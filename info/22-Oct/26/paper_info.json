[
  {
    "id": "arXiv:2210.13454",
    "title": "A Novel Block-Wise Index Modulation Scheme for High-Mobility OTFS  Communications",
    "abstract": "As a promising technique for high-mobility wireless communications,\northogonal time frequency space (OTFS) has been proved to enjoy excellent\nadvantages with respect to traditional orthogonal frequency division\nmultiplexing (OFDM). However, a challenging problem is to design efficient\nsystems to further improve the performance. In this paper, we propose a novel\nblock-wise index modulation (IM) scheme for OTFS systems, named Doppler-IM with\nOTFS (DoIM-OTFS), where a block of Doppler resource bins are activated\nsimultaneously. For practical implementation, we develop a low complexity\ncustomized message passing (CMP) algorithm for our proposed DoIM-OTFS scheme.\nSimulation results demonstrate our proposed DoIM-OTFS system outperforms\ntraditional OTFS system without IM. The proposed CMP algorithm can achieve\ndesired performance and robustness to the imperfect channel state information\n(CSI).",
    "descriptor": "",
    "authors": [
      "Mi Qian",
      "Yao Ge",
      "Miaowen Wen",
      "Fei Ji"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13454"
  },
  {
    "id": "arXiv:2210.13455",
    "title": "Planning with Uncertainty: Deep Exploration in Model-Based Reinforcement  Learning",
    "abstract": "Deep model-based Reinforcement Learning (RL) has shown super-human\nperformance in many challenging domains. Low sample efficiency and limited\nexploration remain as leading obstacles in the field, however. In this paper,\nwe demonstrate deep exploration in model-based RL by incorporating epistemic\nuncertainty into planning trees, circumventing the standard approach of\npropagating uncertainty through value learning. We evaluate this approach with\nthe state of the art model-based RL algorithm MuZero, and extend its training\nprocess to stabilize learning from explicitly-exploratory trajectories. In our\nexperiments planning with uncertainty is able to demonstrate effective deep\nexploration with standard uncertainty estimation mechanisms, and with it\nsignificant gains in sample efficiency.",
    "descriptor": "\nComments: Submitted to ICLR 2023\n",
    "authors": [
      "Yaniv Oren",
      "Matthijs T. J. Spaan",
      "Wendelin B\u00f6hmer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13455"
  },
  {
    "id": "arXiv:2210.13456",
    "title": "An Algorithm and Heuristic based on Normalized Mutual Information for  Dimensionality Reduction and Classification of Hyperspectral images",
    "abstract": "In the feature classification domain, the choice of data affects widely the\nresults. The Hyperspectral image (HSI), is a set of more than a hundred\nbidirectional measures (called bands), of the same region (called ground truth\nmap: GT). The HSI is modelized at a set of N vectors. So we have N features (or\nattributes) expressing N vectors of measures for C substances (called classes).\nThe problematic is that it's pratically impossible to investgate all possible\nsubsets. So we must find K vectors among N, such as relevant and no redundant\nones; in order to classify substances. Here we introduce an algorithm based on\nNormalized Mutual Information to select relevant and no redundant bands,\nnecessary to increase classification accuracy of HSI.\nKeywords: Feature Selection, Normalized Mutual information, Hyperspectral\nimages, Classification, Redundancy.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1211.0613. text overlap with arXiv:2210.12296\n",
    "authors": [
      "Elkebir Sarhrouni",
      "Ahmed Hammouch",
      "Driss Aboutajdine"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13456"
  },
  {
    "id": "arXiv:2210.13457",
    "title": "Mixed Precision Quantization to Tackle Gradient Leakage Attacks in  Federated Learning",
    "abstract": "Federated Learning (FL) enables collaborative model building among a large\nnumber of participants without the need for explicit data sharing. But this\napproach shows vulnerabilities when privacy inference attacks are applied to\nit. In particular, in the event of a gradient leakage attack, which has a\nhigher success rate in retrieving sensitive data from the model gradients, FL\nmodels are at higher risk due to the presence of communication in their\ninherent architecture. The most alarming thing about this gradient leakage\nattack is that it can be performed in such a covert way that it does not hamper\nthe training performance while the attackers backtrack from the gradients to\nget information about the raw data. Two of the most common approaches proposed\nas solutions to this issue are homomorphic encryption and adding noise with\ndifferential privacy parameters. These two approaches suffer from two major\ndrawbacks. They are: the key generation process becomes tedious with the\nincreasing number of clients, and noise-based differential privacy suffers from\na significant drop in global model accuracy. As a countermeasure, we propose a\nmixed-precision quantized FL scheme, and we empirically show that both of the\nissues addressed above can be resolved. In addition, our approach can ensure\nmore robustness as different layers of the deep model are quantized with\ndifferent precision and quantization modes. We empirically proved the validity\nof our method with three benchmark datasets and found a minimal accuracy drop\nin the global model after applying quantization.",
    "descriptor": "",
    "authors": [
      "Pretom Roy Ovi",
      "Emon Dey",
      "Nirmalya Roy",
      "Aryya Gangopadhyay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13457"
  },
  {
    "id": "arXiv:2210.13458",
    "title": "OpenAUC: Towards AUC-Oriented Open-Set Recognition",
    "abstract": "Traditional machine learning follows a close-set assumption that the training\nand test set share the same label space. While in many practical scenarios, it\nis inevitable that some test samples belong to unknown classes (open-set). To\nfix this issue, Open-Set Recognition (OSR), whose goal is to make correct\npredictions on both close-set samples and open-set samples, has attracted\nrising attention. In this direction, the vast majority of literature focuses on\nthe pattern of open-set samples. However, how to evaluate model performance in\nthis challenging task is still unsolved. In this paper, a systematic analysis\nreveals that most existing metrics are essentially inconsistent with the\naforementioned goal of OSR: (1) For metrics extended from close-set\nclassification, such as Open-set F-score, Youden's index, and Normalized\nAccuracy, a poor open-set prediction can escape from a low performance score\nwith a superior close-set prediction. (2) Novelty detection AUC, which measures\nthe ranking performance between close-set and open-set samples, ignores the\nclose-set performance. To fix these issues, we propose a novel metric named\nOpenAUC. Compared with existing metrics, OpenAUC enjoys a concise pairwise\nformulation that evaluates open-set performance and close-set performance in a\ncoupling manner. Further analysis shows that OpenAUC is free from the\naforementioned inconsistency properties. Finally, an end-to-end learning method\nis proposed to minimize the OpenAUC risk, and the experimental results on\npopular benchmark datasets speak to its effectiveness.",
    "descriptor": "",
    "authors": [
      "Zitai Wang",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Yuan He",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13458"
  },
  {
    "id": "arXiv:2210.13459",
    "title": "Adaptive Label Smoothing with Self-Knowledge in Natural Language  Generation",
    "abstract": "Overconfidence has been shown to impair generalization and calibration of a\nneural network. Previous studies remedy this issue by adding a regularization\nterm to a loss function, preventing a model from making a peaked distribution.\nLabel smoothing smoothes target labels with a pre-defined prior label\ndistribution; as a result, a model is learned to maximize the likelihood of\npredicting the soft label. Nonetheless, the amount of smoothing is the same in\nall samples and remains fixed in training. In other words, label smoothing does\nnot reflect the change in probability distribution mapped by a model over the\ncourse of training. To address this issue, we propose a regularization scheme\nthat brings dynamic nature into the smoothing parameter by taking model\nprobability distribution into account, thereby varying the parameter per\ninstance. A model in training self-regulates the extent of smoothing on the fly\nduring forward propagation. Furthermore, inspired by recent work in bridging\nlabel smoothing and knowledge distillation, our work utilizes self-knowledge as\na prior label distribution in softening target labels, and presents theoretical\nsupport for the regularization effect by knowledge distillation and the dynamic\nsmoothing parameter. Our regularizer is validated comprehensively, and the\nresult illustrates marked improvements in model generalization and calibration,\nenhancing robustness and trustworthiness of a model.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Dongkyu Lee",
      "Ka Chun Cheung",
      "Nevin L. Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13459"
  },
  {
    "id": "arXiv:2210.13461",
    "title": "Active Predictive Coding: A Unified Neural Framework for Learning  Hierarchical World Models for Perception and Planning",
    "abstract": "Predictive coding has emerged as a prominent model of how the brain learns\nthrough predictions, anticipating the importance accorded to predictive\nlearning in recent AI architectures such as transformers. Here we propose a new\nframework for predictive coding called active predictive coding which can learn\nhierarchical world models and solve two radically different open problems in\nAI: (1) how do we learn compositional representations, e.g., part-whole\nhierarchies, for equivariant vision? and (2) how do we solve large-scale\nplanning problems, which are hard for traditional reinforcement learning, by\ncomposing complex action sequences from primitive policies? Our approach\nexploits hypernetworks, self-supervised learning and reinforcement learning to\nlearn hierarchical world models that combine task-invariant state transition\nnetworks and task-dependent policy networks at multiple abstraction levels. We\ndemonstrate the viability of our approach on a variety of vision datasets\n(MNIST, FashionMNIST, Omniglot) as well as on a scalable hierarchical planning\nproblem. Our results represent, to our knowledge, the first demonstration of a\nunified solution to the part-whole learning problem posed by Hinton, the nested\nreference frames problem posed by Hawkins, and the integrated state-action\nhierarchy learning problem in reinforcement learning.",
    "descriptor": "\nComments: 15 pages, 10 figures, 2 supplementary figures\n",
    "authors": [
      "Rajesh P. N. Rao",
      "Dimitrios C. Gklezakos",
      "Vishwas Sathish"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.13461"
  },
  {
    "id": "arXiv:2210.13462",
    "title": "Artificial Intelligence-Based Methods for Fusion of Electronic Health  Records and Imaging Data",
    "abstract": "Healthcare data are inherently multimodal, including electronic health\nrecords (EHR), medical images, and multi-omics data. Combining these multimodal\ndata sources contributes to a better understanding of human health and provides\noptimal personalized healthcare. Advances in artificial intelligence (AI)\ntechnologies, particularly machine learning (ML), enable the fusion of these\ndifferent data modalities to provide multimodal insights. To this end, in this\nscoping review, we focus on synthesizing and analyzing the literature that uses\nAI techniques to fuse multimodal medical data for different clinical\napplications. More specifically, we focus on studies that only fused EHR with\nmedical imaging data to develop various AI methods for clinical applications.\nWe present a comprehensive analysis of the various fusion strategies, the\ndiseases and clinical outcomes for which multimodal fusion was used, the ML\nalgorithms used to perform multimodal fusion for each clinical application, and\nthe available multimodal medical datasets. We followed the PRISMA-ScR\nguidelines. We searched Embase, PubMed, Scopus, and Google Scholar to retrieve\nrelevant studies. We extracted data from 34 studies that fulfilled the\ninclusion criteria. In our analysis, a typical workflow was observed: feeding\nraw data, fusing different data modalities by applying conventional machine\nlearning (ML) or deep learning (DL) algorithms, and finally, evaluating the\nmultimodal fusion through clinical outcome predictions. Specifically, early\nfusion was the most used technique in most applications for multimodal learning\n(22 out of 34 studies). We found that multimodality fusion models outperformed\ntraditional single-modality models for the same task. Disease diagnosis and\nprediction were the most common clinical outcomes (reported in 20 and 10\nstudies, respectively) from a clinical outcome perspective.",
    "descriptor": "\nComments: Accepted in Nature Scientific Reports. 20 pages\n",
    "authors": [
      "Farida Mohsen",
      "Hazrat Ali",
      "Nady El Hajj",
      "Zubair Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13462"
  },
  {
    "id": "arXiv:2210.13463",
    "title": "Adversarial Pretraining of Self-Supervised Deep Networks: Past, Present  and Future",
    "abstract": "In this paper, we review adversarial pretraining of self-supervised deep\nnetworks including both convolutional neural networks and vision transformers.\nUnlike the adversarial training with access to labeled examples, adversarial\npretraining is complicated as it only has access to unlabeled examples. To\nincorporate adversaries into pretraining models on either input or feature\nlevel, we find that existing approaches are largely categorized into two\ngroups: memory-free instance-wise attacks imposing worst-case perturbations on\nindividual examples, and memory-based adversaries shared across examples over\niterations. In particular, we review several representative adversarial\npretraining models based on Contrastive Learning (CL) and Masked Image Modeling\n(MIM), respectively, two popular self-supervised pretraining methods in\nliterature. We also review miscellaneous issues about computing overheads,\ninput-/feature-level adversaries, as well as other adversarial pretraining\napproaches beyond the above two groups. Finally, we discuss emerging trends and\nfuture directions about the relations between adversarial and cooperative\npretraining, unifying adversarial CL and MIM pretraining, and the trade-off\nbetween accuracy and robustness in adversarial pretraining.",
    "descriptor": "",
    "authors": [
      "Guo-Jun Qi",
      "Mubarak Shah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13463"
  },
  {
    "id": "arXiv:2210.13464",
    "title": "Graph Reinforcement Learning-based CNN Inference Offloading in Dynamic  Edge Computing",
    "abstract": "This paper studies the computational offloading of CNN inference in dynamic\nmulti-access edge computing (MEC) networks. To address the uncertainties in\ncommunication time and Edge servers' available capacity, we use early-exit\nmechanism to terminate the computation earlier to meet the deadline of\ninference tasks. We design a reward function to trade off the communication,\ncomputation and inference accuracy, and formulate the offloading problem of CNN\ninference as a maximization problem with the goal of maximizing the average\ninference accuracy and throughput in long term. To solve the maximization\nproblem, we propose a graph reinforcement learning-based early-exit mechanism\n(GRLE), which outperforms the state-of-the-art work, deep reinforcement\nlearning-based online offloading (DROO) and its enhanced method, DROO with\nearly-exit mechanism (DROOE), under different dynamic scenarios. The\nexperimental results show that GRLE achieves the average accuracy up to 3.41x\nover graph reinforcement learning (GRL) and 1.45x over DROOE, which shows the\nadvantages of GRLE for offloading decision-making in dynamic MEC.",
    "descriptor": "\nComments: Accepted by the 2022 IEEE Global Communications Conference (Globecom 2022)\n",
    "authors": [
      "Nan Li",
      "Alexandros Iosifidis",
      "Qi Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13464"
  },
  {
    "id": "arXiv:2210.13466",
    "title": "Machine learning-based approach for online fault Diagnosis of Discrete  Event System",
    "abstract": "The problem considered in this paper is the online diagnosis of Automated\nProduction Systems with sensors and actuators delivering discrete binary\nsignals that can be modeled as Discrete Event Systems. Even though there are\nnumerous diagnosis methods, none of them can meet all the criteria of\nimplementing an efficient diagnosis system (such as an intelligent solution, an\naverage effort, a reasonable cost, an online diagnosis, fewer false alarms,\netc.). In addition, these techniques require either a correct, robust, and\nrepresentative model of the system or relevant data or experts' knowledge that\nrequire continuous updates. In this paper, we propose a Machine Learning-based\napproach of a diagnostic system. It is considered as a multi-class classifier\nthat predicts the plant state: normal or faulty and what fault that has arisen\nin the case of failing behavior.",
    "descriptor": "",
    "authors": [
      "R Saddem",
      "D Baptiste"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13466"
  },
  {
    "id": "arXiv:2210.13468",
    "title": "Fast and Low-Memory Deep Neural Networks Using Binary Matrix  Factorization",
    "abstract": "Despite the outstanding performance of deep neural networks in different\napplications, they are still computationally extensive and require a great\nnumber of memories. This motivates more research on reducing the resources\nrequired for implementing such networks. An efficient approach addressed for\nthis purpose is matrix factorization, which has been shown to be effective on\ndifferent networks. In this paper, we utilize binary matrix factorization and\nshow its great efficiency in reducing the required number of resources in deep\nneural networks. In effect, this technique can lead to the practical\nimplementation of such networks.",
    "descriptor": "",
    "authors": [
      "Alireza Bordbar",
      "Mohammad Hossein Kahaei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13468"
  },
  {
    "id": "arXiv:2210.13470",
    "title": "Graded-Q Reinforcement Learning with Information-Enhanced State Encoder  for Hierarchical Collaborative Multi-Vehicle Pursuit",
    "abstract": "The multi-vehicle pursuit (MVP), as a problem abstracted from various\nreal-world scenarios, is becoming a hot research topic in Intelligent\nTransportation System (ITS). The combination of Artificial Intelligence (AI)\nand connected vehicles has greatly promoted the research development of MVP.\nHowever, existing works on MVP pay little attention to the importance of\ninformation exchange and cooperation among pursuing vehicles under the complex\nurban traffic environment. This paper proposed a graded-Q reinforcement\nlearning with information-enhanced state encoder (GQRL-IESE) framework to\naddress this hierarchical collaborative multi-vehicle pursuit (HCMVP) problem.\nIn the GQRL-IESE, a cooperative graded Q scheme is proposed to facilitate the\ndecision-making of pursuing vehicles to improve pursuing efficiency. Each\npursuing vehicle further uses a deep Q network (DQN) to make decisions based on\nits encoded state. A coordinated Q optimizing network adjusts the individual\ndecisions based on the current environment traffic information to obtain the\nglobal optimal action set. In addition, an information-enhanced state encoder\nis designed to extract critical information from multiple perspectives and uses\nthe attention mechanism to assist each pursuing vehicle in effectively\ndetermining the target. Extensive experimental results based on SUMO indicate\nthat the total timestep of the proposed GQRL-IESE is less than other methods on\naverage by 47.64%, which demonstrates the excellent pursuing efficiency of the\nGQRL-IESE. Codes are outsourced in https://github.com/ANT-ITS/GQRL-IESE.",
    "descriptor": "\nComments: 8 pages with 6 figures\n",
    "authors": [
      "Yiying Yang",
      "Xinhang Li",
      "Zheng Yuan",
      "Qinwen Wang",
      "Chen Xu",
      "Lin Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13470"
  },
  {
    "id": "arXiv:2210.13488",
    "title": "LidarAugment: Searching for Scalable 3D LiDAR Data Augmentations",
    "abstract": "Data augmentations are important in training high-performance 3D object\ndetectors for point clouds. Despite recent efforts on designing new data\naugmentations, perhaps surprisingly, most state-of-the-art 3D detectors only\nuse a few simple data augmentations. In particular, different from 2D image\ndata augmentations, 3D data augmentations need to account for different\nrepresentations of input data and require being customized for different\nmodels, which introduces significant overhead. In this paper, we resort to a\nsearch-based approach, and propose LidarAugment, a practical and effective data\naugmentation strategy for 3D object detection. Unlike previous approaches where\nall augmentation policies are tuned in an exponentially large search space, we\npropose to factorize and align the search space of each data augmentation,\nwhich cuts down the 20+ hyperparameters to 2, and significantly reduces the\nsearch complexity. We show LidarAugment can be customized for different model\narchitectures with different input representations by a simple 2D grid search,\nand consistently improve both convolution-based UPillars/StarNet/RSN and\ntransformer-based SWFormer. Furthermore, LidarAugment mitigates overfitting and\nallows us to scale up 3D detectors to much larger capacity. In particular, by\ncombining with latest 3D detectors, our LidarAugment achieves a new\nstate-of-the-art 74.8 mAPH L2 on Waymo Open Dataset.",
    "descriptor": "",
    "authors": [
      "Zhaoqi Leng",
      "Guowang Li",
      "Chenxi Liu",
      "Ekin Dogus Cubuk",
      "Pei Sun",
      "Tong He",
      "Dragomir Anguelov",
      "Mingxing Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13488"
  },
  {
    "id": "arXiv:2210.13497",
    "title": "Subspace Recovery from Heterogeneous Data with Non-isotropic Noise",
    "abstract": "Recovering linear subspaces from data is a fundamental and important task in\nstatistics and machine learning. Motivated by heterogeneity in Federated\nLearning settings, we study a basic formulation of this problem: the principal\ncomponent analysis (PCA), with a focus on dealing with irregular noise. Our\ndata come from $n$ users with user $i$ contributing data samples from a\n$d$-dimensional distribution with mean $\\mu_i$. Our goal is to recover the\nlinear subspace shared by $\\mu_1,\\ldots,\\mu_n$ using the data points from all\nusers, where every data point from user $i$ is formed by adding an independent\nmean-zero noise vector to $\\mu_i$. If we only have one data point from every\nuser, subspace recovery is information-theoretically impossible when the\ncovariance matrices of the noise vectors can be non-spherical, necessitating\nadditional restrictive assumptions in previous work. We avoid these assumptions\nby leveraging at least two data points from each user, which allows us to\ndesign an efficiently-computable estimator under non-spherical and\nuser-dependent noise. We prove an upper bound for the estimation error of our\nestimator in general scenarios where the number of data points and amount of\nnoise can vary across users, and prove an information-theoretic error lower\nbound that not only matches the upper bound up to a constant factor, but also\nholds even for spherical Gaussian noise. This implies that our estimator does\nnot introduce additional estimation error (up to a constant factor) due to\nirregularity in the noise. We show additional results for a linear regression\nproblem in a similar setup.",
    "descriptor": "\nComments: In NeurIPS 2022\n",
    "authors": [
      "John Duchi",
      "Vitaly Feldman",
      "Lunjia Hu",
      "Kunal Talwar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13497"
  },
  {
    "id": "arXiv:2210.13504",
    "title": "Opportunistic Episodic Reinforcement Learning",
    "abstract": "In this paper, we propose and study opportunistic reinforcement learning - a\nnew variant of reinforcement learning problems where the regret of selecting a\nsuboptimal action varies under an external environmental condition known as the\nvariation factor. When the variation factor is low, so is the regret of\nselecting a suboptimal action and vice versa. Our intuition is to exploit more\nwhen the variation factor is high, and explore more when the variation factor\nis low. We demonstrate the benefit of this novel framework for finite-horizon\nepisodic MDPs by designing and evaluating OppUCRL2 and OppPSRL algorithms. Our\nalgorithms dynamically balance the exploration-exploitation trade-off for\nreinforcement learning by introducing variation factor-dependent optimism to\nguide exploration. We establish an $\\tilde{O}(HS \\sqrt{AT})$ regret bound for\nthe OppUCRL2 algorithm and show through simulations that both OppUCRL2 and\nOppPSRL algorithm outperform their original corresponding algorithms.",
    "descriptor": "",
    "authors": [
      "Xiaoxiao Wang",
      "Nader Bouacida",
      "Xueying Guo",
      "Xin Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13504"
  },
  {
    "id": "arXiv:2210.13507",
    "title": "Causal Explanation for Reinforcement Learning: Quantifying State and  Temporal Importance",
    "abstract": "Explainability plays an increasingly important role in machine learning.\nBecause reinforcement learning (RL) involves interactions between states and\nactions over time, explaining an RL policy is more challenging than that of\nsupervised learning. Furthermore, humans view the world from causal lens and\nthus prefer causal explanations over associational ones. Therefore, in this\npaper, we develop a causal explanation mechanism that quantifies the causal\nimportance of states on actions and such importance over time. Moreover, via a\nseries of simulation studies including crop irrigation, Blackjack, collision\navoidance, and lunar lander, we demonstrate the advantages of our mechanism\nover state-of-the-art associational methods in terms of RL policy explanation.",
    "descriptor": "",
    "authors": [
      "Xiaoxiao Wang",
      "Fanyu Meng",
      "Zhaodan Kong",
      "Xin Chen",
      "Xin Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13507"
  },
  {
    "id": "arXiv:2210.13510",
    "title": "Evaluation of Argo Scholar with Observational Study",
    "abstract": "Discovering and making sense of relevant literature is fundamental in any\nscientific field. Node-link diagram-based visualization tools can aid this\nprocess; however, existing tools have been evaluated only on small scales. This\npaper evaluates Argo Scholar, an open-source visualization tool designed for\ninteractive exploration of literature and easy sharing of exploration results.\nA large-scale user study of 122 participants from diverse backgrounds and\nexperiences showed that Argo Scholar is effective at helping users find related\nwork and understand paper connections, and incremental graph-based exploration\nis effective across diverse disciplines. Based on the user study and user\nfeedback, we provide design considerations and feature suggestions for future\nwork.",
    "descriptor": "\nComments: VIS IEEE 22\n",
    "authors": [
      "Kevin Li",
      "Haoyang Yang",
      "Evan Montoya",
      "Anish Upadhayay",
      "Zhiyan Zhou",
      "Jon Saad-Falcon",
      "Duen Horng Chau"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13510"
  },
  {
    "id": "arXiv:2210.13512",
    "title": "Provably Learning Diverse Features in Multi-View Data with Midpoint  Mixup",
    "abstract": "Mixup is a data augmentation technique that relies on training using random\nconvex combinations of data points and their labels. In recent years, Mixup has\nbecome a standard primitive used in the training of state-of-the-art image\nclassification models due to its demonstrated benefits over empirical risk\nminimization with regards to generalization and robustness. In this work, we\ntry to explain some of this success from a feature learning perspective. We\nfocus our attention on classification problems in which each class may have\nmultiple associated features (or views) that can be used to predict the class\ncorrectly. Our main theoretical results demonstrate that, for a non-trivial\nclass of data distributions with two features per class, training a 2-layer\nconvolutional network using empirical risk minimization can lead to learning\nonly one feature for almost all classes while training with a specific\ninstantiation of Mixup succeeds in learning both features for every class. We\nalso show empirically that these theoretical insights extend to the practical\nsettings of image benchmarks modified to have additional synthetic features.",
    "descriptor": "\nComments: 50 pages\n",
    "authors": [
      "Muthu Chidambaram",
      "Xiang Wang",
      "Chenwei Wu",
      "Rong Ge"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13512"
  },
  {
    "id": "arXiv:2210.13513",
    "title": "ExPUNations: Augmenting Puns with Keywords and Explanations",
    "abstract": "The tasks of humor understanding and generation are challenging and\nsubjective even for humans, requiring commonsense and real-world knowledge to\nmaster. Puns, in particular, add the challenge of fusing that knowledge with\nthe ability to interpret lexical-semantic ambiguity. In this paper, we present\nthe ExPUNations (ExPUN) dataset, in which we augment an existing dataset of\npuns with detailed crowdsourced annotations of keywords denoting the most\ndistinctive words that make the text funny, pun explanations describing why the\ntext is funny, and fine-grained funniness ratings. This is the first humor\ndataset with such extensive and fine-grained annotations specifically for puns.\nBased on these annotations, we propose two tasks: explanation generation to aid\nwith pun classification and keyword-conditioned pun generation, to challenge\nthe current state-of-the-art natural language understanding and generation\nmodels' ability to understand and generate humor. We showcase that the\nannotated keywords we collect are helpful for generating better novel humorous\ntexts in human evaluation, and that our natural language explanations can be\nleveraged to improve both the accuracy and robustness of humor classifiers.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference\n",
    "authors": [
      "Jiao Sun",
      "Anjali Narayan-Chen",
      "Shereen Oraby",
      "Alessandra Cervone",
      "Tagyoung Chung",
      "Jing Huang",
      "Yang Liu",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13513"
  },
  {
    "id": "arXiv:2210.13520",
    "title": "Gosper's algorithm and Bell numbers",
    "abstract": "Computers are good at evaluating finite sums in closed form, but there are\nfinite sums which do not have closed forms. Summands which do not produce a\nclosed form can often be ``fixed'' by multiplying them by a suitable\npolynomial. We provide an explicit description of a class of such polynomials\nfor simple hypergeometric summands in terms of the Bell numbers.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Robert Dougherty-Bliss"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Combinatorics (math.CO)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2210.13520"
  },
  {
    "id": "arXiv:2210.13522",
    "title": "Context-Situated Pun Generation",
    "abstract": "Previous work on pun generation commonly begins with a given pun word (a pair\nof homophones for heterographic pun generation and a polyseme for homographic\npun generation) and seeks to generate an appropriate pun. While this may enable\nefficient pun generation, we believe that a pun is most entertaining if it fits\nappropriately within a given context, e.g., a given situation or dialogue. In\nthis work, we propose a new task, context-situated pun generation, where a\nspecific context represented by a set of keywords is provided, and the task is\nto first identify suitable pun words that are appropriate for the context, then\ngenerate puns based on the context keywords and the identified pun words. We\ncollect CUP (Context-sitUated Pun), containing 4.5k tuples of context words and\npun pairs. Based on the new data and setup, we propose a pipeline system for\ncontext-situated pun generation, including a pun word retrieval module that\nidentifies suitable pun words for a given context, and a generation module that\ngenerates puns from context keywords and pun words. Human evaluation shows that\n69% of our top retrieved pun words can be used to generate context-situated\npuns, and our generation module yields successful puns 31% of the time given a\nplausible tuple of context words and pun pair, almost tripling the yield of a\nstate-of-the-art pun generation model. With an end-to-end evaluation, our\npipeline system with the top-1 retrieved pun pair for a given context can\ngenerate successful puns 40% of the time, better than all other modeling\nvariations but 32% lower than the human success rate. This highlights the\ndifficulty of the task, and encourages more research in this direction.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference\n",
    "authors": [
      "Jiao Sun",
      "Anjali Narayan-Chen",
      "Shereen Oraby",
      "Shuyang Gao",
      "Tagyoung Chung",
      "Jing Huang",
      "Yang Liu",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13522"
  },
  {
    "id": "arXiv:2210.13529",
    "title": "Multi-Person 3D Pose and Shape Estimation via Inverse Kinematics and  Refinement",
    "abstract": "Estimating 3D poses and shapes in the form of meshes from monocular RGB\nimages is challenging. Obviously, it is more difficult than estimating 3D poses\nonly in the form of skeletons or heatmaps. When interacting persons are\ninvolved, the 3D mesh reconstruction becomes more challenging due to the\nambiguity introduced by person-to-person occlusions. To tackle the challenges,\nwe propose a coarse-to-fine pipeline that benefits from 1) inverse kinematics\nfrom the occlusion-robust 3D skeleton estimation and 2) Transformer-based\nrelation-aware refinement techniques. In our pipeline, we first obtain\nocclusion-robust 3D skeletons for multiple persons from an RGB image. Then, we\napply inverse kinematics to convert the estimated skeletons to deformable 3D\nmesh parameters. Finally, we apply the Transformer-based mesh refinement that\nrefines the obtained mesh parameters considering intra- and inter-person\nrelations of 3D meshes. Via extensive experiments, we demonstrate the\neffectiveness of our method, outperforming state-of-the-arts on 3DPW, MuPoTS\nand AGORA datasets.",
    "descriptor": "\nComments: Published at ECCV 2022\n",
    "authors": [
      "Junuk Cha",
      "Muhammad Saqlain",
      "GeonU Kim",
      "Mingyu Shin",
      "Seungryul Baek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13529"
  },
  {
    "id": "arXiv:2210.13530",
    "title": "An efficient Monte Carlo scheme for Zakai equations",
    "abstract": "In this paper we develop a numerical method for efficiently approximating\nsolutions of certain Zakai equations in high dimensions. The key idea is to\ntransform a given Zakai SPDE into a PDE with random coefficients. We show that\nunder suitable regularity assumptions on the coefficients of the Zakai equation\nthe corresponding random PDE admits a solution random field which,\nconditionally on the random coefficients, can be written as a classical\nsolution of a second order linear parabolic PDE. This makes it possible to\napply the Feynman--Kac formula to obtain an efficient Monte Carlo scheme for\ncomputing approximate solutions of Zakai equations. The approach achieves good\nresults in up to 100 dimensions with fast run times.",
    "descriptor": "",
    "authors": [
      "Christian Beck",
      "Sebastian Becker",
      "Patrick Cheridito",
      "Arnulf Jentzen",
      "Ariel Neufeld"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.13530"
  },
  {
    "id": "arXiv:2210.13532",
    "title": "Adaptive Top-K in SGD for Communication-Efficient Distributed Learning",
    "abstract": "Distributed stochastic gradient descent (SGD) with gradient compression has\nemerged as a communication-efficient solution to accelerate distributed\nlearning. Top-K sparsification is one of the most popular gradient compression\nmethods that sparsifies the gradient in a fixed degree during model training.\nHowever, there lacks an approach to adaptively adjust the degree of\nsparsification to maximize the potential of model performance or training\nspeed. This paper addresses this issue by proposing a novel adaptive Top-K SGD\nframework, enabling adaptive degree of sparsification for each gradient descent\nstep to maximize the convergence performance by exploring the trade-off between\ncommunication cost and convergence error. Firstly, we derive an upper bound of\nthe convergence error for the adaptive sparsification scheme and the loss\nfunction. Secondly, we design the algorithm by minimizing the convergence error\nunder the communication cost constraints. Finally, numerical results show that\nthe proposed adaptive Top-K in SGD achieves a significantly better convergence\nrate compared with the state-of-the-art methods.",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Mengzhe Ruan",
      "Guangfeng Yan",
      "Yuanzhang Xiao",
      "Linqi Song",
      "Weitao Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.13532"
  },
  {
    "id": "arXiv:2210.13533",
    "title": "Sharpness-aware Minimization for Worst Case Optimization",
    "abstract": "Improvement of worst group performance and generalization performance are\ncore problems of current machine learning. There are diverse efforts to\nincrease performance, such as weight norm penalty and data augmentation, but\nthe improvements are limited. Recently, there have been two promising\napproaches to increase the worst group performance and generalization\nperformance, respectively. Distributionally robust optimization (DRO) focuses\non the worst or hardest group to improve the worst-group performance. Besides,\nsharpness-aware minimization (SAM) finds the flat minima to increase the\ngeneralization ability on an unseen dataset. They show significant performance\nimprovements on the worst-group dataset and unseen dataset, respectively.\nHowever, DRO does not guarantee flatness, and SAM does not guarantee the worst\ngroup performance improvement. In other words, DRO and SAM may fail to increase\nthe worst group performance when the training and test dataset shift occurs. In\nthis study, we propose a new approach, the sharpness-aware group\ndistributionally robust optimization (SGDRO). SGDRO finds the flat-minima that\ngeneralizes well on the worst group dataset. Different from DRO and SAM, SGDRO\ncontributes to improving the generalization ability even the distribution shift\noccurs. We validate that SGDRO shows the smaller maximum eigenvalue and\nimproved performance in the worst group.",
    "descriptor": "",
    "authors": [
      "Taero Kim",
      "Sungjun Lim",
      "Kyungwoo Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13533"
  },
  {
    "id": "arXiv:2210.13534",
    "title": "Classification of Misinformation in New Articles using Natural Language  Processing and a Recurrent Neural Network",
    "abstract": "This paper seeks to address the classification of misinformation in news\narticles using a Long Short Term Memory Recurrent Neural Network. Articles were\ntaken from 2018; a year that was filled with reporters writing about President\nDonald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.\nThe model presented successfully classifies these articles with an accuracy\nscore of 0.779944. We consider this to be successful because the model was\ntrained on articles that included languages other than English as well as\nincomplete, or fragmented, articles.",
    "descriptor": "\nComments: ICWSM Data Conference 2020\n",
    "authors": [
      "Brendan Cunha",
      "Lydia Manikonda"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13534"
  },
  {
    "id": "arXiv:2210.13535",
    "title": "Human-centered XAI for Burn Depth Characterization",
    "abstract": "Approximately 1.25 million people in the United States are treated each year\nfor burn injuries. Precise burn injury classification is an important aspect of\nthe medical AI field. In this work, we propose an explainable human-in-the-loop\nframework for improving burn ultrasound classification models. Our framework\nleverages an explanation system based on the LIME classification explainer to\ncorroborate and integrate a burn expert's knowledge -- suggesting new features\nand ensuring the validity of the model. Using this framework, we discover that\nB-mode ultrasound classifiers can be enhanced by supplying textural features.\nMore specifically, we confirm that texture features based on the Gray Level\nCo-occurance Matrix (GLCM) of ultrasound frames can increase the accuracy of\ntransfer learned burn depth classifiers. We test our hypothesis on real data\nfrom porcine subjects. We show improvements in the accuracy of burn depth\nclassification -- from ~88% to ~94% -- once modified according to our\nframework.",
    "descriptor": "",
    "authors": [
      "Maxwell J. Jacobson",
      "Daniela Chanci Arrubla",
      "Maria Romeo Tricas",
      "Gayle Gordillo",
      "Yexiang Xue",
      "Chandan Sen",
      "Juan Wachs"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13535"
  },
  {
    "id": "arXiv:2210.13536",
    "title": "Effective Pre-Training Objectives for Transformer-based Autoencoders",
    "abstract": "In this paper, we study trade-offs between efficiency, cost and accuracy when\npre-training Transformer encoders with different pre-training objectives. For\nthis purpose, we analyze features of common objectives and combine them to\ncreate new effective pre-training approaches. Specifically, we designed light\ntoken generators based on a straightforward statistical approach, which can\nreplace ELECTRA computationally heavy generators, thus highly reducing cost.\nOur experiments also show that (i) there are more efficient alternatives to\nBERT's MLM, and (ii) it is possible to efficiently pre-train Transformer-based\nmodels using lighter generators without a significant drop in performance.",
    "descriptor": "\nComments: Accepted at EMNLP 2022 Findings\n",
    "authors": [
      "Luca Di Liello",
      "Matteo Gabburo",
      "Alessandro Moschitti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13536"
  },
  {
    "id": "arXiv:2210.13537",
    "title": "Private Online Prediction from Experts: Separations and Faster Rates",
    "abstract": "Online prediction from experts is a fundamental problem in machine learning\nand several works have studied this problem under privacy constraints. We\npropose and analyze new algorithms for this problem that improve over the\nregret bounds of the best existing algorithms for non-adaptive adversaries. For\napproximate differential privacy, our algorithms achieve regret bounds of\n$\\tilde{O}(\\sqrt{T \\log d} + \\log d/\\varepsilon)$ for the stochastic setting\nand $\\tilde O(\\sqrt{T \\log d} + T^{1/3} \\log d/\\varepsilon)$ for oblivious\nadversaries (where $d$ is the number of experts). For pure DP, our algorithms\nare the first to obtain sub-linear regret for oblivious adversaries in the\nhigh-dimensional regime $d \\ge T$. Moreover, we prove new lower bounds for\nadaptive adversaries. Our results imply that unlike the non-private setting,\nthere is a strong separation between the optimal regret for adaptive and\nnon-adaptive adversaries for this problem. Our lower bounds also show a\nseparation between pure and approximate differential privacy for adaptive\nadversaries where the latter is necessary to achieve the non-private\n$O(\\sqrt{T})$ regret.",
    "descriptor": "",
    "authors": [
      "Hilal Asi",
      "Vitaly Feldman",
      "Tomer Koren",
      "Kunal Talwar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13537"
  },
  {
    "id": "arXiv:2210.13540",
    "title": "Video based Object 6D Pose Estimation using Transformers",
    "abstract": "We introduce a Transformer based 6D Object Pose Estimation framework\nVideoPose, comprising an end-to-end attention based modelling architecture,\nthat attends to previous frames in order to estimate accurate 6D Object Poses\nin videos. Our approach leverages the temporal information from a video\nsequence for pose refinement, along with being computationally efficient and\nrobust. Compared to existing methods, our architecture is able to capture and\nreason from long-range dependencies efficiently, thus iteratively refining over\nvideo sequences. Experimental evaluation on the YCB-Video dataset shows that\nour approach is on par with the state-of-the-art Transformer methods, and\nperforms significantly better relative to CNN based approaches. Further, with a\nspeed of 33 fps, it is also more efficient and therefore applicable to a\nvariety of applications that require real-time object pose estimation. Training\ncode and pretrained models are available at\nhttps://github.com/ApoorvaBeedu/VideoPose",
    "descriptor": "",
    "authors": [
      "Apoorva Beedu",
      "Huda Alamri",
      "Irfan Essa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13540"
  },
  {
    "id": "arXiv:2210.13542",
    "title": "Scaling up and Stabilizing Differentiable Planning with Implicit  Differentiation",
    "abstract": "Differentiable planning promises end-to-end differentiability and adaptivity.\nHowever, an issue prevents it from scaling up to larger-scale problems: they\nneed to differentiate through forward iteration layers to compute gradients,\nwhich couples forward computation and backpropagation, and needs to balance\nforward planner performance and computational cost of the backward pass. To\nalleviate this issue, we propose to differentiate through the Bellman\nfixed-point equation to decouple forward and backward passes for Value\nIteration Network and its variants, which enables constant backward cost (in\nplanning horizon) and flexible forward budget and helps scale up to large\ntasks. We study the convergence stability, scalability, and efficiency of the\nproposed implicit version of VIN and its variants and demonstrate their\nsuperiorities on a range of planning tasks: 2D navigation, visual navigation,\nand 2-DOF manipulation in configuration space and workspace.",
    "descriptor": "",
    "authors": [
      "Linfeng Zhao",
      "Huazhe Xu",
      "Lawson L.S. Wong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13542"
  },
  {
    "id": "arXiv:2210.13545",
    "title": "MEET: A Monte Carlo Exploration-Exploitation Trade-off for Buffer  Sampling",
    "abstract": "Data selection is essential for any data-based optimization technique, such\nas Reinforcement Learning. State-of-the-art sampling strategies for the\nexperience replay buffer improve the performance of the Reinforcement Learning\nagent. However, they do not incorporate uncertainty in the Q-Value estimation.\nConsequently, they cannot adapt the sampling strategies, including exploration\nand exploitation of transitions, to the complexity of the task. To address\nthis, this paper proposes a new sampling strategy that leverages the\nexploration-exploitation trade-off. This is enabled by the uncertainty\nestimation of the Q-Value function, which guides the sampling to explore more\nsignificant transitions and, thus, learn a more efficient policy. Experiments\non classical control environments demonstrate stable results across various\nenvironments. They show that the proposed method outperforms state-of-the-art\nsampling strategies for dense rewards w.r.t. convergence and peak performance\nby 26% on average.",
    "descriptor": "\nComments: Submitted at ICASSP 2023\n",
    "authors": [
      "Julius Ott",
      "Lorenzo Servadei",
      "Jose Arjona-Medina",
      "Enrico Rinaldi",
      "Gianfranco Mauro",
      "Daniela S\u00e1nchez Lopera",
      "Michael Stephan",
      "Thomas Stadelmayer",
      "Avik Santra",
      "Robert Wille"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13545"
  },
  {
    "id": "arXiv:2210.13547",
    "title": "Machine and Deep Learning for IoT Security and Privacy: Applications,  Challenges, and Future Directions",
    "abstract": "The integration of the Internet of Things (IoT) connects a number of\nintelligent devices with a minimum of human interference that can interact with\none another. IoT is rapidly emerging in the areas of computer science. However,\nnew security problems were posed by the cross-cutting design of the\nmultidisciplinary elements and IoT systems involved in deploying such schemes.\nIneffective is the implementation of security protocols, i.e., authentication,\nencryption, application security, and access network for IoT systems and their\nessential weaknesses in security. Current security approaches can also be\nimproved to protect the IoT environment effectively. In recent years, deep\nlearning (DL)/ machine learning (ML) has progressed significantly in various\ncritical implementations. Therefore, DL/ML methods are essential to turn IoT\nsystems protection from simply enabling safe contact between IoT systems to\nintelligence systems in security. This review aims to include an extensive\nanalysis of ML systems and state-of-the-art developments in DL methods to\nimprove enhanced IoT device protection methods. On the other hand, various new\ninsights in machine and deep learning for IoT Securities illustrate how it\ncould help future research. IoT protection risks relating to emerging or\nessential threats are identified, as well as future IoT device attacks and\npossible threats associated with each surface. We then carefully analyze DL and\nML IoT protection approaches and present each approach's benefits,\npossibilities, and weaknesses. This review discusses a number of potential\nchallenges and limitations. The future works, recommendations, and suggestions\nof DL/ML in IoT security are also included.",
    "descriptor": "\nComments: 46 pages, 7 figures, 2 tables\n",
    "authors": [
      "Subrato Bharati",
      "Prajoy Podder"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13547"
  },
  {
    "id": "arXiv:2210.13552",
    "title": "Perceptual Image Enhancement for Smartphone Real-Time Applications",
    "abstract": "Recent advances in camera designs and imaging pipelines allow us to capture\nhigh-quality images using smartphones. However, due to the small size and lens\nlimitations of the smartphone cameras, we commonly find artifacts or\ndegradation in the processed images. The most common unpleasant effects are\nnoise artifacts, diffraction artifacts, blur, and HDR overexposure. Deep\nlearning methods for image restoration can successfully remove these artifacts.\nHowever, most approaches are not suitable for real-time applications on mobile\ndevices due to their heavy computation and memory requirements.\nIn this paper, we propose LPIENet, a lightweight network for perceptual image\nenhancement, with the focus on deploying it on smartphones. Our experiments\nshow that, with much fewer parameters and operations, our model can deal with\nthe mentioned artifacts and achieve competitive performance compared with\nstate-of-the-art methods on standard benchmarks. Moreover, to prove the\nefficiency and reliability of our approach, we deployed the model directly on\ncommercial smartphones and evaluated its performance. Our model can process 2K\nresolution images under 1 second in mid-level commercial smartphones.",
    "descriptor": "\nComments: Accepted IEEE/CVF WACV 2023\n",
    "authors": [
      "Marcos V. Conde",
      "Florin Vasluianu",
      "Javier Vazquez-Corral",
      "Radu Timofte"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.13552"
  },
  {
    "id": "arXiv:2210.13554",
    "title": "Weight Fixing Networks",
    "abstract": "Modern iterations of deep learning models contain millions (billions) of\nunique parameters, each represented by a b-bit number. Popular attempts at\ncompressing neural networks (such as pruning and quantisation) have shown that\nmany of the parameters are superfluous, which we can remove (pruning) or\nexpress with less than b-bits (quantisation) without hindering performance.\nHere we look to go much further in minimising the information content of\nnetworks. Rather than a channel or layer-wise encoding, we look to lossless\nwhole-network quantisation to minimise the entropy and number of unique\nparameters in a network. We propose a new method, which we call Weight Fixing\nNetworks (WFN) that we design to realise four model outcome objectives: i) very\nfew unique weights, ii) low-entropy weight encodings, iii) unique weight values\nwhich are amenable to energy-saving versions of hardware multiplication, and\niv) lossless task-performance. Some of these goals are conflicting. To best\nbalance these conflicts, we combine a few novel (and some well-trodden) tricks;\na novel regularisation term, (i, ii) a view of clustering cost as relative\ndistance change (i, ii, iv), and a focus on whole-network re-use of weights (i,\niii). Our Imagenet experiments demonstrate lossless compression using 56x fewer\nunique weights and a 1.9x lower weight-space entropy than SOTA quantisation\napproaches.",
    "descriptor": "\nComments: AMS-LaTeX v1.2, 14 pages with 5 figures\n",
    "authors": [
      "Christopher Subia-Waud",
      "Srinandan Dasmahapatra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13554"
  },
  {
    "id": "arXiv:2210.13555",
    "title": "Energy Pricing in P2P Energy Systems Using Reinforcement Learning",
    "abstract": "The increase in renewable energy on the consumer side gives place to new\ndynamics in the energy grids. Participants in a microgrid can produce energy\nand trade it with their peers (peer-to-peer) with the permission of the energy\nprovider. In such a scenario, the stochastic nature of distributed renewable\nenergy generators and energy consumption increases the complexity of defining\nfair prices for buying and selling energy. In this study, we introduce a\nreinforcement learning framework to help solve this issue by training an agent\nto set the prices that maximize the profit of all components in the microgrid,\naiming to facilitate the implementation of P2P grids in real-life scenarios.\nThe microgrid considers consumers, prosumers, the service provider, and a\ncommunity battery. Experimental results on the \\textit{Pymgrid} dataset show a\nsuccessful approach to price optimization for all components in the microgrid.\nThe proposed framework ensures flexibility to account for the interest of these\ncomponents, as well as the ratio of consumers and prosumers in the microgrid.\nThe results also examine the effect of changing the capacity of the community\nbattery on the profit of the system. The implementation code is available\n\\href{https://github.com/Artifitialleap-MBZUAI/rl-p2p-price-prediction}{here}.",
    "descriptor": "",
    "authors": [
      "Nicolas Avila",
      "Shahad Hardan",
      "Elnura Zhalieva",
      "Moayad Aloqaily",
      "Mohsen Guizani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13555"
  },
  {
    "id": "arXiv:2210.13567",
    "title": "I see what you hear: a vision-inspired method to localize words",
    "abstract": "This paper explores the possibility of using visual object detection\ntechniques for word localization in speech data. Object detection has been\nthoroughly studied in the contemporary literature for visual data. Noting that\nan audio can be interpreted as a 1-dimensional image, object localization\ntechniques can be fundamentally useful for word localization. Building upon\nthis idea, we propose a lightweight solution for word detection and\nlocalization. We use bounding box regression for word localization, which\nenables our model to detect the occurrence, offset, and duration of keywords in\na given audio stream. We experiment with LibriSpeech and train a model to\nlocalize 1000 words. Compared to existing work, our method reduces model size\nby 94%, and improves the F1 score by 6.5\\%.",
    "descriptor": "",
    "authors": [
      "Mohammad Samragh",
      "Arnav Kundu",
      "Ting-Yao Hu",
      "Minsik Cho",
      "Aman Chadha",
      "Ashish Shrivastava",
      "Oncel Tuzel",
      "Devang Naik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13567"
  },
  {
    "id": "arXiv:2210.13569",
    "title": "Characterizing Verbatim Short-Term Memory in Neural Language Models",
    "abstract": "When a language model is trained to predict natural language sequences, its\nprediction at each moment depends on a representation of prior context. What\nkind of information about the prior context can language models retrieve? We\ntested whether language models could retrieve the exact words that occurred\npreviously in a text. In our paradigm, language models (transformers and an\nLSTM) processed English text in which a list of nouns occurred twice. We\noperationalized retrieval as the reduction in surprisal from the first to the\nsecond list. We found that the transformers retrieved both the identity and\nordering of nouns from the first list. Further, the transformers' retrieval was\nmarkedly enhanced when they were trained on a larger corpus and with greater\nmodel depth. Lastly, their ability to index prior tokens was dependent on\nlearned attention patterns. In contrast, the LSTM exhibited less precise\nretrieval, which was limited to list-initial tokens and to short intervening\ntexts. The LSTM's retrieval was not sensitive to the order of nouns and it\nimproved when the list was semantically coherent. We conclude that transformers\nimplemented something akin to a working memory system that could flexibly\nretrieve individual token representations across arbitrary delays; conversely,\nthe LSTM maintained a coarser and more rapidly-decaying semantic gist of prior\ntokens, weighted toward the earliest items.",
    "descriptor": "\nComments: accepted at CoNLL2022\n",
    "authors": [
      "Kristijan Armeni",
      "Christopher Honey",
      "Tal Linzen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13569"
  },
  {
    "id": "arXiv:2210.13570",
    "title": "Strong-TransCenter: Improved Multi-Object Tracking based on Transformers  with Dense Representations",
    "abstract": "Transformer networks have been a focus of research in many fields in recent\nyears, being able to surpass the state-of-the-art performance in different\ncomputer vision tasks. A few attempts have been made to apply this method to\nthe task of Multiple Object Tracking (MOT), among those the state-of-the-art\nwas TransCenter, a transformer-based MOT architecture with dense object queries\nfor accurately tracking all the objects while keeping reasonable runtime.\nTransCenter is the first center-based transformer framework for MOT, and is\nalso among the first to show the benefits of using transformer-based\narchitectures for MOT. In this paper we show an improvement to this tracker\nusing post processing mechanism based in the Track-by-Detection paradigm:\nmotion model estimation using Kalman filter and target Re-identification using\nan embedding network. Our new tracker shows significant improvements in the\nIDF1 and HOTA metrics and comparable results on the MOTA metric (70.9%, 59.8%\nand 75.8% respectively) on the MOTChallenge MOT17 test dataset and improvement\non all 3 metrics (67.5%, 56.3% and 73.0%) on the MOT20 test dataset. Our\ntracker is currently ranked first among transformer-based trackers in these\ndatasets. The code is publicly available at:\nhttps://github.com/amitgalor18/STC_Tracker",
    "descriptor": "",
    "authors": [
      "Amit Galor",
      "Roy Orfaig",
      "Ben-Zion Bobrovsky"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13570"
  },
  {
    "id": "arXiv:2210.13572",
    "title": "Sequential Recommendation with Auxiliary Item Relationships via  Multi-Relational Transformer",
    "abstract": "Sequential Recommendation (SR) models user dynamics and predicts the next\npreferred items based on the user history. Existing SR methods model the 'was\ninteracted before' item-item transitions observed in sequences, which can be\nviewed as an item relationship. However, there are multiple auxiliary item\nrelationships, e.g., items from similar brands and with similar contents in\nreal-world scenarios. Auxiliary item relationships describe item-item\naffinities in multiple different semantics and alleviate the long-lasting cold\nstart problem in the recommendation. However, it remains a significant\nchallenge to model auxiliary item relationships in SR.\nTo simultaneously model high-order item-item transitions in sequences and\nauxiliary item relationships, we propose a Multi-relational Transformer capable\nof modeling auxiliary item relationships for SR (MT4SR). Specifically, we\npropose a novel self-attention module, which incorporates arbitrary item\nrelationships and weights item relationships accordingly. Second, we regularize\nintra-sequence item relationships with a novel regularization module to\nsupervise attentions computations. Third, for inter-sequence item relationship\npairs, we introduce a novel inter-sequence related items modeling module.\nFinally, we conduct experiments on four benchmark datasets and demonstrate the\neffectiveness of MT4SR over state-of-the-art methods and the improvements on\nthe cold start problem. The code is available at\nhttps://github.com/zfan20/MT4SR.",
    "descriptor": "\nComments: Accepted to BigData 2022. The code is at this https URL\n",
    "authors": [
      "Ziwei Fan",
      "Zhiwei Liu",
      "Chen Wang",
      "Peijie Huang",
      "Hao Peng",
      "Philip S. Yu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13572"
  },
  {
    "id": "arXiv:2210.13575",
    "title": "Does Self-Rationalization Improve Robustness to Spurious Correlations?",
    "abstract": "Rationalization is fundamental to human reasoning and learning. NLP models\ntrained to produce rationales along with predictions, called\nself-rationalization models, have been investigated for their interpretability\nand utility to end-users. However, the extent to which training with\nhuman-written rationales facilitates learning remains an under-explored\nquestion. We ask whether training models to self-rationalize can aid in their\nlearning to solve tasks for the right reasons. Specifically, we evaluate how\ntraining self-rationalization models with free-text rationales affects\nrobustness to spurious correlations in fine-tuned encoder-decoder and\ndecoder-only models of six different sizes. We evaluate robustness to spurious\ncorrelations by measuring performance on 1) manually annotated challenge\ndatasets and 2) subsets of original test sets where reliance on spurious\ncorrelations would fail to produce correct answers. We find that while\nself-rationalization can improve robustness to spurious correlations in\nlow-resource settings, it tends to hurt robustness in higher-resource settings.\nFurthermore, these effects depend on model family and size, as well as on\nrationale content. Together, our results suggest that explainability can come\nat the cost of robustness; thus, appropriate care should be taken when training\nself-rationalizing models with the goal of creating more trustworthy models.",
    "descriptor": "",
    "authors": [
      "Alexis Ross",
      "Matthew E. Peters",
      "Ana Marasovi\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13575"
  },
  {
    "id": "arXiv:2210.13576",
    "title": "Spectral Clustering-aware Learning of Embeddings for Speaker Diarisation",
    "abstract": "In speaker diarisation, speaker embedding extraction models often suffer from\nthe mismatch between their training loss functions and the speaker clustering\nmethod. In this paper, we propose the method of spectral clustering-aware\nlearning of embeddings (SCALE) to address the mismatch. Specifically, besides\nan angular prototype cal (AP) loss, SCALE uses a novel affinity matrix loss\nwhich directly minimises the error between the affinity matrix estimated from\nspeaker embeddings and the reference. SCALE also includes p-percentile\nthresholding and Gaussian blur as two important hyper-parameters for spectral\nclustering in training. Experiments on the AMI dataset showed that speaker\nembeddings obtained with SCALE achieved over 50% relative speaker error rate\nreductions using oracle segmentation, and over 30% relative diarisation error\nrate reductions using automatic segmentation when compared to a strong baseline\nwith the AP-loss-based speaker embeddings.",
    "descriptor": "\nComments: Submitted to ICASSP 2023, 5 pages\n",
    "authors": [
      "Evonne P.C. Lee",
      "Guangzhi Sun",
      "Chao Zhang",
      "Philip C. Woodland"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13576"
  },
  {
    "id": "arXiv:2210.13577",
    "title": "A finite difference - discontinuous Galerkin method for the wave  equation in second order form",
    "abstract": "We develop a hybrid spatial discretization for the wave equation in second\norder form, based on high-order accurate finite difference methods and\ndiscontinuous Galerkin methods. The hybridization combines computational\nefficiency of finite difference methods on Cartesian grids and geometrical\nflexibility of discontinuous Galerkin methods on unstructured meshes. The two\nspatial discretizations are coupled by a penalty technique at the interface\nsuch that the overall semidiscretization satisfies a discrete energy estimate\nto ensure stability. In addition, optimal convergence is obtained in the sense\nthat when combining a fourth order finite difference method with a\ndiscontinuous Galerkin method using third order local polynomials, the overall\nconvergence rate is fourth order. Furthermore, we use a novel approach to\nderive an error estimate for the semidiscretization by combining the energy\nmethod and the normal mode analysis for a corresponding one dimensional model\nproblem. The stability and accuracy analysis are verified in numerical\nexperiments.",
    "descriptor": "",
    "authors": [
      "Siyang Wang",
      "Gunilla Kreiss"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13577"
  },
  {
    "id": "arXiv:2210.13578",
    "title": "Speeding Up Question Answering Task of Language Models via Inverted  Index",
    "abstract": "Natural language processing applications, such as conversational agents and\ntheir question-answering capabilities, are widely used in the real world.\nDespite the wide popularity of large language models (LLMs), few real-world\nconversational agents take advantage of LLMs. Extensive resources consumed by\nLLMs disable developers from integrating them into end-user applications. In\nthis study, we leverage an inverted indexing mechanism combined with LLMs to\nimprove the efficiency of question-answering models for closed-domain\nquestions. Our experiments show that using the index improves the average\nresponse time by 97.44%. In addition, due to the reduced search scope, the\naverage BLEU score improved by 0.23 while using the inverted index.",
    "descriptor": "",
    "authors": [
      "Xiang Ji",
      "Yesim Sungu-Eryilmaz",
      "Elaheh Momeni",
      "Reza Rawassizadeh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13578"
  },
  {
    "id": "arXiv:2210.13582",
    "title": "Causal Analysis on the Anchor Store Effect in a Location-based Social  Network",
    "abstract": "A particular phenomenon of interest in Retail Economics is the spillover\neffect of anchor stores (specific stores with a reputable brand) to non-anchor\nstores in terms of customer traffic. Prior works in this area rely on small and\nsurvey-based datasets that are often confidential or expensive to collect on a\nlarge scale. Also, very few works study the underlying causal mechanisms\nbetween factors that underpin the spillover effect. In this work, we analyse\nthe causal relationship between anchor stores and customer traffic to\nnon-anchor stores and employ a propensity score matching framework to\ninvestigate this effect more efficiently. First of all, to demonstrate the\neffect, we leverage open and mobile data from London Datastore and\nLocation-Based Social Networks (LBSNs) such as Foursquare. We then perform a\nlarge-scale empirical analysis on customer visit patterns from anchor stores to\nnon-anchor stores(e.g., non-chain restaurants) located in the Greater London\narea as a case study. By studying over 600 neighbourhoods in the GreaterLondon\nArea, we find that anchor stores cause a 14.2-26.5% increase in customer\ntraffic for the non-anchor stores reinforcing the established economic theory.\nMoreover, we evaluate the efficiency of our methodology by studying the\nconfounder balance, dose difference and performance of matching framework on\nsynthetic data. Through this work, we point decision-makers in the retail\nindustry to a more systematic approach to estimate the anchor store effect and\npave the way for further research to discover more complex causal relationships\nunderlying this effect with open data.",
    "descriptor": "",
    "authors": [
      "Anish K. Vallapuram",
      "Young D. Kwon",
      "Lik-Hang Lee",
      "Fengli Xu",
      "Pan Hui"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13582"
  },
  {
    "id": "arXiv:2210.13583",
    "title": "Learning Latent Structural Causal Models",
    "abstract": "Causal learning has long concerned itself with the accurate recovery of\nunderlying causal mechanisms. Such causal modelling enables better explanations\nof out-of-distribution data. Prior works on causal learning assume that the\nhigh-level causal variables are given. However, in machine learning tasks, one\noften operates on low-level data like image pixels or high-dimensional vectors.\nIn such settings, the entire Structural Causal Model (SCM) -- structure,\nparameters, \\textit{and} high-level causal variables -- is unobserved and needs\nto be learnt from low-level data. We treat this problem as Bayesian inference\nof the latent SCM, given low-level data. For linear Gaussian additive noise\nSCMs, we present a tractable approximate inference method which performs joint\ninference over the causal variables, structure and parameters of the latent SCM\nfrom random, known interventions. Experiments are performed on synthetic\ndatasets and a causally generated image dataset to demonstrate the efficacy of\nour approach. We also perform image generation from unseen interventions,\nthereby verifying out of distribution generalization for the proposed causal\nmodel.",
    "descriptor": "\nComments: 21 pages, 19 figures\n",
    "authors": [
      "Jithendaraa Subramanian",
      "Yashas Annadani",
      "Ivaxi Sheth",
      "Nan Rosemary Ke",
      "Tristan Deleu",
      "Stefan Bauer",
      "Derek Nowrouzezahrai",
      "Samira Ebrahimi Kahou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.13583"
  },
  {
    "id": "arXiv:2210.13589",
    "title": "Embodied, Situated, and Grounded Intelligence: Implications for AI",
    "abstract": "In April of 2022, the Santa Fe Institute hosted a workshop on embodied,\nsituated, and grounded intelligence as part of the Institute's Foundations of\nIntelligence project. The workshop brought together computer scientists,\npsychologists, philosophers, social scientists, and others to discuss the\nscience of embodiment and related issues in human intelligence, and its\nimplications for building robust, human-level AI. In this report, we summarize\neach of the talks and the subsequent discussions. We also draw out a number of\nkey themes and identify important frontiers for future research.",
    "descriptor": "\nComments: 38 pages, workshop report\n",
    "authors": [
      "Tyler Millhouse",
      "Melanie Moses",
      "Melanie Mitchell"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13589"
  },
  {
    "id": "arXiv:2210.13591",
    "title": "Learning by Hallucinating: Vision-Language Pre-training with Weak  Supervision",
    "abstract": "Weakly-supervised vision-language (V-L) pre-training (W-VLP) aims at learning\ncross-modal alignment with little or no paired data, such as aligned images and\ncaptions. Recent W-VLP methods, which pair visual features with object tags,\nhelp achieve performances comparable with some VLP models trained with aligned\npairs in various V-L downstream tasks. This, however, is not the case in\ncross-modal retrieval (XMR). We argue that the learning of such a W-VLP model\nis curbed and biased by the object tags of limited semantics.\nWe address the lack of paired V-L data for model supervision with a novel\nVisual Vocabulary based Feature Hallucinator (WFH), which is trained via weak\nsupervision as a W-VLP model, not requiring images paired with captions. WFH\ngenerates visual hallucinations from texts, which are then paired with the\noriginally unpaired texts, allowing more diverse interactions across\nmodalities.\nEmpirically, WFH consistently boosts the prior W-VLP works, e.g. U-VisualBERT\n(U-VB), over a variety of V-L tasks, i.e. XMR, Visual Question Answering, etc.\nNotably, benchmarked with recall@{1,5,10}, it consistently improves U-VB on\nimage-to-text and text-to-image retrieval on two popular datasets Flickr30K and\nMSCOCO. Meanwhile, it gains by at least 14.5% in cross-dataset generalization\ntests on these XMR tasks. Moreover, in other V-L downstream tasks considered,\nour WFH models are on par with models trained with paired V-L data, revealing\nthe utility of unpaired data. These results demonstrate greater generalization\nof the proposed W-VLP model with WFH.",
    "descriptor": "\nComments: Accepted to WACV'23. Please find supplementary material at this https URL\n",
    "authors": [
      "Tzu-Jui Julius Wang",
      "Jorma Laaksonen",
      "Tomas Langer",
      "Heikki Arponen",
      "Tom E. Bishop"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13591"
  },
  {
    "id": "arXiv:2210.13594",
    "title": "Datavoidant: An AI System for Addressing Political Data Voids on Social  Media",
    "abstract": "The limited information (data voids) on political topics relevant to\nunderrepresented communities has facilitated the spread of disinformation.\nIndependent journalists who combat disinformation in underrepresented\ncommunities have reported feeling overwhelmed because they lack the tools\nnecessary to make sense of the information they monitor and address the data\nvoids. In this paper, we present a system to identify and address political\ndata voids within underrepresented communities. Armed with an interview study,\nindicating that the independent news media has the potential to address them,\nwe designed an intelligent collaborative system, called Datavoidant.\nDatavoidant uses state-of-the-art machine learning models and introduces a\nnovel design space to provide independent journalists with a collective\nunderstanding of data voids to facilitate generating content to cover the\nvoids. We performed a user interface evaluation with independent news media\njournalists (N=22). These journalists reported that Datavoidant's features\nallowed them to more rapidly while easily having a sense of what was taking\nplace in the information ecosystem to address the data voids. They also\nreported feeling more confident about the content they created and the unique\nperspectives they had proposed to cover the voids. We conclude by discussing\nhow Datavoidant enables a new design space wherein individuals can collaborate\nto make sense of their information ecosystem and actively devise strategies to\nprevent disinformation.",
    "descriptor": "",
    "authors": [
      "Claudia Flores-Saviaga",
      "Shangbin Feng",
      "Saiph Savage"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13594"
  },
  {
    "id": "arXiv:2210.13598",
    "title": "Caveats on the first-generation da Vinci Research Kit: latent technical  constraints and essential calibrations",
    "abstract": "Telesurgical robotic systems provide a well established form of assistance in\nthe operating theater, with evidence of growing uptake in recent years. Until\nnow, the da Vinci surgical system (Intuitive Surgical Inc, Sunnyvale,\nCalifornia) has been the most widely adopted robot of this kind, with more than\n6,700 systems in current clinical use worldwide. To accelerate research on\nrobotic-assisted surgery, the retired first-generation da Vinci robots have\nbeen redeployed for research use as \"da Vinci Research Kits\" (dVRKs), which\nhave been distributed to research institutions around the world to support both\ntraining and research in the sector. In the past ten years, a great amount of\nresearch on the dVRK has been carried out across a vast range of research\ntopics. During this extensive and distributed process, common technical issues\nhave been identified that are buried deep within the dVRK research and\ndevelopment architecture, and were found to be common among dVRK user feedback,\nregardless of the breadth and disparity of research directions identified. This\npaper gathers and analyzes the most significant of these, with a focus on the\ntechnical constraints of the first-generation dVRK, which both existing and\nprospective users should be aware of before embarking onto dVRK-related\nresearch. The hope is that this review will aid users in identifying and\naddressing common limitations of the systems promptly, thus helping to\naccelerate progress in the field.",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Zejian Cui",
      "Joao Cartucho",
      "Stamatia Giannarou",
      "Ferdinando Rodriguez y Baena"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13598"
  },
  {
    "id": "arXiv:2210.13599",
    "title": "Noise Injection as a Probe of Deep Learning Dynamics",
    "abstract": "We propose a new method to probe the learning mechanism of Deep Neural\nNetworks (DNN) by perturbing the system using Noise Injection Nodes (NINs).\nThese nodes inject uncorrelated noise via additional optimizable weights to\nexisting feed-forward network architectures, without changing the optimization\nalgorithm. We find that the system displays distinct phases during training,\ndictated by the scale of injected noise. We first derive expressions for the\ndynamics of the network and utilize a simple linear model as a test case. We\nfind that in some cases, the evolution of the noise nodes is similar to that of\nthe unperturbed loss, thus indicating the possibility of using NINs to learn\nmore about the full system in the future.",
    "descriptor": "\nComments: 11 pages, 3 figures\n",
    "authors": [
      "Noam Levi",
      "Itay Bloch",
      "Marat Freytsis",
      "Tomer Volansky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13599"
  },
  {
    "id": "arXiv:2210.13600",
    "title": "LANS: Large-scale Arabic News Summarization Corpus",
    "abstract": "Text summarization has been intensively studied in many languages, and some\nlanguages have reached advanced stages. Yet, Arabic Text Summarization (ATS) is\nstill in its developing stages. Existing ATS datasets are either small or lack\ndiversity. We build, LANS, a large-scale and diverse dataset for Arabic Text\nSummarization task. LANS offers 8.4 million articles and their summaries\nextracted from newspapers websites metadata between 1999 and 2019. The\nhigh-quality and diverse summaries are written by journalists from 22 major\nArab newspapers, and include an eclectic mix of at least more than 7 topics\nfrom each source. We conduct an intrinsic evaluation on LANS by both automatic\nand human evaluations. Human evaluation of 1000 random samples reports 95.4%\naccuracy for our collected summaries, and automatic evaluation quantifies the\ndiversity and abstractness of the summaries. The dataset is publicly available\nupon request.",
    "descriptor": "\nComments: 10 pages, 1 figure\n",
    "authors": [
      "Abdulaziz Alhamadani",
      "Xuchao Zhang",
      "Jianfeng He",
      "Chang-Tien Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13600"
  },
  {
    "id": "arXiv:2210.13601",
    "title": "Active Learning for Single Neuron Models with Lipschitz Non-Linearities",
    "abstract": "We consider the problem of active learning for single neuron models, also\nsometimes called ``ridge functions'', in the agnostic setting (under\nadversarial label noise). Such models have been shown to be broadly effective\nin modeling physical phenomena, and for constructing surrogate data-driven\nmodels for partial differential equations.\nSurprisingly, we show that for a single neuron model with any Lipschitz\nnon-linearity (such as the ReLU, sigmoid, absolute value, low-degree\npolynomial, among others), strong provable approximation guarantees can be\nobtained using a well-known active learning strategy for fitting \\emph{linear\nfunctions} in the agnostic setting. % -- i.e. for the case when there is no\nnon-linearity. Namely, we can collect samples via statistical \\emph{leverage\nscore sampling}, which has been shown to be near-optimal in other active\nlearning scenarios. We support our theoretical results with empirical\nsimulations showing that our proposed active learning strategy based on\nleverage score sampling outperforms (ordinary) uniform sampling when fitting\nsingle neuron models.",
    "descriptor": "",
    "authors": [
      "Aarshvi Gajjar",
      "Chinmay Hegde",
      "Christopher Musco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13601"
  },
  {
    "id": "arXiv:2210.13602",
    "title": "Learned Lifted Linearization Applied to Unstable Dynamic Systems Enabled  by Koopman Direct Encoding",
    "abstract": "This paper presents a Koopman lifting linearization method that is applicable\nto nonlinear dynamical systems having both stable and unstable regions. It is\nknown that DMD and other standard data-driven methods face a fundamental\ndifficulty in constructing a Koopman model when applied to unstable systems.\nHere we solve the problem by incorporating knowledge about a nonlinear state\nequation with a learning method for finding an effective set of observables. In\na lifted space, stable and unstable regions are separated into independent\nsubspaces. Based on this property, we propose to find effective observables\nthrough neural net training where training data are separated into stable and\nunstable trajectories. The resultant learned observables are used for\nconstructing a linear state transition matrix using method known as Direct\nEncoding, which transforms the nonlinear state equation to a state transition\nmatrix through inner product computations with the observables. The proposed\nmethod shows a dramatic improvement over existing DMD and data-driven methods.",
    "descriptor": "\nComments: 6 pages, 5 figures, submitted to joint submission of LCSS and ACC2023\n",
    "authors": [
      "Jerry Ng",
      "H. Harry Asada"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13602"
  },
  {
    "id": "arXiv:2210.13604",
    "title": "The Robustness Limits of SoTA Vision Models to Natural Variation",
    "abstract": "Recent state-of-the-art vision models introduced new architectures, learning\nparadigms, and larger pretraining data, leading to impressive performance on\ntasks such as classification. While previous generations of vision models were\nshown to lack robustness to factors such as pose, it's unclear the extent to\nwhich this next generation of models are more robust. To study this question,\nwe develop a dataset of more than 7 million images with controlled changes in\npose, position, background, lighting, and size. We study not only how robust\nrecent state-of-the-art models are, but also the extent to which models can\ngeneralize variation in factors when they're present during training. We\nconsider a catalog of recent vision models, including vision transformers\n(ViT), self-supervised models such as masked autoencoders (MAE), and models\ntrained on larger datasets such as CLIP. We find out-of-the-box, even today's\nbest models are not robust to common changes in pose, size, and background.\nWhen some samples varied during training, we found models required a\nsignificant portion of diversity to generalize -- though eventually robustness\ndid improve. When diversity is only seen for some classes however, we found\nmodels did not generalize to other classes, unless the classes were very\nsimilar to those seen varying during training. We hope our work will shed\nfurther light on the blind spots of SoTA models and spur the development of\nmore robust vision models.",
    "descriptor": "",
    "authors": [
      "Mark Ibrahim",
      "Quentin Garrido",
      "Ari Morcos",
      "Diane Bouchacourt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13604"
  },
  {
    "id": "arXiv:2210.13605",
    "title": "GliTr: Glimpse Transformers with Spatiotemporal Consistency for Online  Action Prediction",
    "abstract": "Many online action prediction models observe complete frames to locate and\nattend to informative subregions in the frames called glimpses and recognize an\nongoing action based on global and local information. However, in applications\nwith constrained resources, an agent may not be able to observe the complete\nframe, yet must still locate useful glimpses to predict an incomplete action\nbased on local information only. In this paper, we develop Glimpse Transformers\n(GliTr), which observe only narrow glimpses at all times, thus predicting an\nongoing action and the following most informative glimpse location based on the\npartial spatiotemporal information collected so far. In the absence of a ground\ntruth for the optimal glimpse locations for action recognition, we train GliTr\nusing a novel spatiotemporal consistency objective: We require GliTr to attend\nto the glimpses with features similar to the corresponding complete frames\n(i.e. spatial consistency) and the resultant class logits at time t equivalent\nto the ones predicted using whole frames up to t (i.e. temporal consistency).\nInclusion of our proposed consistency objective yields ~10% higher accuracy on\nthe Something-Something-v2 (SSv2) dataset than the baseline cross-entropy\nobjective. Overall, despite observing only ~33% of the total area per frame,\nGliTr achieves 53.02%and 93.91% accuracy on the SSv2 and Jester datasets,\nrespectively.",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Samrudhdhi B Rangrej",
      "Kevin J Liang",
      "Tal Hassner",
      "James J Clark"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13605"
  },
  {
    "id": "arXiv:2210.13609",
    "title": "Model-based Evaluation of Driver Control Workloads in Haptic-based  Driver Assistance Systems",
    "abstract": "This study presents a novel approach for modeling and simulating\nhuman-vehicle interactions in order to examine the effects of automated driving\nsystems (ADS) on driving performance and driver control workload. Existing\ndriver-ADS interaction studies have relied on simulated or real-world human\ndriver experiments that are limited in providing objective evaluation of the\ndynamic interactions and control workloads on the driver. Our approach\nleverages an integrated human model-based active driving system (HuMADS) to\nsimulate the dynamic interaction between the driver model and the haptic-based\nADS during a vehicle overtaking task. Two driver arm-steering models were\ndeveloped for both tense and relaxed human driver conditions and validated\nagainst experimental data. We conducted a simulation study to evaluate the\neffects of three different haptic shared control conditions (based on the\npresence and type of control conflict) on overtaking task performance and\ndriver workloads. We found that No Conflict shared control scenarios result in\nimproved driving performance and reduced control workloads, while Conflict\nscenarios result in unsafe maneuvers and increased workloads. These findings,\nwhich are consistent with experimental studies, demonstrate the potential for\nour approach to improving future ADS design for safer driver assistance\nsystems.",
    "descriptor": "\nComments: 11 pages, 13 figures, submitted to the IEEE Transactions on Intelligent Vehicles\n",
    "authors": [
      "Kenechukwu C. Mbanisi",
      "Hideyuki Kimpara",
      "Zhi Li",
      "Danil Prokhorov",
      "Michael A. Gennert"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13609"
  },
  {
    "id": "arXiv:2210.13611",
    "title": "Understanding the Evolution of Linear Regions in Deep Reinforcement  Learning",
    "abstract": "Policies produced by deep reinforcement learning are typically characterised\nby their learning curves, but they remain poorly understood in many other\nrespects. ReLU-based policies result in a partitioning of the input space into\npiecewise linear regions. We seek to understand how observed region counts and\ntheir densities evolve during deep reinforcement learning using empirical\nresults that span a range of continuous control tasks and policy network\ndimensions. Intuitively, we may expect that during training, the region density\nincreases in the areas that are frequently visited by the policy, thereby\naffording fine-grained control. We use recent theoretical and empirical results\nfor the linear regions induced by neural networks in supervised learning\nsettings for grounding and comparison of our results. Empirically, we find that\nthe region density increases only moderately throughout training, as measured\nalong fixed trajectories coming from the final policy. However, the\ntrajectories themselves also increase in length during training, and thus the\nregion densities decrease as seen from the perspective of the current\ntrajectory. Our findings suggest that the complexity of deep reinforcement\nlearning policies does not principally emerge from a significant growth in the\ncomplexity of functions observed on-and-around trajectories of the policy.",
    "descriptor": "\nComments: NeurIPS 2022 camera ready\n",
    "authors": [
      "Setareh Cohen",
      "Nam Hee Kim",
      "David Rolnick",
      "Michiel van de Panne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13611"
  },
  {
    "id": "arXiv:2210.13617",
    "title": "Adapters for Enhanced Modeling of Multilingual Knowledge and Text",
    "abstract": "Large language models appear to learn facts from the large text corpora they\nare trained on. Such facts are encoded implicitly within their many parameters,\nmaking it difficult to verify or manipulate what knowledge has been learned.\nLanguage models have recently been extended to multilingual language models\n(MLLMs), enabling knowledge to be learned across hundreds of languages.\nMeanwhile, knowledge graphs contain facts in an explicit triple format, which\nrequire careful and costly curation and are only available in a few\nhigh-resource languages, restricting their research and application. To address\nthese issues, we propose to enhance MLLMs with knowledge from multilingual\nknowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks\nacross many languages, including low-resource ones. Specifically, we introduce\na lightweight adapter set to enhance MLLMs with cross-lingual entity alignment\nand facts from MLKGs for many languages. Experiments on common benchmarks show\nthat such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable\nor improved performance for knowledge graph completion and entity alignment\nrelative to baselines, especially for low-resource languages (for which\nknowledge graphs are unavailable); and (2) improved MLLM performance on\nlanguage understanding tasks that require multilingual factual knowledge; all\nwhile maintaining performance on other general language tasks.",
    "descriptor": "\nComments: Our code, models, and data (e.g., integration corpus and extended datasets) are available: this https URL\n",
    "authors": [
      "Yifan Hou",
      "Wenxiang Jiao",
      "Meizhen Liu",
      "Zhaopeng Tu",
      "Carl Allen",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13617"
  },
  {
    "id": "arXiv:2210.13619",
    "title": "A Simpler Method for Understanding Emergency Shelter Access Patterns",
    "abstract": "The Simplified Access Metric (SAM) is a new approach for characterizing\nemergency shelter access patterns as a measure of shelter client vulnerability.\nThe goal of SAM is to provide shelter operators with an intuitive way to\nunderstand access patterns that can be implemented by non-technical staff using\nspreadsheet operations. Client data from a large North American shelter will be\nused to demonstrate that SAM produces similar results to traditional\ntransitional, episodic and chronic client cluster analysis. Since SAM requires\nless data than cluster analysis, it is also able to generate a real time\npicture of how shelter access patterns are affected by external factors.\nTimelines generated from nine years of shelter client data using SAM\ndemonstrate the impact of Housing First programming and the COVID-19 lockdown\non how people access shelter. Finally, SAM allows shelter staff to move beyond\nassigning transitional, episodic and chronic labels and instead use the \"soft\"\noutput of SAM directly as a measure of vulnerability.",
    "descriptor": "",
    "authors": [
      "Geoffrey G. Messier"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13619"
  },
  {
    "id": "arXiv:2210.13621",
    "title": "Experimental Flight Testing of a Fault-Tolerant Adaptive Autopilot for  Fixed-Wing Aircraft",
    "abstract": "This paper presents an adaptive autopilot for fixed-wing aircraft and\ncompares its performance with a fixed-gain autopilot. The adaptive autopilot is\nconstructed by augmenting the autopilot architecture with adaptive control laws\nthat are updated using retrospective cost adaptive control. In order to\ninvestigate the performance of the adaptive autopilot, the default gains of the\nfixed-gain autopilot are scaled to degrade its performance. This scenario\nprovides a venue for determining the ability of the adaptive autopilot to\ncompensate for the degraded fixed-gain autopilot. Next, the performance of the\nadaptive autopilot is examined under failure conditions by simulating a\nscenario where one of the control surfaces is assumed to be stuck at an unknown\nangle. The adaptive autopilot is also tested in physical flight experiments\nunder degraded-nominal conditions, and the resulting performance improvement is\nexamined.",
    "descriptor": "\nComments: 8 pages, submitted to 2023 American Control Conference (ACC). arXiv admin note: substantial text overlap with arXiv:2110.11390\n",
    "authors": [
      "Joonghyun Lee",
      "John Spencer",
      "Siyuan Shao",
      "Juan Augusto Paredes",
      "Dennis S. Bernstein",
      "Ankit Goel"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13621"
  },
  {
    "id": "arXiv:2210.13623",
    "title": "Reinforcement Learning and Bandits for Speech and Language Processing:  Tutorial, Review and Outlook",
    "abstract": "In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.",
    "descriptor": "\nComments: INTERSPEECH 2022 Tutorial\n",
    "authors": [
      "Baihan Lin"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13623"
  },
  {
    "id": "arXiv:2210.13625",
    "title": "Deploying a Steered Query Optimizer in Production at Microsoft",
    "abstract": "Modern analytical workloads are highly heterogeneous and massively complex,\nmaking generic query optimizers untenable for many customers and scenarios. As\na result, it is important to specialize these optimizers to instances of the\nworkloads. In this paper, we continue a recent line of work in steering a query\noptimizer towards better plans for a given workload, and make major strides in\npushing previous research ideas to production deployment. Along the way we\nsolve several operational challenges including, making steering actions more\nmanageable, keeping the costs of steering within budget, and avoiding\nunexpected performance regressions in production. Our resulting system,\nQQ-advisor, essentially externalizes the query planner to a massive offline\npipeline for better exploration and specialization. We discuss various aspects\nof our design and show detailed results over production SCOPE workloads at\nMicrosoft, where the system is currently enabled by default.",
    "descriptor": "",
    "authors": [
      "Wangda Zhang",
      "Matteo Interlandi",
      "Paul Mineiro",
      "Shi Qiao",
      "Nasim Ghazanfari Karlen Lie",
      "Marc Friedman",
      "Rafah Hosn",
      "Hiren Patel",
      "Alekh Jindal"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13625"
  },
  {
    "id": "arXiv:2210.13626",
    "title": "VLC-BERT: Visual Question Answering with Contextualized Commonsense  Knowledge",
    "abstract": "There has been a growing interest in solving Visual Question Answering (VQA)\ntasks that require the model to reason beyond the content present in the image.\nIn this work, we focus on questions that require commonsense reasoning. In\ncontrast to previous methods which inject knowledge from static knowledge\nbases, we investigate the incorporation of contextualized knowledge using\nCommonsense Transformer (COMET), an existing knowledge model trained on\nhuman-curated knowledge bases. We propose a method to generate, select, and\nencode external commonsense knowledge alongside visual and textual cues in a\nnew pre-trained Vision-Language-Commonsense transformer model, VLC-BERT.\nThrough our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets,\nwe show that VLC-BERT is capable of outperforming existing models that utilize\nstatic knowledge bases. Furthermore, through a detailed analysis, we explain\nwhich questions benefit, and which don't, from contextualized commonsense\nknowledge from COMET.",
    "descriptor": "\nComments: Accepted at WACV 2023. For code and supplementary material, see this https URL\n",
    "authors": [
      "Sahithya Ravi",
      "Aditya Chinchure",
      "Leonid Sigal",
      "Renjie Liao",
      "Vered Shwartz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13626"
  },
  {
    "id": "arXiv:2210.13628",
    "title": "Predicting Long-Term Citations from Short-Term Linguistic Influence",
    "abstract": "A standard measure of the influence of a research paper is the number of\ntimes it is cited. However, papers may be cited for many reasons, and citation\ncount offers limited information about the extent to which a paper affected the\ncontent of subsequent publications. We therefore propose a novel method to\nquantify linguistic influence in timestamped document collections. There are\ntwo main steps: first, identify lexical and semantic changes using contextual\nembeddings and word frequencies; second, aggregate information about these\nchanges into per-document influence scores by estimating a high-dimensional\nHawkes process with a low-rank parameter matrix. We show that this measure of\nlinguistic influence is predictive of $\\textit{future}$ citations: the estimate\nof linguistic influence from the two years after a paper's publication is\ncorrelated with and predictive of its citation count in the following three\nyears. This is demonstrated using an online evaluation with incremental\ntemporal training/test splits, in comparison with a strong baseline that\nincludes predictors for initial citation counts, topics, and lexical features.",
    "descriptor": "\nComments: 17 pages, 3 figures, to appear in the Findings of EMNLP 2022\n",
    "authors": [
      "Sandeep Soni",
      "David Bamman",
      "Jacob Eisenstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13628"
  },
  {
    "id": "arXiv:2210.13630",
    "title": "Budget-Constrained Bounds for Mini-Batch Estimation of Optimal Transport",
    "abstract": "Optimal Transport (OT) is a fundamental tool for comparing probability\ndistributions, but its exact computation remains prohibitive for large\ndatasets. In this work, we introduce novel families of upper and lower bounds\nfor the OT problem constructed by aggregating solutions of mini-batch OT\nproblems. The upper bound family contains traditional mini-batch averaging at\none extreme and a tight bound found by optimal coupling of mini-batches at the\nother. In between these extremes, we propose various methods to construct\nbounds based on a fixed computational budget. Through various experiments, we\nexplore the trade-off between computational budget and bound tightness and show\nthe usefulness of these bounds in computer vision applications.",
    "descriptor": "",
    "authors": [
      "David Alvarez-Melis",
      "Nicol\u00f2 Fusi",
      "Lester Mackey",
      "Tal Wagner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.13630"
  },
  {
    "id": "arXiv:2210.13631",
    "title": "On the Robustness of Dataset Inference",
    "abstract": "Machine learning (ML) models are costly to train as they can require a\nsignificant amount of data, computational resources and technical expertise.\nThus, they constitute valuable intellectual property that needs protection from\nadversaries wanting to steal them. Ownership verification techniques allow the\nvictims of model stealing attacks to demonstrate that a suspect model was in\nfact stolen from theirs. Although a number of ownership verification techniques\nbased on watermarking or fingerprinting have been proposed, most of them fall\nshort either in terms of security guarantees (well-equipped adversaries can\nevade verification) or computational cost. A fingerprinting technique\nintroduced at ICLR '21, Dataset Inference (DI), has been shown to offer better\nrobustness and efficiency than prior methods. The authors of DI provided a\ncorrectness proof for linear (suspect) models. However, in the same setting, we\nprove that DI suffers from high false positives (FPs) -- it can incorrectly\nidentify an independent model trained with non-overlapping data from the same\ndistribution as stolen. We further prove that DI also triggers FPs in\nrealistic, non-linear suspect models. We then confirm empirically that DI leads\nto FPs, with high confidence. Second, we show that DI also suffers from false\nnegatives (FNs) -- an adversary can fool DI by regularising a stolen model's\ndecision boundaries using adversarial training, thereby leading to an FN. To\nthis end, we demonstrate that DI fails to identify a model adversarially\ntrained from a stolen dataset -- the setting where DI is the hardest to evade.\nFinally, we discuss the implications of our findings, the viability of\nfingerprinting-based ownership verification in general, and suggest directions\nfor future work.",
    "descriptor": "",
    "authors": [
      "Sebastian Szyller",
      "Rui Zhang",
      "Jian Liu",
      "N. Asokan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13631"
  },
  {
    "id": "arXiv:2210.13634",
    "title": "Vitruvio: 3D Building Meshes via Single Perspective Sketches",
    "abstract": "Today's architectural engineering and construction (AEC) software require a\nlearning curve to generate a three-dimension building representation. This\nlimits the ability to quickly validate the volumetric implications of an\ninitial design idea communicated via a single sketch. Allowing designers to\ntranslate a single sketch to a 3D building will enable owners to instantly\nvisualize 3D project information without the cognitive load required. If\nprevious state-of-the-art (SOTA) data-driven methods for single view\nreconstruction (SVR) showed outstanding results in the reconstruction process\nfrom a single image or sketch, they lacked specific applications, analysis, and\nexperiments in the AEC. Therefore, this research addresses this gap,\nintroducing a deep learning method: Vitruvio. Vitruvio adapts Occupancy Network\nfor SVR tasks on a specific building dataset (Manhattan 1K). This adaptation\nbrings two main improvements. First, it accelerates the inference process by\nmore than 26\\% (from 0.5s to 0.37s). Second, it increases the reconstruction\naccuracy (measured by the Chamfer Distance) by 18\\%. During this adaptation in\nthe AEC domain, we evaluate the effect of the building orientation in the\nlearning procedure since it constitutes an important design factor. While\naligning all the buildings to a canonical pose improved the overall\nquantitative metrics, it did not capture fine-grain details in more complex\nbuilding shapes (as shown in our qualitative analysis). Finally, Vitruvio\noutputs a 3D-printable building mesh with arbitrary topology and genus from a\nsingle perspective sketch, providing a step forward to allow owners and\ndesigners to communicate 3D information via a 2D, effective, intuitive, and\nuniversal communication medium: the sketch.",
    "descriptor": "",
    "authors": [
      "Alberto Tono",
      "Martin Fischer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13634"
  },
  {
    "id": "arXiv:2210.13635",
    "title": "Toward an Intelligent Tutoring System for Argument Mining in Legal Texts",
    "abstract": "We propose an adaptive environment (CABINET) to support caselaw analysis\n(identifying key argument elements) based on a novel cognitive computing\nframework that carefully matches various machine learning (ML) capabilities to\nthe proficiency of a user. CABINET supports law students in their learning as\nwell as professionals in their work. The results of our experiments focused on\nthe feasibility of the proposed framework are promising. We show that the\nsystem is capable of identifying a potential error in the analysis with very\nlow false positives rate (2.0-3.5%), as well as of predicting the key argument\nelement type (e.g., an issue or a holding) with a reasonably high F1-score\n(0.74).",
    "descriptor": "\nComments: Accepted for presentation at the 35th International Conference on Legal Knowledge and Information Systems (JURIX 2022) and publication in the Frontiers of Artificial Intelligence and Applications series of IOS Press\n",
    "authors": [
      "Hannes Westermann",
      "Jaromir Savelka",
      "Vern R. Walker",
      "Kevin D. Ashley",
      "Karim Benyekhlef"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13635"
  },
  {
    "id": "arXiv:2210.13638",
    "title": "Learning Robust Real-World Dexterous Grasping Policies via Implicit  Shape Augmentation",
    "abstract": "Dexterous robotic hands have the capability to interact with a wide variety\nof household objects to perform tasks like grasping. However, learning robust\nreal world grasping policies for arbitrary objects has proven challenging due\nto the difficulty of generating high quality training data. In this work, we\npropose a learning system (ISAGrasp) for leveraging a small number of human\ndemonstrations to bootstrap the generation of a much larger dataset containing\nsuccessful grasps on a variety of novel objects. Our key insight is to use a\ncorrespondence-aware implicit generative model to deform object meshes and\ndemonstrated human grasps in order to generate a diverse dataset of novel\nobjects and successful grasps for supervised learning, while maintaining\nsemantic realism. We use this dataset to train a robust grasping policy in\nsimulation which can be deployed in the real world. We demonstrate grasping\nperformance with a four-fingered Allegro hand in both simulation and the real\nworld, and show this method can handle entirely new semantic classes and\nachieve a 79% success rate on grasping unseen objects in the real world.",
    "descriptor": "\nComments: Accepted by CoRL2022\n",
    "authors": [
      "Zoey Qiuyu Chen",
      "Karl Van Wyk",
      "Yu-Wei Chao",
      "Wei Yang",
      "Arsalan Mousavian",
      "Abhishek Gupta",
      "Dieter Fox"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13638"
  },
  {
    "id": "arXiv:2210.13641",
    "title": "NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields",
    "abstract": "We propose a novel geometric and photometric 3D mapping pipeline for accurate\nand real-time scene reconstruction from monocular images. To achieve this, we\nleverage recent advances in dense monocular SLAM and real-time hierarchical\nvolumetric neural radiance fields. Our insight is that dense monocular SLAM\nprovides the right information to fit a neural radiance field of the scene in\nreal-time, by providing accurate pose estimates and depth-maps with associated\nuncertainty. With our proposed uncertainty-based depth loss, we achieve not\nonly good photometric accuracy, but also great geometric accuracy. In fact, our\nproposed pipeline achieves better geometric and photometric accuracy than\ncompeting approaches (up to 179% better PSNR and 86% better L1 depth), while\nworking in real-time and using only monocular images.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Antoni Rosinol",
      "John J. Leonard",
      "Luca Carlone"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13641"
  },
  {
    "id": "arXiv:2210.13642",
    "title": "MISm: A Medical Image Segmentation Metric for Evaluation of weak labeled  Data",
    "abstract": "Performance measures are an important tool for assessing and comparing\ndifferent medical image segmentation algorithms. Unfortunately, the current\nmeasures have their weaknesses when it comes to assessing certain edge cases.\nThese limitations arouse when images with a very small region of interest or\nwithout a region of interest at all are assessed. As a solution for these\nlimitations, we propose a new medical image segmentation metric: MISm. To\nevaluate MISm, the popular metrics in the medical image segmentation and MISm\nwere compared using images of magnet resonance tomography from several\nscenarios. In order to allow application in the community and reproducibility\nof experimental results, we included MISm in the publicly available evaluation\nframework MISeval:\nhttps://github.com/frankkramer-lab/miseval/tree/master/miseval",
    "descriptor": "\nComments: GitHub: this https URL\n",
    "authors": [
      "Dennis Hartmann",
      "Verena Schmid",
      "Philip Meyer",
      "I\u00f1aki Soto-Rey",
      "Dominik M\u00fcller",
      "Frank Kramer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13642"
  },
  {
    "id": "arXiv:2210.13646",
    "title": "Depth Monocular Estimation with Attention-based Encoder-Decoder Network  from Single Image",
    "abstract": "Depth information is the foundation of perception, essential for autonomous\ndriving, robotics, and other source-constrained applications. Promptly\nobtaining accurate and efficient depth information allows for a rapid response\nin dynamic environments. Sensor-based methods using LIDAR and RADAR obtain high\nprecision at the cost of high power consumption, price, and volume. While due\nto advances in deep learning, vision-based approaches have recently received\nmuch attention and can overcome these drawbacks. In this work, we explore an\nextreme scenario in vision-based settings: estimate a depth map from one\nmonocular image severely plagued by grid artifacts and blurry edges. To address\nthis scenario, We first design a convolutional attention mechanism block (CAMB)\nwhich consists of channel attention and spatial attention sequentially and\ninsert these CAMBs into skip connections. As a result, our novel approach can\nfind the focus of current image with minimal overhead and avoid losses of depth\nfeatures. Next, by combining the depth value, the gradients of X axis, Y axis\nand diagonal directions, and the structural similarity index measure (SSIM), we\npropose our novel loss function. Moreover, we utilize pixel blocks to\naccelerate the computation of the loss function. Finally, we show, through\ncomprehensive experiments on two large-scale image datasets, i.e. KITTI and\nNYU-V2, that our method outperforms several representative baselines.",
    "descriptor": "",
    "authors": [
      "Xin Zhang",
      "Rabab Abdelfattah",
      "Yuqi Song",
      "Samuel A. Dauchert",
      "Xiaofeng wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13646"
  },
  {
    "id": "arXiv:2210.13647",
    "title": "Temporally Disentangled Representation Learning",
    "abstract": "Recently in the field of unsupervised representation learning, strong\nidentifiability results for disentanglement of causally-related latent\nvariables have been established by exploiting certain side information, such as\nclass labels, in addition to independence. However, most existing work is\nconstrained by functional form assumptions such as independent sources or\nfurther with linear transitions, and distribution assumptions such as\nstationary, exponential family distribution. It is unknown whether the\nunderlying latent variables and their causal relations are identifiable if they\nhave arbitrary, nonparametric causal influences in between. In this work, we\nestablish the identifiability theories of nonparametric latent causal processes\nfrom their nonlinear mixtures under fixed temporal causal influences and\nanalyze how distribution changes can further benefit the disentanglement. We\npropose \\textbf{\\texttt{TDRL}}, a principled framework to recover time-delayed\nlatent causal variables and identify their relations from measured sequential\ndata under stationary environments and under different distribution shifts.\nSpecifically, the framework can factorize unknown distribution shifts into\ntransition distribution changes under fixed and time-varying latent causal\nrelations, and under observation changes in observation. Through experiments,\nwe show that time-delayed latent causal influences are reliably identified and\nthat our approach considerably outperforms existing baselines that do not\ncorrectly exploit this modular representation of changes. Our code is available\nat: \\url{https://github.com/weirayao/tdrl}.",
    "descriptor": "\nComments: NeurIPS 2022. arXiv admin note: text overlap with arXiv:2202.04828\n",
    "authors": [
      "Weiran Yao",
      "Guangyi Chen",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13647"
  },
  {
    "id": "arXiv:2210.13648",
    "title": "Learning to forecast vegetation greenness at fine resolution over Africa  with ConvLSTMs",
    "abstract": "Forecasting the state of vegetation in response to climate and weather events\nis a major challenge. Its implementation will prove crucial in predicting crop\nyield, forest damage, or more generally the impact on ecosystems services\nrelevant for socio-economic functioning, which if absent can lead to\nhumanitarian disasters. Vegetation status depends on weather and environmental\nconditions that modulate complex ecological processes taking place at several\ntimescales. Interactions between vegetation and different environmental drivers\nexpress responses at instantaneous but also time-lagged effects, often showing\nan emerging spatial context at landscape and regional scales. We formulate the\nland surface forecasting task as a strongly guided video prediction task where\nthe objective is to forecast the vegetation developing at very fine resolution\nusing topography and weather variables to guide the prediction. We use a\nConvolutional LSTM (ConvLSTM) architecture to address this task and predict\nchanges in the vegetation state in Africa using Sentinel-2 satellite NDVI,\nhaving ERA5 weather reanalysis, SMAP satellite measurements, and topography\n(DEM of SRTMv4.1) as variables to guide the prediction. Ours results highlight\nhow ConvLSTM models can not only forecast the seasonal evolution of NDVI at\nhigh resolution, but also the differential impacts of weather anomalies over\nthe baselines. The model is able to predict different vegetation types, even\nthose with very high NDVI variability during target length, which is promising\nto support anticipatory actions in the context of drought-related disasters.",
    "descriptor": "\nComments: Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022\n",
    "authors": [
      "Claire Robin",
      "Christian Requena-Mesa",
      "Vitus Benson",
      "Lazaro Alonso",
      "Jeran Poehls",
      "Nuno Carvalhais",
      "Markus Reichstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13648"
  },
  {
    "id": "arXiv:2210.13650",
    "title": "ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs",
    "abstract": "Knowledge Graph Question Answering (KGQA) involves retrieving entities as\nanswers from a Knowledge Graph (KG) using natural language queries. The\nchallenge is to learn to reason over question-relevant KG facts that traverse\nKG entities and lead to the question answers. To facilitate reasoning, the\nquestion is decoded into instructions, which are dense question representations\nused to guide the KG traversals. However, if the derived instructions do not\nexactly match the underlying KG information, they may lead to reasoning under\nirrelevant context. Our method, termed ReaRev, introduces a new way to KGQA\nreasoning with respect to both instruction decoding and execution. To improve\ninstruction decoding, we perform reasoning in an adaptive manner, where\nKG-aware information is used to iteratively update the initial instructions. To\nimprove instruction execution, we emulate breadth-first search (BFS) with graph\nneural networks (GNNs). The BFS strategy treats the instructions as a set and\nallows our method to decide on their execution order on the fly. Experimental\nresults on three KGQA benchmarks demonstrate the ReaRev's effectiveness\ncompared with previous state-of-the-art, especially when the KG is incomplete\nor when we tackle complex questions. Our code is publicly available at\nhttps://github.com/cmavro/ReaRev_KGQA.",
    "descriptor": "",
    "authors": [
      "Costas Mavromatis",
      "George Karypis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13650"
  },
  {
    "id": "arXiv:2210.13651",
    "title": "An Effective Approach for Multi-label Classification with Missing Labels",
    "abstract": "Compared with multi-class classification, multi-label classification that\ncontains more than one class is more suitable in real life scenarios. Obtaining\nfully labeled high-quality datasets for multi-label classification problems,\nhowever, is extremely expensive, and sometimes even infeasible, with respect to\nannotation efforts, especially when the label spaces are too large. This\nmotivates the research on partial-label classification, where only a limited\nnumber of labels are annotated and the others are missing. To address this\nproblem, we first propose a pseudo-label based approach to reduce the cost of\nannotation without bringing additional complexity to the existing\nclassification networks. Then we quantitatively study the impact of missing\nlabels on the performance of classifier. Furthermore, by designing a novel loss\nfunction, we are able to relax the requirement that each instance must contain\nat least one positive label, which is commonly used in most existing\napproaches. Through comprehensive experiments on three large-scale multi-label\nimage datasets, i.e. MS-COCO, NUS-WIDE, and Pascal VOC12, we show that our\nmethod can handle the imbalance between positive labels and negative labels,\nwhile still outperforming existing missing-label learning approaches in most\ncases, and in some cases even approaches with fully labeled datasets.",
    "descriptor": "",
    "authors": [
      "Xin Zhang",
      "Rabab Abdelfattah",
      "Yuqi Song",
      "Xiaofeng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13651"
  },
  {
    "id": "arXiv:2210.13654",
    "title": "Boosting Kidney Stone Identification in Endoscopic Images Using Two-Step  Transfer Learning",
    "abstract": "Knowing the cause of kidney stone formation is crucial to establish\ntreatments that prevent recurrence. There are currently different approaches\nfor determining the kidney stone type. However, the reference ex-vivo\nidentification procedure can take up to several weeks, while an in-vivo visual\nrecognition requires highly trained specialists. Machine learning models have\nbeen developed to provide urologists with an automated classification of kidney\nstones during an ureteroscopy; however, there is a general lack in terms of\nquality of the training data and methods. In this work, a two-step transfer\nlearning approach is used to train the kidney stone classifier. The proposed\napproach transfers knowledge learned on a set of images of kidney stones\nacquired with a CCD camera (ex-vivo dataset) to a final model that classifies\nimages from endoscopic images (ex-vivo dataset). The results show that learning\nfeatures from different domains with similar information helps to improve the\nperformance of a model that performs classification in real conditions (for\ninstance, uncontrolled lighting conditions and blur). Finally, in comparison to\nmodels that are trained from scratch or by initializing ImageNet weights, the\nobtained results suggest that the two-step approach extracts features improving\nthe identification of kidney stones in endoscopic images.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Francisco Lopez-Tiro",
      "Juan Pablo Betancur-Rengifo",
      "Arturo Ruiz-Sanchez",
      "Ivan Reyes-Amezcua",
      "Jonathan El-Beze",
      "Jacques Hubert",
      "Michel Daudon",
      "Gilberto Ochoa-Ruiz",
      "Christian Daul"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.13654"
  },
  {
    "id": "arXiv:2210.13658",
    "title": "A fast multilevel dimension iteration algorithm for high dimensional  numerical integration",
    "abstract": "In this paper, we propose and study a fast multilevel dimension iteration\n(MDI) algorithm for computing arbitrary $d$-dimensional integrals based on\ntensor product approximations. It reduces the computational complexity (in\nterms of the CPU time) of a tensor product method from the exponential order\n$O(N^d)$ to the polynomial order {\\color{black} $O(d^3N^2)$ or better}, where\n$N$ stands for the number of quadrature points in each coordinate direction. As\na result, the proposed MDI algorithm effectively circumvents the curse of the\ndimensionality of tensor product methods for high dimensional numerical\nintegration. The main idea of the proposed MDI algorithm is to compute the\nfunction evaluations at all integration points in the cluster and iteratively\nalong each coordinate direction, so lots of computations for function\nevaluations can be reused in each iteration. This idea is also applicable to\nany quadrature rule whose integration points have a lattice-like structure.",
    "descriptor": "\nComments: 25 pages, 12 tables and 8 figures\n",
    "authors": [
      "Xiaobing Feng",
      "Huicong Zhong"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13658"
  },
  {
    "id": "arXiv:2210.13659",
    "title": "Self-Configuring nnU-Nets Detect Clouds in Satellite Images",
    "abstract": "Cloud detection is a pivotal satellite image pre-processing step that can be\nperformed both on the ground and on board a satellite to tag useful images. In\nthe latter case, it can help to reduce the amount of data to downlink by\npruning the cloudy areas, or to make a satellite more autonomous through\ndata-driven acquisition re-scheduling of the cloudy areas. We approach this\nimportant task with nnU-Nets, a self-reconfigurable framework able to perform\nmeta-learning of a segmentation network over various datasets. Our experiments,\nperformed over Sentinel-2 and Landsat-8 multispectral images revealed that\nnnU-Nets deliver state-of-the-art cloud segmentation performance without any\nmanual design. Our approach was ranked within the top 7% best solutions (across\n847 participating teams) in the On Cloud N: Cloud Cover Detection Challenge,\nwhere we reached the Jaccard index of 0.882 over more than 10k unseen\nSentinel-2 image patches (the winners obtained 0.897, whereas the baseline\nU-Net with the ResNet-34 backbone used as an encoder: 0.817, and the classic\nSentinel-2 image thresholding: 0.652).",
    "descriptor": "",
    "authors": [
      "Bartosz Grabowski",
      "Maciej Ziaja",
      "Michal Kawulok",
      "Nicolas Long\u00e9p\u00e9",
      "Bertrand Le Saux",
      "Jakub Nalepa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13659"
  },
  {
    "id": "arXiv:2210.13660",
    "title": "SpacePhish: The Evasion-space of Adversarial Attacks against Phishing  Website Detectors using Machine Learning",
    "abstract": "Existing literature on adversarial Machine Learning (ML) focuses either on\nshowing attacks that break every ML model, or defenses that withstand most\nattacks. Unfortunately, little consideration is given to the actual\n\\textit{cost} of the attack or the defense. Moreover, adversarial samples are\noften crafted in the \"feature-space\", making the corresponding evaluations of\nquestionable value. Simply put, the current situation does not allow to\nestimate the actual threat posed by adversarial attacks, leading to a lack of\nsecure ML systems.\nWe aim to clarify such confusion in this paper. By considering the\napplication of ML for Phishing Website Detection (PWD), we formalize the\n\"evasion-space\" in which an adversarial perturbation can be introduced to fool\na ML-PWD -- demonstrating that even perturbations in the \"feature-space\" are\nuseful. Then, we propose a realistic threat model describing evasion attacks\nagainst ML-PWD that are cheap to stage, and hence intrinsically more attractive\nfor real phishers. Finally, we perform the first statistically validated\nassessment of state-of-the-art ML-PWD against 12 evasion attacks. Our\nevaluation shows (i) the true efficacy of evasion attempts that are more likely\nto occur; and (ii) the impact of perturbations crafted in different\nevasion-spaces. Our realistic evasion attempts induce a statistically\nsignificant degradation (3-10% at $p\\!<$0.05), and their cheap cost makes them\na subtle threat. Notably, however, some ML-PWD are immune to our most realistic\nattacks ($p$=0.22). Our contribution paves the way for a much needed\nre-assessment of adversarial attacks against ML systems for cybersecurity.",
    "descriptor": "",
    "authors": [
      "Giovanni Apruzzese",
      "Mauro Conti",
      "Ying Yuan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13660"
  },
  {
    "id": "arXiv:2210.13661",
    "title": "Understanding Inconsistency in Azure Cosmos DB with TLA+",
    "abstract": "Beyond implementation correctness of a distributed system, it is equally\nimportant to understand exactly what users should expect to see from that\nsystem. Even if the system itself works as designed, insufficient understanding\nof its user-visible semantics can cause bugs in its dependencies. By focusing a\nformal specification effort on precisely defining the expected user-facing\nbehaviors of the Azure Cosmos DB service at Microsoft, we were able to write a\nformal specification of the database that was significantly smaller and\nconceptually simpler than any other specification of Cosmos DB, while\nrepresenting a wider range of valid user-observable behaviors than existing\nmore detailed specifications. Many of the additional behaviors we documented\nwere previously poorly understood outside of the Cosmos DB development team,\neven informally, leading to data consistency errors in Microsoft products that\ndepend on it. Using this model, we were able to raise two key issues in Cosmos\nDB's public-facing documentation, which have since been addressed. We were also\nable to offer a fundamental solution to a previous high-impact outage within\nanother Azure service that depends on Cosmos DB.",
    "descriptor": "",
    "authors": [
      "A. Finn Hackett",
      "Joshua Rowe",
      "Markus Alexander Kuppe"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.13661"
  },
  {
    "id": "arXiv:2210.13662",
    "title": "Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis  Testing: A Lesson From Fano",
    "abstract": "Differential privacy (DP) is by far the most widely accepted framework for\nmitigating privacy risks in machine learning. However, exactly how small the\nprivacy parameter $\\epsilon$ needs to be to protect against certain privacy\nrisks in practice is still not well-understood. In this work, we study data\nreconstruction attacks for discrete data and analyze it under the framework of\nmultiple hypothesis testing. We utilize different variants of the celebrated\nFano's inequality to derive upper bounds on the inferential power of a data\nreconstruction adversary when the model is trained differentially privately.\nImportantly, we show that if the underlying private data takes values from a\nset of size $M$, then the target privacy parameter $\\epsilon$ can be $O(\\log\nM)$ before the adversary gains significant inferential power. Our analysis\noffers theoretical evidence for the empirical effectiveness of DP against data\nreconstruction attacks even at relatively large values of $\\epsilon$.",
    "descriptor": "",
    "authors": [
      "Chuan Guo",
      "Alexandre Sablayrolles",
      "Maziar Sanjabi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.13662"
  },
  {
    "id": "arXiv:2210.13663",
    "title": "Technical Report: Implementation of Single Packet Number Space in  Multi-Path QUIC",
    "abstract": "Over the past few of years, we have witnessed increasing interests in the use\ncases of multi-path QUIC from both industry and academia. For example, Alibaba\ndeployed XLINK, a QoE-driven multi-path QUIC solution, in Taobao short video\nand showed benefits in both reduced tail latency and video re-buffering. For\nthe time being, the multi-path QUIC protocol is in the process of\nstandardization at the IETF QUIC working group, with the draft recently updated\nto version 02. The focus of the draft is to provide basic guidance on the\nimplementation so that we can encourage more exploration, testing, and finally,\nan accelerated adoption of this technology. However, draft-02 has brought up an\nopen issue on whether the multi-path QUIC should be implemented using single\npacket number space (SPNS) or multiple packet number space (MPNS), as in the\ncurrent draft, both options co-exist. Knowing that one cannot draw a solid\nconclusion without experiments, we implement both SPNS and MPNS at Alibaba and\nmeasured their performance. The goal is to help the community better understand\nthe implication, and we hope this report can be a useful resource for engineers\nand researchers who are interested in deploying multi-path QUIC.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Yingqi Tang",
      "Yunfei Ma",
      "Yanmei Liu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13663"
  },
  {
    "id": "arXiv:2210.13664",
    "title": "Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher  Mixture Model",
    "abstract": "In spite of the high performance and reliability of deep learning algorithms\nin a wide range of everyday applications, many investigations tend to show that\na lot of models exhibit biases, discriminating against specific subgroups of\nthe population (e.g. gender, ethnicity). This urges the practitioner to develop\nfair systems with a uniform/comparable performance across sensitive groups. In\nthis work, we investigate the gender bias of deep Face Recognition networks. In\norder to measure this bias, we introduce two new metrics, $\\mathrm{BFAR}$ and\n$\\mathrm{BFRR}$, that better reflect the inherent deployment needs of Face\nRecognition systems. Motivated by geometric considerations, we mitigate gender\nbias through a new post-processing methodology which transforms the deep\nembeddings of a pre-trained model to give more representation power to\ndiscriminated subgroups. It consists in training a shallow neural network by\nminimizing a Fair von Mises-Fisher loss whose hyperparameters account for the\nintra-class variance of each gender. Interestingly, we empirically observe that\nthese hyperparameters are correlated with our fairness metrics. In fact,\nextensive numerical experiments on a variety of datasets show that a careful\nselection significantly reduces gender bias.",
    "descriptor": "",
    "authors": [
      "Jean-R\u00e9my Conti",
      "Nathan Noiry",
      "Vincent Despiegel",
      "St\u00e9phane Gentric",
      "St\u00e9phan Cl\u00e9men\u00e7on"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13664"
  },
  {
    "id": "arXiv:2210.13669",
    "title": "Help me write a poem: Instruction Tuning as a Vehicle for Collaborative  Poetry Writing",
    "abstract": "Recent work in training large language models (LLMs) to follow natural\nlanguage instructions has opened up exciting opportunities for natural language\ninterface design. Building on the prior success of LLMs in the realm of\ncomputer-assisted creativity, we aim to study if LLMs can improve the quality\nof user-generated content through collaboration. We present CoPoet, a\ncollaborative poetry writing system. In contrast to auto-completing a user's\ntext, CoPoet is controlled by user instructions that specify the attributes of\nthe desired text, such as Write a sentence about `love' or Write a sentence\nending in `fly'. The core component of our system is a language model\nfine-tuned on a diverse collection of instructions for poetry writing. Our\nmodel is not only competitive with publicly available LLMs trained on\ninstructions (InstructGPT), but is also capable of satisfying unseen\ncompositional instructions. A study with 15 qualified crowdworkers shows that\nusers successfully write poems with CoPoet on diverse topics ranging from\nMonarchy to Climate change. Further, the collaboratively written poems are\npreferred by third-party evaluators over those written without the system.",
    "descriptor": "\nComments: To appear at EMNLP 2022\n",
    "authors": [
      "Tuhin Chakrabarty",
      "Vishakh Padmakumar",
      "He He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13669"
  },
  {
    "id": "arXiv:2210.13670",
    "title": "Simplified State Storage Rent for EVM Blockchains",
    "abstract": "Uncontrolled growth of blockchain state can adversely affect client\nperformance, decentralization and security. Previous attempts to introduce\nduration-based state storage pricing or 'storage rent' in Ethereum have\nstalled, partly because of complexity. We present a new approach with finer\ngranularity to \"spread\" rent payments across peers. Our proposal shifts the\nburden of state rent from accounts to transaction senders in a quasi-random\nmanner. This proposal offers a simple path for initial adoption on Ethereum\nVirtual Machine (EVM) compatible chains, and serve as a foundation to address\nremaining challenges.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Sergio Demian Lerner",
      "Federico Jinich",
      "Diego Masini",
      "Shreemoy Mishra"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.13670"
  },
  {
    "id": "arXiv:2210.13672",
    "title": "Feng-Shui Compass: A Modern Exploration of Traditional Chinese  Environmental Analysis",
    "abstract": "The technological advancement in data analysis and sensor technology has\ncontributed to a growth in knowledge of the surrounding environments. Feng\nShui, the Chinese philosophy of evaluating a certain environment and how it\ninfluences human well-being, can only be determined by self-claimed specialists\nfor the past thousands of years. We developed a device as well as a procedure\nto evaluate the ambient environment of a room to perform a study that attempts\nto use sensor data to predict the well-being score of a person in that\nenvironment, therefore evaluating the primary aspect of Feng Shui. Our study\nrevealed preliminary results showing great potential for further research with\nlarger experiments.",
    "descriptor": "\nComments: Ubiquitous Computing at Cornell Tech\n",
    "authors": [
      "Xuanyu Fang",
      "Yunzhu Pan",
      "Hongjun Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13672"
  },
  {
    "id": "arXiv:2210.13673",
    "title": "Evaluating Parameter Efficient Learning for Generation",
    "abstract": "Parameter efficient learning methods (PERMs) have recently gained significant\nattention as they provide an efficient way for pre-trained language models\n(PLMs) to adapt to a downstream task. However, these conclusions are mostly\ndrawn from in-domain evaluations over the full training set. In this paper, we\npresent comparisons between PERMs and finetuning from three new perspectives:\n(1) the effect of sample and model size to in-domain evaluations, (2)\ngeneralization to unseen domains and new datasets, and (3) the faithfulness of\ngenerations. Our results show that for in-domain settings (a) there is a cross\npoint of sample size for which PERMs will perform better than finetuning when\ntraining with fewer samples, and (b) larger PLMs have larger cross points. For\ncross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al.,\n2019) performs the best amongst all the PERMs studied here, and (b) it\noutperforms finetuning if the task dataset is below a certain size. We also\ncompare the faithfulness of generations and show that PERMs can achieve better\nfaithfulness score than finetuning, especially for small training set, by as\nmuch as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and\nachieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all\nROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference\n",
    "authors": [
      "Peng Xu",
      "Mostofa Patwary",
      "Shrimai Prabhumoye",
      "Virginia Adams",
      "Ryan J. Prenger",
      "Wei Ping",
      "Nayeon Lee",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13673"
  },
  {
    "id": "arXiv:2210.13676",
    "title": "Computing Medial Axis Transform with Feature Preservation via Restricted  Power Diagram",
    "abstract": "We propose a novel framework for computing the medial axis transform of 3D\nshapes while preserving their medial features via restricted power diagram\n(RPD). Medial features, including external features such as the sharp edges and\ncorners of the input mesh surface and internal features such as the seams and\njunctions of medial axis, are important shape descriptors both topologically\nand geometrically. However, existing medial axis approximation methods fail to\ncapture and preserve them due to the fundamentally under-sampling in the\nvicinity of medial features, and the difficulty to build their correct\nconnections. In this paper we use the RPD of medial spheres and its affiliated\nstructures to help solve these challenges. The dual structure of RPD provides\nthe connectivity of medial spheres. The surface restricted power cell (RPC) of\neach medial sphere provides the tangential surface regions that these spheres\nhave contact with. The connected components (CC) of surface RPC give us the\nclassification of each sphere, to be on a medial sheet, a seam, or a junction.\nThey allow us to detect insufficient sphere sampling around medial features and\ndevelop necessary conditions to preserve them. Using this RPD-based framework,\nwe are able to construct high quality medial meshes with features preserved.\nCompared with existing sampling-based or voxel-based methods, our method is the\nfirst one that can preserve not only external features but also internal\nfeatures of medial axes.",
    "descriptor": "\nComments: 18 pages, 26 figures, SIGGRAPH Asia 2022\n",
    "authors": [
      "Ningna Wang",
      "Bin Wang",
      "Wenping Wang",
      "Xiaohu Guo"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.13676"
  },
  {
    "id": "arXiv:2210.13678",
    "title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval",
    "abstract": "Building dense retrievers requires a series of standard procedures, including\ntraining and validating neural models and creating indexes for efficient\nsearch. However, these procedures are often misaligned in that training\nobjectives do not exactly reflect the retrieval scenario at inference time. In\nthis paper, we explore how the gap between training and inference in dense\nretrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021)\nwhere billions of representations are indexed at inference. Since validating\nevery dense retriever with a large-scale index is practically infeasible, we\npropose an efficient way of validating dense retrievers using a small subset of\nthe entire corpus. This allows us to validate various training strategies\nincluding unifying contrastive loss terms and using hard negatives for phrase\nretrieval, which largely reduces the training-inference discrepancy. As a\nresult, we improve top-1 phrase retrieval accuracy by 2~3 points and top-20\npassage retrieval accuracy by 2~4 points for open-domain question answering.\nOur work urges modeling dense retrievers with careful consideration of training\nand inference via efficient validation while advancing phrase retrieval as a\ngeneral solution for dense retrieval.",
    "descriptor": "\nComments: Findings of EMNLP 2022; 12 pages, 3 figures\n",
    "authors": [
      "Gyuwan Kim",
      "Jinhyuk Lee",
      "Barlas Oguz",
      "Wenhan Xiong",
      "Yizhe Zhang",
      "Yashar Mehdad",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13678"
  },
  {
    "id": "arXiv:2210.13681",
    "title": "BSDF Importance Baking: A Lightweight Neural Solution to Importance  Sampling Parametric BSDFs",
    "abstract": "Parametric BSDFs (Bidirectional Scattering Distribution Functions) are\npervasively used because of their flexibility to represent a large variety of\nmaterial appearances by simply tuning the parameters. While efficient\nevaluation of parametric BSDFs has been well-studied, high-quality importance\nsampling techniques for parametric BSDFs are still scarce. Existing sampling\nstrategies either heavily rely on approximations and result in high variance,\nor solely perform sampling on a portion of the whole BSDF slice. Moreover, many\nof the sampling approaches are specifically paired with certain types of BSDFs.\nIn this paper, we seek an efficient and general way for importance sampling\nparametric BSDFs. We notice that the nature of importance sampling is the\nmapping between a uniform distribution and the target distribution.\nSpecifically, when BSDF parameters are given, the mapping that performs\nimportance sampling on a BSDF slice can be simply recorded as a 2D image that\nwe name as importance map. Following this observation, we accurately precompute\nthe importance maps using a mathematical tool named optimal transport. Then we\npropose a lightweight neural network to efficiently compress the precomputed\nimportance maps. In this way, we have completely brought parametric BSDF\nimportance sampling to the precomputation stage, avoiding heavy runtime\ncomputation. Since this process is similar to light baking where a set of\nimages are precomputed, we name our method importance baking. Together with a\nBSDF evaluation network and a PDF (probability density function) query network,\nour method enables full MIS without any revision to the rendering pipeline. Our\nmethod essentially performs perfect importance sampling. Compared with previous\nmethods, we demonstrate reduced noise levels on rendering results with a rich\nset of appearances, including both conductors and dielectrics with anisotropic\nroughness.",
    "descriptor": "",
    "authors": [
      "Yaoyi Bai",
      "Songyin Wu",
      "Zheng Zeng",
      "Beibei Wang",
      "Ling-Qi Yan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.13681"
  },
  {
    "id": "arXiv:2210.13682",
    "title": "Musings on the HashGraph Protocol: Its Security and Its Limitations",
    "abstract": "The HashGraph Protocol is a Byzantine fault tolerant atomic broadcast\nprotocol. Its novel use of locally stored metadata allows parties to recover a\nconsistent ordering of their log just by examining their local data, removing\nthe need for a voting protocol. Our paper's first contribution is to present a\nrewritten proof of security for the HashGraph Protocol that follows the\nconsistency and liveness paradigm used in the atomic broadcast literature. In\nour second contribution, we show a novel adversarial strategy that stalls the\nprotocol from committing data to the log for an expected exponential number of\nrounds. This proves tight the exponential upper bound conjectured in the\noriginal paper. We believe that our proof of security will make it easier to\ncompare HashGraph with other atomic broadcast protocols and to incorporate its\nideas into new constructions. We also believe that our attack might inspire\nmore research into similar attacks for other DAG-based atomic broadcast\nprotocols.",
    "descriptor": "\nComments: 30 pages, 16 figures\n",
    "authors": [
      "Vinesh Sridhar",
      "Erica Blum",
      "Jonathan Katz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13682"
  },
  {
    "id": "arXiv:2210.13683",
    "title": "Bayesian Methods in Automated Vehicle's Car-following Uncertainties:  Enabling Strategic Decision Making",
    "abstract": "This paper proposes a methodology to estimate uncertainty in automated\nvehicle (AV) dynamics in real time via Bayesian inference. Based on the\nestimated uncertainty, the method aims to continuously monitor the\ncar-following (CF) performance of the AV to support strategic actions to\nmaintain a desired performance. Our methodology consists of three sequential\ncomponents: (i) the Stochastic Gradient Langevin Dynamics (SGLD) is adopted to\nestimate parameter uncertainty relative to vehicular dynamics in real time,\n(ii) dynamic monitoring of car-following stability (local and string-wise), and\n(iii) strategic actions for control adjustment if anomaly is detected. The\nproposed methodology provides means to gauge AV car-following performance in\nreal time and preserve desired performance against real time uncertainty that\nare unaccounted for in the vehicle control algorithm.",
    "descriptor": "",
    "authors": [
      "Wissam Kontar",
      "Soyoung Ahn"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13683"
  },
  {
    "id": "arXiv:2210.13686",
    "title": "FedGRec: Federated Graph Recommender System with Lazy Update of Latent  Embeddings",
    "abstract": "Recommender systems are widely used in industry to improve user experience.\nDespite great success, they have recently been criticized for collecting\nprivate user data. Federated Learning (FL) is a new paradigm for learning on\ndistributed data without direct data sharing. Therefore, Federated Recommender\n(FedRec) systems are proposed to mitigate privacy concerns to non-distributed\nrecommender systems. However, FedRec systems have a performance gap to its\nnon-distributed counterpart. The main reason is that local clients have an\nincomplete user-item interaction graph, thus FedRec systems cannot utilize\nindirect user-item interactions well. In this paper, we propose the Federated\nGraph Recommender System (FedGRec) to mitigate this gap. Our FedGRec system can\neffectively exploit the indirect user-item interactions. More precisely, in our\nsystem, users and the server explicitly store latent embeddings for users and\nitems, where the latent embeddings summarize different orders of indirect\nuser-item interactions and are used as a proxy of missing interaction graph\nduring local training. We perform extensive empirical evaluations to verify the\nefficacy of using latent embeddings as a proxy of missing interaction graph;\nthe experimental results show superior performance of our system compared to\nvarious baselines. A short version of the paper is presented in\n\\href{https://federated-learning.org/fl-neurips-2022/}{the FL-NeurIPS'22\nworkshop}.",
    "descriptor": "",
    "authors": [
      "Junyi Li",
      "Heng Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13686"
  },
  {
    "id": "arXiv:2210.13689",
    "title": "Embedded Design of Automatic Pesticide Spraying Robot Control System",
    "abstract": "In agriculture, crops need to apply pesticide spraying flow control precisely\nto reduce costs, protect the environment, and increase yield production.\nAlthough there have several variable control methods for spraying flow control\nbecause indirect control flow techniques and having a slow response could cause\ninaccuracy and mismanagement, also noted that those systems also suffer from\ncomplicated design and debugging, etc. In this paper, an embedded design of the\nfuzzy PID variable spraying control method is adopted. The experimental results\nshow that the overshoot of Proportional Integral Derivative (PID) control is\n10.76%, and the overshoot of fuzzy PID control is 7.17% which can meet the\nrequirements of an advanced spray control flow system. For further\ninvestigation, a novel spray flow control method based on Programmable Logic\nControl (PLC) is proposed in this paper.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1806.06762 by other authors\n",
    "authors": [
      "Ahamed Mustak",
      "Hongbin Ma",
      "Lepeng Song",
      "Ying Jin"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13689"
  },
  {
    "id": "arXiv:2210.13693",
    "title": "XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for  Cross-lingual Text-to-SQL Semantic Parsing",
    "abstract": "In-context learning using large language models has recently shown surprising\nresults for semantic parsing tasks such as Text-to-SQL translation. Prompting\nGPT-3 or Codex using several examples of question-SQL pairs can produce\nexcellent results, comparable to state-of-the-art finetuning-based models.\nHowever, existing work primarily focuses on English datasets, and it is unknown\nwhether large language models can serve as competitive semantic parsers for\nother languages. To bridge this gap, our work focuses on cross-lingual\nText-to-SQL semantic parsing for translating non-English utterances into SQL\nqueries based on an English schema. We consider a zero-shot transfer learning\nsetting with the assumption that we do not have any labeled examples in the\ntarget language (but have annotated examples in English). This work introduces\nthe XRICL framework, which learns to retrieve relevant English exemplars for a\ngiven query to construct prompts. We also include global translation exemplars\nfor a target language to facilitate the translation process for large language\nmodels. To systematically evaluate our model, we construct two new benchmark\ndatasets, XSpider and XKaggle-dbqa, which include questions in Chinese,\nVietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively\nleverages large pre-trained language models to outperform existing baselines.\nData and code are publicly available at https://github.com/Impavidity/XRICL.",
    "descriptor": "",
    "authors": [
      "Peng Shi",
      "Rui Zhang",
      "He Bai",
      "Jimmy Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13693"
  },
  {
    "id": "arXiv:2210.13694",
    "title": "Worst-Case Adaptive Submodular Cover",
    "abstract": "In this paper, we study the adaptive submodular cover problem under the\nworst-case setting. This problem generalizes many previously studied problems,\nnamely, the pool-based active learning and the stochastic submodular set cover.\nThe input of our problem is a set of items (e.g., medical tests) and each item\nhas a random state (e.g., the outcome of a medical test), whose realization is\ninitially unknown. One must select an item at a fixed cost in order to observe\nits realization. There is an utility function which is defined over items and\ntheir states. Our goal is to sequentially select a group of items to achieve a\n``goal value'' while minimizing the maximum cost across realizations (a.k.a.\nworst-case cost). To facilitate our study, we introduce a broad class of\nstochastic functions, called \\emph{worst-case submodular function}. Assume the\nutility function is worst-case submodular, we develop a tight $(\\log\n(Q/\\eta)+1)$-approximation policy, where $Q$ is the ``goal value'' and $\\eta$\nis the minimum gap between $Q$ and any attainable utility value $\\hat{Q}<Q$. We\nalso study a worst-case maximum-coverage problem, whose goal is to select a\ngroup of items to maximize its worst-case utility subject to a budget\nconstraint. This is a flipped problem of the minimum-cost-cover problem, and to\nsolve this problem, we develop a tight $(1-1/e)$-approximation solution.",
    "descriptor": "",
    "authors": [
      "Jing Yuan",
      "Shaojie Tang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13694"
  },
  {
    "id": "arXiv:2210.13701",
    "title": "Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating  Models to Reflect Conflicting Evidence",
    "abstract": "Question answering models can use rich knowledge sources -- up to one hundred\nretrieved passages and parametric knowledge in the large-scale language model\n(LM). Prior work assumes information in such knowledge sources is consistent\nwith each other, paying little attention to how models blend information stored\nin their LM parameters with that from retrieved evidence documents. In this\npaper, we simulate knowledge conflicts (i.e., where parametric knowledge\nsuggests one answer and different passages suggest different answers) and\nexamine model behaviors. We find retrieval performance heavily impacts which\nsources models rely on, and current models mostly rely on non-parametric\nknowledge in their best-performing settings. We discover a troubling trend that\ncontradictions among knowledge sources affect model confidence only marginally.\nTo address this issue, we present a new calibration study, where models are\ndiscouraged from presenting any single answer when presented with multiple\nconflicting answer candidates in retrieved evidences.",
    "descriptor": "\nComments: Accepted to the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)\n",
    "authors": [
      "Hung-Ting Chen",
      "Michael J.Q. Zhang",
      "Eunsol Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13701"
  },
  {
    "id": "arXiv:2210.13702",
    "title": "DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to  Reality",
    "abstract": "Recent work has demonstrated the ability of deep reinforcement learning (RL)\nalgorithms to learn complex robotic behaviours in simulation, including in the\ndomain of multi-fingered manipulation. However, such models can be challenging\nto transfer to the real world due to the gap between simulation and reality. In\nthis paper, we present our techniques to train a) a policy that can perform\nrobust dexterous manipulation on an anthropomorphic robot hand and b) a robust\npose estimator suitable for providing reliable real-time information on the\nstate of the object being manipulated. Our policies are trained to adapt to a\nwide range of conditions in simulation. Consequently, our vision-based policies\nsignificantly outperform the best vision policies in the literature on the same\nreorientation task and are competitive with policies that are given privileged\nstate information via motion capture systems. Our work reaffirms the\npossibilities of sim-to-real transfer for dexterous manipulation in diverse\nkinds of hardware and simulator setups, and in our case, with the Allegro Hand\nand Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for\nresearchers to achieve such results with commonly-available, affordable robot\nhands and cameras. Videos of the resulting policy and supplementary\ninformation, including experiments and demos, can be found at\n\\url{https://dextreme.org/}",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Ankur Handa",
      "Arthur Allshire",
      "Viktor Makoviychuk",
      "Aleksei Petrenko",
      "Ritvik Singh",
      "Jingzhou Liu",
      "Denys Makoviichuk",
      "Karl Van Wyk",
      "Alexander Zhurkevich",
      "Balakumar Sundaralingam",
      "Yashraj Narang",
      "Jean-Francois Lafleche",
      "Dieter Fox",
      "Gavriel State"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13702"
  },
  {
    "id": "arXiv:2210.13704",
    "title": "Geo-SIC: Learning Deformable Geometric Shapes in Deep Image Classifiers",
    "abstract": "Deformable shapes provide important and complex geometric features of objects\npresented in images. However, such information is oftentimes missing or\nunderutilized as implicit knowledge in many image analysis tasks. This paper\npresents Geo-SIC, the first deep learning model to learn deformable shapes in a\ndeformation space for an improved performance of image classification. We\nintroduce a newly designed framework that (i) simultaneously derives features\nfrom both image and latent shape spaces with large intra-class variations; and\n(ii) gains increased model interpretability by allowing direct access to the\nunderlying geometric features of image data. In particular, we develop a\nboosted classification network, equipped with an unsupervised learning of\ngeometric shape representations characterized by diffeomorphic transformations\nwithin each class. In contrast to previous approaches using pre-extracted\nshapes, our model provides a more fundamental approach by naturally learning\nthe most relevant shape features jointly with an image classifier. We\ndemonstrate the effectiveness of our method on both simulated 2D images and\nreal 3D brain magnetic resonance (MR) images. Experimental results show that\nour model substantially improves the image classification accuracy with an\nadditional benefit of increased model interpretability. Our code is publicly\navailable at https://github.com/jw4hv/Geo-SIC",
    "descriptor": "\nComments: 10 pages, 6 figures, 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Jian Wang",
      "Miaomiao Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13704"
  },
  {
    "id": "arXiv:2210.13705",
    "title": "An Effective Deep Network for Head Pose Estimation without Keypoints",
    "abstract": "Human head pose estimation is an essential problem in facial analysis in\nrecent years that has a lot of computer vision applications such as gaze\nestimation, virtual reality, and driver assistance. Because of the importance\nof the head pose estimation problem, it is necessary to design a compact model\nto resolve this task in order to reduce the computational cost when deploying\non facial analysis-based applications such as large camera surveillance\nsystems, AI cameras while maintaining accuracy. In this work, we propose a\nlightweight model that effectively addresses the head pose estimation problem.\nOur approach has two main steps. 1) We first train many teacher models on the\nsynthesis dataset - 300W-LPA to get the head pose pseudo labels. 2) We design\nan architecture with the ResNet18 backbone and train our proposed model with\nthe ensemble of these pseudo labels via the knowledge distillation process. To\nevaluate the effectiveness of our model, we use AFLW-2000 and BIWI - two\nreal-world head pose datasets. Experimental results show that our proposed\nmodel significantly improves the accuracy in comparison with the\nstate-of-the-art head pose estimation methods. Furthermore, our model has the\nreal-time speed of $\\sim$300 FPS when inferring on Tesla V100.",
    "descriptor": "",
    "authors": [
      "Chien Thai",
      "Viet Tran",
      "Minh Bui",
      "Huong Ninh",
      "Hai Tran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13705"
  },
  {
    "id": "arXiv:2210.13708",
    "title": "MARLlib: Extending RLlib for Multi-agent Reinforcement Learning",
    "abstract": "Despite the fast development of multi-agent reinforcement learning (MARL)\nmethods, there is a lack of commonly-acknowledged baseline implementation and\nevaluation platforms. As a result, an urgent need for MARL researchers is to\ndevelop an integrated library suite, similar to the role of RLlib in\nsingle-agent RL, that delivers reliable MARL implementation and replicable\nevaluation in various benchmarks. To fill such a research gap, in this paper,\nwe propose Multi-Agent RLlib (MARLlib), a comprehensive MARL algorithm library\nthat facilitates RLlib for solving multi-agent problems. With a novel design of\nagent-level distributed dataflow, MARLlib manages to unify tens of algorithms,\nincluding different types of independent learning, centralized critic, and\nvalue decomposition methods; this leads to a highly composable integration of\nMARL algorithms that are not possible to unify before. Furthermore, MARLlib\ngoes beyond current work by integrating diverse environment interfaces and\nproviding flexible parameter sharing strategies; this allows to create\nversatile solutions to cooperative, competitive, and mixed tasks with minimal\ncode modifications for end users. A plethora of experiments are conducted to\nsubstantiate the correctness of our implementation, based on which we further\nderive new insights on the relationship between the performance and the design\nof algorithmic components. With MARLlib, we expect researchers to be able to\ntackle broader real-world multi-agent problems with trustworthy solutions. Our\ncode\\footnote{\\url{https://github.com/Replicable-MARL/MARLlib}} and\ndocumentation\\footnote{\\url{https://marllib.readthedocs.io/}} are released for\nreference.",
    "descriptor": "",
    "authors": [
      "Siyi Hu",
      "Yifan Zhong",
      "Minquan Gao",
      "Weixun Wang",
      "Hao Dong",
      "Zhihui Li",
      "Xiaodan Liang",
      "Xiaojun Chang",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.13708"
  },
  {
    "id": "arXiv:2210.13709",
    "title": "InForecaster: Forecasting Influenza Hemagglutinin Mutations Through the  Lens of Anomaly Detection",
    "abstract": "The influenza virus hemagglutinin is an important part of the virus\nattachment to the host cells. The hemagglutinin proteins are one of the genetic\nregions of the virus with a high potential for mutations. Due to the importance\nof predicting mutations in producing effective and low-cost vaccines, solutions\nthat attempt to approach this problem have recently gained a significant\nattention. A historical record of mutations have been used to train predictive\nmodels in such solutions. However, the imbalance between mutations and the\npreserved proteins is a big challenge for the development of such models that\nneeds to be addressed. Here, we propose to tackle this challenge through\nanomaly detection (AD). AD is a well-established field in Machine Learning (ML)\nthat tries to distinguish unseen anomalies from the normal patterns using only\nnormal training samples. By considering mutations as the anomalous behavior, we\ncould benefit existing rich solutions in this field that have emerged recently.\nSuch methods also fit the problem setup of extreme imbalance between the number\nof unmutated vs. mutated training samples. Motivated by this formulation, our\nmethod tries to find a compact representation for unmutated samples while\nforcing anomalies to be separated from the normal ones. This helps the model to\nlearn a shared unique representation between normal training samples as much as\npossible, which improves the discernibility and detectability of mutated\nsamples from the unmutated ones at the test time. We conduct a large number of\nexperiments on four publicly available datasets, consisting of 3 different\nhemagglutinin protein datasets, and one SARS-CoV-2 dataset, and show the\neffectiveness of our method through different standard criteria.",
    "descriptor": "",
    "authors": [
      "Ali Garjani",
      "Atoosa Malemir Chegini",
      "Mohammadreza Salehi",
      "Alireza Tabibzadeh",
      "Parastoo Yousefi",
      "Mohammad Hossein Razizadeh",
      "Moein Esghaei",
      "Maryam Esghaei",
      "Mohammad Hossein Rohban"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.13709"
  },
  {
    "id": "arXiv:2210.13710",
    "title": "Motif-Backdoor: Rethinking the Backdoor Attack on Graph Neural Networks  via Motifs",
    "abstract": "Graph neural network (GNN) with a powerful representation capability has been\nwidely applied to various areas, such as biological gene prediction, social\nrecommendation, etc. Recent works have exposed that GNN is vulnerable to the\nbackdoor attack, i.e., models trained with maliciously crafted training samples\nare easily fooled by patched samples. Most of the proposed studies launch the\nbackdoor attack using a trigger that either is the randomly generated subgraph\n(e.g., erd\\H{o}s-r\\'enyi backdoor) for less computational burden, or the\ngradient-based generative subgraph (e.g., graph trojaning attack) to enable a\nmore effective attack. However, the interpretation of how is the trigger\nstructure and the effect of the backdoor attack related has been overlooked in\nthe current literature. Motifs, recurrent and statistically significant\nsub-graphs in graphs, contain rich structure information. In this paper, we are\nrethinking the trigger from the perspective of motifs, and propose a\nmotif-based backdoor attack, denoted as Motif-Backdoor. It contributes from\nthree aspects. (i) Interpretation: it provides an in-depth explanation for\nbackdoor effectiveness by the validity of the trigger structure from motifs,\nleading to some novel insights, e.g., using subgraphs that appear less\nfrequently in the graph as the trigger can achieve better attack performance.\n(ii) Effectiveness: Motif-Backdoor reaches the state-of-the-art (SOTA) attack\nperformance in both black-box and defensive scenarios. (iii) Efficiency: based\non the graph motif distribution, Motif-Backdoor can quickly obtain an effective\ntrigger structure without target model feedback or subgraph model generation.\nExtensive experimental results show that Motif-Backdoor realizes the SOTA\nperformance on three popular models and four public datasets compared with five\nbaselines.",
    "descriptor": "",
    "authors": [
      "Haibin Zheng",
      "Haiyang Xiong",
      "Jinyin Chen",
      "Haonan Ma",
      "Guohan Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13710"
  },
  {
    "id": "arXiv:2210.13712",
    "title": "Parameter-Efficient Legal Domain Adaptation",
    "abstract": "Seeking legal advice is often expensive. Recent advancement in machine\nlearning for solving complex problems can be leveraged to help make legal\nservices more accessible to the public. However, real-life applications\nencounter significant challenges. State-of-the-art language models are growing\nincreasingly large, making parameter-efficient learning increasingly important.\nUnfortunately, parameter-efficient methods perform poorly with small amounts of\ndata, which are common in the legal domain (where data labelling costs are\nhigh). To address these challenges, we propose parameter-efficient legal domain\nadaptation, which uses vast unsupervised legal data from public legal forums to\nperform legal pre-training. This method exceeds or matches the fewshot\nperformance of existing models such as LEGAL-BERT on various legal tasks while\ntuning only approximately 0.1% of model parameters. Additionally, we show that\nour method can achieve calibration comparable to existing methods across\nseveral tasks. To the best of our knowledge, this work is among the first to\nexplore parameter-efficient methods of tuning language models toward the legal\ndomain.",
    "descriptor": "\nComments: Accepted into the 2022 NLLP workshop\n",
    "authors": [
      "Jonathan Li",
      "Rohan Bhambhoria",
      "Xiaodan Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13712"
  },
  {
    "id": "arXiv:2210.13715",
    "title": "PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph  Completion",
    "abstract": "This paper presents a parameter-lite transfer learning approach of pretrained\nlanguage models (LM) for knowledge graph (KG) completion. Instead of\nfinetuning, which modifies all LM parameters, we only tune a few new parameters\nwhile keeping the original LM parameters fixed. We establish this via\nreformulating KG completion as a \"fill-in-the-blank\" task, and introducing a\nparameter-lite encoder on top of the original LMs. We show that, by tuning far\nfewer parameters than finetuning, LMs transfer non-trivially to most tasks and\nreach competitiveness with prior state-of-the-art approaches. For instance, we\noutperform the fully finetuning approaches on a KG completion benchmark by\ntuning only 1% of the parameters. The code and datasets are available at\n\\url{https://github.com/yuanyehome/PALT}.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Jianhao Shen",
      "Chenguang Wang",
      "Ye Yuan",
      "Jiawei Han",
      "Heng Ji",
      "Koushik Sen",
      "Ming Zhang",
      "Dawn Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13715"
  },
  {
    "id": "arXiv:2210.13716",
    "title": "ASD: Towards Attribute Spatial Decomposition for Prior-Free Facial  Attribute Recognition",
    "abstract": "Representing the spatial properties of facial attributes is a vital challenge\nfor facial attribute recognition (FAR). Recent advances have achieved the\nreliable performances for FAR, benefiting from the description of spatial\nproperties via extra prior information. However, the extra prior information\nmight not be always available, resulting in the restricted application scenario\nof the prior-based methods. Meanwhile, the spatial ambiguity of facial\nattributes caused by inherent spatial diversities of facial parts is ignored.\nTo address these issues, we propose a prior-free method for attribute spatial\ndecomposition (ASD), mitigating the spatial ambiguity of facial attributes\nwithout any extra prior information. Specifically, assignment-embedding module\n(AEM) is proposed to enable the procedure of ASD, which consists of two\noperations: attribute-to-location assignment and location-to-attribute\nembedding. The attribute-to-location assignment first decomposes the feature\nmap based on latent factors, assigning the magnitude of attribute components on\neach spatial location. Then, the assigned attribute components from all\nlocations to represent the global-level attribute embeddings. Furthermore,\ncorrelation matrix minimization (CMM) is introduced to enlarge the\ndiscriminability of attribute embeddings. Experimental results demonstrate the\nsuperiority of ASD compared with state-of-the-art prior-based methods, while\nthe reliable performance of ASD for the case of limited training data is\nfurther validated.",
    "descriptor": "",
    "authors": [
      "Chuanfei Hu",
      "Hang Shao",
      "Bo Dong",
      "Zhe Wang",
      "Yongxiong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13716"
  },
  {
    "id": "arXiv:2210.13718",
    "title": "Facial Action Units Detection Aided by Global-Local Expression Embedding",
    "abstract": "Since Facial Action Unit (AU) annotations require domain expertise, common AU\ndatasets only contain a limited number of subjects. As a result, a crucial\nchallenge for AU detection is addressing identity overfitting. We find that AUs\nand facial expressions are highly associated, and existing facial expression\ndatasets often contain a large number of identities. In this paper, we aim to\nutilize the expression datasets without AU labels to facilitate AU detection.\nSpecifically, we develop a novel AU detection framework aided by the\nGlobal-Local facial Expressions Embedding, dubbed GLEE-Net. Our GLEE-Net\nconsists of three branches to extract identity-independent expression features\nfor AU detection. We introduce a global branch for modeling the overall facial\nexpression while eliminating the impacts of identities. We also design a local\nbranch focusing on specific local face regions. The combined output of global\nand local branches is firstly pre-trained on an expression dataset as an\nidentity-independent expression embedding, and then finetuned on AU datasets.\nTherefore, we significantly alleviate the issue of limited identities.\nFurthermore, we introduce a 3D global branch that extracts expression\ncoefficients through 3D face reconstruction to consolidate 2D AU descriptions.\nFinally, a Transformer-based multi-label classifier is employed to fuse all the\nrepresentations for AU detection. Extensive experiments demonstrate that our\nmethod significantly outperforms the state-of-the-art on the widely-used DISFA,\nBP4D and BP4D+ datasets.",
    "descriptor": "",
    "authors": [
      "Zhipeng Hu",
      "Wei Zhang",
      "Lincheng Li",
      "Yu Ding",
      "Wei Chen",
      "Zhigang Deng",
      "Xin Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13718"
  },
  {
    "id": "arXiv:2210.13722",
    "title": "ARENA: Towards Informative Alternative Query Plan Selection for Database  Education",
    "abstract": "A key learning goal of learners taking database systems course is to\nunderstand how SQL queries are processed in an RDBMS in practice. To this end,\ncomprehension of the cost-based comparison of different plan choices to select\nthe query execution plan (QEP) of a query is paramount. Unfortunately,\noff-the-shelf RDBMS typically only expose the selected QEP to users without\nrevealing information about representative alternative query plans considered\nduring QEP selection in a learner-friendly manner, hindering the learning\nprocess. In this paper, we present a novel end-to-end and generic framework\ncalled ARENA that facilitates exploration of informative alternative query\nplans of a given SQL query to aid the comprehension of QEP selection. Under the\nhood, ARENA addresses a novel problem called alternative plan selection problem\n(TIPS) which aims to discover a set of k alternative plans from the underlying\nplan space so that the plan interestingness of the set is maximized.\nSpecifically, we explore two variants of the problem, namely batch TIPS and\nincremental TIPS, to cater to diverse set of learners. Due to the computational\nhardness of the problem, we present a 2 approximation algorithm to address it\nefficiently. Exhaustive experimental study with real-world learners\ndemonstrates the effectiveness of arena in enhancing learners' understanding of\nthe alternative plan choices considered during QEP selection.",
    "descriptor": "",
    "authors": [
      "Hu Wang",
      "Hui Li",
      "Sourav S Bhowmick"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.13722"
  },
  {
    "id": "arXiv:2210.13723",
    "title": "S3E: A Large-scale Multimodal Dataset for Collaborative SLAM",
    "abstract": "With the advanced request to employ a team of robots to perform a task\ncollaboratively, the research community has become increasingly interested in\ncollaborative simultaneous localization and mapping. Unfortunately, existing\ndatasets are limited in the scale and variation of the collaborative\ntrajectories they capture, even though generalization between\ninter-trajectories among different agents is crucial to the overall viability\nof collaborative tasks. To help align the research community's contributions\nwith real-world multiagent ordinated SLAM problems, we introduce S3E, a novel\nlarge-scale multimodal dataset captured by a fleet of unmanned ground vehicles\nalong four designed collaborative trajectory paradigms. S3E consists of 7\noutdoor and 5 indoor scenes that each exceed 200 seconds, consisting of well\nsynchronized and calibrated high-quality stereo camera, LiDAR, and\nhigh-frequency IMU data. Crucially, our effort exceeds previous attempts\nregarding dataset size, scene variability, and complexity. It has 4x as much\naverage recording time as the pioneering EuRoC dataset. We also provide careful\ndataset analysis as well as baselines for collaborative SLAM and single\ncounterparts. Find data, code, and more up-to-date information at\nhttps://github.com/PengYu-Team/S3E.",
    "descriptor": "",
    "authors": [
      "Dapeng Feng",
      "Yuhua Qi",
      "Shipeng Zhong",
      "Zhiqiang Chen",
      "Yudu Jiao",
      "Qiming Chen",
      "Tao Jiang",
      "Hongbo Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13723"
  },
  {
    "id": "arXiv:2210.13728",
    "title": "Equivariant Filters are Equivariant",
    "abstract": "Observers for systems with Lie group symmetries are an active area of\nresearch that is seeing significant impact in a number of practical domains,\nincluding aerospace, robotics, and mechatronics. This paper builds on the\ntheory of the recently proposed Equivariant Filter (EqF), which is a general\nobserver design for systems on homogeneous spaces that takes advantage of\nsymmetries to yield significant performance advantages. It is shown that the\nEqF error dynamics are invariant to transformation of the input signal and\nequivariant as a parametrised vector field. The main theorem shows that two\nEqF's with different choices of local coordinates and origins and with\nequivalent noise modelling yield identical performance. In other words, the EqF\nis intrinsic to the system equations and symmetry. This is verified in a\nsimulation of a 2D robot localisation problem, which also shows how the ability\nto choose an origin for the EqF can yield practical performance advantages by\nmitigating floating point precision errors.",
    "descriptor": "\nComments: 11 pages, 2 figures, accepted for publication in IFAC NOLCOS 2022\n",
    "authors": [
      "Hiya Gada",
      "Pieter van Goor",
      "Ravi Banavar",
      "Robert Mahony"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13728"
  },
  {
    "id": "arXiv:2210.13729",
    "title": "Hybrid Reinforced Medical Report Generation with M-Linear Attention and  Repetition Penalty",
    "abstract": "To reduce doctors' workload, deep-learning-based automatic medical report\ngeneration has recently attracted more and more research efforts, where deep\nconvolutional neural networks (CNNs) are employed to encode the input images,\nand recurrent neural networks (RNNs) are used to decode the visual features\ninto medical reports automatically. However, these state-of-the-art methods\nmainly suffer from three shortcomings: (i) incomprehensive optimization, (ii)\nlow-order and unidimensional attention mechanisms, and (iii) repeated\ngeneration. In this article, we propose a hybrid reinforced medical report\ngeneration method with m-linear attention and repetition penalty mechanism\n(HReMRG-MR) to overcome these problems. Specifically, a hybrid reward with\ndifferent weights is employed to remedy the limitations of single-metric-based\nrewards. We also propose a search algorithm with linear complexity to\napproximate the best weight combination. Furthermore, we use m-linear attention\nmodules to explore high-order feature interactions and to achieve multi-modal\nreasoning, while a repetition penalty applies penalties to repeated terms\nduring the model's training process. Extensive experimental studies on two\npublic datasets show that HReMRG-MR greatly outperforms the state-of-the-art\nbaselines in terms of all metrics. We also conducted a series of ablation\nexperiments to prove the effectiveness of all our proposed components. We also\nperformed a reward search toy experiment to give evidence that our proposed\nsearch approach can significantly reduce the search time while approximating\nthe best performance.",
    "descriptor": "\nComments: This paper is current under peer-review in IEEE TNNLS\n",
    "authors": [
      "Wenting Xu",
      "Zhenghua Xu",
      "Junyang Chen",
      "Chang Qi",
      "Thomas Lukasiewicz"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13729"
  },
  {
    "id": "arXiv:2210.13732",
    "title": "Evaluating and Optimizing Hearing-Aid Self-Fitting Methods using  Population Coverage",
    "abstract": "Adults with mild-to-moderate hearing loss can use over-the-counter hearing\naids to treat their hearing loss at a fraction of traditional hearing care\ncosts. These products incorporate self-fitting methods that allow end-users to\nconfigure their hearing aids without the help of an audiologist. A self-fitting\nmethod helps users configure the gain-frequency responses that control the\namplification for each frequency band of the incoming sound. This paper\nconsiders how to design effective self-fitting methods and whether we may\nevaluate certain aspects of their design without resorting to expensive user\nstudies. Most existing fitting methods provide various user interfaces to allow\nusers to select a configuration from a predetermined set of presets. We propose\na novel metric for evaluating the performance of preset-based approaches by\ncomputing their population coverage. The population coverage estimates the\nfraction of users for which it is possible to find a configuration they prefer.\nA unique aspect of our approach is a probabilistic model that captures how a\nuser's unique preferences differ from other users with similar hearing loss.\nNext, we develop methods for determining presets to maximize population\ncoverage. Exploratory results demonstrate that the proposed algorithms can\neffectively select a small number of presets that provide higher population\ncoverage than clustering-based approaches. Moreover, we may use our algorithms\nto configure the number of increments for slider-based methods.",
    "descriptor": "",
    "authors": [
      "Dhruv Vyas",
      "Erik Jorgensen",
      "Yu-Hsiang Wu",
      "Octav Chipara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13732"
  },
  {
    "id": "arXiv:2210.13733",
    "title": "Better Few-Shot Relation Extraction with Label Prompt Dropout",
    "abstract": "Few-shot relation extraction aims to learn to identify the relation between\ntwo entities based on very limited training examples. Recent efforts found that\ntextual labels (i.e., relation names and relation descriptions) could be\nextremely useful for learning class representations, which will benefit the\nfew-shot learning task. However, what is the best way to leverage such label\ninformation in the learning process is an important research question. Existing\nworks largely assume such textual labels are always present during both\nlearning and prediction. In this work, we argue that such approaches may not\nalways lead to optimal results. Instead, we present a novel approach called\nlabel prompt dropout, which randomly removes label descriptions in the learning\nprocess. Our experiments show that our approach is able to lead to improved\nclass representations, yielding significantly better results on the few-shot\nrelation extraction task.",
    "descriptor": "\nComments: Accepted by EMNLP 2022 (long paper)\n",
    "authors": [
      "Peiyuan Zhang",
      "Wei Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13733"
  },
  {
    "id": "arXiv:2210.13734",
    "title": "Kurdish Handwritten Character Recognition using Deep Learning Techniques",
    "abstract": "Handwriting recognition is one of the active and challenging areas of\nresearch in the field of image processing and pattern recognition. It has many\napplications that include: a reading aid for visual impairment, automated\nreading and processing for bank checks, making any handwritten document\nsearchable, and converting them into structural text form, etc. Moreover, high\naccuracy rates have been recorded by handwriting recognition systems for\nEnglish, Chinese Arabic, Persian, and many other languages. Yet there is no\nsuch system available for offline Kurdish handwriting recognition. In this\npaper, an attempt is made to design and develop a model that can recognize\nhandwritten characters for Kurdish alphabets using deep learning techniques.\nKurdish (Sorani) contains 34 characters and mainly employs an Arabic\\Persian\nbased script with modified alphabets. In this work, a Deep Convolutional Neural\nNetwork model is employed that has shown exemplary performance in handwriting\nrecognition systems. Then, a comprehensive dataset was created for handwritten\nKurdish characters, which contains more than 40 thousand images. The created\ndataset has been used for training the Deep Convolutional Neural Network model\nfor classification and recognition tasks. In the proposed system, the\nexperimental results show an acceptable recognition level. The testing results\nreported a 96% accuracy rate, and training accuracy reported a 97% accuracy\nrate. From the experimental results, it is clear that the proposed deep\nlearning model is performing well and is comparable to the similar model of\nother languages' handwriting recognition systems.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Rebin M. Ahmed",
      "Tarik A. Rashid",
      "Polla Fattah",
      "Abeer Alsadoon",
      "Nebojsa Bacanin",
      "Seyedali Mirjalili",
      "S.Vimal",
      "Amit Chhabra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.13734"
  },
  {
    "id": "arXiv:2210.13738",
    "title": "Pruning's Effect on Generalization Through the Lens of Training and  Regularization",
    "abstract": "Practitioners frequently observe that pruning improves model generalization.\nA long-standing hypothesis based on bias-variance trade-off attributes this\ngeneralization improvement to model size reduction. However, recent studies on\nover-parameterization characterize a new model size regime, in which larger\nmodels achieve better generalization. Pruning models in this over-parameterized\nregime leads to a contradiction -- while theory predicts that reducing model\nsize harms generalization, pruning to a range of sparsities nonetheless\nimproves it. Motivated by this contradiction, we re-examine pruning's effect on\ngeneralization empirically.\nWe show that size reduction cannot fully account for the\ngeneralization-improving effect of standard pruning algorithms. Instead, we\nfind that pruning leads to better training at specific sparsities, improving\nthe training loss over the dense model. We find that pruning also leads to\nadditional regularization at other sparsities, reducing the accuracy\ndegradation due to noisy examples over the dense model. Pruning extends model\ntraining time and reduces model size. These two factors improve training and\nadd regularization respectively. We empirically demonstrate that both factors\nare essential to fully explaining pruning's impact on generalization.",
    "descriptor": "\nComments: 49 pages, 20 figures\n",
    "authors": [
      "Tian Jin",
      "Michael Carbin",
      "Daniel M. Roy",
      "Jonathan Frankle",
      "Gintare Karolina Dziugaite"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13738"
  },
  {
    "id": "arXiv:2210.13739",
    "title": "Deterministic Small Vertex Connectivity in Almost Linear Time",
    "abstract": "In the vertex connectivity problem, given an undirected $n$-vertex $m$-edge\ngraph $G$, we need to compute the minimum number of vertices that can\ndisconnect $G$ after removing them. This problem is one of the most\nwell-studied graph problems. From 2019, a new line of work [Nanongkai et\nal.~STOC'19;SODA'20;STOC'21] has used randomized techniques to break the\nquadratic-time barrier and, very recently, culminated in an almost-linear time\nalgorithm via the recently announced maxflow algorithm by Chen et al. In\ncontrast, all known deterministic algorithms are much slower. The fastest\nalgorithm [Gabow FOCS'00] takes $O(m(n+\\min\\{c^{5/2},cn^{3/4}\\}))$ time where\n$c$ is the vertex connectivity. It remains open whether there exists a\nsubquadratic-time deterministic algorithm for any constant $c>3$.\nIn this paper, we give the first deterministic almost-linear time vertex\nconnectivity algorithm for all constants $c$. Our running time is\n$m^{1+o(1)}2^{O(c^{2})}$ time, which is almost-linear for all $c=o(\\sqrt{\\log\nn})$. This is the first deterministic algorithm that breaks the $O(n^{2})$-time\nbound on sparse graphs where $m=O(n)$, which is known for more than 50 years\nago [Kleitman'69]. Towards our result, we give a new reduction framework to\nvertex expanders which in turn exploits our new almost-linear time construction\nof mimicking network for vertex connectivity. The previous construction by\nKratsch and Wahlstr\\\"{o}m [FOCS'12] requires large polynomial time and is\nrandomized.",
    "descriptor": "",
    "authors": [
      "Thatchaphol Saranurak",
      "Sorrachai Yingchareonthawornchai"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13739"
  },
  {
    "id": "arXiv:2210.13740",
    "title": "Latency Aware Multi-Path Data Transmission for URLLC Services",
    "abstract": "5th Generation Mobile Communication Technology (5G) utilizes Access Traffic\nSteering, Switching, and Splitting (ATSSS) rule to enable multi-path data\ntransmission, which is currently being standardized. Recently, the 3rd\nGeneration Partnership Project (3GPP) SA1 and SA2 have been working on\nmulti-path solution for possible improvement from different perspectives.\nHowever, the existing 3GPP multi-path solution has some limitations on URLLC\ntraffic in terms of reliability and latency requirements. In order to capture\nthe potential gains of multi-path architecture in the context of URLLC\nservices, this paper proposes a new traffic splitting technique which can more\nefficiently enjoy the benefit of multi-path architecture in reducing users'\nuplink (UL) End-to-End (E2E) latency. In particular, we formulate an\noptimization framework which minimizes the UL E2E latency of users via\noptimizing the ratio of traffic assigned to each path and corresponding\ntransmit power. The performance of the proposed scheme is evaluated via well\ndesigned simulations.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. 6 pages, 6 figures\n",
    "authors": [
      "Liu Cao",
      "Abbas Kiani",
      "Amanda Xiang",
      "Kaippallimalil John",
      "Tony Saboorian"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13740"
  },
  {
    "id": "arXiv:2210.13743",
    "title": "Online Cross-Layer Knowledge Distillation on Graph Neural Networks with  Deep Supervision",
    "abstract": "Graph neural networks (GNNs) have become one of the most popular research\ntopics in both academia and industry communities for their strong ability in\nhandling irregular graph data. However, large-scale datasets are posing great\nchallenges for deploying GNNs in edge devices with limited resources and model\ncompression techniques have drawn considerable research attention. Existing\nmodel compression techniques such as knowledge distillation (KD) mainly focus\non convolutional neural networks (CNNs). Only limited attempts have been made\nrecently for distilling knowledge from GNNs in an offline manner. As the\nperformance of the teacher model does not necessarily improve as the number of\nlayers increases in GNNs, selecting an appropriate teacher model will require\nsubstantial efforts. To address these challenges, we propose a novel online\nknowledge distillation framework called Alignahead++ in this paper.\nAlignahead++ transfers structure and feature information in a student layer to\nthe previous layer of another simultaneously trained student model in an\nalternating training procedure. Meanwhile, to avoid over-smoothing problem in\nGNNs, deep supervision is employed in Alignahead++ by adding an auxiliary\nclassifier in each intermediate layer to prevent the collapse of the node\nfeature embeddings. Experimental results on four datasets including PPI, Cora,\nPubMed and CiteSeer demonstrate that the student performance is consistently\nboosted in our collaborative training framework without the supervision of a\npre-trained teacher model and its effectiveness can generally be improved by\nincreasing the number of students.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.02468\n",
    "authors": [
      "Jiongyu Guo",
      "Defang Chen",
      "Can Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13743"
  },
  {
    "id": "arXiv:2210.13746",
    "title": "DEMETR: Diagnosing Evaluation Metrics for Translation",
    "abstract": "While machine translation evaluation metrics based on string overlap (e.g.,\nBLEU) have their limitations, their computations are transparent: the BLEU\nscore assigned to a particular candidate translation can be traced back to the\npresence or absence of certain words. The operations of newer learned metrics\n(e.g., BLEURT, COMET), which leverage pretrained language models to achieve\nhigher correlations with human quality judgments than BLEU, are opaque in\ncomparison. In this paper, we shed light on the behavior of these learned\nmetrics by creating DEMETR, a diagnostic dataset with 31K English examples\n(translated from 10 source languages) for evaluating the sensitivity of MT\nevaluation metrics to 35 different linguistic perturbations spanning semantic,\nsyntactic, and morphological error categories. All perturbations were carefully\ndesigned to form minimal pairs with the actual translation (i.e., differ in\nonly one aspect). We find that learned metrics perform substantially better\nthan string-based metrics on DEMETR. Additionally, learned metrics differ in\ntheir sensitivity to various phenomena (e.g., BERTScore is sensitive to\nuntranslated words but relatively insensitive to gender manipulation, while\nCOMET is much more sensitive to word repetition than to aspectual changes). We\npublicly release DEMETR to spur more informed future development of machine\ntranslation evaluation metrics",
    "descriptor": "\nComments: 22 pages, EMNLP 2022 (camera ready)\n",
    "authors": [
      "Marzena Karpinska",
      "Nishant Raj",
      "Katherine Thai",
      "Yixiao Song",
      "Ankita Gupta",
      "Mohit Iyyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13746"
  },
  {
    "id": "arXiv:2210.13749",
    "title": "AugCSE: Contrastive Sentence Embedding with Diverse Augmentations",
    "abstract": "Data augmentation techniques have been proven useful in many applications in\nNLP fields. Most augmentations are task-specific, and cannot be used as a\ngeneral-purpose tool. In our work, we present AugCSE, a unified framework to\nutilize diverse sets of data augmentations to achieve a better, general\npurpose, sentence embedding model. Building upon the latest sentence embedding\nmodels, our approach uses a simple antagonistic discriminator that\ndifferentiates the augmentation types. With the finetuning objective borrowed\nfrom domain adaptation, we show that diverse augmentations, which often lead to\nconflicting contrastive signals, can be tamed to produce a better and more\nrobust sentence representation. Our methods achieve state-of-the-art results on\ndownstream transfer tasks and perform competitively on semantic textual\nsimilarity tasks, using only unsupervised data.",
    "descriptor": "\nComments: AACL 2022, 9 pages, Long paper, oral. arXiv admin note: text overlap with arXiv:2112.02721\n",
    "authors": [
      "Zilu Tang",
      "Muhammed Yusuf Kocyigit",
      "Derry Wijaya"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13749"
  },
  {
    "id": "arXiv:2210.13752",
    "title": "Aboveground carbon biomass estimate with Physics-informed deep network",
    "abstract": "The global carbon cycle is a key process to understand how our climate is\nchanging. However, monitoring the dynamics is difficult because a\nhigh-resolution robust measurement of key state parameters including the\naboveground carbon biomass (AGB) is required. Here, we use deep neural network\nto generate a wall-to-wall map of AGB within the Continental USA (CONUS) with\n30-meter spatial resolution for the year 2021. We combine radar and optical\nhyperspectral imagery, with a physical climate parameter of SIF-based GPP.\nValidation results show that a masked variation of UNet has the lowest\nvalidation RMSE of 37.93 $\\pm$ 1.36 Mg C/ha, as compared to 52.30 $\\pm$ 0.03 Mg\nC/ha for random forest algorithm. Furthermore, models that learn from SIF-based\nGPP in addition to radar and optical imagery reduce validation RMSE by almost\n10% and the standard deviation by 40%. Finally, we apply our model to measure\nlosses in AGB from the recent 2021 Caldor wildfire in California, and validate\nour analysis with Sentinel-based burn index.",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Juan Nathaniel",
      "Levente J. Klein",
      "Campbell D. Watson",
      "Gabrielle Nyirjesy",
      "Conrad M. Albrecht"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13752"
  },
  {
    "id": "arXiv:2210.13755",
    "title": "Online and Bandit Algorithms Beyond $\\ell_p$ Norms",
    "abstract": "Vector norms play a fundamental role in computer science and optimization, so\nthere is an ongoing effort to generalize existing algorithms to settings beyond\n$\\ell_\\infty$ and $\\ell_p$ norms. We show that many online and bandit\napplications for general norms admit good algorithms as long as the norm can be\napproximated by a function that is ``gradient-stable'', a notion that we\nintroduce. Roughly it says that the gradient of the function should not\ndrastically decrease (multiplicatively) in any component as we increase the\ninput vector. We prove that several families of norms, including all monotone\nsymmetric norms, admit a gradient-stable approximation, giving us the first\nonline and bandit algorithms for these norm families.\nIn particular, our notion of gradient-stability gives $O\\big(\\log^2\n(\\text{dimension})\\big)$-competitive algorithms for the symmetric norm\ngeneralizations of Online Generalized Load Balancing and Bandits with\nKnapsacks. Our techniques extend to applications beyond symmetric norms as\nwell, e.g., to Online Vector Scheduling and to Online Generalized Assignment\nwith Convex Costs. Some key properties underlying our applications that are\nimplied by gradient-stable approximations are a ``smooth game inequality'' and\nan approximate converse to Jensen's inequality.",
    "descriptor": "\nComments: Appears in SODA 2023\n",
    "authors": [
      "Thomas Kesselheim",
      "Marco Molinaro",
      "Sahil Singla"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13755"
  },
  {
    "id": "arXiv:2210.13757",
    "title": "Workload Similarity Analysis using Machine Learning Techniques",
    "abstract": "Finding the similarity between two workload behaviors is helpful in 1.\ncreating proxy workloads 2. characterizing an unknown workload's behavior by\nmatching its behavior against known workloads. In this article, we propose a\nmethod to measure the similarity between two workloads using machine\nlearning-based analysis of the performance telemetry data collected for the\nexecution runs of the two workloads. We also demonstrate the accuracy of the\ntechnique by measuring the similarity between a variety of know benchmark\nworkloads.",
    "descriptor": "",
    "authors": [
      "Deepak Mishra",
      "Vineet Singh",
      "Ashish Ledalla"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2210.13757"
  },
  {
    "id": "arXiv:2210.13762",
    "title": "Towards Robust Recommender Systems via Triple Cooperative Defense",
    "abstract": "Recommender systems are often susceptible to well-crafted fake profiles,\nleading to biased recommendations. The wide application of recommender systems\nmakes studying the defense against attack necessary. Among existing defense\nmethods, data-processing-based methods inevitably exclude normal samples, while\nmodel-based methods struggle to enjoy both generalization and robustness.\nConsidering the above limitations, we suggest integrating data processing and\nrobust model and propose a general framework, Triple Cooperative Defense (TCD),\nwhich cooperates to improve model robustness through the co-training of three\nmodels. Specifically, in each round of training, we sequentially use the\nhigh-confidence prediction ratings (consistent ratings) of any two models as\nauxiliary training data for the remaining model, and the three models\ncooperatively improve recommendation robustness. Notably, TCD adds pseudo label\ndata instead of deleting abnormal data, which avoids the cleaning of normal\ndata, and the cooperative training of the three models is also beneficial to\nmodel generalization. Through extensive experiments with five poisoning attacks\non three real-world datasets, the results show that the robustness improvement\nof TCD significantly outperforms baselines. It is worth mentioning that TCD is\nalso beneficial for model generalizations.",
    "descriptor": "\nComments: 15 pages, 4 figures, 5 tables\n",
    "authors": [
      "Qingyang Wang",
      "Defu Lian",
      "Chenwang Wu",
      "Enhong Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13762"
  },
  {
    "id": "arXiv:2210.13763",
    "title": "Teal: Learning-Accelerated Optimization of Traffic Engineering",
    "abstract": "In the last decade, global cloud wide-area networks (WANs) have grown\n10$\\times$ in size due to the deployment of new network sites and datacenters,\nmaking it challenging for commercial optimization engines to solve the network\ntraffic engineering (TE) problem within the temporal budget of a few minutes.\nIn this work, we show that carefully designed deep learning models are key to\naccelerating the running time of intra-WAN TE systems for large deployments\nsince deep learning is both massively parallel and it benefits from the wealth\nof historical traffic allocation data from production WANs. However,\noff-the-shelf deep learning methods fail to perform well on the TE task since\nthey ignore the effects of network connectivity on flow allocations. They are\nalso faced with a tractability challenge posed by the large problem scale of TE\noptimization. Moreover, neural networks do not have mechanisms to readily\nenforce hard constraints on model outputs (e.g., link capacity constraints). We\ntackle these challenges by designing a deep learning-based TE system -- Teal.\nFirst, Teal leverages graph neural networks (GNN) to faithfully capture\nconnectivity and model network flows. Second, Teal devises a multi-agent\nreinforcement learning (RL) algorithm to process individual demands\nindependently in parallel to lower the problem scale. Finally, Teal reduces\nlink capacity violations and improves solution quality using the alternating\ndirection method of multipliers (ADMM). We evaluate Teal on traffic matrices of\na global commercial cloud provider and find that Teal computes near-optimal\ntraffic allocations with a 59$\\times$ speedup over state-of-the-art TE systems\non a WAN topology of over 1,500 nodes.",
    "descriptor": "",
    "authors": [
      "Zhiying Xu",
      "Francis Y. Yan",
      "Rachee Singh",
      "Justin T. Chiu",
      "Alexander M. Rush",
      "Minlan Yu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13763"
  },
  {
    "id": "arXiv:2210.13765",
    "title": "Exact domain truncation for the Morse-Ingard equations",
    "abstract": "Morse and Ingard give a coupled system of time-harmonic equations for the\ntemperature and pressure of an excited gas. These equations form a critical\naspect of modeling trace gas sensors. Like other wave propagation problems, the\ncomputational problem must be closed with suitable far-field boundary\nconditions. Working in a scattered-field formulation, we adapt a nonlocal\nboundary condition proposed earlier for the Helmholtz equation to this coupled\nsystem. This boundary condition uses a Green's formula for the true solution on\nthe boundary, giving rise to a nonlocal perturbation of standard transmission\nboundary conditions. However, the boundary condition is exact and so Galerkin\ndiscretization of the resulting problem converges to the restriction of the\nexact solution to the computational domain. Numerical results demonstrate that\naccuracy can be obtained on relatively coarse meshes on small computational\ndomains, and the resulting algebraic systems may be solved by GMRES using the\nlocal part of the operator as an effective preconditioner.",
    "descriptor": "",
    "authors": [
      "Robert C. Kirby",
      "Xiaoyu Wei",
      "Andreas Kloeckner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13765"
  },
  {
    "id": "arXiv:2210.13766",
    "title": "Optimizing the Homogeneity and Efficiency of an SOEC Based on  Multiphysics Simulation and Data-driven Surrogate Model",
    "abstract": "Inhomogeneous current and temperature distributions are harmful to the\ndurability of the solid oxide electrolysis cell (SOEC). Segmented SOEC\nexperiments reveal that a high steam utilization, which is favorable for system\nefficiency, leads to local steam starvation and enhanced the inhomogeneity. It\nis necessary to consider inhomogeneity and efficiency jointly in optimization\nstudies. Three-dimensional (3D) multiphysics models validated with experiments\ncan simulate the inhomogeneity in a reliable manner, but they are unsuitable\nfor optimization due to the high computational cost. This study proposes a\nmethod that combines segmented SOEC experiments, multiphysics simulation, and\nartificial intelligence to optimize the inhomogeneity and efficiency of SOEC\njointly. A 3D cell model is first built and verified by segmented SOEC\nexperiments. Then, fast neural network surrogate models are built from the\nsimulation data and integrated into a multi-objective optimization problem. Its\nsolutions form a Pareto front reflecting the conflicting relationships among\ndifferent objectives. It is found that the down-stream current is 60%-65% of\nthe up-stream current when the steam utilization is 0.7. To increase the steam\nutilization to 0.8, the down-stream current will further drop to 50%-60% of the\nup-stream current. The Pareto fronts enable system operators to achieve a\nbalance between efficiency and inhomogeneity.",
    "descriptor": "",
    "authors": [
      "Yingtian Chi",
      "Kentaro Yokoo",
      "Hironori Nakajima",
      "Kohei Ito",
      "Jin Lin",
      "Yonghua Song"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13766"
  },
  {
    "id": "arXiv:2210.13768",
    "title": "GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural  Networks",
    "abstract": "Spiking Neural Networks (SNNs) have been studied over decades to incorporate\ntheir biological plausibility and leverage their promising energy efficiency.\nThroughout existing SNNs, the leaky integrate-and-fire (LIF) model is commonly\nadopted to formulate the spiking neuron and evolves into numerous variants with\ndifferent biological features. However, most LIF-based neurons support only\nsingle biological feature in different neuronal behaviors, limiting their\nexpressiveness and neuronal dynamic diversity. In this paper, we propose GLIF,\na unified spiking neuron, to fuse different bio-features in different neuronal\nbehaviors, enlarging the representation space of spiking neurons. In GLIF,\ngating factors, which are exploited to determine the proportion of the fused\nbio-features, are learnable during training. Combining all learnable\nmembrane-related parameters, our method can make spiking neurons different and\nconstantly changing, thus increasing the heterogeneity and adaptivity of\nspiking neurons. Extensive experiments on a variety of datasets demonstrate\nthat our method obtains superior performance compared with other SNNs by simply\nchanging their neuronal formulations to GLIF. In particular, we train a spiking\nResNet-19 with GLIF and achieve $77.35\\%$ top-1 accuracy with six time steps on\nCIFAR-100, which has advanced the state-of-the-art. Codes are available at\n\\url{https://github.com/Ikarosy/Gated-LIF}.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Xingting Yao",
      "Fanrong Li",
      "Zitao Mo",
      "Jian Cheng"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13768"
  },
  {
    "id": "arXiv:2210.13769",
    "title": "GlobalFlowNet: Video Stabilization using Deep Distilled Global Motion  Estimates",
    "abstract": "Videos shot by laymen using hand-held cameras contain undesirable shaky\nmotion. Estimating the global motion between successive frames, in a manner not\ninfluenced by moving objects, is central to many video stabilization\ntechniques, but poses significant challenges. A large body of work uses 2D\naffine transformations or homography for the global motion. However, in this\nwork, we introduce a more general representation scheme, which adapts any\nexisting optical flow network to ignore the moving objects and obtain a\nspatially smooth approximation of the global motion between video frames. We\nachieve this by a knowledge distillation approach, where we first introduce a\nlow pass filter module into the optical flow network to constrain the predicted\noptical flow to be spatially smooth. This becomes our student network, named as\n\\textsc{GlobalFlowNet}. Then, using the original optical flow network as the\nteacher network, we train the student network using a robust loss function.\nGiven a trained \\textsc{GlobalFlowNet}, we stabilize videos using a two stage\nprocess. In the first stage, we correct the instability in affine parameters\nusing a quadratic programming approach constrained by a user-specified cropping\nlimit to control loss of field of view. In the second stage, we stabilize the\nvideo further by smoothing global motion parameters, expressed using a small\nnumber of discrete cosine transform coefficients. In extensive experiments on a\nvariety of different videos, our technique outperforms state of the art\ntechniques in terms of subjective quality and different quantitative measures\nof video stability. The source code is publicly available at\n\\href{https://github.com/GlobalFlowNet/GlobalFlowNet}{https://github.com/GlobalFlowNet/GlobalFlowNet}",
    "descriptor": "",
    "authors": [
      "Jerin Geo James",
      "Devansh Jain",
      "Ajit Rajwade"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13769"
  },
  {
    "id": "arXiv:2210.13770",
    "title": "The Virality of Hate Speech on Social Media",
    "abstract": "Online hate speech is responsible for violent attacks such as, e.g., the\nPittsburgh synagogue shooting in 2018, thereby posing a significant threat to\nvulnerable groups and society in general. However, little is known about what\nmakes hate speech on social media go viral. In this paper, we collected N =\n25,219 Twitter cascades with 65,946 retweets and classify them as hateful vs.\nnormal. Using a generalized linear regression, we then estimate differences in\nthe spread of hateful vs. normal content based on author and content variables.\nWe thereby identify important determinants that explain differences in the\nspreading of hateful vs. normal content. For example, hateful content authored\nby verified users is disproportionally more likely to go viral than hateful\ncontent from non-verified ones: hateful content from a verified user (as\nopposed to normal content) has a 3.5 times larger cascade size, a 3.2 times\nlonger cascade lifetime, and a 1.2 times larger structural virality.\nAltogether, we offer novel insights into the virality of hate speech on social\nmedia.",
    "descriptor": "",
    "authors": [
      "Abdurahman Maarouf",
      "Nicolas Pr\u00f6llochs",
      "Stefan Feuerriegel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13770"
  },
  {
    "id": "arXiv:2210.13774",
    "title": "From Points to Functions: Infinite-dimensional Representations in  Diffusion Models",
    "abstract": "Diffusion-based generative models learn to iteratively transfer unstructured\nnoise to a complex target distribution as opposed to Generative Adversarial\nNetworks (GANs) or the decoder of Variational Autoencoders (VAEs) which produce\nsamples from the target distribution in a single step. Thus, in diffusion\nmodels every sample is naturally connected to a random trajectory which is a\nsolution to a learned stochastic differential equation (SDE). Generative models\nare only concerned with the final state of this trajectory that delivers\nsamples from the desired distribution. Abstreiter et. al showed that these\nstochastic trajectories can be seen as continuous filters that wash out\ninformation along the way. Consequently, it is reasonable to ask if there is an\nintermediate time step at which the preserved information is optimal for a\ngiven downstream task. In this work, we show that a combination of information\ncontent from different time steps gives a strictly better representation for\nthe downstream task. We introduce an attention and recurrence based modules\nthat ``learn to mix'' information content of various time-steps such that the\nresultant representation leads to superior performance in downstream tasks.",
    "descriptor": "",
    "authors": [
      "Sarthak Mittal",
      "Guillaume Lajoie",
      "Stefan Bauer",
      "Arash Mehrjou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13774"
  },
  {
    "id": "arXiv:2210.13777",
    "title": "SciFact-Open: Towards open-domain scientific claim verification",
    "abstract": "While research on scientific claim verification has led to the development of\npowerful systems that appear to approach human performance, these approaches\nhave yet to be tested in a realistic setting against large corpora of\nscientific literature. Moving to this open-domain evaluation setting, however,\nposes unique challenges; in particular, it is infeasible to exhaustively\nannotate all evidence documents. In this work, we present SciFact-Open, a new\ntest collection designed to evaluate the performance of scientific claim\nverification systems on a corpus of 500K research abstracts. Drawing upon\npooling techniques from information retrieval, we collect evidence for\nscientific claims by pooling and annotating the top predictions of four\nstate-of-the-art scientific claim verification models. We find that systems\ndeveloped on smaller corpora struggle to generalize to SciFact-Open, exhibiting\nperformance drops of at least 15 F1. In addition, analysis of the evidence in\nSciFact-Open reveals interesting phenomena likely to appear when claim\nverification systems are deployed in practice, e.g., cases where the evidence\nsupports only a special case of the claim. Our dataset is available at\nhttps://github.com/dwadden/scifact-open.",
    "descriptor": "\nComments: EMNLP Findings 2022. GitHub: this https URL\n",
    "authors": [
      "David Wadden",
      "Kyle Lo",
      "Bailey Kuehl",
      "Arman Cohan",
      "Iz Beltagy",
      "Lucy Lu Wang",
      "Hannaneh Hajishirzi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13777"
  },
  {
    "id": "arXiv:2210.13778",
    "title": "IDK-MRC: Unanswerable Questions for Indonesian Machine Reading  Comprehension",
    "abstract": "Machine Reading Comprehension (MRC) has become one of the essential tasks in\nNatural Language Understanding (NLU) as it is often included in several NLU\nbenchmarks (Liang et al., 2020; Wilie et al., 2020). However, most MRC datasets\nonly have answerable question type, overlooking the importance of unanswerable\nquestions. MRC models trained only on answerable questions will select the span\nthat is most likely to be the answer, even when the answer does not actually\nexist in the given passage (Rajpurkar et al., 2018). This problem especially\nremains in medium- to low-resource languages like Indonesian. Existing\nIndonesian MRC datasets (Purwarianti et al., 2007; Clark et al., 2020) are\nstill inadequate because of the small size and limited question types, i.e.,\nthey only cover answerable questions. To fill this gap, we build a new\nIndonesian MRC dataset called I(n)don'tKnow- MRC (IDK-MRC) by combining the\nautomatic and manual unanswerable question generation to minimize the cost of\nmanual dataset construction while maintaining the dataset quality. Combined\nwith the existing answerable questions, IDK-MRC consists of more than 10K\nquestions in total. Our analysis shows that our dataset significantly improves\nthe performance of Indonesian MRC models, showing a large improvement for\nunanswerable questions.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Rifki Afina Putri",
      "Alice Oh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13778"
  },
  {
    "id": "arXiv:2210.13782",
    "title": "Towards Trustworthy Multi-label Sewer Defect Classification via  Evidential Deep Learning",
    "abstract": "An automatic vision-based sewer inspection plays a key role of sewage system\nin a modern city. Recent advances focus on utilizing deep learning model to\nrealize the sewer inspection system, benefiting from the capability of\ndata-driven feature representation. However, the inherent uncertainty of sewer\ndefects is ignored, resulting in the missed detection of serious unknown sewer\ndefect categories. In this paper, we propose a trustworthy multi-label sewer\ndefect classification (TMSDC) method, which can quantify the uncertainty of\nsewer defect prediction via evidential deep learning. Meanwhile, a novel expert\nbase rate assignment (EBRA) is proposed to introduce the expert knowledge for\ndescribing reliable evidences in practical situations. Experimental results\ndemonstrate the effectiveness of TMSDC and the superior capability of\nuncertainty estimation is achieved on the latest public benchmark.",
    "descriptor": "\nComments: Chenyang Zhao and Chuanfei Hu contributed equally to this work. Corresponding author: Chuanfei Hu\n",
    "authors": [
      "Chenyang Zhao",
      "Chuanfei Hu",
      "Hang Shao",
      "Zhe Wang",
      "Yongxiong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13782"
  },
  {
    "id": "arXiv:2210.13783",
    "title": "Topical Segmentation of Spoken Narratives: A Test Case on Holocaust  Survivor Testimonies",
    "abstract": "The task of topical segmentation is well studied, but previous work has\nmostly addressed it in the context of structured, well-defined segments, such\nas segmentation into paragraphs, chapters, or segmenting text that originated\nfrom multiple sources. We tackle the task of segmenting running (spoken)\nnarratives, which poses hitherto unaddressed challenges. As a test case, we\naddress Holocaust survivor testimonies, given in English. Other than the\nimportance of studying these testimonies for Holocaust research, we argue that\nthey provide an interesting test case for topical segmentation, due to their\nunstructured surface level, relative abundance (tens of thousands of such\ntestimonies were collected), and the relatively confined domain that they\ncover. We hypothesize that boundary points between segments correspond to low\nmutual information between the sentences proceeding and following the boundary.\nBased on this hypothesis, we explore a range of algorithmic approaches to the\ntask, building on previous work on segmentation that uses generative Bayesian\nmodeling and state-of-the-art neural machinery. Compared to manually annotated\nreferences, we find that the developed approaches show considerable\nimprovements over previous work.",
    "descriptor": "",
    "authors": [
      "Eitan Wagner",
      "Renana Keydar",
      "Amit Pinchevski",
      "Omri Abend"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13783"
  },
  {
    "id": "arXiv:2210.13795",
    "title": "Line Graph Contrastive Learning for Link Prediction",
    "abstract": "Link prediction task aims to predict the connection of two nodes in the\nnetwork. Existing works mainly predict links by node pairs similarity\nmeasurements. However, if the local structure doesn't meet such measurement\nassumption, the algorithms' performance will deteriorate rapidly. To overcome\nthese limitations, we propose a Line Graph Contrastive Learning (LGCL) method\nto obtain multiview information. Our framework obtains a subgraph view by h-hop\nsubgraph sampling with target node pairs as the center. After transforming the\nsampled subgraph into a line graph, the edge embedding information is directly\naccessible, and the link prediction task is converted into a node\nclassification task. Then, different graph convolution operators learn\nrepresentations from double perspectives. Finally, contrastive learning is\nadopted to balance the subgraph representations of these perspectives via\nmaximizing mutual information. With experiments on six public datasets, LGCL\noutperforms current benchmarks on link prediction tasks and shows better\ngeneralization performance and robustness.",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Zehua Zhang",
      "Shilin Sun",
      "Guixiang Ma",
      "Caiming Zhong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13795"
  },
  {
    "id": "arXiv:2210.13797",
    "title": "MAROAM: Map-based Radar SLAM through Two-step Feature Selection",
    "abstract": "In this letter, we propose MAROAM, a millimeter wave radar-based SLAM\nframework, which employs a two-step feature selection process to build the\nglobal consistent map. Specifically, we first extract feature points from raw\ndata based on their local geometric properties to filter out those points that\nviolate the principle of millimeter-wave radar imaging. Then, we further employ\nanother round of probabilistic feature selection by examining how often and how\nrecent the feature point has been detected in the proceeding frames. With such\na two-step feature selection, we establish a global consistent map for accurate\nand robust pose estimation as well as other downstream tasks. At last, we\nperform loop closure and graph optimization in the back-end, further reducing\nthe accumulated drift error.\nWe evaluate the performance of MAROAM on the three datasets: the Oxford Radar\nRobotCar Dataset, the MulRan Dataset and the Boreas Dataset. We consider a\nvariety of experimental settings with different scenery, weather, and road\nconditions. The experimental results show that the accuracy of MAROAM is 7.95%,\n37.0% and 8.9% higher than the currently best-performing algorithms on these\nthree datasets, respectively. The ablation results also show that our map-based\nodometry performs 28.6% better than the commonly used scan-to-frames method.\nFinally, as devoted contributors to the open-source community, we will open\nsource the algorithm after the paper is accepted.",
    "descriptor": "",
    "authors": [
      "Dequan Wang",
      "Yifan Duan",
      "Xiaoran Fan",
      "Chengzhen Meng",
      "Jianmin Ji",
      "Yanyong Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13797"
  },
  {
    "id": "arXiv:2210.13800",
    "title": "Referee: Reference-Free Sentence Summarization with Sharper  Controllability through Symbolic Knowledge Distillation",
    "abstract": "We present Referee, a novel framework for sentence summarization that can be\ntrained reference-free (i.e., requiring no gold summaries for supervision),\nwhile allowing direct control for compression ratio. Our work is the first to\ndemonstrate that reference-free, controlled sentence summarization is feasible\nvia the conceptual framework of Symbolic Knowledge Distillation (West et al.,\n2022), where latent knowledge in pre-trained language models is distilled via\nexplicit examples sampled from the teacher models, further purified with three\ntypes of filters: length, fidelity, and Information Bottleneck. Moreover, we\nuniquely propose iterative distillation of knowledge, where student models from\nthe previous iteration of distillation serve as teacher models in the next\niteration. Starting off from a relatively modest set of GPT3-generated\nsummaries, we demonstrate how iterative knowledge distillation can lead to\nconsiderably smaller, but better summarizers with sharper controllability. A\nuseful by-product of this iterative distillation process is a high-quality\ndataset of sentence-summary pairs with varying degrees of compression ratios.\nEmpirical results demonstrate that the final student models vastly outperform\nthe much larger GPT3-Instruct model in terms of the controllability of\ncompression ratios, without compromising the quality of resulting\nsummarization.",
    "descriptor": "",
    "authors": [
      "Melanie Sclar",
      "Peter West",
      "Sachin Kumar",
      "Yulia Tsvetkov",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13800"
  },
  {
    "id": "arXiv:2210.13801",
    "title": "Deep Boosting Robustness of DNN-based Image Watermarking via DBMark",
    "abstract": "In this paper, we present DBMark, a new end-to-end digital image watermarking\nframework to deep boost the robustness of DNN-based image watermarking. The key\nnovelty is the synergy of the Invertible Neural Networks(INNs) and effective\nwatermark features generation. The framework generates watermark features with\nredundancy and error correction ability through message processing, synergized\nwith the powerful information embedding and extraction capabilities of\nInvertible Neural Networks to achieve higher robustness and invisibility.\nExtensive experiment results demonstrate the superiority of the proposed\nframework compared with the state-of-the-art ones under various distortions.",
    "descriptor": "",
    "authors": [
      "Guanhui Ye",
      "Jiashi Gao",
      "Wei Xie",
      "Bo Yin",
      "Xuetao Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13801"
  },
  {
    "id": "arXiv:2210.13803",
    "title": "Adapitch: Adaption Multi-Speaker Text-to-Speech Conditioned on Pitch  Disentangling with Untranscribed Data",
    "abstract": "In this paper, we proposed Adapitch, a multi-speaker TTS method that makes\nadaptation of the supervised module with untranscribed data. We design two self\nsupervised modules to train the text encoder and mel decoder separately with\nuntranscribed data to enhance the representation of text and mel. To better\nhandle the prosody information in a synthesized voice, a supervised TTS module\nis designed conditioned on content disentangling of pitch, text, and speaker.\nThe training phase was separated into two parts, pretrained and fixed the text\nencoder and mel decoder with unsupervised mode, then the supervised mode on the\ndisentanglement of TTS. Experiment results show that the Adaptich achieved much\nbetter quality than baseline methods.",
    "descriptor": "\nComments: Accepted by MSN2022, The 18th International Conference on Mobility, Sensing and Networking\n",
    "authors": [
      "Xulong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13803"
  },
  {
    "id": "arXiv:2210.13805",
    "title": "Improving Speech Representation Learning via Speech-level and  Phoneme-level Masking Approach",
    "abstract": "Recovering the masked speech frames is widely applied in speech\nrepresentation learning. However, most of these models use random masking in\nthe pre-training. In this work, we proposed two kinds of masking approaches:\n(1) speech-level masking, making the model to mask more speech segments than\nsilence segments, (2) phoneme-level masking, forcing the model to mask the\nwhole frames of the phoneme, instead of phoneme pieces. We pre-trained the\nmodel via these two approaches, and evaluated on two downstream tasks, phoneme\nclassification and speaker recognition. The experiments demonstrated that the\nproposed masking approaches are beneficial to improve the performance of speech\nrepresentation.",
    "descriptor": "\nComments: Accepted by MSN2022, The 18th International Conference on Mobility, Sensing and Networking\n",
    "authors": [
      "Xulong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Kexin Zhu",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13805"
  },
  {
    "id": "arXiv:2210.13809",
    "title": "Diagnostic Posture Control System for Seated-Style Echocardiography  Robot",
    "abstract": "Purpose: Conventional robotic ultrasound systems were utilized with patients\nin supine positions. Meanwhile, the limitation of the systems is that it is\ndifficult to evacuate the patients in case of emergency (e.g., patient\ndiscomfort and system failure) because the patients are restricted between the\nrobot system and bed. Then, it is ideal that the patient undergoes the\nexamination in the sitting position in terms of safety. Therefore, we validated\na feasibility study of seated-style echocardiography using a robot. Method:\nPreliminary experiments were conducted to verify the following two points: (1)\nthe possibility of obtaining cardiac disease features in the sitting posture as\nwell as in the conventional examination, and (2) the relationship between\nposture angle and physical burden. For reducing the physical burden, two unique\nmechanisms were incorporated into the system: (1) a leg pendulum base mechanism\nto reduce the load on the legs when the lateral bending angle increases, and\n(2) a roll angle division by a lumbar lateral bending and thoracic rotation\nmechanisms. Results: Preliminary results demonstrated that adjusting the\ndiagnostic posture angle enabled us to obtain the views, including cardiac\ndisease features, as in the conventional examination. The results showed that\nthe body burden increased as the posture's lateral bending angle increased. The\nresults also demonstrated that the body load reduction mechanism incorporated\nin the results could reduce the physical load in the seated echocardiography.\nConclusion: These results showed the potential of the seated-style\nechocardiography robot.",
    "descriptor": "",
    "authors": [
      "Yuuki Shida",
      "Masami Sugawara",
      "Ryosuke Tsumura",
      "Haruaki Chiba",
      "Tokuhisa Uejima",
      "Hiroyasu Iwata"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13809"
  },
  {
    "id": "arXiv:2210.13810",
    "title": "Toward domain generalized pruning by scoring out-of-distribution  importance",
    "abstract": "Filter pruning has been widely used for compressing convolutional neural\nnetworks to reduce computation costs during the deployment stage. Recent\nstudies have shown that filter pruning techniques can achieve lossless\ncompression of deep neural networks, reducing redundant filters (kernels)\nwithout sacrificing accuracy performance. However, the evaluation is done when\nthe training and testing data are from similar environmental conditions\n(independent and identically distributed), and how the filter pruning\ntechniques would affect the cross-domain generalization (out-of-distribution)\nperformance is largely ignored. We conduct extensive empirical experiments and\nreveal that although the intra-domain performance could be maintained after\nfilter pruning, the cross-domain performance will decay to a large extent. As\nscoring a filter's importance is one of the central problems for pruning, we\ndesign the importance scoring estimation by using the variance of domain-level\nrisks to consider the pruning risk in the unseen distribution. As such, we can\nremain more domain generalized filters. The experiments show that under the\nsame pruning ratio, our method can achieve significantly better cross-domain\ngeneralization performance than the baseline filter pruning method. For the\nfirst attempt, our work sheds light on the joint problem of domain\ngeneralization and filter pruning research.",
    "descriptor": "\nComments: Accepted in Workshop on Distribution Shifts, 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Rizhao Cai",
      "Haoliang Li",
      "Alex Kot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13810"
  },
  {
    "id": "arXiv:2210.13811",
    "title": "MetaSpeech: Speech Effects Switch Along with Environment for Metaverse",
    "abstract": "Metaverse expands the physical world to a new dimension, and the physical\nenvironment and Metaverse environment can be directly connected and entered.\nVoice is an indispensable communication medium in the real world and Metaverse.\nFusion of the voice with environment effects is important for user immersion in\nMetaverse. In this paper, we proposed using the voice conversion based method\nfor the conversion of target environment effect speech. The proposed method was\nnamed MetaSpeech, which introduces an environment effect module containing an\neffect extractor to extract the environment information and an effect encoder\nto encode the environment effect condition, in which gradient reversal layer\nwas used for adversarial training to keep the speech content and speaker\ninformation while disentangling the environmental effects. From the experiment\nresults on the public dataset of LJSpeech with four environment effects, the\nproposed model could complete the specific environment effect conversion and\noutperforms the baseline methods from the voice conversion task.",
    "descriptor": "\nComments: Accepted by AI2OT2022, The Third International Workshop on Artificial Intelligence Applications in Internet of Things\n",
    "authors": [
      "Xulong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Jing Xiao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13811"
  },
  {
    "id": "arXiv:2210.13815",
    "title": "FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node  Classification",
    "abstract": "Recently, a lot of research attention has been devoted to exploring Web\nsecurity, a most representative topic is the adversarial robustness of graph\nmining algorithms. Especially, a widely deployed adversarial attacks\nformulation is the graph manipulation attacks by modifying the relational data\nto mislead the Graph Neural Networks' (GNNs) predictions. Naturally, an\nintrinsic question one would ask is whether we can accurately identify the\nmanipulations over graphs - we term this problem as poisoned graph sanitation.\nIn this paper, we present FocusedCleaner, a poisoned graph sanitation framework\nconsisting of two modules: bi-level structural learning and victim node\ndetection. In particular, the structural learning module will reserve the\nattack process to steadily sanitize the graph while the detection module\nprovides the \"focus\" - a narrowed and more accurate search region - to\nstructural learning. These two modules will operate in iterations and reinforce\neach other to sanitize a poisoned graph step by step. Extensive experiments\ndemonstrate that FocusedCleaner outperforms the state-of-the-art baselines both\non poisoned graph sanitation and improving robustness.",
    "descriptor": "",
    "authors": [
      "Yulin Zhu",
      "Liang Tong",
      "Kai Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13815"
  },
  {
    "id": "arXiv:2210.13820",
    "title": "Supporting Electronics Learning through Augmented Reality",
    "abstract": "Understanding electronics is a critical area in the maker scene. Many of the\nmakers' projects require electronics knowledge to connect microcontrollers with\nsensors and actuators. Yet, learning electronics is challenging, as internal\ncomponent processes remain invisible, and students often fear personal harm or\ncomponent damage. Augmented Reality (AR) applications are developed to support\nelectronics learning and visualize complex processes. This paper reflects on\nrelated work around AR and electronics that characterize open research\nchallenges around the four characteristics functionality, fidelity, feedback\ntype, and interactivity.",
    "descriptor": "",
    "authors": [
      "Thomas Kosch",
      "Julian Rasch",
      "Albrecht Schmidt",
      "Sebastian Feger"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13820"
  },
  {
    "id": "arXiv:2210.13821",
    "title": "Salient Object Detection via Dynamic Scale Routing",
    "abstract": "Recent research advances in salient object detection (SOD) could largely be\nattributed to ever-stronger multi-scale feature representation empowered by the\ndeep learning technologies. The existing SOD deep models extract multi-scale\nfeatures via the off-the-shelf encoders and combine them smartly via various\ndelicate decoders. However, the kernel sizes in this commonly-used thread are\nusually \"fixed\". In our new experiments, we have observed that kernels of small\nsize are preferable in scenarios containing tiny salient objects. In contrast,\nlarge kernel sizes could perform better for images with large salient objects.\nInspired by this observation, we advocate the \"dynamic\" scale routing (as a\nbrand-new idea) in this paper. It will result in a generic plug-in that could\ndirectly fit the existing feature backbone. This paper's key technical\ninnovations are two-fold. First, instead of using the vanilla convolution with\nfixed kernel sizes for the encoder design, we propose the dynamic pyramid\nconvolution (DPConv), which dynamically selects the best-suited kernel sizes\nw.r.t. the given input. Second, we provide a self-adaptive bidirectional\ndecoder design to accommodate the DPConv-based encoder best. The most\nsignificant highlight is its capability of routing between feature scales and\ntheir dynamic collection, making the inference process scale-aware. As a\nresult, this paper continues to enhance the current SOTA performance. Both the\ncode and dataset are publicly available at\nhttps://github.com/wuzhenyubuaa/DPNet.",
    "descriptor": "\nComments: 15 pages, 15 figures\n",
    "authors": [
      "Zhenyu Wu",
      "Shuai Li",
      "Chenglizhao Chen",
      "Hong Qin",
      "Aimin Hao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13821"
  },
  {
    "id": "arXiv:2210.13823",
    "title": "A Chinese Spelling Check Framework Based on Reverse Contrastive Learning",
    "abstract": "Chinese spelling check is a task to detect and correct spelling mistakes in\nChinese text. Existing research aims to enhance the text representation and use\nmulti-source information to improve the detection and correction capabilities\nof models, but does not pay too much attention to improving their ability to\ndistinguish between confusable words. Contrastive learning, whose aim is to\nminimize the distance in representation space between similar sample pairs, has\nrecently become a dominant technique in natural language processing. Inspired\nby contrastive learning, we present a novel framework for Chinese spelling\nchecking, which consists of three modules: language representation, spelling\ncheck and reverse contrastive learning. Specifically, we propose a reverse\ncontrastive learning strategy, which explicitly forces the model to minimize\nthe agreement between the similar examples, namely, the phonetically and\nvisually confusable characters. Experimental results show that our framework is\nmodel-agnostic and could be combined with existing Chinese spelling check\nmodels to yield state-of-the-art performance.",
    "descriptor": "",
    "authors": [
      "Nankai Lin",
      "Sihui Fu",
      "Xiaotian Lin",
      "Shengyi Jiang",
      "Aimin Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13823"
  },
  {
    "id": "arXiv:2210.13826",
    "title": "Instance Segmentation for Chinese Character Stroke Extraction, Datasets  and Benchmarks",
    "abstract": "Stroke is the basic element of Chinese character and stroke extraction has\nbeen an important and long-standing endeavor. Existing stroke extraction\nmethods are often handcrafted and highly depend on domain expertise due to the\nlimited training data. Moreover, there are no standardized benchmarks to\nprovide a fair comparison between different stroke extraction methods, which,\nwe believe, is a major impediment to the development of Chinese character\nstroke understanding and related tasks. In this work, we present the first\npublic available Chinese Character Stroke Extraction (CCSE) benchmark, with two\nnew large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW).\nWith the large-scale datasets, we hope to leverage the representation power of\ndeep models such as CNNs to solve the stroke extraction task, which, however,\nremains an open question. To this end, we turn the stroke extraction problem\ninto a stroke instance segmentation problem. Using the proposed datasets to\ntrain a stroke instance segmentation model, we surpass previous methods by a\nlarge margin. Moreover, the models trained with the proposed datasets benefit\nthe downstream font generation and handwritten aesthetic assessment tasks. We\nhope these benchmark results can facilitate further research. The source code\nand datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.",
    "descriptor": "\nComments: 12 pages, 8 pages for the main paper, 4 pages for the supplementary\n",
    "authors": [
      "Lizhao Liu",
      "Kunyang Lin",
      "Shangxin Huang",
      "Zhongli Li",
      "Chao Li",
      "Yunbo Cao",
      "Qingyu Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13826"
  },
  {
    "id": "arXiv:2210.13827",
    "title": "End-to-end Transformer for Compressed Video Quality Enhancement",
    "abstract": "Convolutional neural networks have achieved excellent results in compressed\nvideo quality enhancement task in recent years. State-of-the-art methods\nexplore the spatiotemporal information of adjacent frames mainly by deformable\nconvolution. However, offset fields in deformable convolution are difficult to\ntrain, and its instability in training often leads to offset overflow, which\nreduce the efficiency of correlation modeling. In this work, we propose a\ntransformer-based compressed video quality enhancement (TVQE) method,\nconsisting of Swin-AutoEncoder based Spatio-Temporal feature Fusion (SSTF)\nmodule and Channel-wise Attention based Quality Enhancement (CAQE) module. The\nproposed SSTF module learns both local and global features with the help of\nSwin-AutoEncoder, which improves the ability of correlation modeling.\nMeanwhile, the window mechanism-based Swin Transformer and the encoderdecoder\nstructure greatly improve the execution efficiency. On the other hand, the\nproposed CAQE module calculates the channel attention, which aggregates the\ntemporal information between channels in the feature map, and finally achieves\nthe efficient fusion of inter-frame information. Extensive experimental results\non the JCT-VT test sequences show that the proposed method achieves better\nperformance in average for both subjective and objective quality. Meanwhile,\nour proposed method outperforms existing ones in terms of both inference speed\nand GPU consumption.",
    "descriptor": "",
    "authors": [
      "Li Yu",
      "Wenshuai Chang",
      "Shiyu Wu",
      "Moncef Gabbouj"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13827"
  },
  {
    "id": "arXiv:2210.13829",
    "title": "Information Filter upon Diversity-Improved Decoding for  Diversity-Faithfulness Tradeoff in NLG",
    "abstract": "Some Natural Language Generation (NLG) tasks require both faithfulness and\ndiversity. The decoding strategy is intensively related to the quality of the\ngenerated text. Strategies such as beam search, greedy search, etc., perform\nwith low diversity and high repetition. On the other hand, guided decoding, the\nsolution towards diversity, may generate unfaithful expressions. To this end,\nthis paper presents Information Filter upon Diversity-Improved Decoding (IFDID)\nto obtain the tradeoff between diversity and faithfulness. IFDID is a two-stage\ndecoding strategy leveraging the proposed Enhance-Filter framework, which\nachieves the tradeoff by increasing the probabilities of some typical tokens\nbeing selected and subsequently filtering them by their information amount. To\nverify the effectiveness, we compare our method with other baselines on related\nCommonGEN, RocStories and AdGen benchmarks, which cover Chinese and English\ndatasets. Our numerical experimental results and human evaluation outcomes\nverify the effectiveness of the proposed approach, as our approach achieves a\n1.24 higher ROUGE score describing faithfulness as well as higher diversity\nrepresented by 62.5% higher upon Dist-2 than traditional approaches,\ndemonstrating that IFDID is a novel SOTA decoding strategy for the tradeoff\nbetween diversity and faithfulness.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Han Meng",
      "Xiaosong He",
      "Zexing Chen",
      "Feng Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13829"
  },
  {
    "id": "arXiv:2210.13830",
    "title": "Wikinformetrics: Construction and description of an open Wikipedia  knowledge graph dataset for informetric purposes",
    "abstract": "Wikipedia is one of the most visited websites in the world and is also a\nfrequent subject of scientific research. However, the analytical possibilities\nof Wikipedia information have not yet been analyzed considering at the same\ntime both a large volume of pages and attributes. The main objective of this\nwork is to offer a methodological framework and an open knowledge graph for the\ninformetric large-scale study of Wikipedia. Features of Wikipedia pages are\ncompared with those of scientific publications to highlight the\n(di)similarities between the two types of documents. Based on this comparison,\ndifferent analytical possibilities that Wikipedia and its various data sources\noffer are explored, ultimately offering a set of metrics meant to study\nWikipedia from different analytical dimensions. In parallel, a complete\ndedicated dataset of the English Wikipedia was built (and shared) following a\nrelational model. Finally, a descriptive case study is carried out on the\nEnglish Wikipedia dataset to illustrate the analytical potential of the\nknowledge graph and its metrics.",
    "descriptor": "",
    "authors": [
      "Wenceslao Arroyo-Machado",
      "Daniel Torres-Salinas",
      "Rodrigo Costas"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.13830"
  },
  {
    "id": "arXiv:2210.13832",
    "title": "FineD-Eval: Fine-grained Automatic Dialogue-Level Evaluation",
    "abstract": "Recent model-based reference-free metrics for open-domain dialogue evaluation\nexhibit promising correlations with human judgment. However, they either\nperform turn-level evaluation or look at a single dialogue quality dimension.\nOne would expect a good evaluation metric to assess multiple quality dimensions\nat the dialogue level. To this end, we are motivated to propose a\nmulti-dimensional dialogue-level metric, which consists of three sub-metrics\nwith each targeting a specific dimension. The sub-metrics are trained with\nnovel self-supervised objectives and exhibit strong correlations with human\njudgment for their respective dimensions. Moreover, we explore two approaches\nto combine the sub-metrics: metric ensemble and multitask learning. Both\napproaches yield a holistic metric that significantly outperforms individual\nsub-metrics. Compared to the existing state-of-the-art metric, the combined\nmetrics achieve around 16% relative improvement on average across three\nhigh-quality dialogue-level evaluation benchmarks.",
    "descriptor": "\nComments: EMNLP-2022, 20 pages\n",
    "authors": [
      "Chen Zhang",
      "Luis Fernando D'Haro",
      "Qiquan Zhang",
      "Thomas Friedrichs",
      "Haizhou Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13832"
  },
  {
    "id": "arXiv:2210.13835",
    "title": "Synthetic Data Supervised Salient Object Detection",
    "abstract": "Although deep salient object detection (SOD) has achieved remarkable\nprogress, deep SOD models are extremely data-hungry, requiring large-scale\npixel-wise annotations to deliver such promising results. In this paper, we\npropose a novel yet effective method for SOD, coined SODGAN, which can generate\ninfinite high-quality image-mask pairs requiring only a few labeled data, and\nthese synthesized pairs can replace the human-labeled DUTS-TR to train any\noff-the-shelf SOD model. Its contribution is three-fold. 1) Our proposed\ndiffusion embedding network can address the manifold mismatch and is tractable\nfor the latent code generation, better matching with the ImageNet latent space.\n2) For the first time, our proposed few-shot saliency mask generator can\nsynthesize infinite accurate image synchronized saliency masks with a few\nlabeled data. 3) Our proposed quality-aware discriminator can select\nhighquality synthesized image-mask pairs from noisy synthetic data pool,\nimproving the quality of synthetic data. For the first time, our SODGAN tackles\nSOD with synthetic data directly generated from the generative model, which\nopens up a new research paradigm for SOD. Extensive experimental results show\nthat the saliency model trained on synthetic data can achieve $98.4\\%$\nF-measure of the saliency model trained on the DUTS-TR. Moreover, our approach\nachieves a new SOTA performance in semi/weakly-supervised methods, and even\noutperforms several fully-supervised SOTA methods. Code is available at\nhttps://github.com/wuzhenyubuaa/SODGAN",
    "descriptor": "\nComments: 9 pages, 8 figures\n",
    "authors": [
      "Zhenyu Wu",
      "Lin Wang",
      "Wei Wang",
      "Tengfei Shi",
      "Chenglizhao Chen",
      "Aimin Hao",
      "Shuo Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13835"
  },
  {
    "id": "arXiv:2210.13836",
    "title": "Deconfounding Legal Judgment Prediction for European Court of Human  Rights Cases Towards Better Alignment with Experts",
    "abstract": "This work demonstrates that Legal Judgement Prediction systems without\nexpert-informed adjustments can be vulnerable to shallow, distracting surface\nsignals that arise from corpus construction, case distribution, and confounding\nfactors. To mitigate this, we use domain expertise to strategically identify\nstatistically predictive but legally irrelevant information. We adopt\nadversarial training to prevent the system from relying on it. We evaluate our\ndeconfounded models by employing interpretability techniques and comparing to\nexpert annotations. Quantitative experiments and qualitative analysis show that\nour deconfounded model consistently aligns better with expert rationales than\nbaselines trained for prediction only. We further contribute a set of reference\nexpert annotations to the validation and testing partitions of an existing\nbenchmark dataset of European Court of Human Rights cases.",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "T.Y.S.S Santosh",
      "Shanshan Xu",
      "Oana Ichim",
      "Matthias Grabmair"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13836"
  },
  {
    "id": "arXiv:2210.13838",
    "title": "Multilingual Relation Classification via Efficient and Effective  Prompting",
    "abstract": "Prompting pre-trained language models has achieved impressive performance on\nvarious NLP tasks, especially in low data regimes. Despite the success of\nprompting in monolingual settings, applying prompt-based methods in\nmultilingual scenarios has been limited to a narrow set of tasks, due to the\nhigh cost of handcrafting multilingual prompts. In this paper, we present the\nfirst work on prompt-based multilingual relation classification (RC), by\nintroducing an efficient and effective method that constructs prompts from\nrelation triples and involves only minimal translation for the class labels. We\nevaluate its performance in fully supervised, few-shot and zero-shot scenarios,\nand analyze its effectiveness across 14 languages, prompt variants, and\nEnglish-task training in cross-lingual settings. We find that in both fully\nsupervised and few-shot scenarios, our prompt method beats competitive\nbaselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the\nrandom baseline by a large margin in zero-shot experiments. Our method requires\nlittle in-language knowledge and can be used as a strong baseline for similar\nmultilingual classification tasks.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Yuxuan Chen",
      "David Harbecke",
      "Leonhard Hennig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13838"
  },
  {
    "id": "arXiv:2210.13839",
    "title": "Preference-Learning Emitters for Mixed-Initiative Quality-Diversity  Algorithms",
    "abstract": "In mixed-initiative co-creation tasks, where a human and a machine jointly\ncreate items, it is valuable for the generative system to provide multiple\nrelevant suggestions to the designer. Quality-diversity algorithms have been\ncommonly used for this, as they can provide diverse suggestions that are\nrepresentative of salient areas of the solution space, showcasing solutions\nwith both high fitness and different properties that the designer might be\ninterested in. Since these suggestions are what drives the search process, it\nis important that they provide the right inspiration for the designer, as well\nas not stray too far away from the search trajectory, i.e., they should be\naligned with what the designer is looking for. Additionally, in most cases,\nmany interactions with the system are required before the designer is content\nwith a solution. In this work, we tackle both of these problems with an\ninteractive constrained MAP-Elites system by crafting emitters that are able to\nlearn the preferences of the designer and use them in automated hidden steps.\nBy learning such preferences, we remain aligned with the designer's intentions,\nand by applying automatic steps, we generate more solutions per system\ninteraction, giving a larger number of choices to the designer and speeding up\nthe search process. We propose a general framework for preference-learning\nemitters and test it on a procedural content generation task in the video game\nSpace Engineers. In an internal study, we show that preference-learning\nemitters allow users to more quickly find relevant solutions.",
    "descriptor": "",
    "authors": [
      "Roberto Gallotta",
      "Kai Arulkumaran",
      "L. B. Soros"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13839"
  },
  {
    "id": "arXiv:2210.13845",
    "title": "DialogConv: A Lightweight Fully Convolutional Network for Multi-view  Response Selection",
    "abstract": "Current end-to-end retrieval-based dialogue systems are mainly based on\nRecurrent Neural Networks or Transformers with attention mechanisms. Although\npromising results have been achieved, these models often suffer from slow\ninference or huge number of parameters. In this paper, we propose a novel\nlightweight fully convolutional architecture, called DialogConv, for response\nselection. DialogConv is exclusively built on top of convolution to extract\nmatching features of context and response. Dialogues are modeled in 3D views,\nwhere DialogConv performs convolution operations on embedding view, word view\nand utterance view to capture richer semantic information from multiple\ncontextual views. On the four benchmark datasets, compared with\nstate-of-the-art baselines, DialogConv is on average about 8.5x smaller in\nsize, and 79.39x and 10.64x faster on CPU and GPU devices, respectively. At the\nsame time, DialogConv achieves the competitive effectiveness of response\nselection.",
    "descriptor": "\nComments: EMNLP2022\n",
    "authors": [
      "Yongkang Liu",
      "Shi Feng",
      "Wei Gao",
      "Daling Wang",
      "Yifei Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13845"
  },
  {
    "id": "arXiv:2210.13846",
    "title": "Adaptive Behavior Cloning Regularization for Stable Offline-to-Online  Reinforcement Learning",
    "abstract": "Offline reinforcement learning, by learning from a fixed dataset, makes it\npossible to learn agent behaviors without interacting with the environment.\nHowever, depending on the quality of the offline dataset, such pre-trained\nagents may have limited performance and would further need to be fine-tuned\nonline by interacting with the environment. During online fine-tuning, the\nperformance of the pre-trained agent may collapse quickly due to the sudden\ndistribution shift from offline to online data. While constraints enforced by\noffline RL methods such as a behaviour cloning loss prevent this to an extent,\nthese constraints also significantly slow down online fine-tuning by forcing\nthe agent to stay close to the behavior policy. We propose to adaptively weigh\nthe behavior cloning loss during online fine-tuning based on the agent's\nperformance and training stability. Moreover, we use a randomized ensemble of Q\nfunctions to further increase the sample efficiency of online fine-tuning by\nperforming a large number of learning updates. Experiments show that the\nproposed method yields state-of-the-art offline-to-online reinforcement\nlearning performance on the popular D4RL benchmark. Code is available:\n\\url{https://github.com/zhaoyi11/adaptive_bc}.",
    "descriptor": "",
    "authors": [
      "Yi Zhao",
      "Rinu Boney",
      "Alexander Ilin",
      "Juho Kannala",
      "Joni Pajarinen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13846"
  },
  {
    "id": "arXiv:2210.13848",
    "title": "Connectivity-Aware Contract for Incentivizing IoT Devices in Complex  Wireless Blockchain Networks",
    "abstract": "Blockchain is considered to be the critical backbone technology for secure\nand trusted Internet of Things (IoT) on the future 6G network.However, the IoT\nnetwork is usually with the complex wireless environment, where the\ncommunication is not reliable and the connectivity is complicated. Deploying a\nblockchain system in the complex wireless IoT network is challenging. Due to\nthe limited resources, complex wireless environment and the property of\nself-interest, IoT devices do not have the internal motivation to consume\nenergy and time to maintain the blockchain. Furthermore, existing incentive\nmechanism in blockchain is not compatible well with the wireless IoT network.\nIn this paper, to incentivize IoT devices to join the construction of the\nwireless blockchain network, we propose a multi dimentional contract which\noptimizes the blockchain utility while addressing the issues of adverse\nselection and moral hazard. Specifically, the proposed contract not only\nconsiders the IoT devices' hash rate and transmission power, but also explores\nthe network connectivity from the perspective of the network complexity. We\nincestigate the impact of these factors on energy consumption and the block\nconfirmation probability by the experiments under different network sizes and\naverage link probability. Numerical results demonstrate that our proposed\ncontract mechanism is feasible and effective. Compared with the contract with\nadverse selection, the proposed contract improves blockchain utility by 35%,\nwhich is closer to the perfect information contract.",
    "descriptor": "",
    "authors": [
      "Weiyi Wang",
      "Jin Chen",
      "Yutao Jiao",
      "Jiawen Kang",
      "Wenting Dai",
      "Yuhua Xu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13848"
  },
  {
    "id": "arXiv:2210.13850",
    "title": "Tight analysis of lazy: an improved algorithm for open online  dial-a-ride",
    "abstract": "In the open online dial-a-ride problem, a single server has to carry\ntransportation requests appearing over time in some metric space, subject to\nminimizing the completion time. We improve on the best known upper bounds on\nthe competitive ratio on general metric spaces and on the half-line, in both,\nthe preemptive and non-preemptive version of the problem. We achieve this by\nrevisiting the algorithm Lazy recently suggested in [WAOA, 2022] and giving an\nimproved and tight analysis. More precisely, we show that it is\n$(\\frac{3}{2}+\\sqrt{11/12}\\thickapprox 2.457)$-competitive on general metric\nspaces and $(1+\\frac{1}{2}(1+\\sqrt{3})\\approx 2.366)$-competitive on the\nhalf-line.",
    "descriptor": "",
    "authors": [
      "Julia Baligacs",
      "Yann Disser",
      "David Weckbecker"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.13850"
  },
  {
    "id": "arXiv:2210.13852",
    "title": "TabMixer: Excavating Label Distribution Learning with Small-scale  Features",
    "abstract": "Label distribution learning (LDL) differs from multi-label learning which\naims at representing the polysemy of instances by transforming single-label\nvalues into descriptive degrees. Unfortunately, the feature space of the label\ndistribution dataset is affected by human factors and the inductive bias of the\nfeature extractor causing uncertainty in the feature space. Especially, for\ndatasets with small-scale feature spaces (the feature space dimension $\\approx$\nthe label space), the existing LDL algorithms do not perform well. To address\nthis issue, we seek to model the uncertainty augmentation of the feature space\nto alleviate the problem in LDL tasks. Specifically, we start with augmenting\neach feature value in the feature vector of a sample into a vector (sampling on\na Gaussian distribution function). Which, the variance parameter of the\nGaussian distribution function is learned by using a sub-network, and the mean\nparameter is filled by this feature value. Then, each feature vector is\naugmented to a matrix which is fed into a mixer with local attention\n(\\textit{TabMixer}) to extract the latent feature. Finally, the latent feature\nis squeezed to yield an accurate label distribution via a squeezed network.\nExtensive experiments verify that our proposed algorithm can be competitive\ncompared to other LDL algorithms on several benchmarks.",
    "descriptor": "",
    "authors": [
      "Weiyi Cong",
      "Zhuoran Zheng",
      "Xiuyi Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13852"
  },
  {
    "id": "arXiv:2210.13853",
    "title": "THOR-Net: End-to-end Graformer-based Realistic Two Hands and Object  Reconstruction with Self-supervision",
    "abstract": "Realistic reconstruction of two hands interacting with objects is a new and\nchallenging problem that is essential for building personalized Virtual and\nAugmented Reality environments. Graph Convolutional networks (GCNs) allow for\nthe preservation of the topologies of hands poses and shapes by modeling them\nas a graph. In this work, we propose the THOR-Net which combines the power of\nGCNs, Transformer, and self-supervision to realistically reconstruct two hands\nand an object from a single RGB image. Our network comprises two stages; namely\nthe features extraction stage and the reconstruction stage. In the features\nextraction stage, a Keypoint RCNN is used to extract 2D poses, features maps,\nheatmaps, and bounding boxes from a monocular RGB image. Thereafter, this 2D\ninformation is modeled as two graphs and passed to the two branches of the\nreconstruction stage. The shape reconstruction branch estimates meshes of two\nhands and an object using our novel coarse-to-fine GraFormer shape network. The\n3D poses of the hands and objects are reconstructed by the other branch using a\nGraFormer network. Finally, a self-supervised photometric loss is used to\ndirectly regress the realistic textured of each vertex in the hands' meshes.\nOur approach achieves State-of-the-art results in Hand shape estimation on the\nHO-3D dataset (10.0mm) exceeding ArtiBoost (10.8mm). It also surpasses other\nmethods in hand pose estimation on the challenging two hands and object (H2O)\ndataset by 5mm on the left-hand pose and 1 mm on the right-hand pose.",
    "descriptor": "\nComments: To be published in WACV2023\n",
    "authors": [
      "Ahmed Tawfik Aboukhadra",
      "Jameel Malik",
      "Ahmed Elhayek",
      "Nadia Robertini",
      "Didier Stricker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13853"
  },
  {
    "id": "arXiv:2210.13854",
    "title": "An Improved Algorithm for Open Online Dial-a-Ride",
    "abstract": "We consider the open online dial-a-ride problem, where transportation\nrequests appear online in a metric space and need to be served by a single\nserver. The objective is to minimize the completion time until all requests\nhave been served. We present a new, parameterized algorithm for this problem\nand prove that it attains a competitive ratio of $1 + \\varphi \\approx 2.618$\nfor some choice of its parameter, where $\\varphi$ is the golden ratio. This\nimproves the best known bounds for open online dial-a-ride both for general\nmetric spaces as well as for the real line. We also give a lower bound\nof~$2.457$ for the competitive ratio of our algorithm for any parameter choice.",
    "descriptor": "",
    "authors": [
      "Julia Baligacs",
      "Yann Disser",
      "Nils Mosis",
      "David Weckbecker"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.13854"
  },
  {
    "id": "arXiv:2210.13856",
    "title": "A Framework for Collaborative Multi-Robot Mapping using Spectral Graph  Wavelets",
    "abstract": "The exploration of large-scale unknown environments can benefit from the\ndeployment of multiple robots for collaborative mapping. Each robot explores a\nsection of the environment and communicates onboard pose estimates and maps to\na central server to build an optimized global multi-robot map. Naturally,\ninconsistencies can arise between onboard and server estimates due to onboard\nodometry drift, failures, or degeneracies. The mapping server can correct and\novercome such failure cases using computationally expensive operations such as\ninter-robot loop closure detection and multi-modal mapping. However, the\nindividual robots do not benefit from the collaborative map if the mapping\nserver provides no feedback. Although server updates from the multi-robot map\ncan greatly alleviate the robotic mission strategically, most existing work\nlacks them, due to their associated computational and bandwidth-related costs.\nMotivated by this challenge, this paper proposes a novel collaborative mapping\nframework that enables global mapping consistency among robots and the mapping\nserver. In particular, we propose graph spectral analysis, at different spatial\nscales, to detect structural differences between robot and server graphs, and\nto generate necessary constraints for the individual robot pose graphs. Our\napproach specifically finds the nodes that correspond to the drift's origin\nrather than the nodes where the error becomes too large. We thoroughly analyze\nand validate our proposed framework using several real-world multi-robot field\ndeployments where we show improvements of the onboard system up to 90\\% and can\nrecover the onboard estimation from localization failures and even from the\ndegeneracies within its estimation.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2203.00308\n",
    "authors": [
      "Lukas Bernreiter",
      "Shehryar Khattak",
      "Lionel Ott",
      "Roland Siegwart",
      "Marco Hutter",
      "Cesar Cadena"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13856"
  },
  {
    "id": "arXiv:2210.13858",
    "title": "LAB: Learnable Activation Binarizer for Binary Neural Networks",
    "abstract": "Binary Neural Networks (BNNs) are receiving an upsurge of attention for\nbringing power-hungry deep learning towards edge devices. The traditional\nwisdom in this space is to employ sign() for binarizing featuremaps. We argue\nand illustrate that sign() is a uniqueness bottleneck, limiting information\npropagation throughout the network. To alleviate this, we propose to dispense\nsign(), replacing it with a learnable activation binarizer (LAB), allowing the\nnetwork to learn a fine-grained binarization kernel per layer - as opposed to\nglobal thresholding. LAB is a novel universal module that can seamlessly be\nintegrated into existing architectures. To confirm this, we plug it into four\nseminal BNNs and show a considerable performance boost at the cost of tolerable\nincrease in delay and complexity. Finally, we build an end-to-end BNN (coined\nas LAB-BNN) around LAB, and demonstrate that it achieves competitive\nperformance on par with the state-of-the-art on ImageNet.",
    "descriptor": "\nComments: This paper is accepted to appear in the proceedings of WACV 2023\n",
    "authors": [
      "Sieger Falkena",
      "Hadi Jamali-Rad",
      "Jan van Gemert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13858"
  },
  {
    "id": "arXiv:2210.13861",
    "title": "SUPR: A Sparse Unified Part-Based Human Representation",
    "abstract": "Statistical 3D shape models of the head, hands, and fullbody are widely used\nin computer vision and graphics. Despite their wide use, we show that existing\nmodels of the head and hands fail to capture the full range of motion for these\nparts. Moreover, existing work largely ignores the feet, which are crucial for\nmodeling human movement and have applications in biomechanics, animation, and\nthe footwear industry. The problem is that previous body part models are\ntrained using 3D scans that are isolated to the individual parts. Such data\ndoes not capture the full range of motion for such parts, e.g. the motion of\nhead relative to the neck. Our observation is that full-body scans provide\nimportant information about the motion of the body parts. Consequently, we\npropose a new learning scheme that jointly trains a full-body model and\nspecific part models using a federated dataset of full-body and body-part\nscans. Specifically, we train an expressive human body model called SUPR\n(Sparse Unified Part-Based Human Representation), where each joint strictly\ninfluences a sparse set of model vertices. The factorized representation\nenables separating SUPR into an entire suite of body part models. Note that the\nfeet have received little attention and existing 3D body models have highly\nunder-actuated feet. Using novel 4D scans of feet, we train a model with an\nextended kinematic tree that captures the range of motion of the toes.\nAdditionally, feet deform due to ground contact. To model this, we include a\nnovel non-linear deformation function that predicts foot deformation\nconditioned on the foot pose, shape, and ground contact. We train SUPR on an\nunprecedented number of scans: 1.2 million body, head, hand and foot scans. We\nquantitatively compare SUPR and the separated body parts and find that our\nsuite of models generalizes better than existing models. SUPR is available at\nthis http URL",
    "descriptor": "\nComments: Accepted in ECCV 2022\n",
    "authors": [
      "Ahmed A. A. Osman",
      "Timo Bolkart",
      "Dimitrios Tzionas",
      "Michael J. Black"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13861"
  },
  {
    "id": "arXiv:2210.13865",
    "title": "Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for  Misinformation",
    "abstract": "Misinformation emerges in times of uncertainty when credible information is\nlimited. This is challenging for NLP-based fact-checking as it relies on\ncounter-evidence, which may not yet be available. Despite increasing interest\nin automatic fact-checking, it is still unclear if automated approaches can\nrealistically refute harmful real-world misinformation. Here, we contrast and\ncompare NLP fact-checking with how professional fact-checkers combat\nmisinformation in the absence of counter-evidence. In our analysis, we show\nthat, by design, existing NLP task definitions for fact-checking cannot refute\nmisinformation as professional fact-checkers do for the majority of claims. We\nthen define two requirements that the evidence in datasets must fulfill for\nrealistic fact-checking: It must be (1) sufficient to refute the claim and (2)\nnot leaked from existing fact-checking articles. We survey existing\nfact-checking datasets and find that all of them fail to satisfy both criteria.\nFinally, we perform experiments to demonstrate that models trained on a\nlarge-scale fact-checking dataset rely on leaked evidence, which makes them\nunsuitable in real-world scenarios. Taken together, we show that current NLP\nfact-checking cannot realistically combat real-world misinformation because it\ndepends on unrealistic assumptions about counter-evidence in the data.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Max Glockner",
      "Yufang Hou",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13865"
  },
  {
    "id": "arXiv:2210.13867",
    "title": "A Dynamical System View of Langevin-Based Non-Convex Sampling",
    "abstract": "Non-convex sampling is a key challenge in machine learning, central to\nnon-convex optimization in deep learning as well as to approximate\nprobabilistic inference. Despite its significance, theoretically there remain\nmany important challenges: Existing guarantees (1) typically only hold for the\naveraged iterates rather than the more desirable last iterates, (2) lack\nconvergence metrics that capture the scales of the variables such as\nWasserstein distances, and (3) mainly apply to elementary schemes such as\nstochastic gradient Langevin dynamics. In this paper, we develop a new\nframework that lifts the above issues by harnessing several tools from the\ntheory of dynamical systems. Our key result is that, for a large class of\nstate-of-the-art sampling schemes, their last-iterate convergence in\nWasserstein distances can be reduced to the study of their continuous-time\ncounterparts, which is much better understood. Coupled with standard\nassumptions of MCMC sampling, our theory immediately yields the last-iterate\nWasserstein convergence of many advanced sampling schemes such as proximal,\nrandomized mid-point, and Runge-Kutta integrators. Beyond existing methods, our\nframework also motivates more efficient schemes that enjoy the same rigorous\nguarantees.",
    "descriptor": "",
    "authors": [
      "Mohammad Reza Karimi",
      "Ya-Ping Hsieh",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.13867"
  },
  {
    "id": "arXiv:2210.13868",
    "title": "Time-dependent Steklov--Poincar\u00e9 operators and space-time Robin--Robin  decomposition for the heat equation",
    "abstract": "Domain decomposition methods are a set of widely used tools for\nparallelization of partial differential equation solvers. Convergence is well\nstudied for elliptic equations, but in the case of parabolic equations there\nare hardly any results for general Lipschitz domains in two or more dimensions.\nThe aim of this work is therefore to construct a new framework for analyzing\nnonoverlapping domain decomposition methods for the heat equation in a\nspace-time Lipschitz cylinder. The framework is based on a variational\nformulation, inspired by recent studies of space-time finite elements using\nSobolev spaces with fractional time regularity. In this framework, the\ntime-dependent Steklov--Poincar\\'e operators are introduced and their essential\nproperties are proven. We then derive the interface interpretations of the\nDirichlet--Neumann, Neumann--Neumann and Robin--Robin methods and show that\nthese methods are well defined. Finally, we prove convergence of the\nRobin--Robin method and introduce a modified method with stronger convergence\nproperties.",
    "descriptor": "\nComments: 25 pages, 1 figure\n",
    "authors": [
      "Emil Engstr\u00f6m",
      "Eskil Hansen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13868"
  },
  {
    "id": "arXiv:2210.13873",
    "title": "Program Synthesis Using Example Propagation",
    "abstract": "We present Scrybe, an example-based synthesis tool for a statically-typed\nfunctional programming language, which combines top-down deductive reasoning in\nthe style of $\\lambda^2$ with Smyth-style live bidirectional evaluation. During\nsynthesis, example constraints are propagated through sketches to prune and\nguide the search. This enables Scrybe to make more effective use of functions\nprovided in the context. To evaluate our tool, it is run on the combined,\nlargely disjoint, benchmarks of $\\lambda^2$ and Myth. Scrybe is able to\nsynthesize most of the combined benchmark tasks.",
    "descriptor": "\nComments: 17 pages, 4 figures, 2 tables\n",
    "authors": [
      "Niek Mulleners",
      "Johan Jeuring",
      "Bastiaan Heeren"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.13873"
  },
  {
    "id": "arXiv:2210.13874",
    "title": "Follower--Followee Ratio Category and User Vector for Analyzing  Following Behavior",
    "abstract": "Analyzing following behavior is important in many applications. Following\nbehavior may depend on the main intention of the follower. Users may either\nfollow their friends or they may follow celebrities to know more about them. It\nis difficult to estimate users' intention from their following relationships.\nIn this paper, we propose an approach to analyze following relationships.\nFirst, we investigated the similarity between users. Similar followers and\nfollowees are likely to be friends. However, when the follower and followee are\nnot similar, it is likely that follower seeks to obtain more information on the\nfollowee. Second, we categorized users by the network structure. We then\nproposed analysis of following behavior based on similarity and category of\nusers estimated from tweets and user data. We confirmed the feasibility of the\nproposed method through experiments. Finally, we examined users in different\ncategories and analyzed their following behavior.",
    "descriptor": "\nComments: 2022 9th International Conference on Advanced Informatics: Concepts, Theory and Applications\n",
    "authors": [
      "Hayato Oshimo",
      "Shiori Hironaka",
      "Mitsuo Yoshida",
      "Kyoji Umemura"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13874"
  },
  {
    "id": "arXiv:2210.13876",
    "title": "Towards emotion recognition for virtual environments: an evaluation of  EEG features on benchmark dataset",
    "abstract": "One of the challenges in virtual environments is the difficulty users have in\ninteracting with these increasingly complex systems. Ultimately, endowing\nmachines with the ability to perceive users emotions will enable a more\nintuitive and reliable interaction. Consequently, using the\nelectroencephalogram as a bio-signal sensor, the affective state of a user can\nbe modelled and subsequently utilised in order to achieve a system that can\nrecognise and react to the user's emotions. This paper investigates features\nextracted from electroencephalogram signals for the purpose of affective state\nmodelling based on Russell's Circumplex Model. Investigations are presented\nthat aim to provide the foundation for future work in modelling user affect to\nenhance interaction experience in virtual environments. The DEAP dataset was\nused within this work, along with a Support Vector Machine and Random Forest,\nwhich yielded reasonable classification accuracies for Valence and Arousal\nusing feature vectors based on statistical measurements and band power from the\n\\'z, \\b{eta}, \\'z, and \\'z\\'z waves and High Order Crossing of the EEG signal.",
    "descriptor": "\nComments: Published at Personal and Ubiquitous Computing Journal (PUC)\n",
    "authors": [
      "M. L. Menezes",
      "A. Samara",
      "L. Galway",
      "A. Sant'anna",
      "A. Verikas",
      "F. Alonso-Fernandez",
      "H. Wang",
      "R. Bond"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13876"
  },
  {
    "id": "arXiv:2210.13879",
    "title": "Proximal Mean Field Learning in Shallow Neural Networks",
    "abstract": "Recent mean field interpretations of learning dynamics in over-parameterized\nneural networks offer theoretical insights on the empirical success of first\norder optimization algorithms in finding global minima of the nonconvex risk\nlandscape. In this paper, we explore applying mean field learning dynamics as a\ncomputational algorithm, rather than as an analytical tool. Specifically, we\ndesign a Sinkhorn regularized proximal algorithm to approximate the\ndistributional flow from the learning dynamics in the mean field regime over\nweighted point clouds. In this setting, a contractive fixed point recursion\ncomputes the time-varying weights, numerically realizing the interacting\nWasserstein gradient flow of the parameter distribution supported over the\nneuronal ensemble. An appealing aspect of the proposed algorithm is that the\nmeasure-valued recursions allow meshless computation. We demonstrate the\nproposed computational framework of interacting weighted particle evolution on\nbinary and multi-class classification. Our algorithm performs gradient descent\nof the free energy associated with the risk functional.",
    "descriptor": "",
    "authors": [
      "Alexis Teter",
      "Iman Nodozi",
      "Abhishek Halder"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13879"
  },
  {
    "id": "arXiv:2210.13880",
    "title": "Efficient and Stable Fully Dynamic Facility Location",
    "abstract": "We consider the classic facility location problem in fully dynamic data\nstreams, where elements can be both inserted and deleted. In this problem, one\nis interested in maintaining a stable and high quality solution throughout the\ndata stream while using only little time per update (insertion or deletion). We\nstudy the problem and provide the first algorithm that at the same time\nmaintains a constant approximation and incurs polylogarithmic amortized\nrecourse per update. We complement our theoretical results with an experimental\nanalysis showing the practical efficiency of our method.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022 (oral presentation)\n",
    "authors": [
      "Sayan Bhattacharya",
      "Silvio Lattanzi",
      "Nikos Parotsidis"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13880"
  },
  {
    "id": "arXiv:2210.13885",
    "title": "Real-time AdaBoost cascade face tracker based on likelihood map and  optical flow",
    "abstract": "The authors present a novel face tracking approach where optical flow\ninformation is incorporated into a modified version of the Viola Jones\ndetection algorithm. In the original algorithm, detection is static, as\ninformation from previous frames is not considered. In addition, candidate\nwindows have to pass all stages of the classification cascade, otherwise they\nare discarded as containing no face. In contrast, the proposed tracker\npreserves information about the number of classification stages passed by each\nwindow. Such information is used to build a likelihood map, which represents\nthe probability of having a face located at that position. Tracking\ncapabilities are provided by extrapolating the position of the likelihood map\nto the next frame by optical flow computation. The proposed algorithm works in\nreal time on a standard laptop. The system is verified on the Boston Head\nTracking Database, showing that the proposed algorithm outperforms the standard\nViola Jones detector in terms of detection rate and stability of the output\nbounding box, as well as including the capability to deal with occlusions. The\nauthors also evaluate two recently published face detectors based on\nconvolutional networks and deformable part models with their algorithm showing\na comparable accuracy at a fraction of the computation time.",
    "descriptor": "\nComments: Published at IET Biometrics Journal\n",
    "authors": [
      "Andreas Ranftl",
      "Fernando Alonso-Fernandez",
      "Stefan Karlsson",
      "Josef Bigun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13885"
  },
  {
    "id": "arXiv:2210.13887",
    "title": "High-Throughput Flexible Belief Propagation List Decoder for Polar Codes",
    "abstract": "Due to its high parallelism, belief propagation (BP) decoding can be\nimplemented with high throughput and is a promising solution to meet the\nultra-high peak date rate requirement of future communication systems. However,\nfor polar codes, the error-correcting performance of BP decoding is far\ninferior to that of widely used CRC-aided successive cancellation list (SCL)\ndecoding algorithm. To close the performance gap to SCL, BP list (BPL) decoding\nexpands the exploration of candidate codewords through multiple permuted factor\ngraphs (PFGs). From an implementation perspective, designing a unified and\nflexible hardware architecture of BPL decoding that supports different PFGs and\nvarious code configurations is challenging. In this paper, we propose the first\nhardware implementation of a BPL decoder for polar codes and overcome the\nimplementation challenge by applying a hardware-friendly algorithm that\ngenerates flexible permutations on the fly. First, we derive the permutation\nselection gain and provide a sequential generation (SG) algorithm to obtain a\nnear-optimal PFG set. We further prove that any permutation can be decomposed\ninto a combination of multiple fixed routings, and we design a low-complexity\npermutation network to satisfy the decoding schedule. Our BPL decoder not only\nhas a low decoding latency by executing the decoding and permutation generation\nin parallel, but also supports an arbitrary list size without any area\noverhead. Experimental results show that, for length-${1024}$ polar codes with\na code of one-half, our BPL decoder with a list size ${\\mathbb{L}=32}$ has a\nsimilar error-correcting performance to SCL with ${\\mathbb{L}=4}$ and achieves\na throughput of ${25.63}$ Gbps and an area efficiency of ${29.46}$\nGbps/mm${^2}$ at SNR ${=4.0}$ dB, which is $1.99\\times$ and $7.08\\times$ faster\nthan the state-of-the-art BP flip and SCL decoders, respectively.",
    "descriptor": "",
    "authors": [
      "Yuqing Ren",
      "Yifei Shen",
      "Leyu Zhang",
      "Andreas Toftegaard Kristensen",
      "Alexios Balatsoukas-Stimming",
      "Andreas Burg",
      "Chuan Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.13887"
  },
  {
    "id": "arXiv:2210.13890",
    "title": "Fast multi-encoding to reduce the cost of video streaming",
    "abstract": "The growth in video Internet traffic and advancements in video attributes\nsuch as framerate, resolution, and bit-depth boost the demand to devise a\nlarge-scale, highly efficient video encoding environment. This is even more\nessential for Dynamic Adaptive Streaming over HTTP (DASH)-based content\nprovisioning as it requires encoding numerous representations of the same video\ncontent. High Efficiency Video Coding (HEVC) is one standard video codec that\nsignificantly improves encoding efficiency over its predecessor Advanced Video\nCoding (AVC). This improvement is achieved at the expense of significantly\nincreased time complexity, which is a challenge for content and service\nproviders. As various representations are the same video content encoded at\ndifferent bitrates or resolutions, the encoding analysis information from the\nalready encoded representations can be shared to accelerate the encoding of\nother representations. Several state-of-the-art schemes first encode a single\nrepresentation, called a reference representation. During this encoding, the\nencoder creates analysis metadata with information such as the slicetype\ndecisions, CU, PU, TU partitioning, and the HEVC bitstream itself. The\nremaining representations, called dependent representations, analyze the above\nmetadata and then reuse it to skip searching some partitioning, thus, reducing\nthe computational complexity. With the emergence of cloud-based encoding\nservices, video encoding is accelerated by utilizing an increased number of\nresources, i.e., with multi-core CPUs, multiple representations can be encoded\nin parallel. This paper presents an overview of a wide range of multi-encoding\nschemes with and without the support of machine learning approaches integrated\ninto the HEVC Test Model (HM) and x265, respectively.",
    "descriptor": "\nComments: Accepted in IBC2022\n",
    "authors": [
      "Hadi Amirpour",
      "Vignesh V Menon",
      "Ekrem \u00c7etinkaya",
      "Adithyan Ilangovan",
      "Christian Feldmann",
      "Martin Smole",
      "Christian Timmerer"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.13890"
  },
  {
    "id": "arXiv:2210.13891",
    "title": "Predicting Survival Outcomes in the Presence of Unlabeled Data",
    "abstract": "Many clinical studies require the follow-up of patients over time. This is\nchallenging: apart from frequently observed drop-out, there are often also\norganizational and financial challenges, which can lead to reduced data\ncollection and, in turn, can complicate subsequent analyses. In contrast, there\nis often plenty of baseline data available of patients with similar\ncharacteristics and background information, e.g., from patients that fall\noutside the study time window. In this article, we investigate whether we can\nbenefit from the inclusion of such unlabeled data instances to predict accurate\nsurvival times. In other words, we introduce a third level of supervision in\nthe context of survival analysis, apart from fully observed and censored\ninstances, we also include unlabeled instances. We propose three approaches to\ndeal with this novel setting and provide an empirical comparison over fifteen\nreal-life clinical and gene expression survival datasets. Our results\ndemonstrate that all approaches are able to increase the predictive performance\nover independent test data. We also show that integrating the partial\nsupervision provided by censored data in a semi-supervised wrapper approach\ngenerally provides the best results, often achieving high improvements,\ncompared to not using unlabeled data.",
    "descriptor": "",
    "authors": [
      "Fateme Nateghi Haredasht",
      "Celine Vens"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13891"
  },
  {
    "id": "arXiv:2210.13898",
    "title": "SepLL: Separating Latent Class Labels from Weak Supervision Noise",
    "abstract": "In the weakly supervised learning paradigm, labeling functions automatically\nassign heuristic, often noisy, labels to data samples. In this work, we provide\na method for learning from weak labels by separating two types of complementary\ninformation associated with the labeling functions: information related to the\ntarget label and information specific to one labeling function only. Both types\nof information are reflected to different degrees by all labeled instances. In\ncontrast to previous works that aimed at correcting or removing wrongly labeled\ninstances, we learn a branched deep model that uses all data as-is, but splits\nthe labeling function information in the latent space. Specifically, we propose\nthe end-to-end model SepLL which extends a transformer classifier by\nintroducing a latent space for labeling function specific and task-specific\ninformation. The learning signal is only given by the labeling functions\nmatches, no pre-processing or label model is required for our method. Notably,\nthe task prediction is made from the latent layer without any direct task\nsignal. Experiments on Wrench text classification tasks show that our model is\ncompetitive with the state-of-the-art, and yields a new best average\nperformance.",
    "descriptor": "",
    "authors": [
      "Andreas Stephan",
      "Vasiliki Kougia",
      "Benjamin Roth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13898"
  },
  {
    "id": "arXiv:2210.13900",
    "title": "Deep nurbs -- admissible neural networks",
    "abstract": "In this study, we propose a new numerical scheme for physics-informed neural\nnetworks (PINNs) that enables precise and inexpensive solution for partial\ndifferential equations (PDEs) in case of arbitrary geometries while strictly\nenforcing Dirichlet boundary conditions. The proposed approach combines\nadmissible NURBS parametrizations required to define the physical domain and\nthe Dirichlet boundary conditions with a PINN solver. The fundamental boundary\nconditions are automatically satisfied in this novel Deep NURBS framework. We\nverified our new approach using two-dimensional elliptic PDEs when considering\narbitrary geometries, including non-Lipschitz domains. Compared to the\nclassical PINN solver, the Deep NURBS estimator has a remarkably high\nconvergence rate for all the studied problems. Moreover, a desirable accuracy\nwas realized for most of the studied PDEs using only one hidden layer of neural\nnetworks. This novel approach is considered to pave the way for more effective\nsolutions for high-dimensional problems by allowing for more realistic\nphysics-informed statistical learning to solve PDE-based variational problems.",
    "descriptor": "\nComments: 16 pages, 13 figures\n",
    "authors": [
      "Hamed Saidaoui",
      "Luis Espath",
      "R\u00e1ul Tempone"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13900"
  },
  {
    "id": "arXiv:2210.13901",
    "title": "A Novel Approach for Dimensionality Reduction and Classification of  Hyperspectral Images based on Normalized Synergy",
    "abstract": "During the last decade, hyperspectral images have attracted increasing\ninterest from researchers worldwide. They provide more detailed information\nabout an observed area and allow an accurate target detection and precise\ndiscrimination of objects compared to classical RGB and multispectral images.\nDespite the great potentialities of hyperspectral technology, the analysis and\nexploitation of the large volume data remain a challenging task. The existence\nof irrelevant redundant and noisy images decreases the classification accuracy.\nAs a result, dimensionality reduction is a mandatory step in order to select a\nminimal and effective images subset. In this paper, a new filter approach\nnormalized mutual synergy (NMS) is proposed in order to detect relevant bands\nthat are complementary in the class prediction better than the original\nhyperspectral cube data. The algorithm consists of two steps: images selection\nthrough normalized synergy information and pixel classification. The proposed\napproach measures the discriminative power of the selected bands based on a\ncombination of their maximal normalized synergic information, minimum\nredundancy and maximal mutual information with the ground truth. A comparative\nstudy using the support vector machine (SVM) and k-nearest neighbor (KNN)\nclassifiers is conducted to evaluate the proposed approach compared to the\nstate of art band selection methods. Experimental results on three benchmark\nhyperspectral images proposed by the NASA \"Aviris Indiana Pine\", \"Salinas\" and\n\"Pavia University\" demonstrated the robustness, effectiveness and the\ndiscriminative power of the proposed approach over the literature approaches.\nKeywords: Hyperspectral images; target detection; pixel classification;\ndimensionality reduction; band selection; information theory; mutual\ninformation; normalized synergy",
    "descriptor": "",
    "authors": [
      "Asma Elmaizi",
      "Hasna Nhaila",
      "Elkebir Sarhrouni",
      "Ahmed Hammouch",
      "Nacir Chafik"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13901"
  },
  {
    "id": "arXiv:2210.13904",
    "title": "MICP-L: Fast parallel simulative Range Sensor to Mesh registration for  Robot Localization",
    "abstract": "Triangle mesh-based maps have proven to be a powerful 3D representation of\nthe environment, allowing robots to navigate using universal methods, indoors\nas well as in challenging outdoor environments with tunnels, hills and varying\nslopes. However, any robot that navigates autonomously necessarily requires\nstable, accurate, and continuous localization in such a mesh map where it plans\nits paths and missions. We present MICP-L, a novel and very fast \\textit{Mesh\nICP Localization} method that can register one or more range sensors directly\non a triangle mesh map to continuously localize a robot, determining its 6D\npose in the map. Correspondences between a range sensor and the mesh are found\nthrough simulations accelerated with the latest RTX hardware. With MICP-L, a\ncorrection can be performed quickly and in parallel even with combined data\nfrom different range sensor models. With this work, we aim to significantly\nadvance the development in the field of mesh-based environment representation\nfor autonomous robotic applications. MICP-L is open source and fully integrated\nwith ROS and tf.",
    "descriptor": "",
    "authors": [
      "Alexander Mock",
      "Sebastian P\u00fctz",
      "Thomas Wiemann",
      "Joachim Hertzberg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13904"
  },
  {
    "id": "arXiv:2210.13905",
    "title": "Confidence-Calibrated Face and Kinship Verification",
    "abstract": "In this paper, we investigate the problem of predictive confidence in face\nand kinship verification. Most existing face and kinship verification methods\nfocus on accuracy performance while ignoring confidence estimation for their\nprediction results. However, confidence estimation is essential for modeling\nreliability in such high-risk tasks. To address this issue, we first introduce\na novel yet simple confidence measure for face and kinship verification, which\nallows the verification models to transform the similarity score into a\nconfidence score for a given face pair. We further propose a\nconfidence-calibrated approach called angular scaling calibration (ASC). ASC is\neasy to implement and can be directly applied to existing face and kinship\nverification models without model modifications, yielding accuracy-preserving\nand confidence-calibrated probabilistic verification models. To the best of our\nknowledge, our approach is the first general confidence-calibrated solution to\nface and kinship verification in a modern context. We conduct extensive\nexperiments on four widely used face and kinship verification datasets, and the\nresults demonstrate the effectiveness of our approach.",
    "descriptor": "\nComments: 11 pages, 7 figures, and 8 tables\n",
    "authors": [
      "Min Xu",
      "Ximiao Zhang",
      "Xiuzhuang Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13905"
  },
  {
    "id": "arXiv:2210.13906",
    "title": "One-shot, Offline and Production-Scalable PID Optimisation with Deep  Reinforcement Learning",
    "abstract": "Proportional-integral-derivative (PID) control underlies more than $97\\%$ of\nautomated industrial processes. Controlling these processes effectively with\nrespect to some specified set of performance goals requires finding an optimal\nset of PID parameters to moderate the PID loop. Tuning these parameters is a\nlong and exhaustive process. A method (patent pending) based on deep\nreinforcement learning is presented that learns a relationship between generic\nsystem properties (e.g. resonance frequency), a multi-objective performance\ngoal and optimal PID parameter values. Performance is demonstrated in the\ncontext of a real optical switching product of the foremost manufacturer of\nsuch devices globally. Switching is handled by piezoelectric actuators where\nswitching time and optical loss are derived from the speed and stability of\nactuator-control processes respectively. The method achieves a $5\\times$\nimprovement in the number of actuators that fall within the most challenging\ntarget switching speed, $\\geq 20\\%$ improvement in mean switching speed at the\nsame optical loss and $\\geq 75\\%$ reduction in performance inconsistency when\ntemperature varies between 5 and 73 degrees celcius. Furthermore, once trained\n(which takes $\\mathcal{O}(hours)$), the model generates actuator-unique PID\nparameters in a one-shot inference process that takes $\\mathcal{O}(ms)$ in\ncomparison to up to $\\mathcal{O}(week)$ required for conventional tuning\nmethods, therefore accomplishing these performance improvements whilst\nachieving up to a $10^6\\times$ speed-up. After training, the method can be\napplied entirely offline, incurring effectively zero optimisation-overhead in\nproduction.",
    "descriptor": "\nComments: 26 pages, 7 figures\n",
    "authors": [
      "Zacharaya Shabka",
      "Michael Enrico",
      "Nick Parsons",
      "Georgios Zervas"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13906"
  },
  {
    "id": "arXiv:2210.13907",
    "title": "Finding Early Adopters of Innovation in Social Network",
    "abstract": "Social networks play a fundamental role in the diffusion of innovation\nthrough peers' influence on adoption. Thus, network position including a wide\nrange of network centrality measures have been used to describe individuals'\naffinity to adopt an innovation and their ability to propagate diffusion. Yet,\nsocial networks are assortative in terms of susceptibility and influence and in\nterms of network centralities as well. This makes the identification of\ninfluencers difficult especially since susceptibility and centrality does not\nalways go hand in hand. Here we propose the Top Candidate algorithm, an expert\nrecommendation method, to rank individuals based on their perceived expertise,\nwhich resonates well with the assortative nature of innovators and early\nadopters. Leveraging adoption data from two online social networks that are\nassortative in terms of adoption but represent different levels of\nassortativity of network centralities, we demonstrate that the Top Candidate\nranking is more efficient in capturing early adopters than other widely used\nindices. Top Candidate nodes adopt earlier and have higher reach among\ninnovators, early adopters and early majority than nodes highlighted by other\nmethods. These results suggest that the Top Candidate method can identify good\nseeds for influence maximization campaigns on social networks.",
    "descriptor": "",
    "authors": [
      "Bal\u00e1zs R. Sziklai",
      "Bal\u00e1zs Lengyel"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.13907"
  },
  {
    "id": "arXiv:2210.13908",
    "title": "Quasistatic contact-rich manipulation via linear complementarity  quadratic programming",
    "abstract": "Contact-rich manipulation is challenging due to dynamically-changing physical\nconstraints by the contact mode changes undergone during manipulation. This\npaper proposes a versatile local planning and control framework for\ncontact-rich manipulation that determines the continuous control action under\nvariable contact modes online. We model the physical characteristics of\ncontact-rich manipulation by quasistatic dynamics and complementarity\nconstraints. We then propose a linear complementarity quadratic program (LCQP)\nto efficiently determine the control action that implicitly includes the\ndecisions on the contact modes under these constraints. In the LCQP, we relax\nthe complementarity constraints to alleviate ill-conditioned problems that are\ntypically caused by measure noises or model miss-matches. We conduct dynamical\nsimulations on a 3D physical simulator and demonstrate that the proposed method\ncan achieve various contact-rich manipulation tasks by determining the control\naction including the contact modes in real-time.",
    "descriptor": "\nComments: 8 pages, 7 figures. This work has been accepted to be presented at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)\n",
    "authors": [
      "Sotaro Katayama",
      "Tatsunori Taniai",
      "Kazutoshi Tanaka"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.13908"
  },
  {
    "id": "arXiv:2210.13915",
    "title": "Towards Formal Approximated Minimal Explanations of Neural Networks",
    "abstract": "With the rapid growth of machine learning, deep neural networks (DNNs) are\nnow being used in numerous domains. Unfortunately, DNNs are \"black-boxes\", and\ncannot be interpreted by humans, which is a substantial concern in\nsafety-critical systems. To mitigate this issue, researchers have begun working\non explainable AI (XAI) methods, which can identify a subset of input features\nthat are the cause of a DNN's decision for a given input. Most existing\ntechniques are heuristic, and cannot guarantee the correctness of the\nexplanation provided. In contrast, recent and exciting attempts have shown that\nformal methods can be used to generate provably correct explanations. Although\nthese methods are sound, the computational complexity of the underlying\nverification problem limits their scalability; and the explanations they\nproduce might sometimes be overly complex. Here, we propose a novel approach to\ntackle these limitations. We (1) suggest an efficient, verification-based\nmethod for finding minimal explanations, which constitute a provable\napproximation of the global, minimum explanation; (2) show how DNN verification\ncan assist in calculating lower and upper bounds on the optimal explanation;\n(3) propose heuristics that significantly improve the scalability of the\nverification process; and (4) suggest the use of bundles, which allows us to\narrive at more succinct and interpretable explanations. Our evaluation shows\nthat our approach significantly outperforms state-of-the-art techniques, and\nproduces explanations that are more useful to humans. We thus regard this work\nas a step toward leveraging verification technology in producing DNNs that are\nmore reliable and comprehensible.",
    "descriptor": "",
    "authors": [
      "Shahaf Bassan",
      "Guy Katz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.13915"
  },
  {
    "id": "arXiv:2210.13917",
    "title": "Connective Reconstruction-based Novelty Detection",
    "abstract": "Detection of out-of-distribution samples is one of the critical tasks for\nreal-world applications of computer vision. The advancement of deep learning\nhas enabled us to analyze real-world data which contain unexplained samples,\naccentuating the need to detect out-of-distribution instances more than before.\nGAN-based approaches have been widely used to address this problem due to their\nability to perform distribution fitting; however, they are accompanied by\ntraining instability and mode collapse. We propose a simple yet efficient\nreconstruction-based method that avoids adding complexities to compensate for\nthe limitations of GAN models while outperforming them. Unlike previous\nreconstruction-based works that only utilize reconstruction error or generated\nsamples, our proposed method simultaneously incorporates both of them in the\ndetection task. Our model, which we call \"Connective Novelty Detection\" has two\nsubnetworks, an autoencoder, and a binary classifier. The autoencoder learns\nthe representation of the positive class by reconstructing them. Then, the\nmodel creates negative and connected positive examples using real and generated\nsamples. Negative instances are generated via manipulating the real data, so\ntheir distribution is close to the positive class to achieve a more accurate\nboundary for the classifier. To boost the robustness of the detection to\nreconstruction error, connected positive samples are created by combining the\nreal and generated samples. Finally, the binary classifier is trained using\nconnected positive and negative examples. We demonstrate a considerable\nimprovement in novelty detection over state-of-the-art methods on MNIST and\nCaltech-256 datasets.",
    "descriptor": "",
    "authors": [
      "Seyyed Morteza Hashemi",
      "Parvaneh Aliniya",
      "Parvin Razzaghi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13917"
  },
  {
    "id": "arXiv:2210.13918",
    "title": "Differentially Private Language Models for Secure Data Sharing",
    "abstract": "To protect the privacy of individuals whose data is being shared, it is of\nhigh importance to develop methods allowing researchers and companies to\nrelease textual data while providing formal privacy guarantees to its\noriginators. In the field of NLP, substantial efforts have been directed at\nbuilding mechanisms following the framework of local differential privacy,\nthereby anonymizing individual text samples before releasing them. In practice,\nthese approaches are often dissatisfying in terms of the quality of their\noutput language due to the strong noise required for local differential\nprivacy. In this paper, we approach the problem at hand using global\ndifferential privacy, particularly by training a generative language model in a\ndifferentially private manner and consequently sampling data from it. Using\nnatural language prompts and a new prompt-mismatch loss, we are able to create\nhighly accurate and fluent textual datasets taking on specific desired\nattributes such as sentiment or topic and resembling statistical properties of\nthe training data. We perform thorough experiments indicating that our\nsynthetic datasets do not leak information from our original data and are of\nhigh language quality and highly suitable for training models for further\nanalysis on real-world data. Notably, we also demonstrate that training\nclassifiers on private synthetic data outperforms directly training classifiers\non real data with DP-SGD.",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Justus Mattern",
      "Zhijing Jin",
      "Benjamin Weggenmann",
      "Bernhard Schoelkopf",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13918"
  },
  {
    "id": "arXiv:2210.13923",
    "title": "A Comparative Attention Framework for Better Few-Shot Object Detection  on Aerial Images",
    "abstract": "Few-Shot Object Detection (FSOD) methods are mainly designed and evaluated on\nnatural image datasets such as Pascal VOC and MS COCO. However, it is not clear\nwhether the best methods for natural images are also the best for aerial\nimages. Furthermore, direct comparison of performance between FSOD methods is\ndifficult due to the wide variety of detection frameworks and training\nstrategies. Therefore, we propose a benchmarking framework that provides a\nflexible environment to implement and compare attention-based FSOD methods. The\nproposed framework focuses on attention mechanisms and is divided into three\nmodules: spatial alignment, global attention, and fusion layer. To remain\ncompetitive with existing methods, which often leverage complex training, we\npropose new augmentation techniques designed for object detection. Using this\nframework, several FSOD methods are reimplemented and compared. This comparison\nhighlights two distinct performance regimes on aerial and natural images: FSOD\nperforms worse on aerial images. Our experiments suggest that small objects,\nwhich are harder to detect in the few-shot setting, account for the poor\nperformance. Finally, we develop a novel multiscale alignment method,\nCross-Scales Query-Support Alignment (XQSA) for FSOD, to improve the detection\nof small objects. XQSA outperforms the state-of-the-art significantly on DOTA\nand DIOR.",
    "descriptor": "",
    "authors": [
      "Pierre Le Jeune",
      "Anissa Mokraoui"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13923"
  },
  {
    "id": "arXiv:2210.13927",
    "title": "Deep Crowd Anomaly Detection: State-of-the-Art, Challenges, and Future  Research Directions",
    "abstract": "Crowd anomaly detection is one of the most popular topics in computer vision\nin the context of smart cities. A plethora of deep learning methods have been\nproposed that generally outperform other machine learning solutions. Our review\nprimarily discusses algorithms that were published in mainstream conferences\nand journals between 2020 and 2022. We present datasets that are typically used\nfor benchmarking, produce a taxonomy of the developed algorithms, and discuss\nand compare their performances. Our main findings are that the heterogeneities\nof pre-trained convolutional models have a negligible impact on crowd video\nanomaly detection performance. We conclude our discussion with fruitful\ndirections for future research.",
    "descriptor": "",
    "authors": [
      "Md. Haidar Sharif",
      "Lei Jiao",
      "Christian W. Omlin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13927"
  },
  {
    "id": "arXiv:2210.13929",
    "title": "Control and Evaluation of Event Cameras Output Sharpness via Bias",
    "abstract": "Event cameras also known as neuromorphic sensors are relatively a new\ntechnology with some privilege over the RGB cameras. The most important one is\ntheir difference in capturing the light changes in the environment, each pixel\nchanges independently from the others when it captures a change in the\nenvironment light. To increase the users degree of freedom in controlling the\noutput of these cameras, such as changing the sensitivity of the sensor to\nlight changes, controlling the number of generated events and other similar\noperations, the camera manufacturers usually introduce some tools to make\nsensor level changes in camera settings. The contribution of this research is\nto examine and document the effects of changing the sensor settings on the\nsharpness as an indicator of quality of the generated stream of event data. To\nhave a qualitative understanding this stream of event is converted to frames,\nthen the average image gradient magnitude as an index of the number of edges\nand accordingly sharpness is calculated for these frames. Five different bias\nsettings are explained and the effect of their change in the event output is\nsurveyed and analyzed. In addition, the operation of the event camera sensing\narray is explained with an analogue circuit model and the functions of the bias\nfoundations are linked with this model.",
    "descriptor": "",
    "authors": [
      "Mehdi Sefidgar Dilmaghani",
      "Waseem Shariff",
      "Cian Ryan",
      "Joe Lemley",
      "Peter Corcoran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13929"
  },
  {
    "id": "arXiv:2210.13932",
    "title": "CoLoC: Conditioned Localizer and Classifier for Sound Event Localization  and Detection",
    "abstract": "In this article, we describe Conditioned Localizer and Classifier (CoLoC)\nwhich is a novel solution for Sound Event Localization and Detection (SELD).\nThe solution constitutes of two stages: the localization is done first and is\nfollowed by classification conditioned by the output of the localizer. In order\nto resolve the problem of the unknown number of sources we incorporate the idea\nborrowed from Sequential Set Generation (SSG). Models from both stages are\nSELDnet-like CRNNs, but with single outputs. Conducted reasoning shows that\nsuch two single-output models are fit for SELD task. We show that our solution\nimproves on the baseline system in most metrics on the STARSS22 Dataset.",
    "descriptor": "\nComments: 5 pages, conference\n",
    "authors": [
      "S\u0142awomir Kapka",
      "Jakub Tkaczuk"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13932"
  },
  {
    "id": "arXiv:2210.13937",
    "title": "Multi-Fidelity Bayesian Optimization with Unreliable Information Sources",
    "abstract": "Bayesian optimization (BO) is a powerful framework for optimizing black-box,\nexpensive-to-evaluate functions. Over the past decade, many algorithms have\nbeen proposed to integrate cheaper, lower-fidelity approximations of the\nobjective function into the optimization process, with the goal of converging\ntowards the global optimum at a reduced cost. This task is generally referred\nto as multi-fidelity Bayesian optimization (MFBO). However, MFBO algorithms can\nlead to higher optimization costs than their vanilla BO counterparts,\nespecially when the low-fidelity sources are poor approximations of the\nobjective function, therefore defeating their purpose. To address this issue,\nwe propose rMFBO (robust MFBO), a methodology to make any GP-based MFBO scheme\nrobust to the addition of unreliable information sources. rMFBO comes with a\ntheoretical guarantee that its performance can be bound to its vanilla BO\nanalog, with high controllable probability. We demonstrate the effectiveness of\nthe proposed methodology on a number of numerical benchmarks, outperforming\nearlier MFBO methods on unreliable sources. We expect rMFBO to be particularly\nuseful to reliably include human experts with varying knowledge within BO\nprocesses.",
    "descriptor": "",
    "authors": [
      "Petrus Mikkola",
      "Julien Martinelli",
      "Louis Filstroff",
      "Samuel Kaski"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13937"
  },
  {
    "id": "arXiv:2210.13938",
    "title": "Dual Mechanism Priming Effects in Hindi Word Order",
    "abstract": "Word order choices during sentence production can be primed by preceding\nsentences. In this work, we test the DUAL MECHANISM hypothesis that priming is\ndriven by multiple different sources. Using a Hindi corpus of text productions,\nwe model lexical priming with an n-gram cache model and we capture more\nabstract syntactic priming with an adaptive neural language model. We permute\nthe preverbal constituents of corpus sentences, and then use a logistic\nregression model to predict which sentences actually occurred in the corpus\nagainst artificially generated meaning-equivalent variants. Our results\nindicate that lexical priming and lexically-independent syntactic priming\naffect complementary sets of verb classes. By showing that different priming\ninfluences are separable from one another, our results support the hypothesis\nthat multiple different cognitive mechanisms underlie priming.",
    "descriptor": "\nComments: Accepted to AACL 2022\n",
    "authors": [
      "Sidharth Ranjan",
      "Marten van Schijndel",
      "Sumeet Agarwal",
      "Rajakrishnan Rajkumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.13938"
  },
  {
    "id": "arXiv:2210.13940",
    "title": "Discourse Context Predictability Effects in Hindi Word Order",
    "abstract": "We test the hypothesis that discourse predictability influences Hindi\nsyntactic choice. While prior work has shown that a number of factors (e.g.,\ninformation status, dependency length, and syntactic surprisal) influence Hindi\nword order preferences, the role of discourse predictability is underexplored\nin the literature. Inspired by prior work on syntactic priming, we investigate\nhow the words and syntactic structures in a sentence influence the word order\nof the following sentences. Specifically, we extract sentences from the\nHindi-Urdu Treebank corpus (HUTB), permute the preverbal constituents of those\nsentences, and build a classifier to predict which sentences actually occurred\nin the corpus against artificially generated distractors. The classifier uses a\nnumber of discourse-based features and cognitive features to make its\npredictions, including dependency length, surprisal, and information status. We\nfind that information status and LSTM-based discourse predictability influence\nword order choices, especially for non-canonical object-fronted orders. We\nconclude by situating our results within the broader syntactic priming\nliterature.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Sidharth Ranjan",
      "Marten van Schijndel",
      "Sumeet Agarwal",
      "Rajakrishnan Rajkumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.13940"
  },
  {
    "id": "arXiv:2210.13941",
    "title": "Structural analysis of water networks",
    "abstract": "Liquid water, besides being fundamental for life on Earth, has long\nfascinated scientists due to several anomalies. Different hypotheses have been\nput forward to explain these peculiarities. The most accredited one foresees\nthe presence in the supercooled region of two phases at different densities:\nthe low-density liquid phase and the high-density liquid phase. In our previous\nwork [Faccio et al., J. Mol. Liq. 355 (2022): 118922], we showed that it is\npossible to identify these two forms in water networks through a computational\napproach based on molecular dynamics simulation and on the calculation of the\ntotal communicability of the associated graph, in which the nodes correspond to\nwater molecules and the edges represent the connections (interactions) between\nmolecules. In this paper, we present a more in-depth investigation of the\napplication of graph-theory based approaches to the analysis of the structure\nof water networks. In particular, we investigate different connectivity and\ncentrality measures and we report on the use of a variety of global metrics\naimed at giving a topological and geometrical characterization of liquid water.",
    "descriptor": "\nComments: 25 pages, 22 figures\n",
    "authors": [
      "Michele Benzi",
      "Isabella Daidone",
      "Chiara Faccio",
      "Laura Zanetti-Polzi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Soft Condensed Matter (cond-mat.soft)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.13941"
  },
  {
    "id": "arXiv:2210.13942",
    "title": "Entity Divider with Language Grounding in Multi-Agent Reinforcement  Learning",
    "abstract": "We investigate the use of natural language to drive the generalization of\npolicies in multi-agent settings. Unlike single-agent settings, the\ngeneralization of policies should also consider the influence of other agents.\nBesides, with the increasing number of entities in multi-agent settings, more\nagent-entity interactions are needed for language grounding, and the enormous\nsearch space could impede the learning process. Moreover, given a simple\ngeneral instruction,e.g., beating all enemies, agents are required to decompose\nit into multiple subgoals and figure out the right one to focus on. Inspired by\nprevious work, we try to address these issues at the entity level and propose a\nnovel framework for language grounding in multi-agent reinforcement learning,\nentity divider (EnDi). EnDi enables agents to independently learn subgoal\ndivision at the entity level and act in the environment based on the associated\nentities. The subgoal division is regularized by opponent modeling to avoid\nsubgoal conflicts and promote coordinated strategies. Empirically, EnDi\ndemonstrates the strong generalization ability to unseen games with new\ndynamics and expresses the superiority over existing methods.",
    "descriptor": "",
    "authors": [
      "Ziluo Ding",
      "Wanpeng Zhang",
      "Junpeng Yue",
      "Xiangjun Wang",
      "Tiejun Huang",
      "Zongqing Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.13942"
  },
  {
    "id": "arXiv:2210.13944",
    "title": "A Survey on Artificial Intelligence for Music Generation: Agents,  Domains and Perspectives",
    "abstract": "Music is one of the Gardner's intelligences in his theory of multiple\nintelligences. How humans perceive and understand music is still being studied\nand is crucial to develop artificial intelligence models that imitate such\nprocesses. Music generation with Artificial Intelligence is an emerging field\nthat is gaining much attention in the recent years. In this paper, we describe\nhow humans compose music and how new AI systems could imitate such process by\ncomparing past and recent advances in the field with music composition\ntechniques. To understand how AI models and algorithms generate music and the\npotential applications that might appear in the future, we explore, analyze and\ndescribe the agents that take part of the music generation process: the\ndatasets, models, interfaces, the users and the generated music. We mention\npossible applications that might benefit from this field and we also propose\nnew trends and future research directions that could be explored in the future.",
    "descriptor": "\nComments: Submnitted to IEEE\n",
    "authors": [
      "Carlos Hernandez-Olivan",
      "Javier Hernandez-Olivan",
      "Jose R. Beltran"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13944"
  },
  {
    "id": "arXiv:2210.13945",
    "title": "Comparing neural network training performance between Elixir and Python",
    "abstract": "With a wide range of libraries focused on the machine learning market, such\nas TensorFlow, NumPy, Pandas, Keras, and others, Python has made a name for\nitself as one of the main programming languages. In February 2021, Jos\\'e Valim\nand Sean Moriarity published the first version of the Numerical Elixir (Nx)\nlibrary, a library for tensor operations written in Elixir. Nx aims to allow\nthe language be a good choice for GPU-intensive operations. This work aims to\ncompare the results of Python and Elixir on training convolutional neural\nnetworks (CNN) using MNIST and CIFAR-10 datasets, concluding that Python\nachieved overall better results, and that Elixir is already a viable\nalternative.",
    "descriptor": "",
    "authors": [
      "Lucas C. Tavano",
      "Lucas K. Amin",
      "Adolfo Gustavo Serra-Seca-Neto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.13945"
  },
  {
    "id": "arXiv:2210.13950",
    "title": "Pointly-Supervised Panoptic Segmentation",
    "abstract": "In this paper, we propose a new approach to applying point-level annotations\nfor weakly-supervised panoptic segmentation. Instead of the dense pixel-level\nlabels used by fully supervised methods, point-level labels only provide a\nsingle point for each target as supervision, significantly reducing the\nannotation burden. We formulate the problem in an end-to-end framework by\nsimultaneously generating panoptic pseudo-masks from point-level labels and\nlearning from them. To tackle the core challenge, i.e., panoptic pseudo-mask\ngeneration, we propose a principled approach to parsing pixels by minimizing\npixel-to-point traversing costs, which model semantic similarity, low-level\ntexture cues, and high-level manifold knowledge to discriminate panoptic\ntargets. We conduct experiments on the Pascal VOC and the MS COCO datasets to\ndemonstrate the approach's effectiveness and show state-of-the-art performance\nin the weakly-supervised panoptic segmentation problem. Codes are available at\nhttps://github.com/BraveGroup/PSPS.git.",
    "descriptor": "\nComments: Accepted to ECCV 2022\n",
    "authors": [
      "Junsong Fan",
      "Zhaoxiang Zhang",
      "Tieniu Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13950"
  },
  {
    "id": "arXiv:2210.13952",
    "title": "KnowGL: Knowledge Generation and Linking from Text",
    "abstract": "We propose KnowGL, a tool that allows converting text into structured\nrelational data represented as a set of ABox assertions compliant with the TBox\nof a given Knowledge Graph (KG), such as Wikidata. We address this problem as a\nsequence generation task by leveraging pre-trained sequence-to-sequence\nlanguage models, e.g. BART. Given a sentence, we fine-tune such models to\ndetect pairs of entity mentions and jointly generate a set of facts consisting\nof the full set of semantic annotations for a KG, such as entity labels, entity\ntypes, and their relationships. To showcase the capabilities of our tool, we\nbuild a web application consisting of a set of UI widgets that help users to\nnavigate through the semantic data extracted from a given input text. We make\nthe KnowGL model available at https://huggingface.co/ibm/knowgl-large.",
    "descriptor": "\nComments: AAAI-23 Demo Track\n",
    "authors": [
      "Gaetano Rossiello",
      "Faisal Chowdhury",
      "Nandana Mihindukulasooriya",
      "Owen Cornec",
      "Alfio Gliozzo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.13952"
  },
  {
    "id": "arXiv:2210.13954",
    "title": "I Prefer not to Say: Operationalizing Fair and User-guided Data  Minimization",
    "abstract": "To grant users greater authority over their personal data, policymakers have\nsuggested tighter data protection regulations (e.g., GDPR, CCPA). One key\nprinciple within these regulations is data minimization, which urges companies\nand institutions to only collect data that is relevant and adequate for the\npurpose of the data analysis. In this work, we take a user-centric perspective\non this regulation, and let individual users decide which data they deem\nadequate and relevant to be processed by a machine-learned model. We require\nthat users who decide to provide optional information should appropriately\nbenefit from sharing their data, while users who rely on the mandate to leave\ntheir data undisclosed should not be penalized for doing so. This gives rise to\nthe overlooked problem of fair treatment between individuals providing\nadditional information and those choosing not to. While the classical fairness\nliterature focuses on fair treatment between advantaged and disadvantaged\ngroups, an initial look at this problem through the lens of classical fairness\nnotions reveals that they are incompatible with these desiderata. We offer a\nsolution to this problem by proposing the notion of Optional Feature Fairness\n(OFF) that follows from our requirements. To operationalize OFF, we derive a\nmulti-model strategy and a tractable logistic regression model. We analyze the\neffect and the cost of applying OFF on several real-world data sets.",
    "descriptor": "\nComments: NeurIPS 2022 Workshop on Algorithmic Fairness through the Lens of Causality and Privacy (AFCP)\n",
    "authors": [
      "Tobias Leemann",
      "Martin Pawelczyk",
      "Christian Thomas Eberle",
      "Gjergji Kasneci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13954"
  },
  {
    "id": "arXiv:2210.13956",
    "title": "HiddenGems: Efficient safety boundary detection with active learning",
    "abstract": "Evaluating safety performance in a resource-efficient way is crucial for the\ndevelopment of autonomous systems. Simulation of parameterized scenarios is a\npopular testing strategy but parameter sweeps can be prohibitively expensive.\nTo address this, we propose HiddenGems: a sample-efficient method for\ndiscovering the boundary between compliant and non-compliant behavior via\nactive learning. Given a parameterized scenario, one or more compliance\nmetrics, and a simulation oracle, HiddenGems maps the compliant and\nnon-compliant domains of the scenario. The methodology enables critical test\ncase identification, comparative analysis of different versions of the system\nunder test, as well as verification of design objectives. We evaluate\nHiddenGems on a scenario with a jaywalker crossing in front of an autonomous\nvehicle and obtain compliance boundary estimates for collision, lane keep, and\nacceleration metrics individually and in combination, with 6 times fewer\nsimulations than a parameter sweep. We also show how HiddenGems can be used to\ndetect and rectify a failure mode for an unprotected turn with 86% fewer\nsimulations.",
    "descriptor": "\nComments: Published at IROS 2022\n",
    "authors": [
      "Aleksandar Petrov",
      "Carter Fang",
      "Khang Minh Pham",
      "You Hong Eng",
      "James Guo Ming Fu",
      "Scott Drew Pendleton"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13956"
  },
  {
    "id": "arXiv:2210.13958",
    "title": "Mitigating Health Data Poverty: Generative Approaches versus Resampling  for Time-series Clinical Data",
    "abstract": "Several approaches have been developed to mitigate algorithmic bias stemming\nfrom health data poverty, where minority groups are underrepresented in\ntraining datasets. Augmenting the minority class using resampling (such as\nSMOTE) is a widely used approach due to the simplicity of the algorithms.\nHowever, these algorithms decrease data variability and may introduce\ncorrelations between samples, giving rise to the use of generative approaches\nbased on GAN. Generation of high-dimensional, time-series, authentic data that\nprovides a wide distribution coverage of the real data, remains a challenging\ntask for both resampling and GAN-based approaches. In this work we propose\nCA-GAN architecture that addresses some of the shortcomings of the current\napproaches, where we provide a detailed comparison with both SMOTE and\nWGAN-GP*, using a high-dimensional, time-series, real dataset of 3343\nhypotensive Caucasian and Black patients. We show that our approach is better\nat both generating authentic data of the minority class and remaining within\nthe original distribution of the real data.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research (Neurips 2022 SyntheticData4ML)\n",
    "authors": [
      "Raffaele Marchesi",
      "Nicolo Micheletti",
      "Giuseppe Jurman",
      "Venet Osmani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13958"
  },
  {
    "id": "arXiv:2210.13964",
    "title": "Learning to Reuse Distractors to support Multiple Choice Question  Generation in Education",
    "abstract": "Multiple choice questions (MCQs) are widely used in digital learning systems,\nas they allow for automating the assessment process. However, due to the\nincreased digital literacy of students and the advent of social media\nplatforms, MCQ tests are widely shared online, and teachers are continuously\nchallenged to create new questions, which is an expensive and time-consuming\ntask. A particularly sensitive aspect of MCQ creation is to devise relevant\ndistractors, i.e., wrong answers that are not easily identifiable as being\nwrong. This paper studies how a large existing set of manually created answers\nand distractors for questions over a variety of domains, subjects, and\nlanguages can be leveraged to help teachers in creating new MCQs, by the smart\nreuse of existing distractors. We built several data-driven models based on\ncontext-aware question and distractor representations, and compared them with\nstatic feature-based models. The proposed models are evaluated with automated\nmetrics and in a realistic user test with teachers. Both automatic and human\nevaluations indicate that context-aware models consistently outperform a static\nfeature-based approach. For our best-performing context-aware model, on average\n3 distractors out of the 10 shown to teachers were rated as high-quality\ndistractors. We create a performance benchmark, and make it public, to enable\ncomparison between different approaches and to introduce a more standardized\nevaluation of the task. The benchmark contains a test of 298 educational\nquestions covering multiple subjects & languages and a 77k multilingual pool of\ndistractor vocabulary for future research.",
    "descriptor": "\nComments: 10 pages and 4 figures\n",
    "authors": [
      "Semere Kiros Bitew",
      "Amir Hadifar",
      "Lucas Sterckx",
      "Johannes Deleu",
      "Chris Develder",
      "Thomas Demeester"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13964"
  },
  {
    "id": "arXiv:2210.13965",
    "title": "Exploring the impact of weather on Metro demand forecasting using  machine learning method",
    "abstract": "Urban rail transit provides significant comprehensive benefits such as large\ntraffic volume and high speed, serving as one of the most important components\nof urban traffic construction management and congestion solution. Using real\npassenger flow data of an Asian subway system from April to June of 2018, this\nwork analyzes the space-time distribution of the passenger flow using\nshort-term traffic flow prediction. Stations are divided into four types for\npassenger flow forecasting, and meteorological records are collected for the\nsame period. Then, machine learning methods with different inputs are applied\nand multivariate regression is performed to evaluate the improvement effect of\neach weather element on passenger flow forecasting of representative metro\nstations on hourly basis. Our results show that by inputting weather variables\nthe precision of prediction on weekends enhanced while the performance on\nweekdays only improved marginally, while the contribution of different elements\nof weather differ. Also, different categories of stations are affected\ndifferently by weather. This study provides a possible method to further\nimprove other prediction models, and attests to the promise of data-driven\nanalytics for optimization of short-term scheduling in transit management.",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Yiming Hu",
      "Yangchuan Huang",
      "Shuyin Liu",
      "Yuanyang Qi",
      "Danhui Bai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.13965"
  },
  {
    "id": "arXiv:2210.13966",
    "title": "The Debate Over Understanding in AI's Large Language Models",
    "abstract": "We survey a current, heated debate in the AI research community on whether\nlarge pre-trained language models can be said to \"understand\" language -- and\nthe physical and social situations language encodes -- in any important sense.\nWe describe arguments that have been made for and against such understanding,\nand key questions for the broader sciences of intelligence that have arisen in\nlight of these arguments. We contend that a new science of intelligence can be\ndeveloped that will provide insight into distinct modes of understanding, their\nstrengths and limitations, and the challenge of integrating diverse forms of\ncognition.",
    "descriptor": "\nComments: Under submission as a Perspective article\n",
    "authors": [
      "Melanie Mitchell",
      "David C. Krakauer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13966"
  },
  {
    "id": "arXiv:2210.13977",
    "title": "Cozie Apple: An iOS mobile and smartwatch application for environmental  quality satisfaction and physiological data collection",
    "abstract": "People spend the majority of their time indoors and environmental conditions\naffect their perceptions, performance, health, and well-being. Buildings\nshould, therefore, be designed and operated with the main objective of\nproviding comfortable environments for occupants and meeting their needs.\nHowever, in practice, occupants' perceptions and sensations are rarely used to\noptimize the operation of buildings. This can be partially explained by the\nfact that collecting feedback from occupants in a reliable and non-intrusive\nway has always been a difficult and complex task. Not considering occupants'\nneeds may result in a high number of people being dissatisfied with the quality\nof their indoor environment. We, therefore, developed Cozie an open-source\napplication for the Apple Watch and iPhone. Cozie allows people to complete a\nshort survey and provide real-time feedback about indoor environmental\nconditions via their smartwatch. This approach reduces the effort it takes for\noccupants to provide feedback. In addition, Cozie leverages the watch's inbuilt\nsensors that monitor and log several physiological parameters (e.g., heart\nrate, activity). The data collected using Cozie has, therefore, the potential\nof allowing researchers and practitioners to better understand the needs and\nperceptions of occupants in real time. This may allow them to better design and\noperate new and existing buildings. Cozie could also help to shift towards an\noccupant-centric control of buildings, that adjusts indoor environmental\nconditions in real-time utilizing collected from the occupants.",
    "descriptor": "",
    "authors": [
      "Federico Tartarini",
      "Clayton Miller",
      "Stefano Schiavon"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13977"
  },
  {
    "id": "arXiv:2210.13978",
    "title": "Boosting the Cycle Counting Power of Graph Neural Networks with  I$^2$-GNNs",
    "abstract": "Message Passing Neural Networks (MPNNs) are a widely used class of Graph\nNeural Networks (GNNs). The limited representational power of MPNNs inspires\nthe study of provably powerful GNN architectures. However, knowing one model is\nmore powerful than another gives little insight about what functions they can\nor cannot express. It is still unclear whether these models are able to\napproximate specific functions such as counting certain graph substructures,\nwhich is essential for applications in biology, chemistry and social network\nanalysis. Motivated by this, we propose to study the counting power of Subgraph\nMPNNs, a recent and popular class of powerful GNN models that extract rooted\nsubgraphs for each node, assign the root node a unique identifier and encode\nthe root node's representation within its rooted subgraph. Specifically, we\nprove that Subgraph MPNNs fail to count more-than-4-cycles at node level,\nimplying that node representations cannot correctly encode the surrounding\nsubstructures like ring systems with more than four atoms. To overcome this\nlimitation, we propose I$^2$-GNNs to extend Subgraph MPNNs by assigning\ndifferent identifiers for the root node and its neighbors in each subgraph.\nI$^2$-GNNs' discriminative power is shown to be strictly stronger than Subgraph\nMPNNs and partially stronger than the 3-WL test. More importantly, I$^2$-GNNs\nare proven capable of counting all 3, 4, 5 and 6-cycles, covering common\nsubstructures like benzene rings in organic chemistry, while still keeping\nlinear complexity. To the best of our knowledge, it is the first linear-time\nGNN model that can count 6-cycles with theoretical guarantees. We validate its\ncounting power in cycle counting tasks and demonstrate its competitive\nperformance in molecular prediction benchmarks.",
    "descriptor": "",
    "authors": [
      "Yinan Huang",
      "Xingang Peng",
      "Jianzhu Ma",
      "Muhan Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13978"
  },
  {
    "id": "arXiv:2210.13979",
    "title": "Meta-learning Pathologies from Radiology Reports using Variance Aware  Prototypical Networks",
    "abstract": "Large pretrained Transformer-based language models like BERT and GPT have\nchanged the landscape of Natural Language Processing (NLP). However, fine\ntuning such models still requires a large number of training examples for each\ntarget task, thus annotating multiple datasets and training these models on\nvarious downstream tasks becomes time consuming and expensive. In this work, we\npropose a simple extension of the Prototypical Networks for few-shot text\nclassification. Our main idea is to replace the class prototypes by Gaussians\nand introduce a regularization term that encourages the examples to be\nclustered near the appropriate class centroids. Experimental results show that\nour method outperforms various strong baselines on 13 public and 4 internal\ndatasets. Furthermore, we use the class distributions as a tool for detecting\npotential out-of-distribution (OOD) data points during deployment.",
    "descriptor": "\nComments: EMNLP'22 Industry Track\n",
    "authors": [
      "Arijit Sehanobish",
      "Kawshik Kannan",
      "Nabila Abraham",
      "Anasuya Das",
      "Benjamin Odry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13979"
  },
  {
    "id": "arXiv:2210.13982",
    "title": "Hindering Adversarial Attacks with Implicit Neural Representations",
    "abstract": "We introduce the Lossy Implicit Network Activation Coding (LINAC) defence, an\ninput transformation which successfully hinders several common adversarial\nattacks on CIFAR-$10$ classifiers for perturbations up to $\\epsilon = 8/255$ in\n$L_\\infty$ norm and $\\epsilon = 0.5$ in $L_2$ norm. Implicit neural\nrepresentations are used to approximately encode pixel colour intensities in\n$2\\text{D}$ images such that classifiers trained on transformed data appear to\nhave robustness to small perturbations without adversarial training or large\ndrops in performance. The seed of the random number generator used to\ninitialise and train the implicit neural representation turns out to be\nnecessary information for stronger generic attacks, suggesting its role as a\nprivate key. We devise a Parametric Bypass Approximation (PBA) attack strategy\nfor key-based defences, which successfully invalidates an existing method in\nthis category. Interestingly, our LINAC defence also hinders some transfer and\nadaptive attacks, including our novel PBA strategy. Our results emphasise the\nimportance of a broad range of customised attacks despite apparent robustness\naccording to standard evaluations. LINAC source code and parameters of defended\nclassifier evaluated throughout this submission are available:\nhttps://github.com/deepmind/linac",
    "descriptor": "",
    "authors": [
      "Andrei A. Rusu",
      "Dan A. Calian",
      "Sven Gowal",
      "Raia Hadsell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13982"
  },
  {
    "id": "arXiv:2210.13983",
    "title": "COEP: Cascade Optimization for Inverse Problems with Entropy-Preserving  Hyperparameter Tuning",
    "abstract": "We propose COEP, an automated and principled framework to solve inverse\nproblems with deep generative models. COEP consists of two components, a\ncascade algorithm for optimization and an entropy-preserving criterion for\nhyperparameter tuning. Through COEP, the two components build up an efficient\nand end-to-end solver for inverse problems that require no human evaluation. We\nestablish theoretical guarantees for the proposed methods. We also empirically\nvalidate the strength of COEP on denoising and noisy compressed sensing, which\nare two fundamental tasks in inverse problems.",
    "descriptor": "",
    "authors": [
      "Tianci Liu",
      "Tong Yang",
      "Quan Zhang",
      "Qi Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13983"
  },
  {
    "id": "arXiv:2210.13984",
    "title": "Abductive Action Inference",
    "abstract": "Abductive reasoning aims to make the most likely inference for a given set of\nincomplete observations. In this work, given a situation or a scenario, we aim\nto answer the question 'what is the set of actions that were executed by the\nhuman in order to come to this current state?', which we coin as abductive\naction inference. We provide a solution based on the human-object relations and\ntheir states in the given scene. Specifically, we first detect objects and\nhumans in the scene, and then generate representations for each human-centric\nrelation. Using these human-centric relations, we derive the most likely set of\nactions the human may have executed to arrive in this state. To generate\nhuman-centric relational representations, we investigate several models such as\nTransformers, a novel graph neural network-based encoder-decoder, and a new\nrelational bilinear pooling method. We obtain promising results using these new\nmodels on this challenging task on the Action Genome dataset.",
    "descriptor": "\nComments: 9 pages, 5 figures\n",
    "authors": [
      "Clement Tan",
      "Chai Kiat Yeo",
      "Cheston Tan",
      "Basura Fernando"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13984"
  },
  {
    "id": "arXiv:2210.13985",
    "title": "This joke is [MASK]: Recognizing Humor and Offense with Prompting",
    "abstract": "Humor is a magnetic component in everyday human interactions and\ncommunications. Computationally modeling humor enables NLP systems to entertain\nand engage with users. We investigate the effectiveness of prompting, a new\ntransfer learning paradigm for NLP, for humor recognition. We show that\nprompting performs similarly to finetuning when numerous annotations are\navailable, but gives stellar performance in low-resource humor recognition. The\nrelationship between humor and offense is also inspected by applying influence\nfunctions to prompting; we show that models could rely on offense to determine\nhumor during transfer.",
    "descriptor": "\nComments: Transfer Learning for Natural Language Processing Workshop at NeurIPS 2022\n",
    "authors": [
      "Junze Li",
      "Mengjie Zhao",
      "Yubo Xie",
      "Antonis Maronikolakis",
      "Pearl Pu",
      "Hinrich Sch\u00fctze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13985"
  },
  {
    "id": "arXiv:2210.13988",
    "title": "Attention Based Relation Network for Facial Action Units Recognition",
    "abstract": "Facial action unit (AU) recognition is essential to facial expression\nanalysis. Since there are highly positive or negative correlations between AUs,\nsome existing AU recognition works have focused on modeling AU relations.\nHowever, previous relationship-based approaches typically embed predefined\nrules into their models and ignore the impact of various AU relations in\ndifferent crowds. In this paper, we propose a novel Attention Based Relation\nNetwork (ABRNet) for AU recognition, which can automatically capture AU\nrelations without unnecessary or even disturbing predefined rules. ABRNet uses\nseveral relation learning layers to automatically capture different AU\nrelations. The learned AU relation features are then fed into a self-attention\nfusion module, which aims to refine individual AU features with attention\nweights to enhance the feature robustness. Furthermore, we propose an AU\nrelation dropout strategy and AU relation loss (AUR-Loss) to better model AU\nrelations, which can further improve AU recognition. Extensive experiments show\nthat our approach achieves state-of-the-art performance on the DISFA and DISFA+\ndatasets.",
    "descriptor": "",
    "authors": [
      "Yao Wei",
      "Haoxiang Wang",
      "Mingze Sun",
      "Jiawang Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13988"
  },
  {
    "id": "arXiv:2210.13989",
    "title": "Input-Output Relation and Performance of RIS-Aided OTFS with Fractional  Delay-Doppler",
    "abstract": "Reconfigurable intelligent surfaces (RIS) and orthogonal time-frequency space\n(OTFS) modulation have gained attention in recent wireless research. RIS\ntechnology aids communication by reflecting the incident electromagnetic waves\ntowards the receiver, and OTFS modulation is effective in high-Doppler\nchannels. This paper presents an early investigation of RIS-aided OTFS in\nhigh-Doppler channels. We derive the end-to-end delay-Doppler (DD) domain\ninput-output relation of a RIS-aided OTFS system, considering rectangular\npulses and fractional delay-Doppler values. We also consider a Zak receiver for\nRIS-aided OTFS that converts the received time-domain signal to DD domain in\none step using Zak transform, and derive its end-to-end input-output relation.\nOur simulation results show that $i)$ RIS-aided OTFS performs better than OTFS\nwithout RIS, $ii)$ Zak receiver performs better than a two-step receiver, and\n$iii)$ RIS-aided OTFS achieves superior performance compared to RIS-aided OFDM.",
    "descriptor": "\nComments: Comm Lett. Copyright IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n",
    "authors": [
      "Vighnesh S Bhat",
      "Gandhodi Harshavardhan",
      "A. Chockalingam"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13989"
  },
  {
    "id": "arXiv:2210.13990",
    "title": "OSS Mentor A framework for improving developers contributions via deep  reinforcement learning",
    "abstract": "In open source project governance, there has been a lot of concern about how\nto measure developers' contributions. However, extremely sparse work has\nfocused on enabling developers to improve their contributions, while it is\nsignificant and valuable. In this paper, we introduce a deep reinforcement\nlearning framework named Open Source Software(OSS) Mentor, which can be trained\nfrom empirical knowledge and then adaptively help developers improve their\ncontributions. Extensive experiments demonstrate that OSS Mentor significantly\noutperforms excellent experimental results. Moreover, it is the first time that\nthe presented framework explores deep reinforcement learning techniques to\nmanage open source software, which enables us to design a more robust framework\nto improve developers' contributions.",
    "descriptor": "",
    "authors": [
      "Jiakuan Fan",
      "Haoyue Wang",
      "Wei Wang",
      "Ming Gao",
      "Shengyu Zhao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13990"
  },
  {
    "id": "arXiv:2210.13992",
    "title": "SphNet: A Spherical Network for Semantic Pointcloud Segmentation",
    "abstract": "Semantic segmentation for robotic systems can enable a wide range of\napplications, from self-driving cars and augmented reality systems to domestic\nrobots. We argue that a spherical representation is a natural one for\negocentric pointclouds. Thus, in this work, we present a novel framework\nexploiting such a representation of LiDAR pointclouds for the task of semantic\nsegmentation. Our approach is based on a spherical convolutional neural network\nthat can seamlessly handle observations from various sensor systems (e.g.,\ndifferent LiDAR systems) and provides an accurate segmentation of the\nenvironment. We operate in two distinct stages: First, we encode the projected\ninput pointclouds to spherical features. Second, we decode and back-project the\nspherical features to achieve an accurate semantic segmentation of the\npointcloud. We evaluate our method with respect to state-of-the-art\nprojection-based semantic segmentation approaches using well-known public\ndatasets. We demonstrate that the spherical representation enables us to\nprovide more accurate segmentation and to have a better generalization to\nsensors with different field-of-view and number of beams than what was seen\nduring training.",
    "descriptor": "",
    "authors": [
      "Lukas Bernreiter",
      "Lionel Ott",
      "Roland Siegwart",
      "Cesar Cadena"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13992"
  },
  {
    "id": "arXiv:2210.13994",
    "title": "Minutiae-Guided Fingerprint Embeddings via Vision Transformers",
    "abstract": "Minutiae matching has long dominated the field of fingerprint recognition.\nHowever, deep networks can be used to extract fixed-length embeddings from\nfingerprints. To date, the few studies that have explored the use of CNN\narchitectures to extract such embeddings have shown extreme promise. Inspired\nby these early works, we propose the first use of a Vision Transformer (ViT) to\nlearn a discriminative fixed-length fingerprint embedding. We further\ndemonstrate that by guiding the ViT to focus in on local, minutiae related\nfeatures, we can boost the recognition performance. Finally, we show that by\nfusing embeddings learned by CNNs and ViTs we can reach near parity with a\ncommercial state-of-the-art (SOTA) matcher. In particular, we obtain a\nTAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a\nSOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our\nfixed-length embeddings can be matched orders of magnitude faster than the\ncommercial system (2.5 million matches/second compared to 50K matches/second).\nWe make our code and models publicly available to encourage further research on\nthis topic: https://github.com/tba.",
    "descriptor": "",
    "authors": [
      "Steven A. Grosz",
      "Joshua J. Engelsma",
      "Rajeev Ranjan",
      "Naveen Ramakrishnan",
      "Manoj Aggarwal",
      "Gerard G. Medioni",
      "Anil K. Jain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13994"
  },
  {
    "id": "arXiv:2210.13996",
    "title": "Report on the energy consumption of the IOTA 2.0 prototype network  (GoShimmer 0.8.3) under different testing scenarios",
    "abstract": "The high energy consumption of proof of work-based distributed ledgers has\nbecome an important environmental concern. Bitcoin, for example, consumes as\nmuch energy in a year as a developed country. Alternative consensus mechanisms,\nsuch as proof of stake, have been shown to use drastically less energy than\nproof of work-based DLTs. For example, the IOTA DLT, built upon a directed\nacyclic graph (DAG) architecture, uses an alternative consensus mechanism that\nrequires significantly less energy than other DLTs. Because the (DLT) space is\nconstantly and rapidly evolving, the question of how much energy DLTs actually\nconsume demands to be continuously studied and answered. Previous research into\nthe energy consumption of the IOTA network has shown that an optimization in\nthe overall protocol correlates to an optimization in energy consumption. The\nplanned IOTA 2.0 update, built upon the GoShimmer research prototype, promises\nto further optimize the protocol by removing the network's centralized\nCoordinator. This report presents the results of measuring the energy\nconsumption of a private GoShimmer network while comparing these findings to\nprevious research into the current mainnet, which is called Chrysalis. The main\nfindings of this report are that the IOTA 2.0 research prototype shows both\nimprovements and increase in the energy consumption metrics compared to the\nChrysalis network. Additionally, this report defines a model to estimate the\ntotal annual energy consumption of an IOTA network. This model should be\nsignificant for future research as it enables a way to estimate the total cost\nof running the IOTA network as well as its carbon emissions. Moreover, having\nan annual power consumption metric allows for better objective comparisons to\ndifferent DLTs.",
    "descriptor": "",
    "authors": [
      "Louis Helmer",
      "Andreas Penzkofer"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.13996"
  },
  {
    "id": "arXiv:2210.13999",
    "title": "Unsupervised domain-adaptive person re-identification with multi-camera  constraints",
    "abstract": "Person re-identification is a key technology for analyzing video-based human\nbehavior; however, its application is still challenging in practical situations\ndue to the performance degradation for domains different from those in the\ntraining data. Here, we propose an environment-constrained adaptive network for\nreducing the domain gap. This network refines pseudo-labels estimated via a\nself-training scheme by imposing multi-camera constraints. The proposed method\nincorporates person-pair information without person identity labels obtained\nfrom the environment into the model training. In addition, we develop a method\nthat appropriately selects a person from the pair that contributes to the\nperformance improvement. We evaluate the performance of the network using\npublic and private datasets and confirm the performance surpasses\nstate-of-the-art methods in domains with overlapping camera views. To the best\nof our knowledge, this is the first study on domain-adaptive learning with\nmulti-camera constraints that can be obtained in real environments.",
    "descriptor": "\nComments: ICIP 2022\n",
    "authors": [
      "S. Takeuchi",
      "F. Li",
      "S. Iwasaki",
      "J. Ning",
      "G. Suzuki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13999"
  },
  {
    "id": "arXiv:2210.14003",
    "title": "Dynamic Practical Byzantine Fault Tolerance and Its Blockchain System: A  Large-Scale Markov Modeling",
    "abstract": "In a practical Byzantine fault tolerance (PBFT) blockchain network, the\nvoting nodes may always leave the network while some new nodes can also enter\nthe network, thus the number of voting nodes is constantly changing. Such a new\nPBFT with dynamic nodes is called a dynamic PBFT. Clearly, the dynamic PBFT can\nmore strongly support the decentralization and distributed structure of\nblockchain. However, analyzing dynamic PBFT blockchain systems will become more\ninteresting and challenging.\nIn this paper, we propose a large-scale Markov modeling technique to analyze\nthe dynamic PBFT voting processes and its dynamic PBFT blockchain system. To\nthis end, we set up a large-scale Markov process (and further a\nmulti-dimensional Quasi-Birth-and-Death (QBD) process) and provide performance\nanalysis for both the dynamic PBFT voting processes and the dynamic PBFT\nblockchain system. In particular, we obtain an effective computational method\nfor the throughput of the complicated dynamic PBFT blockchain system. Finally,\nwe use numerical examples to check the validity of our theoretical results and\nindicate how some key system parameters influence the performance measures of\nthe dynamic PBFT voting processes and of the dynamic PBFT blockchain system.\nTherefore, by using the theory of multi-dimensional QBD processes and the\nRG-factorization technique, we hope that the methodology and results developed\nin this paper shed light on the study of dynamic PBFT blockchain systems such\nthat a series of promising research can be developed potentially.",
    "descriptor": "\nComments: 46 pages, 13 figures\n",
    "authors": [
      "Yan-Xia Chang",
      "Quan-Lin Li",
      "Qing Wang",
      "Xing-Shuo Song"
    ],
    "subjectives": [
      "Performance (cs.PF)",
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.14003"
  },
  {
    "id": "arXiv:2210.14005",
    "title": "Parametric PDF for Goodness of Fit",
    "abstract": "The goodness of fit methods for classification problems relies traditionally\non confusion matrices. This paper aims to enrich these methods with a risk\nevaluation and stability analysis tools. For this purpose, we present a\nparametric PDF framework.",
    "descriptor": "",
    "authors": [
      "Natan Katz",
      "Uri Itai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14005"
  },
  {
    "id": "arXiv:2210.14006",
    "title": "Non-binary Two-Deletion Correcting Codes and Burst-Deletion Correcting  Codes",
    "abstract": "In this paper, we construct systematic $q$-ary two-deletion correcting codes\nand burst-deletion correcting codes, where $q\\geq 2$ is an even integer. For\ntwo-deletion codes, our construction has redundancy $5\\log n+O(\\log q\\log\\log\nn)$ and has encoding complexity near-linear in $n$, where $n$ is the length of\nthe message sequences. For burst-deletion codes, we first present a\nconstruction of binary codes with redundancy $\\log n+9\\log\\log\nn+\\gamma_t+o(\\log\\log n)$ bits $(\\gamma_t$ is a constant that depends only on\n$t)$ and capable of correcting a burst of at most $t$ deletions, which improves\nthe Lenz-Polyanskii Construction (ISIT 2020). Then we give a construction of\n$q$-ary codes with redundancy $\\log n+(8\\log q+9)\\log\\log n+o(\\log q\\log\\log\nn)+\\gamma_t$ bits and capable of correcting a burst of at most $t$ deletions.",
    "descriptor": "",
    "authors": [
      "Wentu Song",
      "Kui Cai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14006"
  },
  {
    "id": "arXiv:2210.14011",
    "title": "Are All Spurious Features in Natural Language Alike? An Analysis through  a Causal Lens",
    "abstract": "The term `spurious correlations' has been used in NLP to informally denote\nany undesirable feature-label correlations. However, a correlation can be\nundesirable because (i) the feature is irrelevant to the label (e.g.\npunctuation in a review), or (ii) the feature's effect on the label depends on\nthe context (e.g. negation words in a review), which is ubiquitous in language\ntasks. In case (i), we want the model to be invariant to the feature, which is\nneither necessary nor sufficient for prediction. But in case (ii), even an\nideal model (e.g. humans) must rely on the feature, since it is necessary (but\nnot sufficient) for prediction. Therefore, a more fine-grained treatment of\nspurious features is needed to specify the desired model behavior. We formalize\nthis distinction using a causal model and probabilities of necessity and\nsufficiency, which delineates the causal relations between a feature and a\nlabel. We then show that this distinction helps explain results of existing\ndebiasing methods on different spurious features, and demystifies surprising\nresults such as the encoding of spurious features in model representations\nafter debiasing.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Nitish Joshi",
      "Xiang Pan",
      "He He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14011"
  },
  {
    "id": "arXiv:2210.14012",
    "title": "Gradient-based Weight Density Balancing for Robust Dynamic Sparse  Training",
    "abstract": "Training a sparse neural network from scratch requires optimizing connections\nat the same time as the weights themselves. Typically, the weights are\nredistributed after a predefined number of weight updates, removing a fraction\nof the parameters of each layer and inserting them at different locations in\nthe same layers. The density of each layer is determined using heuristics,\noften purely based on the size of the parameter tensor. While the connections\nper layer are optimized multiple times during training, the density of each\nlayer typically remains constant. This leaves great unrealized potential,\nespecially in scenarios with a high sparsity of 90% and more. We propose Global\nGradient-based Redistribution, a technique which distributes weights across all\nlayers - adding more weights to the layers that need them most. Our evaluation\nshows that our approach is less prone to unbalanced weight distribution at\ninitialization than previous work and that it is able to find better performing\nsparse subnetworks at very high sparsity levels.",
    "descriptor": "",
    "authors": [
      "Mathias Parger",
      "Alexander Ertl",
      "Paul Eibensteiner",
      "Joerg H. Mueller",
      "Martin Winter",
      "Markus Steinberger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14012"
  },
  {
    "id": "arXiv:2210.14013",
    "title": "Modelling Residential Supply Tasks Based on Digital Orthophotography  Using Machine Learning",
    "abstract": "In order to achieve the climate targets, electrification of individual\nmobility is essential. However, grid integration of electrical vehicles poses\nchallenges for the electrical distribution network due to high charging power\nand simultaneity. To investigate these challenges in research studies, the\nnetwork-referenced supply task needs to be modeled. Previous research work\nutilizes data that is not always complete or sufficiently granular in space.\nThis is why this paper presents a methodology which allows a holistic\ndetermination of residential supply tasks based on orthophotos. To do this,\nbuildings are first identified from orthophotos, then residential building\ntypes are classified, and finally the electricity demand of each building is\ndetermined. In an exemplary case study, we validate the presented methodology\nand compare the results with another supply task methodology. The results show\nthat the electricity demand deviates from the results of a reference method by\nan average 9%. Deviations result mainly from the parameterization of the\nselected residential building types. Thus, the presented methodology is able to\nmodel supply tasks similarly as other methods but more granular.",
    "descriptor": "",
    "authors": [
      "Klemens Schumann",
      "Luis B\u00f6ttcher",
      "Philipp H\u00e4lsig",
      "Daniel Zelenak",
      "Andreas Ulbig"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14013"
  },
  {
    "id": "arXiv:2210.14016",
    "title": "Shortest Edit Path Crossover: A Theory-driven Solution to the  Permutation Problem in Evolutionary Neural Architecture Search",
    "abstract": "Evolutionary algorithms (EAs) have gained attention recently due to their\nsuccess in neural architecture search (NAS). However, whereas traditional EAs\ndraw much power from crossover operations, most evolutionary NAS methods deploy\nonly mutation operators. The main reason is the permutation problem: The\nmapping between genotype and phenotype in traditional graph representations is\nmany-to-one, leading to a disruptive effect of standard crossover. This work\nconducts the first theoretical analysis of the behaviors of crossover and\nmutation in the NAS context, and proposes a new crossover operator based on the\nshortest edit path (SEP) in graph space. The SEP crossover is shown to overcome\nthe permutation problem, and as a result, offspring generated by the SEP\ncrossover is theoretically proved to have a better expected improvement in\nterms of graph edit distance to global optimum, compared to mutation and\nstandard crossover. Experiments further show that the SEP crossover\nsignificantly outperforms mutation and standard crossover on three\nstate-of-the-art NAS benchmarks. The SEP crossover therefore allows taking full\nadvantage of evolution in NAS, and potentially other similar design problems as\nwell.",
    "descriptor": "\nComments: 17 pages, 6 figures\n",
    "authors": [
      "Xin Qiu",
      "Risto Miikkulainen"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14016"
  },
  {
    "id": "arXiv:2210.14018",
    "title": "A White-Box Adversarial Attack Against a Digital Twin",
    "abstract": "Recent research has shown that Machine Learning/Deep Learning (ML/DL) models\nare particularly vulnerable to adversarial perturbations, which are small\nchanges made to the input data in order to fool a machine learning classifier.\nThe Digital Twin, which is typically described as consisting of a physical\nentity, a virtual counterpart, and the data connections in between, is\nincreasingly being investigated as a means of improving the performance of\nphysical entities by leveraging computational techniques, which are enabled by\nthe virtual counterpart. This paper explores the susceptibility of Digital Twin\n(DT), a virtual model designed to accurately reflect a physical object using\nML/DL classifiers that operate as Cyber Physical Systems (CPS), to adversarial\nattacks. As a proof of concept, we first formulate a DT of a vehicular system\nusing a deep neural network architecture and then utilize it to launch an\nadversarial attack. We attack the DT model by perturbing the input to the\ntrained model and show how easily the model can be broken with white-box\nattacks.",
    "descriptor": "",
    "authors": [
      "Wilson Patterson",
      "Ivan Fernandez",
      "Subash Neupane",
      "Milan Parmar",
      "Sudip Mittal",
      "Shahram Rahimi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14018"
  },
  {
    "id": "arXiv:2210.14019",
    "title": "The Curious Case of Benign Memorization",
    "abstract": "Despite the empirical advances of deep learning across a variety of learning\ntasks, our theoretical understanding of its success is still very restricted.\nOne of the key challenges is the overparametrized nature of modern models,\nenabling complete overfitting of the data even if the labels are randomized,\ni.e. networks can completely memorize all given patterns. While such a\nmemorization capacity seems worrisome, in this work we show that under training\nprotocols that include data augmentation, neural networks learn to memorize\nentirely random labels in a benign way, i.e. they learn embeddings that lead to\nhighly non-trivial performance under nearest neighbour probing. We demonstrate\nthat deep models have the surprising ability to separate noise from signal by\ndistributing the task of memorization and feature learning to different layers.\nAs a result, only the very last layers are used for memorization, while\npreceding layers encode performant features which remain largely unaffected by\nthe label noise. We explore the intricate role of the augmentations used for\ntraining and identify a memorization-generalization trade-off in terms of their\ndiversity, marking a clear distinction to all previous works. Finally, we give\na first explanation for the emergence of benign memorization by showing that\nmalign memorization under data augmentation is infeasible due to the\ninsufficient capacity of the model for the increased sample size. As a\nconsequence, the network is forced to leverage the correlated nature of the\naugmentations and as a result learns meaningful features. To complete the\npicture, a better theory of feature learning in deep neural networks is\nrequired to fully understand the origins of this phenomenon.",
    "descriptor": "",
    "authors": [
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Lorenzo Noci",
      "Thomas Hofmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14019"
  },
  {
    "id": "arXiv:2210.14022",
    "title": "A Hybrid Microscopic Model for Multimodal Traffic with Empirical  Observations from Aerial Footage",
    "abstract": "Microscopic traffic flow models can be distinguished in lane-based or\nlane-free depending on the degree of lane-discipline. This distinction holds\ntrue only if motorcycles are neglected in lane-based traffic. In cities, as\nopposed to highways, this is an oversimplification and it would be more\naccurate to speak of hybrid situations, where lane discipline can be made\nmode-dependent. Empirical evidence shows that cars follow the lanes as defined\nby the infrastructure, while motorcycles do not necessarily adhere to\npredefined norms and may participate in self-organized formation of virtual\nlanes. This phenomenon is the result of complex interactions between different\ntraffic participants competing for limited space. In order to better understand\nthe dynamics of modal interaction microscopically, we first analyze empirical\ndata from detailed trajectories obtained by the pNEUMA experiment and observe\npatterns of mixed traffic. Then, we propose a hybrid model for multimodal\nvehicular traffic. The hybrid model is inspired by the pedestrian flow\nliterature, featuring collision-free and anticipatory properties, and we\ndemonstrate that it is able to reproduce empirical observations from aerial\nfootage.",
    "descriptor": "",
    "authors": [
      "Georg Anagnostopoulos",
      "Nikolas Geroliminis"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14022"
  },
  {
    "id": "arXiv:2210.14026",
    "title": "SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model  Communication",
    "abstract": "The decentralized Federated Learning (FL) setting avoids the role of a\npotentially unreliable or untrustworthy central host by utilizing groups of\nclients to collaboratively train a model via localized training and\nmodel/gradient sharing. Most existing decentralized FL algorithms require\nsynchronization of client models where the speed of synchronization depends\nupon the slowest client. In this work, we propose SWIFT: a novel wait-free\ndecentralized FL algorithm that allows clients to conduct training at their own\nspeed. Theoretically, we prove that SWIFT matches the gold-standard iteration\nconvergence rate $\\mathcal{O}(1/\\sqrt{T})$ of parallel stochastic gradient\ndescent for convex and non-convex smooth optimization (total iterations $T$).\nFurthermore, we provide theoretical results for IID and non-IID settings\nwithout any bounded-delay assumption for slow clients which is required by\nother asynchronous decentralized FL algorithms. Although SWIFT achieves the\nsame iteration convergence rate with respect to $T$ as other state-of-the-art\n(SOTA) parallel stochastic algorithms, it converges faster with respect to\nrun-time due to its wait-free structure. Our experimental results demonstrate\nthat SWIFT's run-time is reduced due to a large reduction in communication time\nper epoch, which falls by an order of magnitude compared to synchronous\ncounterparts. Furthermore, SWIFT produces loss levels for image classification,\nover IID and non-IID data settings, upwards of 50% faster than existing SOTA\nalgorithms.",
    "descriptor": "\nComments: 30 pages, 9 figures\n",
    "authors": [
      "Marco Bornstein",
      "Tahseen Rabbani",
      "Evan Wang",
      "Amrit Singh Bedi",
      "Furong Huang"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14026"
  },
  {
    "id": "arXiv:2210.14030",
    "title": "UNIFY: a Unified Policy Designing Framework for Solving Constrained  Optimization Problems with Machine Learning",
    "abstract": "The interplay between Machine Learning (ML) and Constrained Optimization (CO)\nhas recently been the subject of increasing interest, leading to a new and\nprolific research area covering (e.g.) Decision Focused Learning and\nConstrained Reinforcement Learning. Such approaches strive to tackle complex\ndecision problems under uncertainty over multiple stages, involving both\nexplicit (cost function, constraints) and implicit knowledge (from data), and\npossibly subject to execution time restrictions. While a good degree of success\nhas been achieved, the existing methods still have limitations in terms of both\napplicability and effectiveness. For problems in this class, we propose UNIFY,\na unified framework to design a solution policy for complex decision-making\nproblems. Our approach relies on a clever decomposition of the policy in two\nstages, namely an unconstrained ML model and a CO problem, to take advantage of\nthe strength of each approach while compensating for its weaknesses. With a\nlittle design effort, UNIFY can generalize several existing approaches, thus\nextending their applicability. We demonstrate the method effectiveness on two\npractical problems, namely an Energy Management System and the Set Multi-cover\nwith stochastic coverage requirements. Finally, we highlight some current\nchallenges of our method and future research directions that can benefit from\nthe cross-fertilization of the two fields.",
    "descriptor": "",
    "authors": [
      "Mattia Silvestri",
      "Allegra De Filippo",
      "Michele Lombardi",
      "Michela Milano"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.14030"
  },
  {
    "id": "arXiv:2210.14031",
    "title": "A Comparative Study on Deep-Learning Methods for Dense Image Matching of  Multi-angle and Multi-date Remote Sensing Stereo Images",
    "abstract": "Deep learning (DL) stereo matching methods gained great attention in remote\nsensing satellite datasets. However, most of these existing studies conclude\nassessments based only on a few/single stereo images lacking a systematic\nevaluation on how robust DL methods are on satellite stereo images with varying\nradiometric and geometric configurations. This paper provides an evaluation of\nfour DL stereo matching methods through hundreds of multi-date multi-site\nsatellite stereo pairs with varying geometric configurations, against the\ntraditional well-practiced Census-SGM (Semi-global matching), to\ncomprehensively understand their accuracy, robustness, generalization\ncapabilities, and their practical potential. The DL methods include a\nlearning-based cost metric through convolutional neural networks (MC-CNN)\nfollowed by SGM, and three end-to-end (E2E) learning models using Geometry and\nContext Network (GCNet), Pyramid Stereo Matching Network (PSMNet), and\nLEAStereo. Our experiments show that E2E algorithms can achieve upper limits of\ngeometric accuracies, while may not generalize well for unseen data. The\nlearning-based cost metric and Census-SGM are rather robust and can\nconsistently achieve acceptable results. All DL algorithms are robust to\ngeometric configurations of stereo pairs and are less sensitive in comparison\nto the Census-SGM, while learning-based cost metrics can generalize on\nsatellite images when trained on different datasets (airborne or ground-view).",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Hessah Albanwan",
      "Rongjun Qin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14031"
  },
  {
    "id": "arXiv:2210.14032",
    "title": "Whitening Convergence Rate of Coupling-based Normalizing Flows",
    "abstract": "Coupling-based normalizing flows (e.g. RealNVP) are a popular family of\nnormalizing flow architectures that work surprisingly well in practice. This\ncalls for theoretical understanding. Existing work shows that such flows weakly\nconverge to arbitrary data distributions. However, they make no statement about\nthe stricter convergence criterion used in practice, the maximum likelihood\nloss. For the first time, we make a quantitative statement about this kind of\nconvergence: We prove that all coupling-based normalizing flows perform\nwhitening of the data distribution (i.e. diagonalize the covariance matrix) and\nderive corresponding convergence bounds that show a linear convergence rate in\nthe depth of the flow. Numerical experiments demonstrate the implications of\nour theory and point at open questions.",
    "descriptor": "\nComments: Proceedings of 36th Conference on Neural Information Processing System (NeurIPS 2022)\n",
    "authors": [
      "Felix Draxler",
      "Christoph Schn\u00f6rr",
      "Ullrich K\u00f6the"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14032"
  },
  {
    "id": "arXiv:2210.14036",
    "title": "A Task Allocation Framework for Human Multi-Robot Collaborative Settings",
    "abstract": "The requirements of modern production systems together with more advanced\nrobotic technologies have fostered the integration of teams comprising humans\nand autonomous robots. However, along with the potential benefits also comes\nthe question of how to effectively handle these teams considering the different\ncharacteristics of the involved agents. For this reason, this paper presents a\nframework for task allocation in a human multi-robot collaborative scenario.\nThe proposed solution combines an optimal offline allocation with an online\nreallocation strategy which accounts for inaccuracies of the offline plan\nand/or unforeseen events, human subjective preferences and cost of switching\nfrom one task to another so as to increase human satisfaction and team\nefficiency. Experiments are presented for the case of two manipulators\ncooperating with a human operator for performing a box filling task.",
    "descriptor": "",
    "authors": [
      "Martina Lippi",
      "Paolo Di Lillo",
      "Alessandro Marino"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14036"
  },
  {
    "id": "arXiv:2210.14037",
    "title": "Revisiting Softmax for Uncertainty Approximation in Text Classification",
    "abstract": "Uncertainty approximation in text classification is an important area with\napplications in domain adaptation and interpretability. The most widely used\nuncertainty approximation method is Monte Carlo Dropout, which is\ncomputationally expensive as it requires multiple forward passes through the\nmodel. A cheaper alternative is to simply use a softmax to estimate model\nuncertainty. However, prior work has indicated that the softmax can generate\noverconfident uncertainty estimates and can thus be tricked into producing\nincorrect predictions. In this paper, we perform a thorough empirical analysis\nof both methods on five datasets with two base neural architectures in order to\nreveal insight into the trade-offs between the two. We compare the methods'\nuncertainty approximations and downstream text classification performance,\nwhile weighing their performance against their computational complexity as a\ncost-benefit analysis, by measuring runtime (cost) and the downstream\nperformance (benefit). We find that, while Monte Carlo produces the best\nuncertainty approximations, using a simple softmax leads to competitive\nuncertainty estimation for text classification at a much lower computational\ncost, suggesting that softmax can in fact be a sufficient uncertainty estimate\nwhen computational resources are a concern.",
    "descriptor": "",
    "authors": [
      "Andreas Nugaard Holm",
      "Dustin Wright",
      "Isabelle Augenstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14037"
  },
  {
    "id": "arXiv:2210.14046",
    "title": "Numerical surgery for mean curvature flow of surfaces",
    "abstract": "A numerical algorithm for mean curvature flow of surfaces with surgery is\nproposed. The method uses a convergent finite element based mean curvature flow\nalgorithm based on a coupled partial differential equation system which\ndirectly provides an approximation for mean curvature and outward unit normal.\nThe proposed numerical surgery process closely follows the analytical surgery\nof Huisken & Sinestrari, and Brendle & Huisken. The numerical surgery approach\nis described in detail, along with extensions to other geometric flows and\nmethods. Numerical experiments report on the performance of the numerical\nsurgery process.",
    "descriptor": "",
    "authors": [
      "Bal\u00e1zs Kov\u00e1cs"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14046"
  },
  {
    "id": "arXiv:2210.14047",
    "title": "OneProvenance: Efficient Extraction of Dynamic Coarse-Grained Provenance  from Database Logs",
    "abstract": "Provenance encodes information that connects datasets, their generation\nworkflows, and associated metadata (e.g., who or when executed a query). As\nsuch, it is instrumental for a wide range of critical applications, including\ngovernance, observability, and auditing. Unfortunately, in the context of\ndatabase systems, extracting meaningful coarse-grained provenance is a\nlong-standing problem due to the complexity and sheer volume of database\nworkflows. Provenance extraction from query event logs has been recently\nproposed as favorable because, in principle, can result in meaningful\nprovenance graphs for provenance applications. Current approaches, however, (a)\nadd substantial overhead to the database and provenance extraction workflows\nand (b) extract provenance that is noisy, omits query execution dependencies,\nand is not rich enough to support upstream applications. To address these\nproblems, we introduce OneProvenance: an engine for efficient extraction of\nmeaningful coarse-grained provenance from query event logs. OneProvenance\naddresses the unique challenges of log-based extraction by (a) identifying\nquery execution dependencies through efficient log analysis, (b) extracting\nprovenance through novel event transformations that account for query\ndependencies, and (c) introducing effective filtering and compression\noptimizations. Our thorough experimental analysis shows that OneProvenance can\nimprove the extraction process by up to 18X compared to state-of-the-art\nbaselines -- while our optimizations reduce the extraction noise and optimize\nperformance even further. OneProvenance is used by Microsoft Purview to support\ndynamic provenance extraction.",
    "descriptor": "",
    "authors": [
      "Fotis Psallidas",
      "Ashvin Agrawal",
      "Chandru Sugunan",
      "Khaled Ibrahim",
      "Konstantinos Karanasos",
      "Jes\u00fas Camacho-Rodr\u00edguez",
      "Avrilia Floratou",
      "Carlo Curino",
      "Raghu Ramakrishnan"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.14047"
  },
  {
    "id": "arXiv:2210.14051",
    "title": "Bridging Distributional and Risk-sensitive Reinforcement Learning with  Provable Regret Bounds",
    "abstract": "We study the regret guarantee for risk-sensitive reinforcement learning\n(RSRL) via distributional reinforcement learning (DRL) methods. In particular,\nwe consider finite episodic Markov decision processes whose objective is the\nentropic risk measure (EntRM) of return. We identify a key property of the\nEntRM, the monotonicity-preserving property, which enables the risk-sensitive\ndistributional dynamic programming framework. We then propose two novel DRL\nalgorithms that implement optimism through two different schemes, including a\nmodel-free one and a model-based one.\nWe prove that both of them attain $\\tilde{\\mathcal{O}}(\\frac{\\exp(|\\beta|\nH)-1}{|\\beta|H}H\\sqrt{HS^2AT})$ regret upper bound, where $S$ is the number of\nstates, $A$ the number of states, $H$ the time horizon and $T$ the number of\ntotal time steps. It matches RSVI2 proposed in \\cite{fei2021exponential} with a\nmuch simpler regret analysis. To the best of our knowledge, this is the first\nregret analysis of DRL, which bridges DRL and RSRL in terms of sample\ncomplexity. Finally, we improve the existing lower bound by proving a tighter\nbound of $\\Omega(\\frac{\\exp(\\beta H/6)-1}{\\beta H}H\\sqrt{SAT})$ for $\\beta>0$\ncase, which recovers the tight lower bound $\\Omega(H\\sqrt{SAT})$ in the\nrisk-neutral setting.",
    "descriptor": "",
    "authors": [
      "Hao Liang",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14051"
  },
  {
    "id": "arXiv:2210.14055",
    "title": "Policy-Guided Lazy Search with Feedback for Task and Motion Planning",
    "abstract": "PDDLStream solvers have recently emerged as viable solutions for Task and\nMotion Planning (TAMP) problems, extending PDDL to problems with continuous\naction spaces. Prior work has shown how PDDLStream problems can be reduced to a\nsequence of PDDL planning problems, which can then be solved using\noff-the-shelf planners. However, this approach can suffer from long runtimes.\nIn this paper we propose LAZY, a solver for PDDLStream problems that maintains\na single integrated search over action skeletons, which gets progressively more\ngeometrically informed as samples of possible motions are lazily drawn during\nmotion planning. We explore how learned models of goal-directed policies and\ncurrent motion sampling data can be incorporated in LAZY to adaptively guide\nthe task planner. We show that this leads to significant speed-ups in the\nsearch for a feasible solution evaluated over unseen test environments of\nvarying numbers of objects, goals, and initial conditions. We evaluate our TAMP\napproach by comparing to existing solvers for PDDLStream problems on a range of\nsimulated 7DoF rearrangement/manipulation problems.",
    "descriptor": "",
    "authors": [
      "Mohamed Khodeir",
      "Atharv Sonwane",
      "Florian Shkurti"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14055"
  },
  {
    "id": "arXiv:2210.14056",
    "title": "Unsupervised Anomaly Detection for Auditing Data and Impact of  Categorical Encodings",
    "abstract": "In this paper, we introduce the Vehicle Claims dataset, consisting of\nfraudulent insurance claims for automotive repairs. The data belongs to the\nmore broad category of Auditing data, which includes also Journals and Network\nIntrusion data. Insurance claim data are distinctively different from other\nauditing data (such as network intrusion data) in their high number of\ncategorical attributes. We tackle the common problem of missing benchmark\ndatasets for anomaly detection: datasets are mostly confidential, and the\npublic tabular datasets do not contain relevant and sufficient categorical\nattributes. Therefore, a large-sized dataset is created for this purpose and\nreferred to as Vehicle Claims (VC) dataset. The dataset is evaluated on shallow\nand deep learning methods. Due to the introduction of categorical attributes,\nwe encounter the challenge of encoding them for the large dataset. As One Hot\nencoding of high cardinal dataset invokes the \"curse of dimensionality\", we\nexperiment with GEL encoding and embedding layer for representing categorical\nattributes. Our work compares competitive learning, reconstruction-error,\ndensity estimation and contrastive learning approaches for Label, One Hot, GEL\nencoding and embedding layer to handle categorical values.",
    "descriptor": "\nComments: This work has been accepted at Proceedings of the Neurips 2022 Workshop on Synthetic Data 4ML\n",
    "authors": [
      "Ajay Chawda",
      "Stefanie Grimm",
      "Marius Kloft"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14056"
  },
  {
    "id": "arXiv:2210.14061",
    "title": "From exemplar to copy: the scribal appropriation of a Hadewijch  manuscript computationally explored",
    "abstract": "This study is devoted to two of the oldest known manuscripts in which the\noeuvre of the medieval mystical author Hadewijch has been preserved: Brussels,\nKBR, 2879-2880 (ms. A) and Brussels, KBR, 2877-2878 (ms. B). On the basis of\ncodicological and contextual arguments, it is assumed that the scribe who\nproduced B used A as an exemplar. While the similarities in both layout and\ncontent between the two manuscripts are striking, the present article seeks to\nidentify the differences. After all, regardless of the intention to produce a\ncopy that closely follows the exemplar, subtle linguistic variation is\napparent. Divergences relate to spelling conventions, but also to the way in\nwhich words are abbreviated (and the extent to which abbreviations occur). The\npresent study investigates the spelling profiles of the scribes who produced\nmss. A and B in a computational way. In the first part of this study, we will\npresent both manuscripts in more detail, after which we will consider prior\nresearch carried out on scribal profiling. The current study both builds and\nexpands on Kestemont (2015). Next, we outline the methodology used to analyse\nand measure the degree of scribal appropriation that took place when ms. B was\ncopied off the exemplar ms. A. After this, we will discuss the results\nobtained, focusing on the scribal variation that can be found both at the level\nof individual words and n-grams. To this end, we use machine learning to\nidentify the most distinctive features that separate manuscript A from B.\nFinally, we look at possible diachronic trends in the appropriation by B's\nscribe of his exemplar. We argue that scribal takeovers in the exemplar impacts\nthe practice of the copying scribe, while transitions to a different content\nmatter cause little to no effect.",
    "descriptor": "",
    "authors": [
      "Wouter Haverals",
      "Mike Kestemont"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14061"
  },
  {
    "id": "arXiv:2210.14063",
    "title": "On a mixed FEM and a FOSLS with $H^{-1}$ loads",
    "abstract": "We study variants of the mixed finite element method (mixed FEM) and the\nfirst-order system least-squares finite element (FOSLS) for the Poisson problem\nwhere we replace the load by a suitable regularization which permits to use\n$H^{-1}$ loads. We prove that any bounded $H^{-1}$ projector onto piecewise\nconstants can be used to define the regularization and yields quasi-optimality\nof the lowest-order mixed FEM resp. FOSLS in weaker norms. Examples for the\nconstruction of such projectors are given. One is based on the adjoint of a\nweighted Cl\\'ement quasi-interpolator. We prove that this Cl\\'ement operator\nhas second-order approximation properties. For the modified mixed method we\nshow optimal convergence rates of a postprocessed solution under minimal\nregularity assumptions -- a result not valid for the lowest-order mixed FEM\nwithout regularization. Numerical examples conclude this work.",
    "descriptor": "",
    "authors": [
      "Thomas F\u00fchrer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14063"
  },
  {
    "id": "arXiv:2210.14064",
    "title": "Learning Low Dimensional State Spaces with Overparameterized Recurrent  Neural Network",
    "abstract": "Overparameterization in deep learning typically refers to settings where a\ntrained Neural Network (NN) has representational capacity to fit the training\ndata in many ways, some of which generalize well, while others do not. In the\ncase of Recurrent Neural Networks (RNNs), there exists an additional layer of\noverparameterization, in the sense that a model may exhibit many solutions that\ngeneralize well for sequence lengths seen in training, some of which\nextrapolate to longer sequences, while others do not. Numerous works studied\nthe tendency of Gradient Descent (GD) to fit overparameterized NNs with\nsolutions that generalize well. On the other hand, its tendency to fit\noverparameterized RNNs with solutions that extrapolate has been discovered only\nlately, and is far less understood. In this paper, we analyze the extrapolation\nproperties of GD when applied to overparameterized linear RNNs. In contrast to\nrecent arguments suggesting an implicit bias towards short-term memory, we\nprovide theoretical evidence for learning low dimensional state spaces, which\ncan also model long-term memory. Our result relies on a dynamical\ncharacterization which shows that GD (with small step size and near-zero\ninitialization) strives to maintain a certain form of balancedness, as well as\non tools developed in the context of the moment problem from statistics\n(recovery of a probability distribution from its moments). Experiments\ncorroborate our theory, demonstrating extrapolation via learning low\ndimensional state spaces with both linear and non-linear RNNs",
    "descriptor": "\nComments: preprint, 9 pages, 2 figures plus supplementary\n",
    "authors": [
      "Edo Cohen-Karlik",
      "Itamar Menuhin-Gruman",
      "Nadav Cohen",
      "Raja Giryes",
      "Amir Globerson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14064"
  },
  {
    "id": "arXiv:2210.14067",
    "title": "Clustering of Threat Information to Mitigate Information Overload for  Computer Emergency Response Teams",
    "abstract": "The constantly increasing number of threats and the existing diversity of\ninformation sources pose challenges for Computer Emergency Response Teams\n(CERTs). In order to respond to new threats, CERTs need to gather information\nin a timely and comprehensive manner. However, the volume of information and\nsources can lead to information overload. This paper answers the question of\nhow to reduce information overload for CERTs with the help of clustering\nmethods. Conditions for such a framework were established and subsequently\ntested. In order to perform an evaluation, different types of evaluation\nmetrics were introduced and selected in relation to the framework conditions.\nFurthermore, different vectorizations and distance measures in combination with\nthe clustering methods were evaluated and interpreted. Two different\nground-truth datasets were used for the evaluation, one containing threat\nmessages and a dataset with messages from different news categories. The work\nshows that the K-means clustering method along with TF-IDF vectorization and\ncosine distance provide the best results in the domain of threat messages.",
    "descriptor": "\nComments: 12 pages, 7 figures\n",
    "authors": [
      "Philipp Kuehn",
      "Moritz Kerk",
      "Marc Wendelborn",
      "Christian Reuter"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.14067"
  },
  {
    "id": "arXiv:2210.14070",
    "title": "Useful Confidence Measures: Beyond the Max Score",
    "abstract": "An important component in deploying machine learning (ML) in safety-critic\napplications is having a reliable measure of confidence in the ML model's\npredictions. For a classifier $f$ producing a probability vector $f(x)$ over\nthe candidate classes, the confidence is typically taken to be $\\max_i f(x)_i$.\nThis approach is potentially limited, as it disregards the rest of the\nprobability vector. In this work, we derive several confidence measures that\ndepend on information beyond the maximum score, such as margin-based and\nentropy-based measures, and empirically evaluate their usefulness, focusing on\nNLP tasks with distribution shifts and Transformer-based models. We show that\nwhen models are evaluated on the out-of-distribution data ``out of the box'',\nusing only the maximum score to inform the confidence measure is highly\nsuboptimal. In the post-processing regime (where the scores of $f$ can be\nimproved using additional in-distribution held-out data), this remains true,\nalbeit less significant. Overall, our results suggest that entropy-based\nconfidence is a surprisingly useful measure.",
    "descriptor": "\nComments: Short paper; appeared in the Workshop on Distribution Shifts @ NeurIPS 2022\n",
    "authors": [
      "Gal Yona",
      "Amir Feder",
      "Itay Laish"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14070"
  },
  {
    "id": "arXiv:2210.14072",
    "title": "Leveraging the Verifier's Dilemma to Double Spend in Bitcoin",
    "abstract": "We describe and analyze perishing mining, a novel block-withholding mining\nstrategy that lures profit-driven miners away from doing useful work on the\npublic chain by releasing block headers from a privately maintained chain. We\nthen introduce the dual private chain (DPC) attack, where an adversary that\naims at double spending increases its success rate by intermittently dedicating\npart of its hash power to perishing mining. We detail the DPC attack's Markov\ndecision process, evaluate its double spending success rate using Monte Carlo\nsimulations. We show that the DPC attack lowers Bitcoin's security bound in the\npresence of profit-driven miners that do not wait to validate the transactions\nof a block before mining on it.",
    "descriptor": "",
    "authors": [
      "Tong Cao",
      "J\u00e9r\u00e9mie Decouchant",
      "Jiangshan Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.14072"
  },
  {
    "id": "arXiv:2210.14075",
    "title": "Non-Oscillatory Limited-Time Integration for Conservation Laws and  Convection-Diffusion Equations",
    "abstract": "In this study we consider unconditionally non-oscillatory, high order\nimplicit time marching based on time-limiters. The first aspect of our work is\nto propose the high resolution Limited-DIRK3 (L-DIRK3) scheme for conservation\nlaws and convection-diffusion equations in the method-of-lines framework. The\nscheme can be used in conjunction with an arbitrary high order spatial\ndiscretization scheme such as 5th order WENO scheme. It can be shown that the\nstrongly S-stable DIRK3 scheme is not SSP and may introduce strong oscillations\nunder large time step. To overcome the oscillatory nature of DIRK3, the key\nidea of L-DIRK3 scheme is to apply local time-limiters (K.Duraisamy,\nJ.D.Baeder, J-G Liu), with which the order of accuracy in time is locally\ndropped to first order in the regions where the evolution of solution is not\nsmooth. In this way, the monotonicity condition is locally satisfied, while a\nhigh order of accuracy is still maintained in most of the solution domain. For\nconvenience of applications to systems of equations, we propose a new and\nsimple construction of time-limiters which allows flexible choice of reference\nquantity with minimal computation cost. Another key aspect of our work is to\nextend the application of time-limiter schemes to multidimensional problems and\nconvection-diffusion equations. Numerical experiments for scalar/systems of\nequations in one- and two-dimensions confirm the high resolution and the\nimproved stability of L-DIRK3 under large time steps. Moreover, the results\nindicate the potential of time-limiter schemes to serve as a generic and\nconvenient methodology to improve the stability of arbitrary DIRK methods.",
    "descriptor": "",
    "authors": [
      "Jingcheng Lu",
      "James D.Baeder"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14075"
  },
  {
    "id": "arXiv:2210.14077",
    "title": "Eigen Memory Tree",
    "abstract": "This work introduces the Eigen Memory Tree (EMT), a novel online memory model\nfor sequential learning scenarios. EMTs store data at the leaves of a binary\ntree and route new samples through the structure using the principal components\nof previous experiences, facilitating efficient (logarithmic) access to\nrelevant memories. We demonstrate that EMT outperforms existing online memory\napproaches, and provide a hybridized EMT-parametric algorithm that enjoys\ndrastically improved performance over purely parametric methods with nearly no\ndownsides. Our findings are validated using 206 datasets from the OpenML\nrepository in both bounded and infinite memory budget situations.",
    "descriptor": "",
    "authors": [
      "Mark Rucker",
      "Joran T. Ash",
      "John Langford",
      "Paul Mineiro",
      "Ida Momennejad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14077"
  },
  {
    "id": "arXiv:2210.14080",
    "title": "Learning Individual Treatment Effects under Heterogeneous Interference  in Networks",
    "abstract": "Estimates of individual treatment effects from networked observational data\nare attracting increasing attention these days. One major challenge in network\nscenarios is the violation of the stable unit treatment value assumption\n(SUTVA), which assumes that the treatment assignment of a unit does not\ninfluence others' outcomes. In network data, due to interference, the outcome\nof a unit is influenced not only by its treatment (i.e., direct effects) but\nalso by others' treatments (i.e., spillover effects). Furthermore, the\ninfluences from other units are always heterogeneous (e.g., friends with\nsimilar interests affect a person differently than friends with different\ninterests). In this paper, we focus on the problem of estimating individual\ntreatment effects (both direct and spillover effects) under heterogeneous\ninterference. To address this issue, we propose a novel Dual Weighting\nRegression (DWR) algorithm by simultaneously learning attention weights that\ncapture the heterogeneous interference and sample weights to eliminate the\ncomplex confounding bias in networks. We formulate the entire learning process\nas a bi-level optimization problem. In theory, we present generalization error\nbounds for individual treatment effect estimation. Extensive experiments on\nfour benchmark datasets demonstrate that the proposed DWR algorithm outperforms\nstate-of-the-art methods for estimating individual treatment effects under\nheterogeneous interference.",
    "descriptor": "",
    "authors": [
      "Ziyu Zhao",
      "Kun Kuang",
      "Ruoxuan Xiong",
      "Fei Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.14080"
  },
  {
    "id": "arXiv:2210.14082",
    "title": "Specialization of Run-time Configuration Space at Compile-time: An  Exploratory Study",
    "abstract": "Numerous software systems are highly configurable through run-time options,\nsuch as command-line parameters. Users can tune some of the options to meet\nvarious functional and non-functional requirements such as footprint, security,\nor execution time. However, some options are never set for a given system\ninstance, and their values remain the same whatever the use cases of the\nsystem. Herein, we design a controlled experiment in which the system's\nrun-time configuration space can be specialized at compile-time and\ncombinations of options can be removed on demand. We perform an in-depth study\nof the well-known x264 video encoder and quantify the effects of its\nspecialization to its non-functional properties, namely on binary size, attack\nsurface, and performance while ensuring its validity. Our exploratory study\nsuggests that the configurable specialization of a system has statistically\nsignificant benefits on most of its analysed non-functional properties, which\nbenefits depend on the number of the debloated options. While our empirical\nresults and insights show the importance of removing code related to unused\nrun-time options to improve software systems, an open challenge is to further\nautomate the specialization process.",
    "descriptor": "\nComments: 14 pages, 3 Figures (not counted the subfigures), 5 Tables, and 1 Algorithm\n",
    "authors": [
      "Xhevahire T\u00ebrnava",
      "Mathieu Acher",
      "Benoit Combemale"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.14082"
  },
  {
    "id": "arXiv:2210.14083",
    "title": "On Fine-Tuned Deep Features for Unsupervised Domain Adaptation",
    "abstract": "Prior feature transformation based approaches to Unsupervised Domain\nAdaptation (UDA) employ the deep features extracted by pre-trained deep models\nwithout fine-tuning them on the specific source or target domain data for a\nparticular domain adaptation task. In contrast, end-to-end learning based\napproaches optimise the pre-trained backbones and the customised adaptation\nmodules simultaneously to learn domain-invariant features for UDA. In this\nwork, we explore the potential of combining fine-tuned features and feature\ntransformation based UDA methods for improved domain adaptation performance.\nSpecifically, we integrate the prevalent progressive pseudo-labelling\ntechniques into the fine-tuning framework to extract fine-tuned features which\nare subsequently used in a state-of-the-art feature transformation based domain\nadaptation method SPL (Selective Pseudo-Labeling). Thorough experiments with\nmultiple deep models including ResNet-50/101 and DeiT-small/base are conducted\nto demonstrate the combination of fine-tuned features and SPL can achieve\nstate-of-the-art performance on several benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Qian Wang",
      "Toby P. Breckon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14083"
  },
  {
    "id": "arXiv:2210.14085",
    "title": "Audio MFCC-gram Transformers for respiratory insufficiency detection in  COVID-19",
    "abstract": "This work explores speech as a biomarker and investigates the detection of\nrespiratory insufficiency (RI) by analyzing speech samples. Previous work\n\\cite{spira2021} constructed a dataset of respiratory insufficiency COVID-19\npatient utterances and analyzed it by means of a convolutional neural network\nachieving an accuracy of $87.04\\%$, validating the hypothesis that one can\ndetect RI through speech. Here, we study how Transformer neural network\narchitectures can improve the performance on RI detection. This approach\nenables construction of an acoustic model. By choosing the correct pretraining\ntechnique, we generate a self-supervised acoustic model, leading to improved\nperformance ($96.53\\%$) of Transformers for RI detection.",
    "descriptor": "",
    "authors": [
      "Marcelo Matheus Gauy",
      "Marcelo Finger"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.14085"
  },
  {
    "id": "arXiv:2210.14093",
    "title": "Deautoconvolution in the two-dimensional case",
    "abstract": "There is extensive mathematical literature on the inverse problem of\ndeautoconvolution for a function with support in the unit interval $[0,1]\n\\subset \\mathbb R$, but little is known about the multidimensional situation.\nThis article tries to fill this gap with analytical and numerical studies on\nthe reconstruction of a real function of two real variables over the unit\nsquare from observations of its autoconvolution on $[0,2]^2 \\subset \\mathbb\nR^2$ (full data case) or on $[0,1]^2$ (limited data case). In an $L^2$-setting,\ntwofoldness and uniqueness assertions are proven for the deautoconvolution\nproblem in 2D. Moreover, its ill-posedness is characterized and illustrated.\nExtensive numerical case studies give an overview of the behaviour of stable\napproximate solutions to the two-dimensional deautoconvolution problem obtained\nby Tikhonov-type regularization with different penalties and the iteratively\nregularized Gauss-Newton method.",
    "descriptor": "",
    "authors": [
      "Yu Deng",
      "Bernd Hofmann",
      "Frank Werner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14093"
  },
  {
    "id": "arXiv:2210.14100",
    "title": "The capacity of a finite field matrix channel",
    "abstract": "The Additive-Multiplicative Matrix Channel (AMMC) was introduced by Silva,\nKschischang and K\\\"otter in 2010 to model data transmission using random linear\nnetwork coding. The input and output of the channel are $n\\times m$ matrices\nover a finite field $\\mathbb{F}_q$. On input the matrix $X$, the channel\noutputs $Y=A(X+W)$ where $A$ is a uniformly chosen $n\\times n$ invertible\nmatrix over $\\mathbb{F}_q$ and where $W$ is a uniformly chosen $n\\times m$\nmatrix over $\\mathbb{F}_q$ of rank $t$.\nSilva \\emph{et al} considered the case when $2n\\leq m$. They determined the\nasymptotic capacity of the AMMC when $t$, $n$ and $m$ are fixed and\n$q\\rightarrow\\infty$. They also determined the leading term of the capacity\nwhen $q$ is fixed, and $t$, $n$ and $m$ grow linearly. We generalise these\nresults, showing that the condition $2n\\geq m$ can be removed. (Our formula for\nthe capacity falls into two cases, one of which generalises the $2n\\geq m$\ncase.) We also improve the error term in the case when $q$ is fixed.",
    "descriptor": "\nComments: 32 pages, 1 figure\n",
    "authors": [
      "Simon R. Blackburn",
      "Jessica Claridge"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.14100"
  },
  {
    "id": "arXiv:2210.14101",
    "title": "SPAD-Based Optical Wireless Communication with ACO-OFDM",
    "abstract": "The sensitivity of the optical wireless communication (OWC) can be\neffectively improved by employing the highly sensitive single-photon avalanche\ndiode (SPAD) arrays. However, the nonlinear distortion introduced by the dead\ntime strongly limits the throughput of the SPAD-based OWC systems. Optical\northogonal frequency division multiplexing (OFDM) can be employed in the\nsystems with SPAD arrays to improve the spectral efficiency. In this work, a\ntheoretical performance analysis of SPAD-based OWC system with\nasymmetrically-clipped optical OFDM (ACO-OFDM) is presented. The impact of the\nSPAD nonlinearity on the system performance is investigated. In addition, the\ncomparison of the considered scheme with direct-current-biased optical OFDM\n(DCO-OFDM) is presented showing the distinct reliable operation regimes of the\ntwo schemes. In the low power regimes, ACO-OFDM outperforms DCO-OFDM; whereas,\nthe latter is more preferable in the high power regimes.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2206.02062\n",
    "authors": [
      "Shenjie Huang",
      "Cheng Chen",
      "Mohammad Dehghani Soltani",
      "Robert Henderson",
      "Harald Haas",
      "Majid Safari"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14101"
  },
  {
    "id": "arXiv:2210.14102",
    "title": "Exploring Mode Connectivity for Pre-trained Language Models",
    "abstract": "Recent years have witnessed the prevalent application of pre-trained language\nmodels (PLMs) in NLP. From the perspective of parameter space, PLMs provide\ngeneric initialization, starting from which high-performance minima could be\nfound. Although plenty of works have studied how to effectively and efficiently\nadapt PLMs to high-performance minima, little is known about the connection of\nvarious minima reached under different adaptation configurations. In this\npaper, we investigate the geometric connections of different minima through the\nlens of mode connectivity, which measures whether two minima can be connected\nwith a low-loss path. We conduct empirical analyses to investigate three\nquestions: (1) how could hyperparameters, specific tuning methods, and training\ndata affect PLM's mode connectivity? (2) How does mode connectivity change\nduring pre-training? (3) How does the PLM's task knowledge change along the\npath connecting two minima? In general, exploring the mode connectivity of PLMs\nconduces to understanding the geometric connection of different minima, which\nmay help us fathom the inner workings of PLM downstream adaptation.",
    "descriptor": "\nComments: EMNLP 2022, main conference\n",
    "authors": [
      "Yujia Qin",
      "Cheng Qian",
      "Jing Yi",
      "Weize Chen",
      "Yankai Lin",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14102"
  },
  {
    "id": "arXiv:2210.14103",
    "title": "Bit Error and Block Error Rate Training for ML-Assisted Communication",
    "abstract": "Even though machine learning (ML) techniques are being widely used in\ncommunications, the question of how to train communication systems has received\nsurprisingly little attention. In this paper, we show that the commonly used\nbinary cross-entropy (BCE) loss is a sensible choice in uncoded systems, e.g.,\nfor training ML-assisted data detectors, but may not be optimal in coded\nsystems. We propose new loss functions targeted at minimizing the block error\nrate and SNR de-weighting, a novel method that trains communication systems for\noptimal performance over a range of signal-to-noise ratios. The utility of the\nproposed loss functions as well as of SNR de-weighting is shown through\nsimulations in NVIDIA Sionna.",
    "descriptor": "\nComments: A shorter version of this paper has been submitted to the 2023 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)\n",
    "authors": [
      "Reinhard Wiesmayr",
      "Gian Marti",
      "Chris Dick",
      "Haochuan Song",
      "Christoph Studer"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.14103"
  },
  {
    "id": "arXiv:2210.14112",
    "title": "Bidirectional Integrated Sensing and Communication: Full-Duplex or  Half-Duplex?",
    "abstract": "A bidirectional integrated sensing and communication (ISAC) system is\nproposed, in which a pair of transceivers carry out two-way communication and\nmutual sensing. Both full-duplex and half-duplex operations in narrowband and\nwideband systems are conceived for the bidirectional ISAC. 1) For the\nnarrowband system, the conventional full-duplex and half-duplex operations are\nredesigned to take into account sensing echo signals. Then, the transmit\nbeamforming design of both transceivers is proposed for addressing the sensing\nand communication (S\\&C) tradeoff. A one-layer iterative algorithm relying on\nsuccessive convex approximation (SCA) is proposed to obtain Karush-Kuhn-Tucker\n(KKT) optimal solutions. 2) For the wideband system, the new full-duplex and\nhalf-duplex operations are proposed for the bidirectional ISAC. In particular,\nthe frequency-selective fading channel is tackled by delay pre-compensation and\npath-based beamforming. By redesigning the proposed SCA-based algorithm, the\nKKT optimal solutions for path-based beamforming for characterizing the S\\&C\ntradeoff are obtained. Finally, the numerical results show that: i) For both\nbandwidth scenarios, the existence of the interference introduced by sensing\nresults in full-duplex may not always outperform half-duplex, especially in the\nsensing-prior regime or when the communication channel is\nline-of-sight-dominated; and ii) For both duplex operations, it is sufficient\nto reuse communication signals for sensing in the narrowband system, while an\nadditional dedicated sensing signal is required in the wideband system.",
    "descriptor": "\nComments: 30 pages, 10 figures\n",
    "authors": [
      "Zhaolin Wang",
      "Xidong Mu",
      "Yuanwei Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14112"
  },
  {
    "id": "arXiv:2210.14114",
    "title": "An adaptive multi-fidelity framework for safety analysis of connected  and automated vehicles",
    "abstract": "Testing and evaluation are expensive but critical steps in the development\nand deployment of connected and automated vehicles (CAVs). In this paper, we\ndevelop an adaptive sampling framework to efficiently evaluate the accident\nrate of CAVs, particularly for scenario-based tests where the probability\ndistribution of input parameters is known from the Naturalistic Driving Data.\nOur framework relies on a surrogate model to approximate the CAV performance\nand a novel acquisition function to maximize the benefit (information to\naccident rate) of the next sample formulated through an information-theoretic\nconsideration. In addition to the standard application with only a single\nhigh-fidelity model of CAV performance, we also extend our approach to the\nbi-fidelity context where an additional low-fidelity model can be used at a\nlower computational cost to approximate the CAV performance. Accordingly for\nthe second case, our approach is formulated such that it allows the choice of\nthe next sample, in terms of both fidelity level (i.e., which model to use) and\nsampling location to maximize the benefit per cost. Our framework is tested in\na widely-considered two-dimensional cut-in problem for CAVs, where Intelligent\nDriving Model (IDM) with different time resolutions are used to construct the\nhigh and low-fidelity models. We show that our single-fidelity method\noutperforms the existing approach for the same problem, and the bi-fidelity\nmethod can further save half of the computational cost to reach a similar\naccuracy in estimating the accident rate.",
    "descriptor": "",
    "authors": [
      "Xianliang Gong",
      "Shuo Feng",
      "Yulin Pan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14114"
  },
  {
    "id": "arXiv:2210.14124",
    "title": "Lafite2: Few-shot Text-to-Image Generation",
    "abstract": "Text-to-image generation models have progressed considerably in recent years,\nwhich can now generate impressive realistic images from arbitrary text. Most of\nsuch models are trained on web-scale image-text paired datasets, which may not\nbe affordable for many researchers. In this paper, we propose a novel method\nfor pre-training text-to-image generation model on image-only datasets. It\nconsiders a retrieval-then-optimization procedure to synthesize pseudo text\nfeatures: for a given image, relevant pseudo text features are first retrieved,\nthen optimized for better alignment. The low requirement of the proposed method\nyields high flexibility and usability: it can be beneficial to a wide range of\nsettings, including the few-shot, semi-supervised and fully-supervised\nlearning; it can be applied on different models including generative\nadversarial networks (GANs) and diffusion models. Extensive experiments\nillustrate the effectiveness of the proposed method. On MS-COCO dataset, our\nGAN model obtains Fr\\'echet Inception Distance (FID) of 6.78 which is the new\nstate-of-the-art (SoTA) of GANs under fully-supervised setting. Our diffusion\nmodel obtains FID of 8.42 and 4.28 on zero-shot and supervised setting\nrespectively, which are competitive to SoTA diffusion models with a much\nsmaller model size.",
    "descriptor": "",
    "authors": [
      "Yufan Zhou",
      "Chunyuan Li",
      "Changyou Chen",
      "Jianfeng Gao",
      "Jinhui Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14124"
  },
  {
    "id": "arXiv:2210.14127",
    "title": "Goal-Driven Context-Aware Next Service Recommendation for Mashup  Composition",
    "abstract": "As service-oriented architecture becoming one of the most prevalent\ntechniques to rapidly deliver functionalities to customers, increasingly more\nreusable software components have been published online in forms of web\nservices. To create a mashup, it gets not only time-consuming but also\nerror-prone for developers to find suitable services from such a sea of\nservices. Service discovery and recommendation has thus attracted significant\nmomentum in both academia and industry. This paper proposes a novel incremental\nrecommend-as-you-go approach to recommending next potential service based on\nthe context of a mashup under construction, considering services that have been\nselected to the current step as well as its mashup goal. The core technique is\nan algorithm of learning the embedding of services, which learns their past\ngoal-driven context-aware decision making behaviors in addition to their\nsemantic descriptions and co-occurrence history. A goal exclusionary negative\nsampling mechanism tailored for mashup development is also developed to improve\ntraining performance. Extensive experiments on a real-world dataset demonstrate\nthe effectiveness of our approach.",
    "descriptor": "",
    "authors": [
      "Xihao Xie",
      "Jia Zhang",
      "Rahul Ramachandran",
      "Tsengdar J. Lee",
      "Seungwon Lee"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14127"
  },
  {
    "id": "arXiv:2210.14128",
    "title": "IELM: An Open Information Extraction Benchmark for Pre-Trained Language  Models",
    "abstract": "We introduce a new open information extraction (OIE) benchmark for\npre-trained language models (LM). Recent studies have demonstrated that\npre-trained LMs, such as BERT and GPT, may store linguistic and relational\nknowledge. In particular, LMs are able to answer ``fill-in-the-blank''\nquestions when given a pre-defined relation category. Instead of focusing on\npre-defined relations, we create an OIE benchmark aiming to fully examine the\nopen relational information present in the pre-trained LMs. We accomplish this\nby turning pre-trained LMs into zero-shot OIE systems. Surprisingly,\npre-trained LMs are able to obtain competitive performance on both standard OIE\ndatasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets\n(TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For\ninstance, the zero-shot pre-trained LMs outperform the F1 score of the\nstate-of-the-art supervised OIE methods on our factual OIE datasets without\nneeding to use any training sets. Our code and datasets are available at\nhttps://github.com/cgraywang/IELM",
    "descriptor": "\nComments: EMNLP 2022. arXiv admin note: substantial text overlap with arXiv:2010.11967\n",
    "authors": [
      "Chenguang Wang",
      "Xiao Liu",
      "Dawn Song"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14128"
  },
  {
    "id": "arXiv:2210.14129",
    "title": "A Deep Fourier Residual Method for solving PDEs using Neural Networks",
    "abstract": "When using Neural Networks as trial functions to numerically solve PDEs, a\nkey choice to be made is the loss function to be minimised, which should\nideally correspond to a norm of the error. In multiple problems, this error\nnorm coincides with--or is equivalent to--the $H^{-1}$-norm of the residual;\nhowever, it is often difficult to accurately compute it. This work assumes\nrectangular domains and proposes the use of a Discrete Sine/Cosine Transform to\naccurately and efficiently compute the $H^{-1}$ norm. The resulting Deep\nFourier-based Residual (DFR) method efficiently and accurately approximate\nsolutions to PDEs. This is particularly useful when solutions lack $H^{2}$\nregularity and methods involving strong formulations of the PDE fail. We\nobserve that the $H^1$-error is highly correlated with the discretised loss\nduring training, which permits accurate error estimation via the loss.",
    "descriptor": "",
    "authors": [
      "Jamie M. Taylor",
      "David Pardo",
      "Ignacio Muga"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.14129"
  },
  {
    "id": "arXiv:2210.14136",
    "title": "PolyHope: Dataset Creation for a Two-Level Hope Speech Detection Task  from Tweets",
    "abstract": "Hope is characterized as openness of spirit toward the future, a desire,\nexpectation, and wish for something to happen or to be true that remarkably\naffects human's state of mind, emotions, behaviors, and decisions. Hope is\nusually associated with concepts of desired expectations and\npossibility/probability concerning the future. Despite its importance, hope has\nrarely been studied as a social media analysis task. This paper presents a hope\nspeech dataset that classifies each tweet first into \"Hope\" and \"Not Hope\",\nthen into three fine-grained hope categories: \"Generalized Hope\", \"Realistic\nHope\", and \"Unrealistic Hope\" (along with \"Not Hope\"). English tweets in the\nfirst half of 2022 were collected to build this dataset. Furthermore, we\ndescribe our annotation process and guidelines in detail and discuss the\nchallenges of classifying hope and the limitations of the existing hope speech\ndetection corpora. In addition, we reported several baselines based on\ndifferent learning approaches, such as traditional machine learning, deep\nlearning, and transformers, to benchmark our dataset. We evaluated our\nbaselines using weighted-averaged and macro-averaged F1-scores. Observations\nshow that a strict process for annotator selection and detailed annotation\nguidelines enhanced the dataset's quality. This strict annotation process\nresulted in promising performance for simple machine learning classifiers with\nonly bi-grams; however, binary and multiclass hope speech detection results\nreveal that contextual embedding models have higher performance in this\ndataset.",
    "descriptor": "\nComments: 20 pages, 9 figures\n",
    "authors": [
      "Fazlourrahman Balouchzahi",
      "Grigori Sidorov",
      "Alexander Gelbukh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14136"
  },
  {
    "id": "arXiv:2210.14139",
    "title": "Learning Explicit Object-Centric Representations with Vision  Transformers",
    "abstract": "With the recent successful adaptation of transformers to the vision domain,\nparticularly when trained in a self-supervised fashion, it has been shown that\nvision transformers can learn impressive object-reasoning-like behaviour and\nfeatures expressive for the task of object segmentation in images. In this\npaper, we build on the self-supervision task of masked autoencoding and explore\nits effectiveness for explicitly learning object-centric representations with\ntransformers. To this end, we design an object-centric autoencoder using\ntransformers only and train it end-to-end to reconstruct full images from\nunmasked patches. We show that the model efficiently learns to decompose simple\nscenes as measured by segmentation metrics on several multi-object benchmarks.",
    "descriptor": "",
    "authors": [
      "Oscar Vikstr\u00f6m",
      "Alexander Ilin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14139"
  },
  {
    "id": "arXiv:2210.14140",
    "title": "Contrastive Search Is What You Need For Neural Text Generation",
    "abstract": "Generating text with autoregressive language models (LMs) is of great\nimportance to many natural language processing (NLP) applications. Previous\nsolutions for this task often produce text that contains degenerative\nexpressions or lacks semantic consistency. Recently, Su et al. introduced a new\ndecoding method, contrastive search, based on the isotropic representation\nspace of the language model and obtained new state of the art on various\nbenchmarks. Additionally, Su et al. argued that the representations of\nautoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also\nshared by previous study. Therefore, to ensure the language model follows an\nisotropic distribution, Su et al. proposed a contrastive learning scheme,\nSimCTG, which calibrates the language model's representations through\nadditional training.\nIn this study, we first answer the question: \"Are autoregressive LMs really\nanisotropic?\". To this end, we extensively evaluate the isotropy of LMs across\n16 major languages. Surprisingly, we find that the anisotropic problem only\nexists in the two specific English GPT-2-small/medium models. On the other\nhand, all other evaluated LMs are naturally isotropic which is in contrast to\nthe conclusion drawn by previous studies. Based on our findings, we further\nassess the contrastive search decoding method using off-the-shelf LMs on four\ngeneration tasks across 16 languages. Our experimental results demonstrate that\ncontrastive search significantly outperforms previous decoding methods without\nany additional training. More notably, on 12 out of 16 evaluated languages,\ncontrastive search performs comparably with human-level performances as judged\nby human evaluations.",
    "descriptor": "\nComments: 20 pages, 5 figures, 14 tables. Work in progress\n",
    "authors": [
      "Yixuan Su",
      "Nigel Collier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14140"
  },
  {
    "id": "arXiv:2210.14142",
    "title": "From colouring-in to pointillism: revisiting semantic segmentation  supervision",
    "abstract": "The prevailing paradigm for producing semantic segmentation training data\nrelies on densely labelling each pixel of each image in the training set, akin\nto colouring-in books. This approach becomes a bottleneck when scaling up in\nthe number of images, classes, and annotators. Here we propose instead a\npointillist approach for semantic segmentation annotation, where only\npoint-wise yes/no questions are answered. We explore design alternatives for\nsuch an active learning approach, measure the speed and consistency of human\nannotators on this task, show that this strategy enables training good\nsegmentation models, and that it is suitable for evaluating models at test\ntime. As concrete proof of the scalability of our method, we collected and\nreleased 22.6M point labels over 4,171 classes on the Open Images dataset. Our\nresults enable to rethink the semantic segmentation pipeline of annotation,\ntraining, and evaluation from a pointillism point of view.",
    "descriptor": "\nComments: Open Images V7 available at this https URL\n",
    "authors": [
      "Rodrigo Benenson",
      "Vittorio Ferrari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14142"
  },
  {
    "id": "arXiv:2210.14145",
    "title": "GlassesGAN: Eyewear Personalization using Synthetic Appearance Discovery  and Targeted Subspace Modeling",
    "abstract": "We present GlassesGAN, a novel image editing framework for custom design of\nglasses, that sets a new standard in terms of image quality, edit realism, and\ncontinuous multi-style edit capability. To facilitate the editing process with\nGlassesGAN, we propose a Targeted Subspace Modelling (TSM) procedure that,\nbased on a novel mechanism for (synthetic) appearance discovery in the latent\nspace of a pre-trained GAN generator, constructs an eyeglasses-specific\n(latent) subspace that the editing framework can utilize. To improve the\nreliability of our learned edits, we also introduce an appearance-constrained\nsubspace initialization (SI) technique able to center the latent representation\nof a given input image in the well-defined part of the constructed subspace. We\ntest GlassesGAN on three diverse datasets (CelebA-HQ, SiblingsDB-HQf, and\nMetFaces) and compare it against three state-of-the-art competitors, i.e.,\nInterfaceGAN, GANSpace, and MaskGAN. Our experimental results show that\nGlassesGAN achieves photo-realistic, multi-style edits to eyeglasses while\ncomparing favorably to its competitors. The source code is made freely\navailable.",
    "descriptor": "\nComments: 16 pages, 14 figures, 3 tables\n",
    "authors": [
      "Richard Plesh",
      "Peter Peer",
      "Vitomir \u0160truc"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.14145"
  },
  {
    "id": "arXiv:2210.14147",
    "title": "Food Ingredients Recognition through Multi-label Learning",
    "abstract": "The ability to recognize various food-items in a generic food plate is a key\ndeterminant for an automated diet assessment system. This study motivates the\nneed for automated diet assessment and proposes a framework to achieve this.\nWithin this framework, we focus on one of the core functionalities to visually\nrecognize various ingredients. To this end, we employed a deep multi-label\nlearning approach and evaluated several state-of-the-art neural networks for\ntheir ability to detect an arbitrary number of ingredients in a dish image. The\nmodels evaluated in this work follow a definite meta-structure, consisting of\nan encoder and a decoder component. Two distinct decoding schemes, one based on\nglobal average pooling and the other on attention mechanism, are evaluated and\nbenchmarked. Whereas for encoding, several well-known architectures, including\nDenseNet, EfficientNet, MobileNet, Inception and Xception, were employed. We\npresent promising preliminary results for deep learning-based ingredients\ndetection, using a challenging dataset, Nutrition5K, and establish a strong\nbaseline for future explorations.",
    "descriptor": "",
    "authors": [
      "Rameez Ismail",
      "Zhaorui Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14147"
  },
  {
    "id": "arXiv:2210.14149",
    "title": "Atlas flow : compatible local structures on the manifold",
    "abstract": "In this paper, we focus on the intersections of a manifold's local structures\nto analyze the global structure of a manifold. We obtain local regions on data\nmanifolds such as the latent space of StyleGAN2, using Mapper, a tool from\ntopological data analysis. We impose gluing compatibility conditions on\noverlapping local regions, which guarantee that the local structures can be\nglued together to the global structure of a manifold. We propose a novel\ngenerative flow model called Atlas flow that uses compatibility to reattach the\nlocal regions. Our model shows that the generating processes perform well on\nsynthetic dataset samples of well-known manifolds with noise. Furthermore, we\ninvestigate the style vector manifold of StyleGAN2 using our model.",
    "descriptor": "\nComments: 23 pages, 10 figures, 2 tables, 8 algorithms\n",
    "authors": [
      "Taejin Paik",
      "Jaemin Park",
      "Jung Ho Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14149"
  },
  {
    "id": "arXiv:2210.14151",
    "title": "Drastically Reducing the Number of Trainable Parameters in Deep CNNs by  Inter-layer Kernel-sharing",
    "abstract": "Deep convolutional neural networks (DCNNs) have become the state-of-the-art\n(SOTA) approach for many computer vision tasks: image classification, object\ndetection, semantic segmentation, etc. However, most SOTA networks are too\nlarge for edge computing. Here, we suggest a simple way to reduce the number of\ntrainable parameters and thus the memory footprint: sharing kernels between\nmultiple convolutional layers. Kernel-sharing is only possible between\n``isomorphic\" layers, i.e.layers having the same kernel size, input and output\nchannels. This is typically the case inside each stage of a DCNN. Our\nexperiments on CIFAR-10 and CIFAR-100, using the ConvMixer and SE-ResNet\narchitectures show that the number of parameters of these models can\ndrastically be reduced with minimal cost on accuracy. The resulting networks\nare appealing for certain edge computing applications that are subject to\nsevere memory constraints, and even more interesting if leveraging \"frozen\nweights\" hardware accelerators. Kernel-sharing is also an efficient\nregularization method, which can reduce overfitting. The codes are publicly\navailable at https://github.com/AlirezaAzadbakht/kernel-sharing.",
    "descriptor": "",
    "authors": [
      "Alireza Azadbakht",
      "Saeed Reza Kheradpisheh",
      "Ismail Khalfaoui-Hassani",
      "Timoth\u00e9e Masquelier"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14151"
  },
  {
    "id": "arXiv:2210.14153",
    "title": "Detection of Real-time DeepFakes in Video Conferencing with Active  Probing and Corneal Reflection",
    "abstract": "The COVID pandemic has led to the wide adoption of online video calls in\nrecent years. However, the increasing reliance on video calls provides\nopportunities for new impersonation attacks by fraudsters using the advanced\nreal-time DeepFakes. Real-time DeepFakes pose new challenges to detection\nmethods, which have to run in real-time as a video call is ongoing. In this\npaper, we describe a new active forensic method to detect real-time DeepFakes.\nSpecifically, we authenticate video calls by displaying a distinct pattern on\nthe screen and using the corneal reflection extracted from the images of the\ncall participant's face. This pattern can be induced by a call participant\ndisplaying on a shared screen or directly integrated into the video-call\nclient. In either case, no specialized imaging or lighting hardware is\nrequired. Through large-scale simulations, we evaluate the reliability of this\napproach under a range in a variety of real-world imaging scenarios.",
    "descriptor": "",
    "authors": [
      "Hui Guo",
      "Xin Wang",
      "Siwei Lyu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14153"
  },
  {
    "id": "arXiv:2210.14157",
    "title": "Isomorphic mesh generation from point clouds with multilayer perceptrons",
    "abstract": "We propose a new neural network, called isomorphic mesh generator (iMG),\nwhich generates isomorphic meshes from point clouds containing noise and\nmissing parts. Isomorphic meshes of arbitrary objects have a unified mesh\nstructure even though the objects belong to different classes. This unified\nrepresentation enables surface models to be handled by DNNs. Moreover, the\nunified mesh structure of isomorphic meshes enables the same process to be\napplied to all isomorphic meshes; although in the case of general mesh models,\nwe need to consider the processes depending on their mesh structures.\nTherefore, the use of isomorphic meshes leads to efficient memory usage and\ncalculation time compared with general mesh models. As iMG is a data-free\nmethod, preparing any point clouds as training data in advance is unnecessary,\nexcept a point cloud of the target object used as the input data of iMG.\nAdditionally, iMG outputs an isomorphic mesh obtained by mapping a reference\nmesh to a given input point cloud. To estimate the mapping function stably, we\nintroduce a step-by-step mapping strategy. This strategy achieves a flexible\ndeformation while maintaining the structure of the reference mesh. From\nsimulation and experiments using a mobile phone, we confirmed that iMG can\ngenerate isomorphic meshes of given objects reliably even when the input point\ncloud includes noise and missing parts.",
    "descriptor": "",
    "authors": [
      "Shoko Miyauchi",
      "Ken'ichi Morooka",
      "Ryo Kurazume"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14157"
  },
  {
    "id": "arXiv:2210.14161",
    "title": "Aligning MAGMA by Few-Shot Learning and Finetuning",
    "abstract": "The goal of vision-language modeling is to allow models to tie language\nunderstanding with visual inputs. The aim of this paper is to evaluate and\nalign the Visual Language Model (VLM) called Multimodal Augmentation of\nGenerative Models through Adapter-based finetuning (MAGMA) with human values.\nMAGMA is a VLM that is capable of image captioning and visual\nquestion-answering. We will evaluate its alignment in three different\nscenarios. To begin, we assess MAGMA's out-of-the-box alignment through the\ncheckpoint provided by Hugging Face. Then, we measure if few-shot learning\nmanages to improve the results. Finally, we finetune the model on aligned\nexamples and evaluate its behavior.",
    "descriptor": "\nComments: Accepted by the Montreal AI Symposium conference in 2022\n",
    "authors": [
      "Jean-Charles Layoun",
      "Alexis Roger",
      "Irina Rish"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14161"
  },
  {
    "id": "arXiv:2210.14162",
    "title": "Commonsense Knowledge from Scene Graphs for Textual Environments",
    "abstract": "Text-based games are becoming commonly used in reinforcement learning as\nreal-world simulation environments. They are usually imperfect information\ngames, and their interactions are only in the textual modality. To challenge\nthese games, it is effective to complement the missing information by providing\nknowledge outside the game, such as human common sense. However, such knowledge\nhas only been available from textual information in previous works. In this\npaper, we investigate the advantage of employing commonsense reasoning obtained\nfrom visual datasets such as scene graph datasets. In general, images convey\nmore comprehensive information compared with text for humans. This property\nenables to extract commonsense relationship knowledge more useful for acting\neffectively in a game. We compare the statistics of spatial relationships\navailable in Visual Genome (a scene graph dataset) and ConceptNet (a text-based\nknowledge) to analyze the effectiveness of introducing scene graph datasets. We\nalso conducted experiments on a text-based game task that requires commonsense\nreasoning. Our experimental results demonstrated that our proposed methods have\nhigher and competitive performance than existing state-of-the-art methods.",
    "descriptor": "\nComments: AAAI-22 Workshop on Reinforcement Learning in Games\n",
    "authors": [
      "Tsunehiko Tanaka",
      "Daiki Kimura",
      "Michiaki Tatsubori"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14162"
  },
  {
    "id": "arXiv:2210.14163",
    "title": "Multi-Granularity Cross-Modality Representation Learning for Named  Entity Recognition on Social Media",
    "abstract": "Named Entity Recognition (NER) on social media refers to discovering and\nclassifying entities from unstructured free-form content, and it plays an\nimportant role for various applications such as intention understanding and\nuser recommendation. With social media posts tending to be multimodal,\nMultimodal Named Entity Recognition (MNER) for the text with its accompanying\nimage is attracting more and more attention since some textual components can\nonly be understood in combination with visual information. However, there are\ntwo drawbacks in existing approaches: 1) Meanings of the text and its\naccompanying image do not match always, so the text information still plays a\nmajor role. However, social media posts are usually shorter and more informal\ncompared with other normal contents, which easily causes incomplete semantic\ndescription and the data sparsity problem. 2) Although the visual\nrepresentations of whole images or objects are already used, existing methods\nignore either fine-grained semantic correspondence between objects in images\nand words in text or the objective fact that there are misleading objects or no\nobjects in some images. In this work, we solve the above two problems by\nintroducing the multi-granularity cross-modality representation learning. To\nresolve the first problem, we enhance the representation by semantic\naugmentation for each word in text. As for the second issue, we perform the\ncross-modality semantic interaction between text and vision at the different\nvision granularity to get the most effective multimodal guidance representation\nfor every word. Experiments show that our proposed approach can achieve the\nSOTA or approximate SOTA performance on two benchmark datasets of tweets. The\ncode, data and the best performing models are available at\nhttps://github.com/LiuPeiP-CS/IIE4MNER",
    "descriptor": "",
    "authors": [
      "Peipei Liu",
      "Gaosheng Wang",
      "Hong Li",
      "Jie Liu",
      "Yimo Ren",
      "Hongsong Zhu",
      "Limin Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.14163"
  },
  {
    "id": "arXiv:2210.14164",
    "title": "Model-Free Prediction of Adversarial Drop Points in 3D Point Clouds",
    "abstract": "Adversarial attacks pose serious challenges for deep neural network\n(DNN)-based analysis of various input signals. In the case of 3D point clouds,\nmethods have been developed to identify points that play a key role in the\nnetwork decision, and these become crucial in generating existing adversarial\nattacks. For example, a saliency map approach is a popular method for\nidentifying adversarial drop points, whose removal would significantly impact\nthe network decision. Generally, methods for identifying adversarial points\nrely on the deep model itself in order to determine which points are critically\nimportant for the model's decision. This paper aims to provide a novel\nviewpoint on this problem, in which adversarial points can be predicted\nindependently of the model. To this end, we define 14 point cloud features and\nuse multiple linear regression to examine whether these features can be used\nfor model-free adversarial point prediction, and which combination of features\nis best suited for this purpose. Experiments show that a suitable combination\nof features is able to predict adversarial points of three different networks\n-- PointNet, PointNet++, and DGCNN -- significantly better than a random guess.\nThe results also provide further insight into DNNs for point cloud analysis, by\nshowing which features play key roles in their decision-making process.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Hanieh Naderi",
      "Chinthaka Dinesh",
      "Ivan V. Bajic",
      "Shohreh Kasaei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14164"
  },
  {
    "id": "arXiv:2210.14165",
    "title": "MEEV: Body Mesh Estimation On Egocentric Video",
    "abstract": "This technical report introduces our solution, MEEV, proposed to the EgoBody\nChallenge at ECCV 2022. Captured from head-mounted devices, the dataset\nconsists of human body shape and motion of interacting people. The EgoBody\ndataset has challenges such as occluded body or blurry image. In order to\novercome the challenges, MEEV is designed to exploit multiscale features for\nrich spatial information. Besides, to overcome the limited size of dataset, the\nmodel is pre-trained with the dataset aggregated 2D and 3D pose estimation\ndatasets. Achieving 82.30 for MPJPE and 92.93 for MPVPE, MEEV has won the\nEgoBody Challenge at ECCV 2022, which shows the effectiveness of the proposed\nmethod. The code is available at https://github.com/clovaai/meev",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Nicolas Monet",
      "Dongyoon Wee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14165"
  },
  {
    "id": "arXiv:2210.14169",
    "title": "Weakly Supervised Data Augmentation Through Prompting for Dialogue  Understanding",
    "abstract": "Dialogue understanding tasks often necessitate abundant annotated data to\nachieve good performance and that presents challenges in low-resource settings.\nTo alleviate this barrier, we explore few-shot data augmentation for dialogue\nunderstanding by prompting large pre-trained language models and present a\nnovel approach that iterates on augmentation quality by applying\nweakly-supervised filters. We evaluate our methods on the emotion and act\nclassification tasks in DailyDialog and the intent classification task in\nFacebook Multilingual Task-Oriented Dialogue. Models fine-tuned on our\naugmented data mixed with few-shot ground truth data are able to approach or\nsurpass existing state-of-the-art performance on both datasets. For DailyDialog\nspecifically, using 10% of the ground truth data we outperform the current\nstate-of-the-art model which uses 100% of the data.",
    "descriptor": "\nComments: To appear in SyntheticData4ML @ NeurIPS 2022. 16 pages, 10 figures, 3 tables\n",
    "authors": [
      "Maximillian Chen",
      "Alexandros Papangelis",
      "Chenyang Tao",
      "Andy Rosenbaum",
      "Seokhwan Kim",
      "Yang Liu",
      "Zhou Yu",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14169"
  },
  {
    "id": "arXiv:2210.14174",
    "title": "Towards Interpretable Summary Evaluation via Allocation of Contextual  Embeddings to Reference Text Topics",
    "abstract": "Despite extensive recent advances in summary generation models, evaluation of\nauto-generated summaries still widely relies on single-score systems\ninsufficient for transparent assessment and in-depth qualitative analysis.\nTowards bridging this gap, we propose the multifaceted interpretable summary\nevaluation method (MISEM), which is based on allocation of a summary's\ncontextual token embeddings to semantic topics identified in the reference\ntext. We further contribute an interpretability toolbox for automated summary\nevaluation and interactive visual analysis of summary scoring, topic\nidentification, and token-topic allocation. MISEM achieves a promising .404\nPearson correlation with human judgment on the TAC'08 dataset.",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Ben Schaper",
      "Christopher Lohse",
      "Marcell Streile",
      "Andrea Giovannini",
      "Richard Osuala"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14174"
  },
  {
    "id": "arXiv:2210.14177",
    "title": "Influence Functions for Sequence Tagging Models",
    "abstract": "Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,\nand Semantic Role Labeling) are naturally framed as sequence tagging problems.\nHowever, there has been comparatively little work on interpretability methods\nfor sequence tagging models. In this paper, we extend influence functions -\nwhich aim to trace predictions back to the training points that informed them -\nto sequence tagging tasks. We define the influence of a training instance\nsegment as the effect that perturbing the labels within this segment has on a\ntest segment level prediction. We provide an efficient approximation to compute\nthis, and show that it tracks with the true segment influence, measured\nempirically. We show the practical utility of segment influence by using the\nmethod to identify systematic annotation errors in two named entity recognition\ncorpora. Code to reproduce our results is available at\nhttps://github.com/successar/Segment_Influence_Functions.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022\n",
    "authors": [
      "Sarthak Jain",
      "Varun Manjunatha",
      "Byron C. Wallace",
      "Ani Nenkova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.14177"
  },
  {
    "id": "arXiv:2210.14179",
    "title": "Practical Program Repair in the Era of Large Pre-trained Language Models",
    "abstract": "Automated Program Repair (APR) aims to help developers automatically patch\nsoftware bugs. However, current state-of-the-art traditional and learning-based\nAPR techniques face the problem of limited patch variety, failing to fix\ncomplicated bugs. This is mainly due to the reliance on bug-fixing datasets to\ncraft fix templates or directly predict potential patches. Large Pre-Trained\nLanguage Models (PLMs), trained using billions of text/code tokens, can\npotentially help avoid this issue. Very recently, researchers have directly\nleveraged PLMs for APR without relying on any bug-fixing datasets. Meanwhile,\nsuch existing work either failed to include state-of-the-art PLMs or was not\nevaluated on realistic datasets.\nIn this work, we perform the first extensive study on directly applying PLMs\nfor APR. We select 9 recent state-of-the-art PLMs, including both generative\nand infilling models, ranging from 125M to 20B in size. We designed 3 different\nrepair settings to evaluate the different ways we can use PLMs to generate\npatches. We apply the PLMs under these repair settings on 5 datasets across 3\ndifferent languages and compare different PLMs in the number of bugs fixed,\ngeneration speed and compilation rate. Our study demonstrates that directly\napplying state-of-the-art PLMs can already substantially outperform all\nexisting APR techniques on all our datasets. Among the studied PLMs, the\nscaling effect exists for APR where larger models tend to achieve better\nperformance. Also, we show for the first time that suffix code after the buggy\nline (adopted in infilling-style APR) is important in not only generating more\nfixes but more patches with higher compilation rate. Besides patch generation,\nthe PLMs consider correct patches to be more natural than other ones, and can\neven be leveraged for effective patch ranking or patch correctness checking.",
    "descriptor": "",
    "authors": [
      "Chunqiu Steven Xia",
      "Yuxiang Wei",
      "Lingming Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.14179"
  },
  {
    "id": "arXiv:2210.14188",
    "title": "MOFormer: Self-Supervised Transformer model for Metal-Organic Framework  Property Prediction",
    "abstract": "Metal-Organic Frameworks (MOFs) are materials with a high degree of porosity\nthat can be used for applications in energy storage, water desalination, gas\nstorage, and gas separation. However, the chemical space of MOFs is close to an\ninfinite size due to the large variety of possible combinations of building\nblocks and topology. Discovering the optimal MOFs for specific applications\nrequires an efficient and accurate search over an enormous number of potential\ncandidates. Previous high-throughput screening methods using computational\nsimulations like DFT can be time-consuming. Such methods also require\noptimizing 3D atomic structure of MOFs, which adds one extra step when\nevaluating hypothetical MOFs. In this work, we propose a structure-agnostic\ndeep learning method based on the Transformer model, named as MOFormer, for\nproperty predictions of MOFs. The MOFormer takes a text string representation\nof MOF (MOFid) as input, thus circumventing the need of obtaining the 3D\nstructure of hypothetical MOF and accelerating the screening process.\nFurthermore, we introduce a self-supervised learning framework that pretrains\nthe MOFormer via maximizing the cross-correlation between its\nstructure-agnostic representations and structure-based representations of\ncrystal graph convolutional neural network (CGCNN) on >400k publicly available\nMOF data. Using self-supervised learning allows the MOFormer to intrinsically\nlearn 3D structural information though it is not included in the input.\nExperiments show that pretraining improved the prediction accuracy of both\nmodels on various downstream prediction tasks. Furthermore, we revealed that\nMOFormer can be more data-efficient on quantum-chemical property prediction\nthan structure-based CGCNN when training data is limited. Overall, MOFormer\nprovides a novel perspective on efficient MOF design using deep learning.",
    "descriptor": "\nComments: ZC and RM share joint first authorship. Main+SI have 34 pages, 5 figures and 2 tables in the main manuscript\n",
    "authors": [
      "Zhonglin Cao",
      "Rishikesh Magar",
      "Yuyang Wang",
      "Amir Barati Farimani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14188"
  },
  {
    "id": "arXiv:2210.14189",
    "title": "Benchmarking Graph Neural Networks for Internet Routing Data",
    "abstract": "The Internet is composed of networks, called Autonomous Systems (or, ASes),\ninterconnected to each other, thus forming a large graph. While both the\nAS-graph is known and there is a multitude of data available for the ASes\n(i.e., node attributes), the research on applying graph machine learning (ML)\nmethods on Internet data has not attracted a lot of attention. In this work, we\nprovide a benchmarking framework aiming to facilitate research on Internet data\nusing graph-ML and graph neural network (GNN) methods. Specifically, we compile\na dataset with heterogeneous node/AS attributes by collecting data from\nmultiple online sources, and preprocessing them so that they can be easily used\nas input in GNN architectures. Then, we create a framework/pipeline for\napplying GNNs on the compiled data. For a set of tasks, we perform a\nbenchmarking of different GNN models (as well as, non-GNN ML models) to test\ntheir efficiency; our results can serve as a common baseline for future\nresearch and provide initial insights for the application of GNNs on Internet\ndata.",
    "descriptor": "",
    "authors": [
      "Dimitrios Panteleimon Giakatos",
      "Sofia Kostoglou",
      "Pavlos Sermpezis",
      "Athena Vakali"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.14189"
  },
  {
    "id": "arXiv:2210.14190",
    "title": "CrisisLTLSum: A Benchmark for Local Crisis Event Timeline Extraction and  Summarization",
    "abstract": "Social media has increasingly played a key role in emergency response: first\nresponders can use public posts to better react to ongoing crisis events and\ndeploy the necessary resources where they are most needed. Timeline extraction\nand abstractive summarization are critical technical tasks to leverage large\nnumbers of social media posts about events. Unfortunately, there are few\ndatasets for benchmarking technical approaches for those tasks. This paper\npresents CrisisLTLSum, the largest dataset of local crisis event timelines\navailable to date. CrisisLTLSum contains 1,000 crisis event timelines across\nfour domains: wildfires, local fires, traffic, and storms. We built\nCrisisLTLSum using a semi-automated cluster-then-refine approach to collect\ndata from the public Twitter stream. Our initial experiments indicate a\nsignificant gap between the performance of strong baselines compared to the\nhuman performance on both tasks. Our dataset, code, and models are publicly\navailable.",
    "descriptor": "",
    "authors": [
      "Hossein Rajaby Faghihi",
      "Bashar Alhafni",
      "Ke Zhang",
      "Shihao Ran",
      "Joel Tetreault",
      "Alejandro Jaimes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.14190"
  },
  {
    "id": "arXiv:2210.14199",
    "title": "Same Pre-training Loss, Better Downstream: Implicit Bias Matters for  Language Models",
    "abstract": "Language modeling on large-scale datasets leads to impressive performance\ngains on various downstream language tasks. The validation pre-training loss\n(or perplexity in autoregressive language modeling) is often used as the\nevaluation metric when developing language models since the pre-training loss\ntends to be well-correlated with downstream performance (which is itself\ndifficult to evaluate comprehensively). Contrary to this conventional wisdom,\nthis paper shows that 1) pre-training loss cannot fully explain downstream\nperformance and 2) flatness of the model is well-correlated with downstream\nperformance where pre-training loss is not. On simplified datasets, we identify\nthree ways to produce models with the same (statistically optimal) pre-training\nloss but different downstream performance: continue pre-training after\nconvergence, increasing the model size, and changing the training algorithm.\nThese experiments demonstrate the existence of implicit bias of pre-training\nalgorithms/optimizers -- among models with the same minimal pre-training loss,\nthey implicitly prefer more transferable ones. Toward understanding this\nimplicit bias, we prove that SGD with standard mini-batch noise implicitly\nprefers flatter minima in language models, and empirically observe a strong\ncorrelation between flatness and downstream performance among models with the\nsame minimal pre-training loss. We also prove in a synthetic language setting\nthat among the models with the minimal pre-training loss, the flattest model\ntransfers to downstream tasks.",
    "descriptor": "",
    "authors": [
      "Hong Liu",
      "Sang Michael Xie",
      "Zhiyuan Li",
      "Tengyu Ma"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14199"
  },
  {
    "id": "arXiv:2210.14204",
    "title": "pmuBAGE: The Benchmarking Assortment of Generated PMU Data for Power  System Events",
    "abstract": "This paper introduces pmuGE (phasor measurement unit Generator of Events),\none of the first data-driven generative model for power system event data. We\nhave trained this model on thousands of actual events and created a dataset\ndenoted pmuBAGE (the Benchmarking Assortment of Generated PMU Events). The\ndataset consists of almost 1000 instances of labeled event data to encourage\nbenchmark evaluations on phasor measurement unit (PMU) data analytics. PMU data\nare challenging to obtain, especially those covering event periods.\nNevertheless, power system problems have recently seen phenomenal advancements\nvia data-driven machine learning solutions. A highly accessible standard\nbenchmarking dataset would enable a drastic acceleration of the development of\nsuccessful machine learning techniques in this field. We propose a novel\nlearning method based on the Event Participation Decomposition of Power System\nEvents, which makes it possible to learn a generative model of PMU data during\nsystem anomalies. The model can create highly realistic event data without\ncompromising the differential privacy of the PMUs used to train it. The dataset\nis available online for any researcher or practitioner to use at the pmuBAGE\nGithub Repository: https://github.com/NanpengYu/pmuBAGE.",
    "descriptor": "\nComments: 13 pages. arXiv admin note: substantial text overlap with arXiv:2204.01095\n",
    "authors": [
      "Brandon Foggo",
      "Koji Yamashita",
      "Nanpeng Yu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14204"
  },
  {
    "id": "arXiv:2210.14208",
    "title": "Orchestrating Networked Robotic Applications",
    "abstract": "In this letter, we formulate the orchestration problem for networked robotic\napplications considering contextual information. Any solution to the proposed\nformulation provides adequate routing updates, migration, and radio handover\ndecisions as the robot moves. We prove the NP-hard nature of the problem, and\nsolve it for a remote driving robotic application with or without some\ncontextual information, as in state-of-the-art. Results show that without\ncontextual information it is impossible to meet the latency requirements of a\nremote-driving robotic application.",
    "descriptor": "\nComments: 4 Pages, 3 figures, submitted to IEEE Communications letter\n",
    "authors": [
      "Khasa Gillani",
      "Jorge Mart\u00edn P\u00e9rez",
      "Milan Groshev",
      "Antonio de la Oliva",
      "Carlos J. Bernardos",
      "Robert Gazda"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.14208"
  },
  {
    "id": "arXiv:2210.14210",
    "title": "MidasTouch: Monte-Carlo inference over distributions across sliding  touch",
    "abstract": "We present MidasTouch, a tactile perception system for online global\nlocalization of a vision-based touch sensor sliding on an object surface. This\nframework takes in posed tactile images over time, and outputs an evolving\ndistribution of sensor pose on the object's surface, without the need for\nvisual priors. Our key insight is to estimate local surface geometry with\ntactile sensing, learn a compact representation for it, and disambiguate these\nsignals over a long time horizon. The backbone of MidasTouch is a Monte-Carlo\nparticle filter, with a measurement model based on a tactile code network\nlearned from tactile simulation. This network, inspired by LIDAR place\nrecognition, compactly summarizes local surface geometries. These generated\ncodes are efficiently compared against a precomputed tactile codebook\nper-object, to update the pose distribution. We further release the YCB-Slide\ndataset of real-world and simulated forceful sliding interactions between a\nvision-based tactile sensor and standard YCB objects. While single-touch\nlocalization can be inherently ambiguous, we can quickly localize our sensor by\ntraversing salient surface geometries. Project page:\nhttps://suddhu.github.io/midastouch-tactile/",
    "descriptor": "\nComments: Accepted at CoRL 2022 (Oral). Project website: this https URL\n",
    "authors": [
      "Sudharshan Suresh",
      "Zilin Si",
      "Stuart Anderson",
      "Michael Kaess",
      "Mustafa Mukadam"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.14210"
  },
  {
    "id": "arXiv:2210.14215",
    "title": "In-context Reinforcement Learning with Algorithm Distillation",
    "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement\nlearning (RL) algorithms into neural networks by modeling their training\nhistories with a causal sequence model. Algorithm Distillation treats learning\nto reinforcement learn as an across-episode sequential prediction problem. A\ndataset of learning histories is generated by a source RL algorithm, and then a\ncausal transformer is trained by autoregressively predicting actions given\ntheir preceding learning histories as context. Unlike sequential policy\nprediction architectures that distill post-learning or expert sequences, AD is\nable to improve its policy entirely in-context without updating its network\nparameters. We demonstrate that AD can reinforcement learn in-context in a\nvariety of environments with sparse rewards, combinatorial task structure, and\npixel-based observations, and find that AD learns a more data-efficient RL\nalgorithm than the one that generated the source data.",
    "descriptor": "",
    "authors": [
      "Michael Laskin",
      "Luyu Wang",
      "Junhyuk Oh",
      "Emilio Parisotto",
      "Stephen Spencer",
      "Richie Steigerwald",
      "DJ Strouse",
      "Steven Hansen",
      "Angelos Filos",
      "Ethan Brooks",
      "Maxime Gazeau",
      "Himanshu Sahni",
      "Satinder Singh",
      "Volodymyr Mnih"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.14215"
  },
  {
    "id": "arXiv:2210.14219",
    "title": "Redistributor: Transforming Empirical Data Distributions",
    "abstract": "We present an algorithm and package, Redistributor, which forces a collection\nof scalar samples to follow a desired distribution. When given independent and\nidentically distributed samples of some random variable $S$ and the continuous\ncumulative distribution function of some desired target $T$, it provably\nproduces a consistent estimator of the transformation $R$ which satisfies\n$R(S)=T$ in distribution. As the distribution of $S$ or $T$ may be unknown, we\nalso include algorithms for efficiently estimating these distributions from\nsamples. This allows for various interesting use cases in image processing,\nwhere Redistributor serves as a remarkably simple and easy-to-use tool that is\ncapable of producing visually appealing results. The package is implemented in\nPython and is optimized to efficiently handle large data sets, making it also\nsuitable as a preprocessing step in machine learning. The source code is\navailable at https://gitlab.com/paloha/redistributor.",
    "descriptor": "\nComments: 19 pages, 8 figures\n",
    "authors": [
      "Pavol Harar",
      "Dennis Elbr\u00e4chter",
      "Monika D\u00f6rfler",
      "Kory D. Johnson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Mathematical Software (cs.MS)"
    ],
    "url": "https://arxiv.org/abs/2210.14219"
  },
  {
    "id": "arXiv:2210.14220",
    "title": "Characterizing information loss in a chaotic double pendulum with the  Information Bottleneck",
    "abstract": "A hallmark of chaotic dynamics is the loss of information with time. Although\ninformation loss is often expressed through a connection to Lyapunov exponents\n-- valid in the limit of high information about the system state -- this\npicture misses the rich spectrum of information decay across different levels\nof granularity. Here we show how machine learning presents new opportunities\nfor the study of information loss in chaotic dynamics, with a double pendulum\nserving as a model system. We use the Information Bottleneck as a training\nobjective for a neural network to extract information from the state of the\nsystem that is optimally predictive of the future state after a prescribed time\nhorizon. We then decompose the optimally predictive information by distributing\na bottleneck to each state variable, recovering the relative importance of the\nvariables in determining future evolution. The framework we develop is broadly\napplicable to chaotic systems and pragmatic to apply, leveraging data and\nmachine learning to monitor the limits of predictability and map out the loss\nof information.",
    "descriptor": "\nComments: NeurIPS 2022 workshop paper (Machine learning and the physical sciences); project page: distributed-information-bottleneck.github.io\n",
    "authors": [
      "Kieran A. Murphy",
      "Dani S. Bassett"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2210.14220"
  },
  {
    "id": "arXiv:2210.14222",
    "title": "PlanT: Explainable Planning Transformers via Object-Level  Representations",
    "abstract": "Planning an optimal route in a complex environment requires efficient\nreasoning about the surrounding scene. While human drivers prioritize important\nobjects and ignore details not relevant to the decision, learning-based\nplanners typically extract features from dense, high-dimensional grid\nrepresentations containing all vehicle and road context information. In this\npaper, we propose PlanT, a novel approach for planning in the context of\nself-driving that uses a standard transformer architecture. PlanT is based on\nimitation learning with a compact object-level input representation. On the\nLongest6 benchmark for CARLA, PlanT outperforms all prior methods (matching the\ndriving score of the expert) while being 5.3x faster than equivalent\npixel-based planning baselines during inference. Combining PlanT with an\noff-the-shelf perception module provides a sensor-based driving system that is\nmore than 10 points better in terms of driving score than the existing state of\nthe art. Furthermore, we propose an evaluation protocol to quantify the ability\nof planners to identify relevant objects, providing insights regarding their\ndecision-making. Our results indicate that PlanT can focus on the most relevant\nobject in the scene, even when this object is geometrically distant.",
    "descriptor": "\nComments: CoRL 2022. Project Page: this https URL\n",
    "authors": [
      "Katrin Renz",
      "Kashyap Chitta",
      "Otniel-Bogdan Mercea",
      "A. Sophia Koepke",
      "Zeynep Akata",
      "Andreas Geiger"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14222"
  },
  {
    "id": "arXiv:2210.13329",
    "title": "Decimated Prony's Method for Stable Super-resolution",
    "abstract": "We study recovery of amplitudes and nodes of a finite impulse train from a\nlimited number of equispaced noisy frequency samples. This problem is known as\nsuper-resolution (SR) under sparsity constraints and has numerous applications,\nincluding direction of arrival and finite rate of innovation sampling. Prony's\nmethod is an algebraic technique which fully recovers the signal parameters in\nthe absence of measurement noise. In the presence of noise, Prony's method may\nexperience significant loss of accuracy, especially when the separation between\nDirac pulses is smaller than the Nyquist-Shannon-Rayleigh (NSR) limit. In this\nwork we combine Prony's method with a recently established decimation technique\nfor analyzing the SR problem in the regime where the distance between two or\nmore pulses is much smaller than the NSR limit. We show that our approach\nattains optimal asymptotic stability in the presence of noise. Our result\nchallenges the conventional belief that Prony-type methods tend to be highly\nnumerically unstable.",
    "descriptor": "\nComments: 5 pages, 10 figures\n",
    "authors": [
      "Rami Katz",
      "Nuha Diab",
      "Dmitry Batenkov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13329"
  },
  {
    "id": "arXiv:2210.13453",
    "title": "A multi-category inverse design neural network and its application to  diblock copolymers",
    "abstract": "In this work, we design a multi-category inverse design neural network to map\nordered periodic structure to physical parameters. The neural network model\nconsists of two parts, a classifier and Structure-Parameter-Mapping (SPM)\nsubnets. The classifier is used to identify structure, and the SPM subnets are\nused to predict physical parameters for desired structures. We also present an\nextensible reciprocal-space data augmentation method to guarantee the rotation\nand translation invariant of periodic structures. We apply the proposed network\nmodel and data augmentation method to two-dimensional diblock copolymers based\non the Landau-Brazovskii model. Results show that the multi-category inverse\ndesign neural network is high accuracy in predicting physical parameters for\ndesired structures. Moreover, the idea of multi-categorization can also be\nextended to other inverse design problems.",
    "descriptor": "",
    "authors": [
      "Dan Wei",
      "Tiejun Zhou",
      "Yunqing Huang",
      "Kai Jiang"
    ],
    "subjectives": [
      "Soft Condensed Matter (cond-mat.soft)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13453"
  },
  {
    "id": "arXiv:2210.13473",
    "title": "$\\texttt{Mangrove}$: Learning Galaxy Properties from Merger Trees",
    "abstract": "Efficiently mapping baryonic properties onto dark matter is a major challenge\nin astrophysics. Although semi-analytic models (SAMs) and hydrodynamical\nsimulations have made impressive advances in reproducing galaxy observables\nacross cosmologically significant volumes, these methods still require\nsignificant computation times, representing a barrier to many applications.\nGraph Neural Networks (GNNs) have recently proven to be the natural choice for\nlearning physical relations. Among the most inherently graph-like structures\nfound in astrophysics are the dark matter merger trees that encode the\nevolution of dark matter halos. In this paper we introduce a new, graph-based\nemulator framework, $\\texttt{Mangrove}$, and show that it emulates the galactic\nstellar mass, cold gas mass and metallicity, instantaneous and time-averaged\nstar formation rate, and black hole mass -- as predicted by a SAM -- with root\nmean squared error up to two times lower than other methods across a $(75\nMpc/h)^3$ simulation box in 40 seconds, 4 orders of magnitude faster than the\nSAM. We show that $\\texttt{Mangrove}$ allows for quantification of the\ndependence of galaxy properties on merger history. We compare our results to\nthe current state of the art in the field and show significant improvements for\nall target properties. $\\texttt{Mangrove}$ is publicly available.",
    "descriptor": "\nComments: 15 pages, 9 figures, 3 tables, 10 pages of Appendices. Accepted for publication in ApJ\n",
    "authors": [
      "Christian Kragh Jespersen",
      "Miles Cranmer",
      "Peter Melchior",
      "Shirley Ho",
      "Rachel S. Somerville",
      "Austen Gabrielpillai"
    ],
    "subjectives": [
      "Astrophysics of Galaxies (astro-ph.GA)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13473"
  },
  {
    "id": "arXiv:2210.13475",
    "title": "Maximizing the geometric measure of entanglement",
    "abstract": "The characterization of the maximally achievable entanglement in a given\nphysical system is relevant, as entanglement is known to be a resource for\nvarious quantum information tasks. This holds especially for pure multiparticle\nquantum states, where the problem of maximal entanglement is not only of\nphysical interest, but also closely related to fundamental mathematical\nproblems in multilinear algebra and tensor analysis. We propose an algorithmic\nmethod to find maximally entangled states of several particles in terms of the\ngeometric measure of entanglement. Besides identifying physically interesting\nstates our results deliver insights to the problem of absolutely maximally\nentangled states; moreover, our methods can be generalized to identify\nmaximally entangled subspaces.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Jonathan Steinberg",
      "Otfried G\u00fchne"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13475"
  },
  {
    "id": "arXiv:2210.13526",
    "title": "Computational Inference in Cognitive Science: Operational, Societal and  Ethical Considerations",
    "abstract": "Emerging research frontiers and computational advances have gradually\ntransformed cognitive science into a multidisciplinary and data-driven field.\nAs a result, there is a proliferation of cognitive theories investigated and\ninterpreted from different academic lens and in different levels of\nabstraction. We formulate this applied aspect of this challenge as the\ncomputational cognitive inference, and describe the major routes of\ncomputational approaches. To balance the potential optimism alongside the speed\nand scale of the data-driven era of cognitive science, we propose to inspect\nthis trend in more empirical terms by identifying the operational challenges,\nsocietal impacts and ethical guidelines in conducting research and interpreting\nresults from the computational inference in cognitive science.",
    "descriptor": "",
    "authors": [
      "Baihan Lin"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13526"
  },
  {
    "id": "arXiv:2210.13573",
    "title": "Conditionally Risk-Averse Contextual Bandits",
    "abstract": "We desire to apply contextual bandits to scenarios where average-case\nstatistical guarantees are inadequate. Happily, we discover the composition of\nreduction to online regression and expectile loss is analytically tractable,\ncomputationally convenient, and empirically effective. The result is the first\nrisk-averse contextual bandit algorithm with an online regret guarantee. We\nstate our precise regret guarantee and conduct experiments from diverse\nscenarios in dynamic pricing, inventory management, and self-tuning software;\nincluding results from a production exascale cloud data processing system.",
    "descriptor": "",
    "authors": [
      "M\u00f3nika Farsang",
      "Paul Mineiro",
      "Wangda Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13573"
  },
  {
    "id": "arXiv:2210.13595",
    "title": "DilatedSegNet: A Deep Dilated Segmentation Network for Polyp  Segmentation",
    "abstract": "Colorectal cancer (CRC) is the second leading cause of cancer-related death\nworldwide. Excision of polyps during colonoscopy helps reduce mortality and\nmorbidity for CRC. Powered by deep learning, computer-aided diagnosis (CAD)\nsystems can detect regions in the colon overlooked by physicians during\ncolonoscopy. Lacking high accuracy and real-time speed are the essential\nobstacles to be overcome for successful clinical integration of such systems.\nWhile literature is focused on improving accuracy, the speed parameter is often\nignored. Toward this critical need, we intend to develop a novel real-time deep\nlearning-based architecture, DilatedSegNet, to perform polyp segmentation on\nthe fly. DilatedSegNet is an encoder-decoder network that uses pre-trained\nResNet50 as the encoder from which we extract four levels of feature maps. Each\nof these feature maps is passed through a dilated convolution pooling (DCP)\nblock. The outputs from the DCP blocks are concatenated and passed through a\nseries of four decoder blocks that predicts the segmentation mask. The proposed\nmethod achieves a real-time operation speed of 33.68 frames per second with an\naverage dice coefficient of 0.90 and mIoU of 0.83. Additionally, we also\nprovide heatmap along with the qualitative results that shows the explanation\nfor the polyp location, which increases the trustworthiness of the method. The\nresults on the publicly available Kvasir-SEG and BKAI-IGH datasets suggest that\nDilatedSegNet can give real-time feedback while retaining a high \\ac{DSC},\nindicating high potential for using such models in real clinical settings in\nthe near future. The GitHub link of the source code can be found here:\n\\url{https://github.com/nikhilroxtomar/DilatedSegNet}.",
    "descriptor": "\nComments: Accepted at MMM 2023\n",
    "authors": [
      "Nikhil Kumar Tomar",
      "Debesh Jha",
      "Ulas Bagci"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13595"
  },
  {
    "id": "arXiv:2210.13622",
    "title": "Characterization of singular flows of zeroth-order pseudo-differential  operators via elliptic eigenfunctions: a numerical study",
    "abstract": "The propagation of internal gravity waves in stratified media, such as those\nfound in ocean basins and lakes, leads to the development of geometrical\npatterns called \"attractors\". These structures accumulate much of the wave\nenergy and make the fluid flow highly singular. In more analytical terms, the\ncause of this phenomenon has been attributed to the presence of a continuous\nspectrum in some nonlocal zeroth-order pseudo-differential operators. In this\nwork, we analyze the generation of these attractors from a numerical analysis\nperspective. First, we propose a high-order pseudo-spectral method to solve the\nevolution problem (whose long-term behaviour is known to be not\nsquare-integrable). Then, we use similar tools to discretize the corresponding\neigenvalue problem. Since the eigenvalues are embedded in a continuous\nspectrum, we compute them using viscous approximations. Finally, we explore the\neffect that the embedded eigenmodes have on the long-term evolution of the\nsystem.",
    "descriptor": "\nComments: 23 pages, 16 figures, MATLAB codes available at this http URL\n",
    "authors": [
      "Javier A. Almonacid",
      "Nilima Nigam"
    ],
    "subjectives": [
      "Spectral Theory (math.SP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13622"
  },
  {
    "id": "arXiv:2210.13668",
    "title": "ConnectedUNets++: Mass Segmentation from Whole Mammographic Images",
    "abstract": "Deep learning has made a breakthrough in medical image segmentation in recent\nyears due to its ability to extract high-level features without the need for\nprior knowledge. In this context, U-Net is one of the most advanced medical\nimage segmentation models, with promising results in mammography. Despite its\nexcellent overall performance in segmenting multimodal medical images, the\ntraditional U-Net structure appears to be inadequate in various ways. There are\ncertain U-Net design modifications, such as MultiResUNet, Connected-UNets, and\nAU-Net, that have improved overall performance in areas where the conventional\nU-Net architecture appears to be deficient. Following the success of UNet and\nits variants, we have presented two enhanced versions of the Connected-UNets\narchitecture: ConnectedUNets+ and ConnectedUNets++. In ConnectedUNets+, we have\nreplaced the simple skip connections of Connected-UNets architecture with\nresidual skip connections, while in ConnectedUNets++, we have modified the\nencoder-decoder structure along with employing residual skip connections. We\nhave evaluated our proposed architectures on two publicly available datasets,\nthe Curated Breast Imaging Subset of Digital Database for Screening Mammography\n(CBIS-DDSM) and INbreast.",
    "descriptor": "",
    "authors": [
      "Prithul Sarker",
      "Sushmita Sarker",
      "George Bebis",
      "Alireza Tavakkoli"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13668"
  },
  {
    "id": "arXiv:2210.13690",
    "title": "Highly Efficient Real-Time Streaming and Fully On-Device Speaker  Diarization with Multi-Stage Clustering",
    "abstract": "While recent research advances in speaker diarization mostly focus on\nimproving the quality of diarization results, there is also an increasing\ninterest in improving the efficiency of diarization systems. In this paper, we\npropose a multi-stage clustering strategy, that uses different clustering\nalgorithms for input of different lengths. Specifically, a fallback clusterer\nis used to handle short-form inputs; a main clusterer is used to handle\nmedium-length inputs; and a pre-clusterer is used to compress long-form inputs\nbefore they are processed by the main clusterer. Both the main clusterer and\nthe pre-clusterer can be configured with an upper bound of the computational\ncomplexity to adapt to devices with different constraints. This multi-stage\nclustering strategy is critical for streaming on-device speaker diarization\nsystems, where the budgets of CPU, memory and battery are tight.",
    "descriptor": "",
    "authors": [
      "Quan Wang",
      "Yiling Huang",
      "Han Lu",
      "Guanlong Zhao",
      "Ignacio Lopez Moreno"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13690"
  },
  {
    "id": "arXiv:2210.13692",
    "title": "Sequential Decision Making on Unmatched Data using Bayesian Kernel  Embeddings",
    "abstract": "The problem of sequentially maximizing the expectation of a function seeks to\nmaximize the expected value of a function of interest without having direct\ncontrol on its features. Instead, the distribution of such features depends on\na given context and an action taken by an agent. In contrast to Bayesian\noptimization, the arguments of the function are not under agent's control, but\nare indirectly determined by the agent's action based on a given context. If\nthe information of the features is to be included in the maximization problem,\nthe full conditional distribution of such features, rather than its expectation\nonly, needs to be accounted for. Furthermore, the function is itself unknown,\nonly counting with noisy observations of such function, and potentially\nrequiring the use of unmatched data sets. We propose a novel algorithm for the\naforementioned problem which takes into consideration the uncertainty derived\nfrom the estimation of both the conditional distribution of the features and\nthe unknown function, by modeling the former as a Bayesian conditional mean\nembedding and the latter as a Gaussian process. Our algorithm empirically\noutperforms the current state-of-the-art algorithm in the experiments\nconducted.",
    "descriptor": "",
    "authors": [
      "Diego Martinez-Taboada",
      "Dino Sejdinovic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13692"
  },
  {
    "id": "arXiv:2210.13695",
    "title": "Structure-based Drug Design with Equivariant Diffusion Models",
    "abstract": "Structure-based drug design (SBDD) aims to design small-molecule ligands that\nbind with high affinity and specificity to pre-determined protein targets.\nTraditional SBDD pipelines start with large-scale docking of compound libraries\nfrom public databases, thus limiting the exploration of chemical space to\nexistent previously studied regions. Recent machine learning methods approached\nthis problem using an atom-by-atom generation approach, which is\ncomputationally expensive. In this paper, we formulate SBDD as a 3D-conditional\ngeneration problem and present DiffSBDD, an E(3)-equivariant 3D-conditional\ndiffusion model that generates novel ligands conditioned on protein pockets.\nFurthermore, we curate a new dataset of experimentally determined binding\ncomplex data from Binding MOAD to provide a realistic binding scenario that\ncomplements the synthetic CrossDocked dataset. Comprehensive in silico\nexperiments demonstrate the efficiency of DiffSBDD in generating novel and\ndiverse drug-like ligands that engage protein pockets with high binding\nenergies as predicted by in silico docking.",
    "descriptor": "",
    "authors": [
      "Arne Schneuing",
      "Yuanqi Du",
      "Charles Harris",
      "Arian Jamasb",
      "Ilia Igashov",
      "Weitao Du",
      "Tom Blundell",
      "Pietro Li\u00f3",
      "Carla Gomes",
      "Max Welling",
      "Michael Bronstein",
      "Bruno Correia"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13695"
  },
  {
    "id": "arXiv:2210.13700",
    "title": "Does Joint Training Really Help Cascaded Speech Translation?",
    "abstract": "Currently, in speech translation, the straightforward approach - cascading a\nrecognition system with a translation system - delivers state-of-the-art\nresults. However, fundamental challenges such as error propagation from the\nautomatic speech recognition system still remain. To mitigate these problems,\nrecently, people turn their attention to direct data and propose various joint\ntraining methods. In this work, we seek to answer the question of whether joint\ntraining really helps cascaded speech translation. We review recent papers on\nthe topic and also investigate a joint training criterion by marginalizing the\ntranscription posterior probabilities. Our findings show that a strong cascaded\nbaseline can diminish any improvements obtained using joint training, and we\nsuggest alternatives to joint training. We hope this work can serve as a\nrefresher of the current speech translation landscape, and motivate research in\nfinding more efficient and creative ways to utilize the direct data for speech\ntranslation.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Viet Anh Khoa Tran",
      "David Thulke",
      "Yingbo Gao",
      "Christian Herold",
      "Hermann Ney"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13700"
  },
  {
    "id": "arXiv:2210.13706",
    "title": "Gaussian Mean Testing Made Simple",
    "abstract": "We study the following fundamental hypothesis testing problem, which we term\nGaussian mean testing. Given i.i.d. samples from a distribution $p$ on\n$\\mathbb{R}^d$, the task is to distinguish, with high probability, between the\nfollowing cases: (i) $p$ is the standard Gaussian distribution,\n$\\mathcal{N}(0,I_d)$, and (ii) $p$ is a Gaussian $\\mathcal{N}(\\mu,\\Sigma)$ for\nsome unknown covariance $\\Sigma$ and mean $\\mu \\in \\mathbb{R}^d$ satisfying\n$\\|\\mu\\|_2 \\geq \\epsilon$. Recent work gave an algorithm for this testing\nproblem with the optimal sample complexity of $\\Theta(\\sqrt{d}/\\epsilon^2)$.\nBoth the previous algorithm and its analysis are quite complicated. Here we\ngive an extremely simple algorithm for Gaussian mean testing with a one-page\nanalysis. Our algorithm is sample optimal and runs in sample linear time.",
    "descriptor": "\nComments: To appear in SIAM Symposium on Simplicity in Algorithms (SOSA) 2023\n",
    "authors": [
      "Ilias Diakonikolas",
      "Daniel M. Kane",
      "Ankit Pensia"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13706"
  },
  {
    "id": "arXiv:2210.13711",
    "title": "A Spectral Method for Assessing and Combining Multiple Data  Visualizations",
    "abstract": "Dimension reduction and data visualization aim to project a high-dimensional\ndataset to a low-dimensional space while capturing the intrinsic structures in\nthe data. It is an indispensable part of modern data science, and many\ndimensional reduction and visualization algorithms have been developed.\nHowever, different algorithms have their own strengths and weaknesses, making\nit critically important to evaluate their relative performance for a given\ndataset, and to leverage and combine their individual strengths. In this paper,\nwe propose an efficient spectral method for assessing and combining multiple\nvisualizations of a given dataset produced by diverse algorithms. The proposed\nmethod provides a quantitative measure -- the visualization eigenscore -- of\nthe relative performance of the visualizations for preserving the structure\naround each data point. Then it leverages the eigenscores to obtain a consensus\nvisualization, which has much improved { quality over the individual\nvisualizations in capturing the underlying true data structure.} Our approach\nis flexible and works as a wrapper around any visualizations. We analyze\nmultiple simulated and real-world datasets from diverse applications to\ndemonstrate the effectiveness of the eigenscores for evaluating visualizations\nand the superiority of the proposed consensus visualization. Furthermore, we\nestablish rigorous theoretical justification of our method based on a general\nstatistical framework, yielding fundamental principles behind the empirical\nsuccess of consensus visualization along with practical guidance.",
    "descriptor": "\nComments: Under revision of Nature Communications\n",
    "authors": [
      "Rong Ma",
      "Eric D. Sun",
      "James Zou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)",
      "Applications (stat.AP)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.13711"
  },
  {
    "id": "arXiv:2210.13721",
    "title": "Multi-modal Dynamic Graph Network: Coupling Structural and Functional  Connectome for Disease Diagnosis and Classification",
    "abstract": "Multi-modal neuroimaging technology has greatlly facilitated the efficiency\nand diagnosis accuracy, which provides complementary information in discovering\nobjective disease biomarkers. Conventional deep learning methods, e.g.\nconvolutional neural networks, overlook relationships between nodes and fail to\ncapture topological properties in graphs. Graph neural networks have been\nproven to be of great importance in modeling brain connectome networks and\nrelating disease-specific patterns. However, most existing graph methods\nexplicitly require known graph structures, which are not available in the\nsophisticated brain system. Especially in heterogeneous multi-modal brain\nnetworks, there exists a great challenge to model interactions among brain\nregions in consideration of inter-modal dependencies. In this study, we propose\na Multi-modal Dynamic Graph Convolution Network (MDGCN) for structural and\nfunctional brain network learning. Our method benefits from modeling\ninter-modal representations and relating attentive multi-model associations\ninto dynamic graphs with a compositional correspondence matrix. Moreover, a\nbilateral graph convolution layer is proposed to aggregate multi-modal\nrepresentations in terms of multi-modal associations. Extensive experiments on\nthree datasets demonstrate the superiority of our proposed method in terms of\ndisease classification, with the accuracy of 90.4%, 85.9% and 98.3% in\npredicting Mild Cognitive Impairment (MCI), Parkinson's disease (PD), and\nschizophrenia (SCHZ) respectively. Furthermore, our statistical evaluations on\nthe correspondence matrix exhibit a high correspondence with previous evidence\nof biomarkers.",
    "descriptor": "",
    "authors": [
      "Yanwu Yang",
      "Xutao Guo",
      "Zhikai Chang",
      "Chenfei Ye",
      "Yang Xiang",
      "Ting Ma"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13721"
  },
  {
    "id": "arXiv:2210.13741",
    "title": "Deep Neural Networks as the Semi-classical Limit of Topological Quantum  Neural Networks: The problem of generalisation",
    "abstract": "Deep Neural Networks miss a principled model of their operation. A novel\nframework for supervised learning based on Topological Quantum Field Theory\nthat looks particularly well suited for implementation on quantum processors\nhas been recently explored. We propose the use of this framework for\nunderstanding the problem of generalization in Deep Neural Networks. More\nspecifically, in this approach Deep Neural Networks are viewed as the\nsemi-classical limit of Topological Quantum Neural Networks. A framework of\nthis kind explains easily the overfitting behavior of Deep Neural Networks\nduring the training step and the corresponding generalization capabilities.",
    "descriptor": "\nComments: 17 pages (two columns), 4 figures. Comments are welcome!\n",
    "authors": [
      "Antonino Marciano",
      "Deen Chen",
      "Filippo Fabrocini",
      "Chris Fields",
      "Matteo Lulli",
      "Emanuele Zappala"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Geometry (cs.CG)",
      "Machine Learning (cs.LG)",
      "Mathematical Physics (math-ph)",
      "Geometric Topology (math.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.13741"
  },
  {
    "id": "arXiv:2210.13745",
    "title": "Numerical Analysis for Real-time Nonlinear Model Predictive Control of  Ethanol Steam Reformers",
    "abstract": "The utilization of renewable energy technologies, particularly hydrogen, has\nseen a boom in interest and has spread throughout the world. Ethanol steam\nreformation is one of the primary methods capable of producing hydrogen\nefficiently and reliably. This paper provides an in-depth study of the\nreformulated system both theoretically and numerically, as well as a plan to\nexplore the possibility of converting the system into its conservation form.\nLastly, we offer an overview of several numerical approaches for solving the\ngeneral first-order quasi-linear hyperbolic equation to the particular model\nfor ethanol steam reforming (ESR). We conclude by presenting some results that\nwould enable the usage of these ODE/PDE solvers to be used in non-linear model\npredictive control (NMPC) algorithms and discuss the limitations of our\napproach and directions for future work.",
    "descriptor": "",
    "authors": [
      "Robert Joseph George"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13745"
  },
  {
    "id": "arXiv:2210.13756",
    "title": "Mixed Emotion Modelling for Emotional Voice Conversion",
    "abstract": "Emotional voice conversion (EVC) aims to convert the emotional state of an\nutterance from one emotion to another while preserving the linguistic content\nand speaker identity. Current studies mostly focus on modelling the conversion\nbetween several specific emotion types. Synthesizing mixed effects of emotions\ncould help us to better imitate human emotions, and facilitate more natural\nhuman-computer interaction. In this research, for the first time, we formulate\nand study the research problem of mixed emotion synthesis for EVC. We regard\nemotional styles as a series of emotion attributes that are learnt from a\nranking-based support vector machine (SVM). Each attribute measures the degree\nof the relevance between the speech recordings belonging to different emotion\ntypes. We then incorporate those attributes into a sequence-to-sequence\n(seq2seq) emotional voice conversion framework. During the training, the\nframework not only learns to characterize the input emotional style, but also\nquantifies its relevance with other emotion types. At run-time, various\nemotional mixtures can be produced by manually defining the attributes. We\nconduct objective and subjective evaluations to validate our idea in terms of\nmixed emotion synthesis. We further build an emotion triangle as an application\nof emotion transition. Codes and speech samples are publicly available.",
    "descriptor": "\nComments: Submitted to ICASSP 2023. arXiv admin note: text overlap with arXiv:2208.05890\n",
    "authors": [
      "Kun Zhou",
      "Berrak Sisman",
      "Carlos Busso",
      "Haizhou Li"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13756"
  },
  {
    "id": "arXiv:2210.13761",
    "title": "Streaming Parrotron for on-device speech-to-speech conversion",
    "abstract": "We present a fully on-device and streaming Speech-To-Speech (STS) conversion\nmodel that normalizes a given input speech directly to synthesized output\nspeech (a.k.a. Parrotron). Deploying such an end-to-end model locally on mobile\ndevices pose significant challenges in terms of memory footprint and\ncomputation requirements. In this paper, we present a streaming-based approach\nto produce an acceptable delay, with minimal loss in speech conversion quality,\nwhen compared to a non-streaming server-based approach. Our approach consists\nof first streaming the encoder in real time while the speaker is speaking.\nThen, as soon as the speaker stops speaking, we run the spectrogram decoder in\nstreaming mode along the side of a streaming vocoder to generate output speech\nin real time. To achieve an acceptable delay quality trade-off, we study a\nnovel hybrid approach for look-ahead in the encoder which combines a look-ahead\nfeature stacker with a look-ahead self-attention. We also compare the model\nwith int4 quantization aware training and int8 post training quantization and\nshow that our streaming approach is 2x faster than real time on the Pixel4 CPU.",
    "descriptor": "\nComments: submitted to ICASSP\n",
    "authors": [
      "Oleg Rybakov",
      "Fadi Biadsy",
      "Xia Zhang",
      "Liyang Jiang",
      "Phoenix Meadowlark",
      "Shivani Agrawal"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13761"
  },
  {
    "id": "arXiv:2210.13767",
    "title": "Networked Signal and Information Processing",
    "abstract": "The article reviews significant advances in networked signal and information\nprocessing, which have enabled in the last 25 years extending decision making\nand inference, optimization, control, and learning to the increasingly\nubiquitous environments of distributed agents. As these interacting agents\ncooperate, new collective behaviors emerge from local decisions and actions.\nMoreover, and significantly, theory and applications show that networked\nagents, through cooperation and sharing, are able to match the performance of\ncloud or federated solutions, while preserving privacy, increasing resilience,\nand saving resources.",
    "descriptor": "",
    "authors": [
      "Stefan Vlaski",
      "Soummya Kar",
      "Ali H. Sayed",
      "Jos\u00e9 M. F. Moura"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.13767"
  },
  {
    "id": "arXiv:2210.13771",
    "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual  Voice Conversion Using $\u03b2$-VAE",
    "abstract": "We propose an unsupervised learning method to disentangle speech into content\nrepresentation and speaker identity representation. We apply this method to the\nchallenging one-shot cross-lingual voice conversion task to demonstrate the\neffectiveness of the disentanglement. Inspired by $\\beta$-VAE, we introduce a\nlearning objective that balances between the information captured by the\ncontent and speaker representations. In addition, the inductive biases from the\narchitectural design and the training dataset further encourage the desired\ndisentanglement. Both objective and subjective evaluations show the\neffectiveness of the proposed method in speech disentanglement and in one-shot\ncross-lingual voice conversion.",
    "descriptor": "\nComments: Accepted at SLT 2022\n",
    "authors": [
      "Hui Lu",
      "Disong Wang",
      "Xixin Wu",
      "Zhiyong Wu",
      "Xunying Liu",
      "Helen Meng"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13771"
  },
  {
    "id": "arXiv:2210.13772",
    "title": "Deformation Theory of Boltzmann Distributions",
    "abstract": "Consider a one-parameter family of Boltzmann distributions $p_t(x) =\n\\tfrac{1}{Z_t}e^{-S_t(x)}$. In this paper we study the problem of sampling from\n$p_{t_0}$ by first sampling from $p_{t_1}$ and then applying a transformation\n$\\Psi_{t_1}^{t_0}$ to the samples so that to they follow $p_{t_0}$. We derive\nan equation relating $\\Psi$ and the corresponding family of unnormalized\nlog-likelihoods $S_t$. We demonstrate the utility of this idea on the $\\phi^4$\nlattice field theory by extending its defining action $S_0$ to a family of\nactions $S_t$ and finding a $\\tau$ such that normalizing flows perform better\nat learning the Boltzmann distribution $p_\\tau$ than at learning $p_0$.",
    "descriptor": "\nComments: Machine Learning for the Physical Science Workshop at NeurIPS '22\n",
    "authors": [
      "B\u00e1lint M\u00e1t\u00e9",
      "Fran\u00e7ois Fleuret"
    ],
    "subjectives": [
      "High Energy Physics - Lattice (hep-lat)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13772"
  },
  {
    "id": "arXiv:2210.13773",
    "title": "Variational Bayesian Inference Clustering Based Joint User Activity and  Data Detection for Grant-Free Random Access in mMTC",
    "abstract": "Tailor-made for massive connectivity and sporadic access, grant-free random\naccess has become a promising candidate access protocol for massive\nmachine-type communications (mMTC). Compared with conventional grant-based\nprotocols, grant-free random access skips the exchange of scheduling\ninformation to reduce the signaling overhead, and facilitates sharing of access\nresources to enhance access efficiency. However, some challenges remain to be\naddressed in the receiver design, such as unknown identity of active users and\nmulti-user interference (MUI) on shared access resources. In this work, we deal\nwith the problem of joint user activity and data detection for grant-free\nrandom access. Specifically, the approximate message passing (AMP) algorithm is\nfirst employed to mitigate MUI and decouple the signals of different users.\nThen, we extend the data symbol alphabet to incorporate the null symbols from\ninactive users. In this way, the joint user activity and data detection problem\nis formulated as a clustering problem under the Gaussian mixture model.\nFurthermore, in conjunction with the AMP algorithm, a variational Bayesian\ninference based clustering (VBIC) algorithm is developed to solve this\nclustering problem. Simulation results show that, compared with state-of-art\nsolutions, the proposed AMP-combined VBIC (AMP-VBIC) algorithm achieves a\nsignificant performance gain in detection accuracy.",
    "descriptor": "\nComments: 10 pages, 5 figures, submitted to Internet-of-Things Journal\n",
    "authors": [
      "Zhaoji Zhang",
      "Qinghua Guo",
      "Ying Li",
      "Ming Jin",
      "Chongwen Huang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.13773"
  },
  {
    "id": "arXiv:2210.13785",
    "title": "Some Simulation and Empirical Results for Semi-Supervised Learning of  the Bayes Rule of Allocation",
    "abstract": "There has been increasing attention to semi-supervised learning (SSL)\napproaches in machine learning to forming a classifier in situations where the\ntraining data consists of some feature vectors that have their class labels\nmissing. In this study, we consider the generative model approach proposed by\nAhfock&McLachlan(2020) who introduced a framework with a missingness mechanism\nfor the missing labels of the unclassified features. In the case of two\nmultivariate normal classes with a common covariance matrix, they showed that\nthe error rate of the estimated Bayes' rule formed by this SSL approach can\nactually have lower error rate than the one that could be formed from a\ncompletely classified sample. In this study we consider this rather surprising\nresult in cases where there may be more than two normal classes with not\nnecessarily common covariance matrices.",
    "descriptor": "\nComments: 27 pages. arXiv admin note: text overlap with arXiv:2104.04046\n",
    "authors": [
      "Ziyang Lyu",
      "Daniel Ahfock",
      "Geoffrey J. McLachlan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13785"
  },
  {
    "id": "arXiv:2210.13799",
    "title": "Does Medical Imaging learn different Convolution Filters?",
    "abstract": "Recent work has investigated the distributions of learned convolution filters\nthrough a large-scale study containing hundreds of heterogeneous image models.\nSurprisingly, on average, the distributions only show minor drifts in\ncomparisons of various studied dimensions including the learned task, image\ndomain, or dataset. However, among the studied image domains, medical imaging\nmodels appeared to show significant outliers through \"spikey\" distributions,\nand, therefore, learn clusters of highly specific filters different from other\ndomains. Following this observation, we study the collected medical imaging\nmodels in more detail. We show that instead of fundamental differences, the\noutliers are due to specific processing in some architectures. Quite the\ncontrary, for standardized architectures, we find that models trained on\nmedical data do not significantly differ in their filter distributions from\nsimilar architectures trained on data from other domains. Our conclusions\nreinforce previous hypotheses stating that pre-training of imaging models can\nbe done with any kind of diverse image data.",
    "descriptor": "\nComments: Accepted at MedNeurIPS 2022\n",
    "authors": [
      "Paul Gavrikov",
      "Janis Keuper"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13799"
  },
  {
    "id": "arXiv:2210.13816",
    "title": "Federated Bayesian Computation via Piecewise Deterministic Markov  Processes",
    "abstract": "When performing Bayesian computations in practice, one is often faced with\nthe challenge that the constituent model components and/or the data are only\navailable in a distributed fashion, e.g. due to privacy concerns or sheer\nvolume. While various methods have been proposed for performing posterior\ninference in such federated settings, these either make very strong assumptions\non the data and/or model or otherwise introduce significant bias when the local\nposteriors are combined to form an approximation of the target posterior. By\nleveraging recently developed methods for Markov Chain Monte Carlo (MCMC) based\non Piecewise Deterministic Markov Processes (PDMPs), we develop a computation\n-- and communication -- efficient family of posterior inference algorithms\n(Fed-PDMC) which provides asymptotically exact approximations of the full\nposterior over a large class of Bayesian models, allowing heterogenous model\nand data contributions from each client. We show that communication between\nclients and the server preserves the privacy of the individual data sources by\nestablishing differential privacy guarantees. We quantify the performance of\nFed-PDMC over a class of illustrative analytical case-studies and demonstrate\nits efficacy on a number of synthetic examples along with realistic Bayesian\ncomputation benchmarks.",
    "descriptor": "\nComments: 17 pages, 7 figures\n",
    "authors": [
      "Joris Bierkens",
      "Andrew Duncan"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.13816"
  },
  {
    "id": "arXiv:2210.13817",
    "title": "Online model error correction with neural networks in the incremental  4D-Var framework",
    "abstract": "Recent studies have demonstrated that it is possible to combine machine\nlearning with data assimilation to reconstruct the dynamics of a physical model\npartially and imperfectly observed. Data assimilation is used to estimate the\nsystem state from the observations, while machine learning computes a surrogate\nmodel of the dynamical system based on those estimated states. The surrogate\nmodel can be defined as an hybrid combination where a physical model based on\nprior knowledge is enhanced with a statistical model estimated by a neural\nnetwork. The training of the neural network is typically done offline, once a\nlarge enough dataset of model state estimates is available. By contrast, with\nonline approaches the surrogate model is improved each time a new system state\nestimate is computed. Online approaches naturally fit the sequential framework\nencountered in geosciences where new observations become available with time.\nIn a recent methodology paper, we have developed a new weak-constraint 4D-Var\nformulation which can be used to train a neural network for online model error\ncorrection. In the present article, we develop a simplified version of that\nmethod, in the incremental 4D-Var framework adopted by most operational weather\ncentres. The simplified method is implemented in the ECMWF Object-Oriented\nPrediction System, with the help of a newly developed Fortran neural network\nlibrary, and tested with a two-layer two-dimensional quasi geostrophic model.\nThe results confirm that online learning is effective and yields a more\naccurate model error correction than offline learning. Finally, the simplified\nmethod is compatible with future applications to state-of-the-art models such\nas the ECMWF Integrated Forecasting System.",
    "descriptor": "",
    "authors": [
      "Alban Farchi",
      "Marcin Chrust",
      "Marc Bocquet",
      "Patrick Laloyaux",
      "Massimo Bonavita"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13817"
  },
  {
    "id": "arXiv:2210.13834",
    "title": "Stable deep MRI reconstruction using Generative Priors",
    "abstract": "Data-driven approaches recently achieved remarkable success in medical image\nreconstruction, but integration into clinical routine remains challenging due\nto a lack of generalizability and interpretability. Existing approaches usually\nrequire high-quality data-image pairs for training, but such data is not easily\navailable for any imaging protocol and the reconstruction quality can quickly\ndegrade even if only minor changes are made to the protocol. In addition,\ndata-driven methods may create artificial features that can influence the\nclinicians decision-making. This is unacceptable if the clinician is unaware of\nthe uncertainty associated with the reconstruction. In this paper, we address\nthese challenges in a unified framework based on generative image priors. We\npropose a novel deep neural network based regularizer which is trained in an\nunsupervised setting on reference images without requiring any data-image\npairs. After training, the regularizer can be used as part of a classical\nvariational approach in combination with any acquisition protocols and shows\nstable behavior even if the test data deviates significantly from the training\ndata. Furthermore, our probabilistic interpretation provides a distribution of\nreconstructions and hence allows uncertainty quantification. We demonstrate our\napproach on parallel magnetic resonance imaging, where results show competitive\nperformance with SotA end-to-end deep learning methods, while preserving the\nflexibility of the acquisition protocol and allowing for uncertainty\nquantification.",
    "descriptor": "",
    "authors": [
      "Martin Zach",
      "Florian Knoll",
      "Thomas Pock"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13834"
  },
  {
    "id": "arXiv:2210.13882",
    "title": "A deep learning approach for brain tumor detection using magnetic  resonance imaging",
    "abstract": "The growth of abnormal cells in the brain's tissue causes brain tumors. Brain\ntumors are considered one of the most dangerous disorders in children and\nadults. It develops quickly, and the patient's survival prospects are slim if\nnot appropriately treated. Proper treatment planning and precise diagnoses are\nessential to improving a patient's life expectancy. Brain tumors are mainly\ndiagnosed using magnetic resonance imaging (MRI). As part of a convolution\nneural network (CNN)-based illustration, an architecture containing five\nconvolution layers, five max-pooling layers, a Flatten layer, and two dense\nlayers has been proposed for detecting brain tumors from MRI images. The\nproposed model includes an automatic feature extractor, modified hidden layer\narchitecture, and activation function. Several test cases were performed, and\nthe proposed model achieved 98.6% accuracy and 97.8% precision score with a low\ncross-entropy rate. Compared with other approaches such as adjacent feature\npropagation network (AFPNet), mask region-based CNN (mask RCNN), YOLOv5, and\nFourier CNN (FCNN), the proposed model has performed better in detecting brain\ntumors.",
    "descriptor": "",
    "authors": [
      "Al-Akhir Nayan",
      "Ahamad Nokib Mozumder",
      "Md. Rakibul Haque",
      "Fahim Hossain Sifat",
      "Khan Raqib Mahmud",
      "Abul Kalam Al Azad",
      "Muhammad Golam Kibria"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13882"
  },
  {
    "id": "arXiv:2210.13889",
    "title": "Clinically-Inspired Multi-Agent Transformers for Disease Trajectory  Forecasting from Multimodal Data",
    "abstract": "Deep neural networks are often applied to medical images to automate the\nproblem of medical diagnosis. However, a more clinically relevant question that\npractitioners usually face is how to predict the future trajectory of a\ndisease. Current methods for prognosis or disease trajectory forecasting often\nrequire domain knowledge and are complicated to apply. In this paper, we\nformulate the prognosis prediction problem as a one-to-many prediction problem.\nInspired by a clinical decision-making process with two agents -- a radiologist\nand a general practitioner -- we predict prognosis with two transformer-based\ncomponents that share information with each other. The first transformer in\nthis framework aims to analyze the imaging data, and the second one leverages\nits internal states as inputs, also fusing them with auxiliary clinical data.\nThe temporal nature of the problem is modeled within the transformer states,\nallowing us to treat the forecasting problem as a multi-task classification,\nfor which we propose a novel loss. We show the effectiveness of our approach in\npredicting the development of structural knee osteoarthritis changes and\nforecasting Alzheimer's disease clinical status directly from raw multi-modal\ndata. The proposed method outperforms multiple state-of-the-art baselines with\nrespect to performance and calibration, both of which are needed for real-world\napplications. An open-source implementation of our method is made publicly\navailable at \\url{https://github.com/Oulu-IMEDS/CLIMATv2}.",
    "descriptor": "\nComments: 10 pages, under review\n",
    "authors": [
      "Huy Hoang Nguyen",
      "Matthew B. Blaschko",
      "Simo Saarakkala",
      "Aleksei Tiulpin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13889"
  },
  {
    "id": "arXiv:2210.13931",
    "title": "An Optimal Stochastic Algorithm for Decentralized Nonconvex Finite-sum  Optimization",
    "abstract": "This paper studies the synchronized decentralized nonconvex optimization\nproblem of the form $\\min_{x\\in{\\mathbb R}^d} f(x)\\triangleq\n\\frac{1}{m}\\sum_{i=1}^m f_i(x)$, where $f_i(x)\\triangleq\n\\frac{1}{n}\\sum_{j=1}^n f_{i,j}(x)$ is the local function on $i$-th agent of\nthe connected network. We propose a novel stochastic algorithm called\nDEcentralized probAbilistic Recursive gradiEnt deScenT (DEAREST), which\nintegrates the techniques of variance reduction, gradient tracking and\nmulti-consensus. We construct a Lyapunov function that simultaneously\ncharacterizes the function value, the gradient estimation error and the\nconsensus error for the convergence analysis. Based on this measure, we provide\na concise proof to show DEAREST requires at most ${\\mathcal\nO}(mn+\\sqrt{mn}L\\varepsilon^{-2})$ incremental first-order oracle (IFO) calls\nand ${\\mathcal O}(L\\varepsilon^{-2}/\\sqrt{1-\\lambda_2(W)}\\,)$ communication\nrounds to find an $\\varepsilon$-stationary point in expectation, where $L$ is\nthe smoothness parameter and $\\lambda_2(W)$ is the second-largest eigenvalues\nof the gossip matrix $W$. We can verify both of the IFO complexity and\ncommunication complexity match the lower bounds. To the best of our knowledge,\nDEAREST is the first optimal algorithm for decentralized nonconvex finite-sum\noptimization.",
    "descriptor": "",
    "authors": [
      "Luo Luo",
      "Haishan Ye"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13931"
  },
  {
    "id": "arXiv:2210.13968",
    "title": "Faster Projection-Free Augmented Lagrangian Methods via Weak Proximal  Oracle",
    "abstract": "This paper considers a convex composite optimization problem with affine\nconstraints, which includes problems that take the form of minimizing a smooth\nconvex objective function over the intersection of (simple) convex sets, or\nregularized with multiple (simple) functions. Motivated by high-dimensional\napplications in which exact projection/proximal computations are not tractable,\nwe propose a \\textit{projection-free} augmented Lagrangian-based method, in\nwhich primal updates are carried out using a \\textit{weak proximal oracle}\n(WPO). In an earlier work, WPO was shown to be more powerful than the standard\n\\textit{linear minimization oracle} (LMO) that underlies conditional\ngradient-based methods (aka Frank-Wolfe methods). Moreover, WPO is\ncomputationally tractable for many high-dimensional problems of interest,\nincluding those motivated by recovery of low-rank matrices and tensors, and\noptimization over polytopes which admit efficient LMOs. The main result of this\npaper shows that under a certain curvature assumption (which is weaker than\nstrong convexity), our WPO-based algorithm achieves an ergodic rate of\nconvergence of $O(1/T)$ for both the objective residual and feasibility gap.\nThis result, to the best of our knowledge, improves upon the $O(1/\\sqrt{T})$\nrate for existing LMO-based projection-free methods for this class of problems.\nEmpirical experiments on a low-rank and sparse covariance matrix estimation\ntask and the Max Cut semidefinite relaxation demonstrate the superiority of our\nmethod over state-of-the-art LMO-based Lagrangian-based methods.",
    "descriptor": "",
    "authors": [
      "Dan Garber",
      "Tsur Livney",
      "Shoham Sabac"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13968"
  },
  {
    "id": "arXiv:2210.13971",
    "title": "Cellular Automata: Temporal Stochasticity and Computability",
    "abstract": "In this dissertation, we study temporally stochasticity in cellular automata\nand the behavior of such cellular automata. The work also explores the\ncomputational ability of such cellular automaton that illustrates the\ncomputability of solving the affinity classification problem. In addition to\nthat, a cellular automaton, defined over Cayley tree, is shown as the classical\nsearching problem solver. The proposed temporally stochastic cellular automata\ndeals with two elementary cellular automata rules, say $f$ and $g$. The $f$ is\nthe default rule, however, $g$ is temporally applied to the overall system with\nsome probability $\\tau$ which acts as a noise in the system. After exploring\nthe dynamics of temporally stochastic cellular automata (TSCAs), we study the\ndynamical behavior of these temporally stochastic cellular automata (TSCAs) to\nidentify the TSCAs that converge to a fixed point from any seed. We apply each\nof the convergent TSCAs to some standard datasets and observe the effectiveness\nof each TSCA as a pattern classifier. It is observed that the proposed\nTSCA-based classifier shows competitive performance in comparison with existing\nclassifier algorithms. We use temporally stochastic cellular automata to solve\na new problem in the field of cellular automata, named as, affinity\nclassification problem which is a generalization of the density classification\nproblem . We show that this model can be used in several applications, like\nmodeling self-healing systems. Finally, we introduce a new model of computing\nunit developed around cellular automata to reduce the workload of the Central\nProcessing Unit (CPU) of a machine to compute. Each cell of the computing unit\nacts as a tiny processing element with attached memory. Such a CA is\nimplemented on the Cayley Tree to realize efficient solutions for diverse\ncomputational problems.",
    "descriptor": "\nComments: This is my M.Tech thesis under the guidance of Dr. Sukanta Das, Associate Professor, Department of Information Technology, Indian Institute of Engineering Science and Technology\n",
    "authors": [
      "Subrata Paul"
    ],
    "subjectives": [
      "Cellular Automata and Lattice Gases (nlin.CG)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2210.13971"
  },
  {
    "id": "arXiv:2210.14007",
    "title": "MEW-UNet: Multi-axis representation learning in frequency domain for  medical image segmentation",
    "abstract": "Recently, Visual Transformer (ViT) has been widely used in various fields of\ncomputer vision due to applying self-attention mechanism in the spatial domain\nto modeling global knowledge. Especially in medical image segmentation (MIS),\nmany works are devoted to combining ViT and CNN, and even some works directly\nutilize pure ViT-based models. However, recent works improved models in the\naspect of spatial domain while ignoring the importance of frequency domain\ninformation. Therefore, we propose Multi-axis External Weights UNet (MEW-UNet)\nfor MIS based on the U-shape architecture by replacing self-attention in ViT\nwith our Multi-axis External Weights block. Specifically, our block performs a\nFourier transform on the three axes of the input feature and assigns the\nexternal weight in the frequency domain, which is generated by our Weights\nGenerator. Then, an inverse Fourier transform is performed to change the\nfeatures back to the spatial domain. We evaluate our model on four datasets and\nachieve state-of-the-art performances. In particular, on the Synapse dataset,\nour method outperforms MT-UNet by 10.15mm in terms of HD95. Code is available\nat https://github.com/JCruan519/MEW-UNet.",
    "descriptor": "\nComments: 5 pages, 3 figures, 4 tables\n",
    "authors": [
      "Jiacheng Ruan",
      "Mingye Xie",
      "Suncheng Xiang",
      "Ting Liu",
      "Yuzhuo Fu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14007"
  },
  {
    "id": "arXiv:2210.14041",
    "title": "Enhanced Fuzzy Decomposition of Sound Into Sines, Transients, and Noise",
    "abstract": "The decomposition of sounds into sines, transients, and noise is a\nlong-standing research problem in audio processing. The current solutions for\nthis three-way separation detect either horizontal and vertical structures or\nanisotropy and orientations in the spectrogram to identify the properties of\neach spectral bin and classify it as sinusoidal, transient, or noise. This\npaper proposes an enhanced three-way decomposition method based on fuzzy logic,\nenabling soft masking while preserving the perfect reconstruction property. The\nproposed method allows each spectral bin to simultaneously belong to two\nclasses, sine and noise or transient and noise. Results of a subjective\nlistening test against three other techniques are reported, showing that the\nproposed decomposition yields a better or comparable quality. The main\nimprovement appears in transient separation, which enjoys little or no loss of\nenergy or leakage from the other components and performs well for test signals\npresenting strong transients. The audio quality of the separation is shown to\ndepend on the complexity of the input signal for all tested methods. The\nproposed method helps improve the quality of various audio processing\napplications. A successful implementation over a state-of-the-art time-scale\nmodification method is reported as an example.",
    "descriptor": "\nComments: Submitted for publication to the Journal of Audio Engineering Society on October 20th, 2022\n",
    "authors": [
      "Leonardo Fierro",
      "Vesa V\u00e4lim\u00e4ki"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14041"
  },
  {
    "id": "arXiv:2210.14044",
    "title": "SeismicNet: Physics-informed neural networks for seismic wave modeling  in semi-infinite domain",
    "abstract": "There has been an increasing interest in integrating physics knowledge and\nmachine learning for modeling dynamical systems. However, very limited studies\nhave been conducted on seismic wave modeling tasks. A critical challenge is\nthat these geophysical problems are typically defined in large domains (i.e.,\nsemi-infinite), which leads to high computational cost. In this paper, we\npresent a novel physics-informed neural network (PINN) model for seismic wave\nmodeling in semi-infinite domain without the nedd of labeled data. In specific,\nthe absorbing boundary condition is introduced into the network as a soft\nregularizer for handling truncated boundaries. In terms of computational\nefficiency, we consider a sequential training strategy via temporal domain\ndecomposition to improve the scalability of the network and solution accuracy.\nMoreover, we design a novel surrogate modeling strategy for parametric loading,\nwhich estimates the wave propagation in semin-infinite domain given the seismic\nloading at different locations. Various numerical experiments have been\nimplemented to evaluate the performance of the proposed PINN model in the\ncontext of forward modeling of seismic wave propagation. In particular, we\ndefine diverse material distributions to test the versatility of this approach.\nThe results demonstrate excellent solution accuracy under distinctive\nscenarios.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Pu Ren",
      "Chengping Rao",
      "Hao Sun",
      "Yang Liu"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14044"
  },
  {
    "id": "arXiv:2210.14088",
    "title": "Enabling Quantum Speedup of Markov Chains using a Multi-level Approach",
    "abstract": "Quantum speedup for mixing a Markov chain can be achieved based on the\nconstruction of slowly-varying $r$ Markov chains where the initial chain can be\neasily prepared and the spectral gaps have uniform lower bound. The overall\ncomplexity is proportional to $r$. We present a multi-level approach to\nconstruct such a sequence of $r$ Markov chains by varying a resolution\nparameter $h.$ We show that the density function of a low-resolution Markov\nchain can be used to warm start the Markov chain with high resolution. We prove\nthat in terms of the chain length the new algorithm has $O(1)$ complexity\nrather than $O(r).$",
    "descriptor": "",
    "authors": [
      "Xiantao Li"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.14088"
  },
  {
    "id": "arXiv:2210.14090",
    "title": "EBEN: Extreme bandwidth extension network applied to speech signals  captured with noise-resilient microphones",
    "abstract": "In this paper, we present Extreme Bandwidth Extension Network (EBEN), a\ngenerative adversarial network (GAN) that enhances audio measured with\nnoise-resilient microphones. This type of capture equipment suppresses ambient\nnoise at the expense of speech bandwidth, thereby requiring signal enhancement\ntechniques to recover the wideband speech signal. EBEN leverages a multiband\ndecomposition of the raw captured speech to decrease the data time-domain\ndimensions, and give better control over the full-band signal. This multiband\nrepresentation is fed to a U-Net-like model, which adopts a combination of\nfeature and adversarial losses to recover an enhanced audio signal. We also\nbenefit from this original representation in the proposed discriminator\narchitecture. Our approach can achieve state-of-the-art results with a\nlightweight generator and real-time compatible operation.",
    "descriptor": "\nComments: 5 pages, 5 figures, submitted to ICASSP 2023\n",
    "authors": [
      "Julien Hauret",
      "Thomas Joubaud",
      "V\u00e9ronique Zimpfer",
      "\u00c9ric Bavu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.14090"
  },
  {
    "id": "arXiv:2210.14143",
    "title": "Entanglement Purification with Quantum LDPC Codes and Iterative Decoding",
    "abstract": "Recent constructions of quantum low-density parity-check (QLDPC) codes\nprovide optimal scaling of the number of logical qubits and the minimum\ndistance in terms of the code length, thereby opening the door to\nfault-tolerant quantum systems with minimal resource overhead. However, the\nhardware path from nearest-neighbor-connection-based topological codes to\nlong-range-interaction-demanding QLDPC codes is likely a challenging one. Given\nthe practical difficulty in building a monolithic architecture for quantum\nsystems, such as computers, based on optimal QLDPC codes, it is worth\nconsidering a distributed implementation of such codes over a network of\ninterconnected medium-sized quantum processors. In such a setting, all syndrome\nmeasurements and logical operations must be performed through the use of\nhigh-fidelity shared entangled states between the processing nodes. Since\nprobabilistic many-to-1 distillation schemes for purifying entanglement are\ninefficient, we investigate quantum error correction based entanglement\npurification in this work. Specifically, we employ QLDPC codes to distill GHZ\nstates, as the resulting high-fidelity logical GHZ states can interact directly\nwith the code used to perform distributed quantum computing (DQC), e.g. for\nfault-tolerant Steane syndrome extraction. This protocol is applicable beyond\nthe application of DQC since entanglement distribution and purification is a\nquintessential task of any quantum network. We use the min-sum algorithm (MSA)\nbased iterative decoder with a sequential schedule for distilling 3-qubit GHZ\nstates using a rate 0.118 family of lifted product QLDPC codes and obtain a\nthreshold of 10.7% under depolarizing noise. Our results apply to larger size\nGHZ states as well, where we extend our technical result about a measurement\nproperty of 3-qubit GHZ states to construct a scalable GHZ purification\nprotocol.",
    "descriptor": "\nComments: 20 pages (main) + 9 pages (appendix); includes a new algorithm to generate logical Pauli operators for stabilizer codes; partial text overlap with arXiv:2109.06248 but this is new work about a simpler protocol with quantum LDPC codes, including extensive simulation results. Comments welcome!\n",
    "authors": [
      "Narayanan Rengaswamy",
      "Nithin Raveendran",
      "Ankur Raina",
      "Bane Vasi\u0107"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.14143"
  },
  {
    "id": "arXiv:2210.14152",
    "title": "SleepMore: Sleep Prediction at Scale via Multi-Device WiFi Sensing",
    "abstract": "The availability of commercial wearable trackers equipped with features to\nmonitor sleep duration and quality has enabled more useful sleep health\nmonitoring applications and analyses. However, much research has reported the\nchallenge of long-term user retention in sleep monitoring through these\nmodalities. Since modern Internet users own multiple mobile devices, our work\nexplores the possibility of employing ubiquitous mobile devices and passive\nWiFi sensing techniques to predict sleep duration as the fundamental measure\nfor complementing long-term sleep monitoring initiatives. In this paper, we\npropose SleepMore, an accurate and easy-to-deploy sleep-tracking approach based\non machine learning over the user's WiFi network activity. It first employs a\nsemi-personalized random forest model with an infinitesimal jackknife variance\nestimation method to classify a user's network activity behavior into sleep and\nawake states per minute granularity. Through a moving average technique, the\nsystem uses these state sequences to estimate the user's nocturnal sleep period\nand its uncertainty rate. Uncertainty quantification enables SleepMore to\novercome the impact of noisy WiFi data that can yield large prediction errors.\nWe validate SleepMore using data from a month-long user study involving 46\ncollege students and draw comparisons with the Oura Ring wearable. Beyond the\ncollege campus, we evaluate SleepMore on non-student users of different housing\nprofiles. Our results demonstrate that SleepMore produces statistically\nindistinguishable sleep statistics from the Oura ring baseline for predictions\nmade within a 5% uncertainty rate. These errors range between 15-28 minutes for\ndetermining sleep time and 7-29 minutes for determining wake time, proving\nstatistically significant improvements over prior work. Our in-depth analysis\nexplains the sources of errors.",
    "descriptor": "\nComments: 29 pages, 24 figures, 14 tables\n",
    "authors": [
      "Camellia Zakaria",
      "Gizem Yilmaz",
      "Priyanka Mammen",
      "Michael Chee",
      "Prashant Shenoy",
      "Rajesh Balan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14152"
  },
  {
    "id": "arXiv:2210.14156",
    "title": "Motion correction in MRI using deep learning and a novel hybrid loss  function",
    "abstract": "Purpose To develop and evaluate a deep learning-based method (MC-Net) to\nsuppress motion artifacts in brain magnetic resonance imaging (MRI). Methods\nMC-Net was derived from a UNet combined with a two-stage multi-loss function.\nT1-weighted axial brain images contaminated with synthetic motions were used to\ntrain the network. Evaluation used simulated T1 and T2-weighted axial, coronal,\nand sagittal images unseen during training, as well as T1-weighted images with\nmotion artifacts from real scans. Performance indices included the peak signal\nto noise ratio (PSNR), structural similarity index measure (SSIM), and visual\nreading scores. Two clinical readers scored the images. Results The MC-Net\noutperformed other methods implemented in terms of PSNR and SSIM on the T1\naxial test set. The MC-Net significantly improved the quality of all\nT1-weighted images (for all directions and for simulated as well as real motion\nartifacts), both on quantitative measures and visual scores. However, the\nMC-Net performed poorly on images of untrained contrast (T2-weighted).\nConclusion The proposed two-stage multi-loss MC-Net can effectively suppress\nmotion artifacts in brain MRI without compromising image context. Given the\nefficiency of the MC-Net (single image processing time ~40ms), it can\npotentially be used in real clinical settings. To facilitate further research,\nthe code and trained model are available at\nhttps://github.com/MRIMoCo/DL_Motion_Correction.",
    "descriptor": "",
    "authors": [
      "Lei Zhang",
      "Xiaoke Wang",
      "Michael Rawson",
      "Radu Balan",
      "Edward H. Herskovits",
      "Elias Melhem",
      "Linda Chang",
      "Ze Wang",
      "Thomas Ernst"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.14156"
  },
  {
    "id": "arXiv:2210.14184",
    "title": "Learning Ability of Interpolating Convolutional Neural Networks",
    "abstract": "It is frequently observed that overparameterized neural networks generalize\nwell. Regarding such phenomena, existing theoretical work mainly devotes to\nlinear settings or fully connected neural networks. This paper studies learning\nability of an important family of deep neural networks, deep convolutional\nneural networks (DCNNs), under underparameterized and overparameterized\nsettings. We establish the best learning rates of underparameterized DCNNs\nwithout parameter restrictions presented in the literature. We also show that,\nby adding well defined layers to an underparameterized DCNN, we can obtain some\ninterpolating DCNNs that maintain the good learning rates of the\nunderparameterized DCNN. This result is achieved by a novel network deepening\nscheme designed for DCNNs. Our work provides theoretical verification on how\noverfitted DCNNs generalize well.",
    "descriptor": "",
    "authors": [
      "Tian-Yi Zhou",
      "Xiaoming Huo"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14184"
  },
  {
    "id": "arXiv:2210.14191",
    "title": "A Database of Ultrastable MOFs Reassembled from Stable Fragments with  Machine Learning Models",
    "abstract": "High-throughput screening of large hypothetical databases of metal-organic\nframeworks (MOFs) can uncover new materials, but their stability in real-world\napplications is often unknown. We leverage community knowledge and machine\nlearning (ML) models to identify MOFs that are thermally stable and stable upon\nactivation. We separate these MOFs into their building blocks and recombine\nthem to make a new hypothetical MOF database of over 50,000 structures that\nsamples orders of magnitude more connectivity nets and inorganic building\nblocks than prior databases. This database shows an order of magnitude\nenrichment of ultrastable MOF structures that are stable upon activation and\nmore than one standard deviation more thermally stable than the average\nexperimentally characterized MOF. For the nearly 10,000 ultrastable MOFs, we\ncompute bulk elastic moduli to confirm these materials have good mechanical\nstability, and we report methane deliverable capacities. Our work identifies\nprivileged metal nodes in ultrastable MOFs that optimize gas storage and\nmechanical stability simultaneously.",
    "descriptor": "",
    "authors": [
      "Aditya Nandy",
      "Shuwen Yue",
      "Changhwan Oh",
      "Chenru Duan",
      "Gianmarco G. Terrones",
      "Yongchul G. Chung",
      "Heather J. Kulik"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.14191"
  },
  {
    "id": "arXiv:2210.14195",
    "title": "Using Deep Learning to Find the Next Unicorn: A Practical Synthesis",
    "abstract": "Startups often represent newly established business models associated with\ndisruptive innovation and high scalability. They are commonly regarded as\npowerful engines for economic and social development. Meanwhile, startups are\nheavily constrained by many factors such as limited financial funding and human\nresources. Therefore the chance for a startup to eventually succeed is as rare\nas ``spotting a unicorn in the wild''. Venture Capital (VC) strives to identify\nand invest in unicorn startups during their early stages, hoping to gain a high\nreturn. To avoid entirely relying on human domain expertise and intuition,\ninvestors usually employ data-driven approaches to forecast the success\nprobability of startups. Over the past two decades, the industry has gone\nthrough a paradigm shift moving from conventional statistical approaches\ntowards becoming machine-learning (ML) based. Notably, the rapid growth of data\nvolume and variety is quickly ushering in deep learning (DL), a subset of ML,\nas a potentially superior approach in terms capacity and expressivity. In this\nwork, we carry out a literature review and synthesis on DL-based approaches,\ncovering the entire DL life cycle. The objective is a) to obtain a thorough and\nin-depth understanding of the methodologies for startup evaluation using DL,\nand b) to distil valuable and actionable learning for practitioners. To the\nbest of our knowledge, our work is the first of this kind.",
    "descriptor": "\nComments: 48 pages, 18 figures\n",
    "authors": [
      "Lele Cao",
      "Vilhelm von Ehrenheim",
      "Sebastian Krakowski",
      "Xiaoxue Li",
      "Alexandra Lutz"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.14195"
  },
  {
    "id": "arXiv:1710.04656",
    "title": "Behavioral Communities and the Atomic Structure of Networks",
    "abstract": "Behavioral Communities and the Atomic Structure of Networks",
    "descriptor": "",
    "authors": [
      "Matthew O. Jackson",
      "Evan C. Storms"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/1710.04656"
  },
  {
    "id": "arXiv:1806.06537",
    "title": "Boolean-like algebras of finite dimension",
    "abstract": "Boolean-like algebras of finite dimension",
    "descriptor": "",
    "authors": [
      "Antonio Bucciarelli",
      "Antonio Ledda",
      "Francesco Paoli",
      "Antonino Salibra"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/1806.06537"
  },
  {
    "id": "arXiv:2004.07671",
    "title": "Isomorphism Testing for Graphs Excluding Small Minors",
    "abstract": "Comments: 33 pages, 7 figures, full version of a paper accepted at FOCS 2020; second version improves the presentation of the results",
    "descriptor": "\nComments: 33 pages, 7 figures, full version of a paper accepted at FOCS 2020; second version improves the presentation of the results\n",
    "authors": [
      "Martin Grohe",
      "Daniel Neuen",
      "Daniel Wiebking"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2004.07671"
  },
  {
    "id": "arXiv:2006.15160",
    "title": "Dissecting power of intersection of two context free languages",
    "abstract": "Dissecting power of intersection of two context free languages",
    "descriptor": "",
    "authors": [
      "Josef Rukavicka"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2006.15160"
  },
  {
    "id": "arXiv:2010.12676",
    "title": "A Differentiable Relaxation of Graph Segmentation and Alignment for AMR  Parsing",
    "abstract": "A Differentiable Relaxation of Graph Segmentation and Alignment for AMR  Parsing",
    "descriptor": "",
    "authors": [
      "Chunchuan Lyu",
      "Shay B. Cohen",
      "Ivan Titov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.12676"
  },
  {
    "id": "arXiv:2011.00905",
    "title": "Advanced Semantics for Commonsense Knowledge Extraction",
    "abstract": "Comments: 12 pages, 3 figures, 11 tables",
    "descriptor": "\nComments: 12 pages, 3 figures, 11 tables\n",
    "authors": [
      "Tuan-Phong Nguyen",
      "Simon Razniewski",
      "Gerhard Weikum"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2011.00905"
  },
  {
    "id": "arXiv:2011.02980",
    "title": "Using Five Cards to Encode Each Integer in $\\mathbb{Z}/6\\mathbb{Z}$",
    "abstract": "Comments: This paper has appeared at SecITC 2021",
    "descriptor": "\nComments: This paper has appeared at SecITC 2021\n",
    "authors": [
      "Suthee Ruangwises"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2011.02980"
  },
  {
    "id": "arXiv:2011.04295",
    "title": "Interactive Oracle Proofs of Proximity to Algebraic Geometry Codes",
    "abstract": "Comments: Published version with two additional authors",
    "descriptor": "\nComments: Published version with two additional authors\n",
    "authors": [
      "Sarah Bordage",
      "Mathieu Lhotel",
      "Jade Nardi",
      "Hugues Randriam"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2011.04295"
  },
  {
    "id": "arXiv:2011.08069",
    "title": "Reconciling Security and Utility in Next-Generation Epidemic Risk  Mitigation Systems",
    "abstract": "Reconciling Security and Utility in Next-Generation Epidemic Risk  Mitigation Systems",
    "descriptor": "",
    "authors": [
      "Pierfrancesco Ingo",
      "Nichole Boufford",
      "Ming Cheng Jiang",
      "Rowan Lindsay",
      "Roberta De Viti",
      "Matthew Lentz",
      "Gilles Barthe",
      "Manuel Gomez-Rodriguez",
      "Bernhard Sch\u00f6lkopf",
      "Deepak Garg",
      "Peter Druschel",
      "Aastha Mehta"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2011.08069"
  },
  {
    "id": "arXiv:2012.01059",
    "title": "A Temporally Consistent Image-based Sun Tracking Algorithm for Solar  Energy Forecasting Applications",
    "abstract": "Comments: Accepted as a workshop paper at NeurIPS 2020",
    "descriptor": "\nComments: Accepted as a workshop paper at NeurIPS 2020\n",
    "authors": [
      "Quentin Paletta",
      "Joan Lasenby"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.01059"
  },
  {
    "id": "arXiv:2012.07261",
    "title": "OCTA-500: A Retinal Dataset for Optical Coherence Tomography Angiography  Study",
    "abstract": "OCTA-500: A Retinal Dataset for Optical Coherence Tomography Angiography  Study",
    "descriptor": "",
    "authors": [
      "Mingchao Li",
      "Kun Huang",
      "Qiuzhuo Xu",
      "Jiadong Yang",
      "Yuhan Zhang",
      "Zexuan Ji",
      "Keren Xie",
      "Songtao Yuan",
      "Qinghuai Liu",
      "Qiang Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.07261"
  },
  {
    "id": "arXiv:2101.07518",
    "title": "BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring",
    "abstract": "Comments: TIP 2022, Code: this https URL",
    "descriptor": "\nComments: TIP 2022, Code: this https URL\n",
    "authors": [
      "Fu-Jen Tsai",
      "Yan-Tsung Peng",
      "Yen-Yu Lin",
      "Chung-Chi Tsai",
      "Chia-Wen Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2101.07518"
  },
  {
    "id": "arXiv:2101.11442",
    "title": "Magnetic Resonance Spectroscopy Deep Learning Denoising Using Few In  Vivo Data",
    "abstract": "Magnetic Resonance Spectroscopy Deep Learning Denoising Using Few In  Vivo Data",
    "descriptor": "",
    "authors": [
      "Dicheng Chen",
      "Wanqi Hu",
      "Huiting Liu",
      "Yirong Zhou",
      "Tianyu Qiu",
      "Yihui Huang",
      "Zi Wang",
      "Jiazheng Wang",
      "Liangjie Lin",
      "Zhigang Wu",
      "Hao Chen",
      "Xi Chen",
      "Gen Yan",
      "Di Guo",
      "Jianzhong Lin",
      "Xiaobo Qu"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2101.11442"
  },
  {
    "id": "arXiv:2102.06626",
    "title": "Do-calculus enables estimation of causal effects in partially observed  biomolecular pathways",
    "abstract": "Comments: this https URL",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Sara Mohammad-Taheri",
      "Jeremy Zucker",
      "Charles Tapley Hoyt",
      "Karen Sachs",
      "Vartika Tewari",
      "Robert Ness",
      "and Olga Vitek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.06626"
  },
  {
    "id": "arXiv:2102.09683",
    "title": "Community Structure Recovery and Interaction Probability Estimation for  Gossip Opinion Dynamics",
    "abstract": "Community Structure Recovery and Interaction Probability Estimation for  Gossip Opinion Dynamics",
    "descriptor": "",
    "authors": [
      "Yu Xing",
      "Xingkang He",
      "Haitao Fang",
      "Karl H. Johansson"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2102.09683"
  },
  {
    "id": "arXiv:2103.11072",
    "title": "Local Interpretations for Explainable Natural Language Processing: A  Survey",
    "abstract": "Comments: Under review by ACM Computing Surveys",
    "descriptor": "\nComments: Under review by ACM Computing Surveys\n",
    "authors": [
      "Siwen Luo",
      "Hamish Ivison",
      "Caren Han",
      "Josiah Poon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2103.11072"
  },
  {
    "id": "arXiv:2103.15510",
    "title": "Photoacoustic image synthesis with generative adversarial networks",
    "abstract": "Comments: 10 pages, 6 figures, 2 tables, update with paper published at Photoacoustics",
    "descriptor": "\nComments: 10 pages, 6 figures, 2 tables, update with paper published at Photoacoustics\n",
    "authors": [
      "Melanie Schellenberg",
      "Janek Gr\u00f6hl",
      "Kris K. Dreher",
      "Jan-Hinrich N\u00f6lke",
      "Niklas Holzwarth",
      "Minu D. Tizabi",
      "Alexander Seitel",
      "Lena Maier-Hein"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2103.15510"
  },
  {
    "id": "arXiv:2104.09120",
    "title": "SAS: A Simple, Accurate and Scalable Node Classification Algorithm",
    "abstract": "Comments: add IEEE copyright",
    "descriptor": "\nComments: add IEEE copyright\n",
    "authors": [
      "Ziyuan Wang",
      "Feiming Yang",
      "Rui Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.09120"
  },
  {
    "id": "arXiv:2104.09570",
    "title": "Extracting Temporal Event Relation with Syntax-guided Graph Transformer",
    "abstract": "Extracting Temporal Event Relation with Syntax-guided Graph Transformer",
    "descriptor": "",
    "authors": [
      "Shuaicheng Zhang",
      "Lifu Huang",
      "Qiang Ning"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2104.09570"
  },
  {
    "id": "arXiv:2104.13636",
    "title": "Point Cloud Learning with Transformer",
    "abstract": "Comments: 10 pages, 4 figures",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Qi Zhong",
      "Xian-Feng Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2104.13636"
  },
  {
    "id": "arXiv:2107.06304",
    "title": "Privacy Vulnerability of Split Computing to Data-Free Model Inversion  Attacks",
    "abstract": "Comments: A new data-free inversion method to reverse neural networks and get input from intermediate feature maps. BMVC'22",
    "descriptor": "\nComments: A new data-free inversion method to reverse neural networks and get input from intermediate feature maps. BMVC'22\n",
    "authors": [
      "Xin Dong",
      "Hongxu Yin",
      "Jose M. Alvarez",
      "Jan Kautz",
      "Pavlo Molchanov",
      "H.T. Kung"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2107.06304"
  },
  {
    "id": "arXiv:2107.12416",
    "title": "Asynchronous Distributed Reinforcement Learning for LQR Control via  Zeroth-Order Block Coordinate Descent",
    "abstract": "Asynchronous Distributed Reinforcement Learning for LQR Control via  Zeroth-Order Block Coordinate Descent",
    "descriptor": "",
    "authors": [
      "Gangshan Jing",
      "He Bai",
      "Jemin George",
      "Aranya Chakrabortty",
      "Piyush K. Sharma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2107.12416"
  },
  {
    "id": "arXiv:2108.04718",
    "title": "Sampling-Based Approximations to Minimum Bayes Risk Decoding for Neural  Machine Translation",
    "abstract": "Comments: EMNLP 2022 camera-ready",
    "descriptor": "\nComments: EMNLP 2022 camera-ready\n",
    "authors": [
      "Bryan Eikema",
      "Wilker Aziz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.04718"
  },
  {
    "id": "arXiv:2108.09306",
    "title": "D-DARTS: Distributed Differentiable Architecture Search",
    "abstract": "D-DARTS: Distributed Differentiable Architecture Search",
    "descriptor": "",
    "authors": [
      "Alexandre Heuillet",
      "Hedi Tabia",
      "Hichem Arioui",
      "Kamal Youcef-Toumi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.09306"
  },
  {
    "id": "arXiv:2109.01902",
    "title": "Barycentric-alignment and reconstruction loss minimization for domain  generalization",
    "abstract": "Comments: Theoretical and experimental results are updated",
    "descriptor": "\nComments: Theoretical and experimental results are updated\n",
    "authors": [
      "Boyang Lyu",
      "Thuan Nguyen",
      "Prakash Ishwar",
      "Matthias Scheutz",
      "Shuchin Aeron"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.01902"
  },
  {
    "id": "arXiv:2109.03780",
    "title": "Bayesian Over-The-Air Computation",
    "abstract": "Comments: Multi-tier computing, over-the-air computation, Bayesian estimation, sum-product algorithm. 18 pages, 11 figures. arXiv admin note: text overlap with arXiv:2102.13604",
    "descriptor": "\nComments: Multi-tier computing, over-the-air computation, Bayesian estimation, sum-product algorithm. 18 pages, 11 figures. arXiv admin note: text overlap with arXiv:2102.13604\n",
    "authors": [
      "Yulin Shao",
      "Deniz Gunduz",
      "Soung Chang Liew"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2109.03780"
  },
  {
    "id": "arXiv:2109.06915",
    "title": "Reconstruction on Trees and Low-Degree Polynomials",
    "abstract": "Comments: 28 pages, comments welcome. V2: added new result and more exposition. abstract shortened for arxiv",
    "descriptor": "\nComments: 28 pages, comments welcome. V2: added new result and more exposition. abstract shortened for arxiv\n",
    "authors": [
      "Frederic Koehler",
      "Elchanan Mossel"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2109.06915"
  },
  {
    "id": "arXiv:2110.01822",
    "title": "Verified eigenvalue and eigenvector computations using complex moments  and the Rayleigh$\\unicode{x2013}$Ritz procedure for generalized Hermitian  eigenvalue problems",
    "abstract": "Comments: 23 pages, 4 figures",
    "descriptor": "\nComments: 23 pages, 4 figures\n",
    "authors": [
      "Akira Imakura",
      "Keiichi Morikuni",
      "Akitoshi Takayasu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.01822"
  },
  {
    "id": "arXiv:2110.06870",
    "title": "Junkyard Computing: Repurposing Discarded Smartphones to Minimize Carbon",
    "abstract": "Junkyard Computing: Repurposing Discarded Smartphones to Minimize Carbon",
    "descriptor": "",
    "authors": [
      "Jennifer Switzer",
      "Gabriel Marcano",
      "Ryan Kastner",
      "Pat Pannuto"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2110.06870"
  },
  {
    "id": "arXiv:2110.10211",
    "title": "Learning Partial Equivariances from Data",
    "abstract": "Comments: Published at NeurIPS 2022",
    "descriptor": "\nComments: Published at NeurIPS 2022\n",
    "authors": [
      "David W. Romero",
      "Suhas Lohit"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.10211"
  },
  {
    "id": "arXiv:2110.13798",
    "title": "Deeper-GXX: Deepening Arbitrary GNNs",
    "abstract": "Deeper-GXX: Deepening Arbitrary GNNs",
    "descriptor": "",
    "authors": [
      "Lecheng Zheng",
      "Dongqi Fu",
      "Ross Maciejewski",
      "Jingrui He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13798"
  },
  {
    "id": "arXiv:2110.14332",
    "title": "Rainbow cycles for families of matchings",
    "abstract": "Comments: 5 pages; minor edits; to appear in Israel Journal of Mathematics",
    "descriptor": "\nComments: 5 pages; minor edits; to appear in Israel Journal of Mathematics\n",
    "authors": [
      "Ron Aharoni",
      "He Guo"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2110.14332"
  },
  {
    "id": "arXiv:2111.00597",
    "title": "Reduced Order Model Predictive Control for Parametrized Parabolic  Partial Differential Equations",
    "abstract": "Reduced Order Model Predictive Control for Parametrized Parabolic  Partial Differential Equations",
    "descriptor": "",
    "authors": [
      "Saskia Dietze",
      "Martin A. Grepl"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.00597"
  },
  {
    "id": "arXiv:2111.02168",
    "title": "Graph Neural Networks for Nomination and Representation Learning of Web  Elements",
    "abstract": "Comments: 12 pages, 8 figures, 3 tables, under review",
    "descriptor": "\nComments: 12 pages, 8 figures, 3 tables, under review\n",
    "authors": [
      "Alexandra Hotti",
      "Riccardo Sven Risuleo",
      "Stefan Magureanu",
      "Aref Moradi",
      "Jens Lagergren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2111.02168"
  },
  {
    "id": "arXiv:2111.02295",
    "title": "The Parameterized Complexity of the Survivable Network Design Problem",
    "abstract": "The Parameterized Complexity of the Survivable Network Design Problem",
    "descriptor": "",
    "authors": [
      "Andreas Emil Feldmann",
      "Anish Mukherjee",
      "Erik Jan van Leeuwen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2111.02295"
  },
  {
    "id": "arXiv:2111.02719",
    "title": "SPEEDEX: A Scalable, Parallelizable, and Economically Efficient Digital  EXchange",
    "abstract": "Comments: 22 pages, 5 figures",
    "descriptor": "\nComments: 22 pages, 5 figures\n",
    "authors": [
      "Geoffrey Ramseyer",
      "Ashish Goel",
      "David Mazi\u00e8res"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2111.02719"
  },
  {
    "id": "arXiv:2111.04066",
    "title": "Fast sampling via spectral independence beyond bounded-degree graphs",
    "abstract": "Fast sampling via spectral independence beyond bounded-degree graphs",
    "descriptor": "",
    "authors": [
      "Ivona Bez\u00e1kov\u00e1",
      "Andreas Galanis",
      "Leslie Ann Goldberg",
      "Daniel \u0160tefankovi\u010d"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2111.04066"
  },
  {
    "id": "arXiv:2111.04090",
    "title": "Learn-Morph-Infer: a new way of solving the inverse problem for brain  tumor modeling",
    "abstract": "Learn-Morph-Infer: a new way of solving the inverse problem for brain  tumor modeling",
    "descriptor": "",
    "authors": [
      "Ivan Ezhov",
      "Kevin Scibilia",
      "Katharina Franitza",
      "Felix Steinbauer",
      "Suprosanna Shit",
      "Lucas Zimmer",
      "Jana Lipkova",
      "Florian Kofler",
      "Johannes Paetzold",
      "Luca Canalini",
      "Diana Waldmannstetter",
      "Martin Menten",
      "Marie Metz",
      "Benedikt Wiestler",
      "Bjoern Menze"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2111.04090"
  },
  {
    "id": "arXiv:2111.05060",
    "title": "View Birdification in the Crowd: Ground-Plane Localization from  Perceived Movements",
    "abstract": "Comments: Extended journal version of the original paper at BMVC 2021",
    "descriptor": "\nComments: Extended journal version of the original paper at BMVC 2021\n",
    "authors": [
      "Mai Nishimura",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.05060"
  },
  {
    "id": "arXiv:2111.10984",
    "title": "Topological Regularization for Dense Prediction",
    "abstract": "Topological Regularization for Dense Prediction",
    "descriptor": "",
    "authors": [
      "Deqing Fu",
      "Bradley J. Nelson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2111.10984"
  },
  {
    "id": "arXiv:2112.05253",
    "title": "MAGMA -- Multimodal Augmentation of Generative Models through  Adapter-based Finetuning",
    "abstract": "Comments: 13 pages, 6 figures, 2 tables. Minor improvements. Accepted at EMNLP 2022",
    "descriptor": "\nComments: 13 pages, 6 figures, 2 tables. Minor improvements. Accepted at EMNLP 2022\n",
    "authors": [
      "Constantin Eichenberg",
      "Sidney Black",
      "Samuel Weinbach",
      "Letitia Parcalabescu",
      "Anette Frank"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.05253"
  },
  {
    "id": "arXiv:2112.12042",
    "title": "Physical ZKP for Makaro Using a Standard Deck of Cards",
    "abstract": "Comments: This paper has appeared at TAMC 2022",
    "descriptor": "\nComments: This paper has appeared at TAMC 2022\n",
    "authors": [
      "Suthee Ruangwises",
      "Toshiya Itoh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2112.12042"
  },
  {
    "id": "arXiv:2201.09862",
    "title": "Learning to Act with Affordance-Aware Multimodal Neural SLAM",
    "abstract": "Comments: Accepted by IROS 2022",
    "descriptor": "\nComments: Accepted by IROS 2022\n",
    "authors": [
      "Zhiwei Jia",
      "Kaixiang Lin",
      "Yizhou Zhao",
      "Qiaozi Gao",
      "Govind Thattai",
      "Gaurav Sukhatme"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.09862"
  },
  {
    "id": "arXiv:2201.10890",
    "title": "One Student Knows All Experts Know: From Sparse to Dense",
    "abstract": "One Student Knows All Experts Know: From Sparse to Dense",
    "descriptor": "",
    "authors": [
      "Fuzhao Xue",
      "Xiaoxin He",
      "Xiaozhe Ren",
      "Yuxuan Lou",
      "Yang You"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.10890"
  },
  {
    "id": "arXiv:2201.11624",
    "title": "LiteLSTM Architecture for Deep Recurrent Neural Networks",
    "abstract": "Comments: Accepted in the IEEE International Symposium on Circuits and Systems (ISCAS) 2022",
    "descriptor": "\nComments: Accepted in the IEEE International Symposium on Circuits and Systems (ISCAS) 2022\n",
    "authors": [
      "Nelly Elsayed",
      "Zag ElSayed",
      "Anthony S. Maida"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.11624"
  },
  {
    "id": "arXiv:2202.00964",
    "title": "What Has Been Enhanced in my Knowledge-Enhanced Language Model?",
    "abstract": "Comments: Our code, demo, and instructions of the usage can be found in this https URL",
    "descriptor": "\nComments: Our code, demo, and instructions of the usage can be found in this https URL\n",
    "authors": [
      "Yifan Hou",
      "Guoji Fu",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.00964"
  },
  {
    "id": "arXiv:2202.02973",
    "title": "SpotLake: Diverse Spot Instance Dataset Archive Service",
    "abstract": "Comments: 14 pages, 11 figures. This paper is accepted to IISWC 2022",
    "descriptor": "\nComments: 14 pages, 11 figures. This paper is accepted to IISWC 2022\n",
    "authors": [
      "Sungjae Lee",
      "Jaeil Hwang",
      "Kyungyong Lee"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.02973"
  },
  {
    "id": "arXiv:2202.04933",
    "title": "Energy-Based Contrastive Learning of Visual Representations",
    "abstract": "Comments: NeurIPS 2022 Oral Paper",
    "descriptor": "\nComments: NeurIPS 2022 Oral Paper\n",
    "authors": [
      "Beomsu Kim",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04933"
  },
  {
    "id": "arXiv:2202.06152",
    "title": "On Dual-Based PI Controllers for Online Allocation Problems",
    "abstract": "On Dual-Based PI Controllers for Online Allocation Problems",
    "descriptor": "",
    "authors": [
      "Santiago R. Balseiro",
      "Haihao Lu",
      "Vahab Mirrokni",
      "Balasubramanian Sivan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.06152"
  },
  {
    "id": "arXiv:2202.06934",
    "title": "Slicing Aided Hyper Inference and Fine-tuning for Small Object Detection",
    "abstract": "Comments: Presented at ICIP 2022, 5 pages, 4 figures, 2 tables",
    "descriptor": "\nComments: Presented at ICIP 2022, 5 pages, 4 figures, 2 tables\n",
    "authors": [
      "Fatih Cagatay Akyon",
      "Sinan Onur Altinuc",
      "Alptekin Temizel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.06934"
  },
  {
    "id": "arXiv:2202.08409",
    "title": "Million.js: A Fast Compiler-Augmented Virtual DOM for the Web",
    "abstract": "Comments: 10 pages, 18 figures, submitted to ACM SAC",
    "descriptor": "\nComments: 10 pages, 18 figures, submitted to ACM SAC\n",
    "authors": [
      "Aiden Bai"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2202.08409"
  },
  {
    "id": "arXiv:2202.09788",
    "title": "How to Physically Verify a Rectangle in a Grid: A Physical ZKP for  Shikaku",
    "abstract": "Comments: This paper has appeared at FUN 2022",
    "descriptor": "\nComments: This paper has appeared at FUN 2022\n",
    "authors": [
      "Suthee Ruangwises",
      "Toshiya Itoh"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2202.09788"
  },
  {
    "id": "arXiv:2202.10036",
    "title": "Guided Visual Attention Model Based on Interactions Between Top-down and  Bottom-up Information for Robot Pose Prediction",
    "abstract": "Guided Visual Attention Model Based on Interactions Between Top-down and  Bottom-up Information for Robot Pose Prediction",
    "descriptor": "",
    "authors": [
      "Hyogo Hiruma",
      "Hiroki Mori",
      "Hiroshi Ito",
      "Tetsuya Ogata"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2202.10036"
  },
  {
    "id": "arXiv:2202.10170",
    "title": "Entropy of Generating Series for Nonlinear Input-Output Systems and  Their Interconnections",
    "abstract": "Entropy of Generating Series for Nonlinear Input-Output Systems and  Their Interconnections",
    "descriptor": "",
    "authors": [
      "W. Steven Gray"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2202.10170"
  },
  {
    "id": "arXiv:2202.11099",
    "title": "Rotationally Equivariant Super-Resolution of Velocity Fields in  Two-Dimensional Fluids Using Convolutional Neural Networks",
    "abstract": "Rotationally Equivariant Super-Resolution of Velocity Fields in  Two-Dimensional Fluids Using Convolutional Neural Networks",
    "descriptor": "",
    "authors": [
      "Yuki Yasuda",
      "Ryo Onishi"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2202.11099"
  },
  {
    "id": "arXiv:2202.12498",
    "title": "Diffeomorphic Image Registration with Neural Velocity Field",
    "abstract": "Diffeomorphic Image Registration with Neural Velocity Field",
    "descriptor": "",
    "authors": [
      "Kun Han",
      "Shanlin sun",
      "Xiangyi Yan",
      "Chenyu You",
      "Hao Tang",
      "Junayed Naushad",
      "Haoyu Ma",
      "Deying Kong",
      "Xiaohui Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.12498"
  },
  {
    "id": "arXiv:2203.03899",
    "title": "Noisy Low-rank Matrix Optimization: Geometry of Local Minima and  Convergence Rate",
    "abstract": "Noisy Low-rank Matrix Optimization: Geometry of Local Minima and  Convergence Rate",
    "descriptor": "",
    "authors": [
      "Ziye Ma",
      "Somayeh Sojoudi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.03899"
  },
  {
    "id": "arXiv:2203.05406",
    "title": "Disentangled Multimodal Representation Learning for Recommendation",
    "abstract": "Comments: IEEE Transactions on Multimedia (TMM)",
    "descriptor": "\nComments: IEEE Transactions on Multimedia (TMM)\n",
    "authors": [
      "Fan Liu",
      "Huilin Chen",
      "Zhiyong Cheng",
      "Anan Liu",
      "Liqiang Nie",
      "Mohan Kankanhalli"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2203.05406"
  },
  {
    "id": "arXiv:2203.06314",
    "title": "Tensor Radiomics: Paradigm for Systematic Incorporation of  Multi-Flavoured Radiomics Features",
    "abstract": "Tensor Radiomics: Paradigm for Systematic Incorporation of  Multi-Flavoured Radiomics Features",
    "descriptor": "",
    "authors": [
      "Arman Rahmim",
      "Amirhosein Toosi",
      "Mohammad R. Salmanpour",
      "Natalia Dubljevic",
      "Ian Janzen",
      "Isaac Shiri",
      "Ren Yuan",
      "Cheryl Ho",
      "Habib Zaidi",
      "Calum MacAulay",
      "Carlos Uribe",
      "Fereshteh Yousefirizi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2203.06314"
  },
  {
    "id": "arXiv:2203.07738",
    "title": "GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning",
    "abstract": "GCT: Graph Co-Training for Semi-Supervised Few-Shot Learning",
    "descriptor": "",
    "authors": [
      "Shuai Shao",
      "Lei Xing",
      "Weifeng Liu",
      "Yanjiang Wang",
      "Baodi Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.07738"
  },
  {
    "id": "arXiv:2203.08378",
    "title": "Transforming Sequence Tagging Into A Seq2Seq Task",
    "abstract": "Comments: Accepted at EMNLP 2022",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Karthik Raman",
      "Iftekhar Naim",
      "Jiecao Chen",
      "Kazuma Hashimoto",
      "Kiran Yalasangi",
      "Krishna Srinivasan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08378"
  },
  {
    "id": "arXiv:2203.09334",
    "title": "Stronger 3SUM-Indexing Lower Bounds",
    "abstract": "Comments: To be published in SODA2023",
    "descriptor": "\nComments: To be published in SODA2023\n",
    "authors": [
      "Eldon Chung",
      "Kasper Green Larsen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2203.09334"
  },
  {
    "id": "arXiv:2203.10115",
    "title": "Introducing causal inference in the energy-efficient building design  process",
    "abstract": "Comments: 20 pages, 10 figures",
    "descriptor": "\nComments: 20 pages, 10 figures\n",
    "authors": [
      "Xia Chen",
      "Jimmy Abualdenien",
      "Manav Mahan Singh",
      "Andr\u00e9 Borrmann",
      "Philipp Geyer"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2203.10115"
  },
  {
    "id": "arXiv:2203.16034",
    "title": "Monitored Distillation for Positive Congruent Depth Completion",
    "abstract": "Monitored Distillation for Positive Congruent Depth Completion",
    "descriptor": "",
    "authors": [
      "Tian Yu Liu",
      "Parth Agrawal",
      "Allison Chen",
      "Byung-Woo Hong",
      "Alex Wong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.16034"
  },
  {
    "id": "arXiv:2203.16288",
    "title": "Region of Interest focused MRI to Synthetic CT Translation using  Regression and Classification Multi-task Network",
    "abstract": "Comments: Submitted to Physics in Medicine & Biology",
    "descriptor": "\nComments: Submitted to Physics in Medicine & Biology\n",
    "authors": [
      "Sandeep Kaushik",
      "Mikael Bylund",
      "Cristina Cozzini",
      "Dattesh Shanbhag",
      "Steven F Petit",
      "Jonathan J Wyatt",
      "Marion I Menzel",
      "Carolin Pirkl",
      "Bhairav Mehta",
      "Vikas Chauhan",
      "Kesavadas Chandrasekharan",
      "Joakim Jonsson",
      "Tufve Nyholm",
      "Florian Wiesinger",
      "Bjoern Menze"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2203.16288"
  },
  {
    "id": "arXiv:2203.17139",
    "title": "Prefix Filter: Practically and Theoretically Better Than Bloom",
    "abstract": "Comments: Full version of VLDB'22 paper",
    "descriptor": "\nComments: Full version of VLDB'22 paper\n",
    "authors": [
      "Tomer Even",
      "Guy Even",
      "Adam Morrison"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2203.17139"
  },
  {
    "id": "arXiv:2204.01888",
    "title": "ConceptExplainer: Interactive Explanation for Deep Neural Networks from  a Concept Perspective",
    "abstract": "Comments: 9 pages, 6 figures",
    "descriptor": "\nComments: 9 pages, 6 figures\n",
    "authors": [
      "Jinbin Huang",
      "Aditi Mishra",
      "Bum Chul Kwon",
      "Chris Bryan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2204.01888"
  },
  {
    "id": "arXiv:2204.03200",
    "title": "The Distressing Ads That Persist: Uncovering The Harms of Targeted  Weight-Loss Ads Among Users with Histories of Disordered Eating",
    "abstract": "The Distressing Ads That Persist: Uncovering The Harms of Targeted  Weight-Loss Ads Among Users with Histories of Disordered Eating",
    "descriptor": "",
    "authors": [
      "Liza Gak",
      "Seyi Olojo",
      "Niloufar Salehi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2204.03200"
  },
  {
    "id": "arXiv:2204.03495",
    "title": "Covariance matrix preparation for quantum principal component analysis",
    "abstract": "Comments: 13 + 3 pages, 8 figures",
    "descriptor": "\nComments: 13 + 3 pages, 8 figures\n",
    "authors": [
      "Max Hunter Gordon",
      "M. Cerezo",
      "Lukasz Cincio",
      "Patrick J. Coles"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.03495"
  },
  {
    "id": "arXiv:2204.04887",
    "title": "Research on Cross-media Science and Technology Information Data  Retrieval",
    "abstract": "Comments: We found some errors in the algorithm and need to withdraw this paper",
    "descriptor": "\nComments: We found some errors in the algorithm and need to withdraw this paper\n",
    "authors": [
      "Yang Jiang",
      "Zhe Xue",
      "Ang Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.04887"
  },
  {
    "id": "arXiv:2204.06683",
    "title": "Revisiting Transformer-based Models for Long Document Classification",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Xiang Dai",
      "Ilias Chalkidis",
      "Sune Darkner",
      "Desmond Elliott"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.06683"
  },
  {
    "id": "arXiv:2204.09333",
    "title": "Profiling and Evolution of Intellectual Property",
    "abstract": "Comments: There are some problems in the conclusions and analysis of this paper, and we need to withdraw this paper",
    "descriptor": "\nComments: There are some problems in the conclusions and analysis of this paper, and we need to withdraw this paper\n",
    "authors": [
      "Bowen Yu",
      "Yingxia Shao",
      "Ang Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.09333"
  },
  {
    "id": "arXiv:2204.09616",
    "title": "Assembly Planning from Observations under Physical Constraints",
    "abstract": "Comments: IROS 2022. See the project webpage at this https URL",
    "descriptor": "\nComments: IROS 2022. See the project webpage at this https URL\n",
    "authors": [
      "Thomas Chabal",
      "Robin Strudel",
      "Etienne Arlaud",
      "Jean Ponce",
      "Cordelia Schmid"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.09616"
  },
  {
    "id": "arXiv:2205.03656",
    "title": "Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken  Language Understanding",
    "abstract": "Comments: EMNLP 2022 Long paper",
    "descriptor": "\nComments: EMNLP 2022 Long paper\n",
    "authors": [
      "Shining Liang",
      "Linjun Shou",
      "Jian Pei",
      "Ming Gong",
      "Wanli Zuo",
      "Xianglin Zuo",
      "Daxin Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.03656"
  },
  {
    "id": "arXiv:2205.03770",
    "title": "Transformer-Empowered 6G Intelligent Networks: From Massive MIMO  Processing to Semantic Communication",
    "abstract": "Transformer-Empowered 6G Intelligent Networks: From Massive MIMO  Processing to Semantic Communication",
    "descriptor": "",
    "authors": [
      "Yang Wang",
      "Zhen Gao",
      "Dezhi Zheng",
      "Sheng Chen",
      "Deniz G\u00fcnd\u00fcz",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.03770"
  },
  {
    "id": "arXiv:2205.07058",
    "title": "RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis",
    "abstract": "Comments: ECCV 2022 Workshop on Learning to Generate 3D Shapes and Scenes. Project page at this http URL",
    "descriptor": "\nComments: ECCV 2022 Workshop on Learning to Generate 3D Shapes and Scenes. Project page at this http URL\n",
    "authors": [
      "Jonathan Tremblay",
      "Moustafa Meshry",
      "Alex Evans",
      "Jan Kautz",
      "Alexander Keller",
      "Sameh Khamis",
      "Thomas M\u00fcller",
      "Charles Loop",
      "Nathan Morrical",
      "Koki Nagano",
      "Towaki Takikawa",
      "Stan Birchfield"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.07058"
  },
  {
    "id": "arXiv:2205.07257",
    "title": "Not to Overfit or Underfit the Source Domains? An Empirical Study of  Domain Generalization in Question Answering",
    "abstract": "Comments: Accepted at EMNLP 2022",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Md Arafat Sultan",
      "Avirup Sil",
      "Radu Florian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.07257"
  },
  {
    "id": "arXiv:2205.07805",
    "title": "Iterated Gauss-Seidel GMRES",
    "abstract": "Comments: Updates to multiple sections",
    "descriptor": "\nComments: Updates to multiple sections\n",
    "authors": [
      "Stephen Thomas",
      "Erin Carson",
      "Miro Rozlo\u017en\u00edk",
      "Arielle Carr",
      "Kasia \u015awirydowicz"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.07805"
  },
  {
    "id": "arXiv:2205.08532",
    "title": "New Lower Bounds for Private Estimation and a Generalized Fingerprinting  Lemma",
    "abstract": "Comments: NeurIPS 2022. Fixed a bug and improved the presentation",
    "descriptor": "\nComments: NeurIPS 2022. Fixed a bug and improved the presentation\n",
    "authors": [
      "Gautam Kamath",
      "Argyris Mouzakis",
      "Vikrant Singhal"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.08532"
  },
  {
    "id": "arXiv:2205.09548",
    "title": "ODBO: Bayesian Optimization with Search Space Prescreening for Directed  Protein Evolution",
    "abstract": "Comments: 27 pages, 13 figures",
    "descriptor": "\nComments: 27 pages, 13 figures\n",
    "authors": [
      "Lixue Cheng",
      "Ziyi Yang",
      "Changyu Hsieh",
      "Benben Liao",
      "Shengyu Zhang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.09548"
  },
  {
    "id": "arXiv:2205.10293",
    "title": "DELATOR: Money Laundering Detection via Multi-Task Learning on Large  Transaction Graphs",
    "abstract": "Comments: Accepted for publication in the 2022 IEEE International Conference on Big Data (IEEE BigData) as a short paper",
    "descriptor": "\nComments: Accepted for publication in the 2022 IEEE International Conference on Big Data (IEEE BigData) as a short paper\n",
    "authors": [
      "Henrique S. Assump\u00e7\u00e3o",
      "Fabr\u00edcio Souza",
      "Leandro Lacerda Campos",
      "Vin\u00edcius T. de Castro Pires",
      "Paulo M. Laurentys de Almeida",
      "Fabricio Murai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2205.10293"
  },
  {
    "id": "arXiv:2205.11482",
    "title": "Towards Tracing Factual Knowledge in Language Models Back to the  Training Data",
    "abstract": "Comments: Findings of EMNLP, 2022",
    "descriptor": "\nComments: Findings of EMNLP, 2022\n",
    "authors": [
      "Ekin Aky\u00fcrek",
      "Tolga Bolukbasi",
      "Frederick Liu",
      "Binbin Xiong",
      "Ian Tenney",
      "Jacob Andreas",
      "Kelvin Guu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.11482"
  },
  {
    "id": "arXiv:2205.11713",
    "title": "Thalamus: a brain-inspired algorithm for biologically-plausible  continual learning and disentangled representations",
    "abstract": "Comments: Submitted to ICLR 2023",
    "descriptor": "\nComments: Submitted to ICLR 2023\n",
    "authors": [
      "Ali Hummos"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.11713"
  },
  {
    "id": "arXiv:2205.11822",
    "title": "Maieutic Prompting: Logically Consistent Reasoning with Recursive  Explanations",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Jaehun Jung",
      "Lianhui Qin",
      "Sean Welleck",
      "Faeze Brahman",
      "Chandra Bhagavatula",
      "Ronan Le Bras",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11822"
  },
  {
    "id": "arXiv:2205.12148",
    "title": "Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer",
    "abstract": "Comments: Accepted at EMNLP 2022 (Main Conference)",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (Main Conference)\n",
    "authors": [
      "Ahmet \u00dcst\u00fcn",
      "Arianna Bisazza",
      "Gosse Bouma",
      "Gertjan van Noord",
      "Sebastian Ruder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12148"
  },
  {
    "id": "arXiv:2205.12253",
    "title": "Evaluating the Impact of Model Scale for Compositional Generalization in  Semantic Parsing",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Linlu Qiu",
      "Peter Shaw",
      "Panupong Pasupat",
      "Tianze Shi",
      "Jonathan Herzig",
      "Emily Pitler",
      "Fei Sha",
      "Kristina Toutanova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12253"
  },
  {
    "id": "arXiv:2205.12382",
    "title": "Challenges and Opportunities in Information Manipulation Detection: An  Examination of Wartime Russian Media",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Chan Young Park",
      "Julia Mendelsohn",
      "Anjalie Field",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12382"
  },
  {
    "id": "arXiv:2205.12528",
    "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly  Supervised Text Classification",
    "abstract": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly  Supervised Text Classification",
    "descriptor": "",
    "authors": [
      "Dheeraj Mekala",
      "Chengyu Dong",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12528"
  },
  {
    "id": "arXiv:2205.12604",
    "title": "Leveraging QA Datasets to Improve Generative Data Augmentation",
    "abstract": "Leveraging QA Datasets to Improve Generative Data Augmentation",
    "descriptor": "",
    "authors": [
      "Dheeraj Mekala",
      "Tu Vu",
      "Timo Schick",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12604"
  },
  {
    "id": "arXiv:2205.12674",
    "title": "Training Language Models with Memory Augmentation",
    "abstract": "Comments: EMNLP 2022. Our code and models are available at this https URL v2 added updated results on machine translation, enwik8 and domain adaptation",
    "descriptor": "\nComments: EMNLP 2022. Our code and models are available at this https URL v2 added updated results on machine translation, enwik8 and domain adaptation\n",
    "authors": [
      "Zexuan Zhong",
      "Tao Lei",
      "Danqi Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12674"
  },
  {
    "id": "arXiv:2205.12688",
    "title": "ProsocialDialog: A Prosocial Backbone for Conversational Agents",
    "abstract": "Comments: EMNLP 2022 camera ready; Dataset and model can be found at this https URL",
    "descriptor": "\nComments: EMNLP 2022 camera ready; Dataset and model can be found at this https URL\n",
    "authors": [
      "Hyunwoo Kim",
      "Youngjae Yu",
      "Liwei Jiang",
      "Ximing Lu",
      "Daniel Khashabi",
      "Gunhee Kim",
      "Yejin Choi",
      "Maarten Sap"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12688"
  },
  {
    "id": "arXiv:2205.12696",
    "title": "Revisiting DocRED -- Addressing the False Negative Problem in Relation  Extraction",
    "abstract": "Comments: Accepted by EMNLP 2022",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Qingyu Tan",
      "Lu Xu",
      "Lidong Bing",
      "Hwee Tou Ng",
      "Sharifah Mahani Aljunied"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.12696"
  },
  {
    "id": "arXiv:2205.12840",
    "title": "SALAD: Source-free Active Label-Agnostic Domain Adaptation for  Classification, Segmentation and Detection",
    "abstract": "SALAD: Source-free Active Label-Agnostic Domain Adaptation for  Classification, Segmentation and Detection",
    "descriptor": "",
    "authors": [
      "Divya Kothandaraman",
      "Sumit Shekhar",
      "Abhilasha Sancheti",
      "Manoj Ghuhan",
      "Tripti Shukla",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12840"
  },
  {
    "id": "arXiv:2205.14794",
    "title": "Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing  Mechanisms in Sequence Learning",
    "abstract": "Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing  Mechanisms in Sequence Learning",
    "descriptor": "",
    "authors": [
      "Aniket Didolkar",
      "Kshitij Gupta",
      "Anirudh Goyal",
      "Nitesh B. Gundavarapu",
      "Alex Lamb",
      "Nan Rosemary Ke",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14794"
  },
  {
    "id": "arXiv:2205.14926",
    "title": "CalFAT: Calibrated Federated Adversarial Training with Label Skewness",
    "abstract": "Comments: Accepted to the Conference on the Advances in Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: Accepted to the Conference on the Advances in Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Chen Chen",
      "Yuchen Liu",
      "Xingjun Ma",
      "Lingjuan Lyu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.14926"
  },
  {
    "id": "arXiv:2205.15124",
    "title": "Mixed-Effect Thompson Sampling",
    "abstract": "Mixed-Effect Thompson Sampling",
    "descriptor": "",
    "authors": [
      "Imad Aouali",
      "Branislav Kveton",
      "Sumeet Katariya"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15124"
  },
  {
    "id": "arXiv:2205.15884",
    "title": "An Effective and Efficient Evolutionary Algorithm for Many-Objective  Optimization",
    "abstract": "Comments: 25 pages, 5 figures, to appear in Information Sciences",
    "descriptor": "\nComments: 25 pages, 5 figures, to appear in Information Sciences\n",
    "authors": [
      "Yani Xue",
      "Miqing Li",
      "Xiaohui Liu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.15884"
  },
  {
    "id": "arXiv:2206.00133",
    "title": "Pre-training via Denoising for Molecular Property Prediction",
    "abstract": "Pre-training via Denoising for Molecular Property Prediction",
    "descriptor": "",
    "authors": [
      "Sheheryar Zaidi",
      "Michael Schaarschmidt",
      "James Martens",
      "Hyunjik Kim",
      "Yee Whye Teh",
      "Alvaro Sanchez-Gonzalez",
      "Peter Battaglia",
      "Razvan Pascanu",
      "Jonathan Godwin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00133"
  },
  {
    "id": "arXiv:2206.01552",
    "title": "Is an encoder within reach?",
    "abstract": "Comments: 11 pages, 10 figures",
    "descriptor": "\nComments: 11 pages, 10 figures\n",
    "authors": [
      "Helene Hauschultz",
      "Rasmus Berg Palm. Pablo Moreno-Mu\u00f1os",
      "Nicki Skafte Detlefsen",
      "Andrew Allan du Plessis",
      "S\u00f8ren Hauberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01552"
  },
  {
    "id": "arXiv:2206.03277",
    "title": "Driving and charging an EV in Australia: A real-world analysis",
    "abstract": "Comments: This work has been published in Australasian Transport Research Forum (ATRF), proceedings (2022)",
    "descriptor": "\nComments: This work has been published in Australasian Transport Research Forum (ATRF), proceedings (2022)\n",
    "authors": [
      "Thara Philip",
      "Kai Li Lim",
      "Jake Whitehead"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.03277"
  },
  {
    "id": "arXiv:2206.03687",
    "title": "A Unified Model for Multi-class Anomaly Detection",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Zhiyuan You",
      "Lei Cui",
      "Yujun Shen",
      "Kai Yang",
      "Xin Lu",
      "Yu Zheng",
      "Xinyi Le"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.03687"
  },
  {
    "id": "arXiv:2206.07085",
    "title": "Understanding the Generalization Benefit of Normalization Layers:  Sharpness Reduction",
    "abstract": "Comments: 76 pages, many figures; NeurIPS 2022 camera-ready version",
    "descriptor": "\nComments: 76 pages, many figures; NeurIPS 2022 camera-ready version\n",
    "authors": [
      "Kaifeng Lyu",
      "Zhiyuan Li",
      "Sanjeev Arora"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07085"
  },
  {
    "id": "arXiv:2206.07571",
    "title": "Efficient decoding up to a constant fraction of the code length for  asymptotically good quantum codes",
    "abstract": "Comments: 43 pages",
    "descriptor": "\nComments: 43 pages\n",
    "authors": [
      "Anthony Leverrier",
      "Gilles Z\u00e9mor"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.07571"
  },
  {
    "id": "arXiv:2206.07758",
    "title": "Reconstructing Training Data from Trained Neural Networks",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Niv Haim",
      "Gal Vardi",
      "Gilad Yehudai",
      "Ohad Shamir",
      "Michal Irani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.07758"
  },
  {
    "id": "arXiv:2206.09065",
    "title": "Free-form Lesion Synthesis Using a Partial Convolution Generative  Adversarial Network for Enhanced Deep Learning Liver Tumor Segmentation",
    "abstract": "Comments: The paper is under review by JACMP-Journal of Applied Medical Physics",
    "descriptor": "\nComments: The paper is under review by JACMP-Journal of Applied Medical Physics\n",
    "authors": [
      "Yingao Liu",
      "Fei Yang",
      "Yidong Yang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.09065"
  },
  {
    "id": "arXiv:2206.09759",
    "title": "An Input-Queueing TSN Switching Architecture to Achieve Zero Packet Loss  for Timely Traffic",
    "abstract": "An Input-Queueing TSN Switching Architecture to Achieve Zero Packet Loss  for Timely Traffic",
    "descriptor": "",
    "authors": [
      "Ming Li",
      "Lei Deng",
      "Yunghsiang S. Han"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.09759"
  },
  {
    "id": "arXiv:2206.11821",
    "title": "A Survey of DeFi Security: Challenges and Opportunities",
    "abstract": "A Survey of DeFi Security: Challenges and Opportunities",
    "descriptor": "",
    "authors": [
      "Wenkai Li",
      "Jiuyang Bu",
      "Xiaoqi Li",
      "Hongli Peng",
      "Yuanzheng Niu",
      "Yuqing Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.11821"
  },
  {
    "id": "arXiv:2206.12038",
    "title": "BYOL-S: Learning Self-supervised Speech Representations by Bootstrapping",
    "abstract": "Comments: Submitted to HEAR-PMLR 2021",
    "descriptor": "\nComments: Submitted to HEAR-PMLR 2021\n",
    "authors": [
      "Gasser Elbanna",
      "Neil Scheidwasser-Clow",
      "Mikolaj Kegler",
      "Pierre Beckmann",
      "Karl El Hajal",
      "Milos Cernak"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.12038"
  },
  {
    "id": "arXiv:2206.12412",
    "title": "Dynamic Propagation of Mode III Cracks in a Lattice Boltzmann Method for  Solids",
    "abstract": "Comments: accepted for publication in Archive of Applied Mechanics",
    "descriptor": "\nComments: accepted for publication in Archive of Applied Mechanics\n",
    "authors": [
      "Henning M\u00fcller",
      "Ali Touil",
      "Alexander Schl\u00fcter",
      "Ralf M\u00fcller"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.12412"
  },
  {
    "id": "arXiv:2206.13909",
    "title": "QTI Submission to DCASE 2021: residual normalization for  device-imbalanced acoustic scene classification with efficient design",
    "abstract": "Comments: tech report; won 1st place in DCASE2021 challenge. arXiv admin note: substantial text overlap with arXiv:2111.06531",
    "descriptor": "\nComments: tech report; won 1st place in DCASE2021 challenge. arXiv admin note: substantial text overlap with arXiv:2111.06531\n",
    "authors": [
      "Byeonggeun Kim",
      "Seunghan Yang",
      "Jangho Kim",
      "Simyung Chang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.13909"
  },
  {
    "id": "arXiv:2206.14348",
    "title": "What Can Secondary Predictions Tell Us? An Exploration on  Question-Answering with SQuAD-v2.0",
    "abstract": "Comments: 18 pages, 2 appendices additional 5 pages, 16 figures",
    "descriptor": "\nComments: 18 pages, 2 appendices additional 5 pages, 16 figures\n",
    "authors": [
      "Michael Kamfonas",
      "Gabriel Alon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.14348"
  },
  {
    "id": "arXiv:2206.14486",
    "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
    "abstract": "Comments: Oral @ NeurIPS 2022 (camera ready version)",
    "descriptor": "\nComments: Oral @ NeurIPS 2022 (camera ready version)\n",
    "authors": [
      "Ben Sorscher",
      "Robert Geirhos",
      "Shashank Shekhar",
      "Surya Ganguli",
      "Ari S. Morcos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.14486"
  },
  {
    "id": "arXiv:2206.14774",
    "title": "TweetNLP: Cutting-Edge Natural Language Processing for Social Media",
    "abstract": "Comments: EMNLP 2022 Demo paper. TweetNLP: this https URL",
    "descriptor": "\nComments: EMNLP 2022 Demo paper. TweetNLP: this https URL\n",
    "authors": [
      "Jose Camacho-Collados",
      "Kiamehr Rezaee",
      "Talayeh Riahi",
      "Asahi Ushio",
      "Daniel Loureiro",
      "Dimosthenis Antypas",
      "Joanne Boisson",
      "Luis Espinosa-Anke",
      "Fangyu Liu",
      "Eugenio Mart\u00ednez-C\u00e1mara",
      "Gonzalo Medina",
      "Thomas Buhrmann",
      "Leonardo Neves",
      "Francesco Barbieri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2206.14774"
  },
  {
    "id": "arXiv:2207.00377",
    "title": "Anisotropic, Sparse and Interpretable Physics-Informed Neural Networks  for PDEs",
    "abstract": "Comments: 10 pages, 17 figures",
    "descriptor": "\nComments: 10 pages, 17 figures\n",
    "authors": [
      "Amuthan A. Ramabathiran",
      "Prabhu Ramachandran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.00377"
  },
  {
    "id": "arXiv:2207.00710",
    "title": "Separating and Collapsing Electoral Control Types",
    "abstract": "Comments: The arXiv.org metadata abstract is an abridged version; please see the paper for the full abstract",
    "descriptor": "\nComments: The arXiv.org metadata abstract is an abridged version; please see the paper for the full abstract\n",
    "authors": [
      "Benjamin Carleton",
      "Michael C. Chavrimootoo",
      "Lane A. Hemaspaandra",
      "David E. Narv\u00e1ez",
      "Conor Taliancich",
      "Henry B. Welles"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2207.00710"
  },
  {
    "id": "arXiv:2207.01590",
    "title": "The Dichotomy of Cloud and IoT: Cloud-Assisted IoT From a Security  Perspective",
    "abstract": "Comments: The authors metadata field and some parts of paper during replacement has been updated in the new version",
    "descriptor": "\nComments: The authors metadata field and some parts of paper during replacement has been updated in the new version\n",
    "authors": [
      "Behrouz Zolfaghari",
      "Abbas Yazdinejad",
      "Ali Dehghantanha",
      "Jacob Krzciok",
      "Khodakhast Bibak"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2207.01590"
  },
  {
    "id": "arXiv:2207.03049",
    "title": "Search versus Search for Collapsing Electoral Control Types",
    "abstract": "Comments: The metadata's abstract is abridged due to arXiv.org's abstract-length limit. The paper itself has the unabridged (i.e., full) abstract",
    "descriptor": "\nComments: The metadata's abstract is abridged due to arXiv.org's abstract-length limit. The paper itself has the unabridged (i.e., full) abstract\n",
    "authors": [
      "Benjamin Carleton",
      "Michael C. Chavrimootoo",
      "Lane A. Hemaspaandra",
      "David E. Narv\u00e1ez",
      "Conor Taliancich",
      "Henry B. Welles"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2207.03049"
  },
  {
    "id": "arXiv:2207.04248",
    "title": "A Statistically-Based Approach to Feedforward Neural Network Model  Selection",
    "abstract": "A Statistically-Based Approach to Feedforward Neural Network Model  Selection",
    "descriptor": "",
    "authors": [
      "Andrew McInerney",
      "Kevin Burke"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.04248"
  },
  {
    "id": "arXiv:2207.06030",
    "title": "Cost-Effective Online Contextual Model Selection",
    "abstract": "Cost-Effective Online Contextual Model Selection",
    "descriptor": "",
    "authors": [
      "Xuefeng Liu",
      "Fangfang Xia",
      "Rick L. Stevens",
      "Yuxin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.06030"
  },
  {
    "id": "arXiv:2207.07061",
    "title": "Confident Adaptive Language Modeling",
    "abstract": "Comments: NeurIPS 2022 (selected as Oral)",
    "descriptor": "\nComments: NeurIPS 2022 (selected as Oral)\n",
    "authors": [
      "Tal Schuster",
      "Adam Fisch",
      "Jai Gupta",
      "Mostafa Dehghani",
      "Dara Bahri",
      "Vinh Q. Tran",
      "Yi Tay",
      "Donald Metzler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.07061"
  },
  {
    "id": "arXiv:2207.08080",
    "title": "Neural Color Operators for Sequential Image Retouching",
    "abstract": "Comments: Accepted to ECCV 2022. Code is available at this https URL",
    "descriptor": "\nComments: Accepted to ECCV 2022. Code is available at this https URL\n",
    "authors": [
      "Yili Wang",
      "Xin Li",
      "Kun Xu",
      "Dongliang He",
      "Qi Zhang",
      "Fu Li",
      "Errui Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.08080"
  },
  {
    "id": "arXiv:2207.08348",
    "title": "Fast Swimming Robots Based on Elastic Instability",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2206.14867",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2206.14867\n",
    "authors": [
      "Zechen Xiong",
      "Liqi Chen",
      "Wenxiong Hao",
      "Yufeng Su",
      "Hod Lipson"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2207.08348"
  },
  {
    "id": "arXiv:2207.14219",
    "title": "A general framework for multi-step ahead adaptive conformal  heteroscedastic time series forecasting",
    "abstract": "Comments: 13 pages, 8 figures",
    "descriptor": "\nComments: 13 pages, 8 figures\n",
    "authors": [
      "Martim Sousa",
      "Ana Maria Tom\u00e9",
      "Jos\u00e9 Moreira"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.14219"
  },
  {
    "id": "arXiv:2208.00755",
    "title": "Off-Policy Correction for Actor-Critic Methods without Importance  Sampling",
    "abstract": "Off-Policy Correction for Actor-Critic Methods without Importance  Sampling",
    "descriptor": "",
    "authors": [
      "Baturay Saglam",
      "Dogan C. Cicek",
      "Furkan B. Mutlu",
      "Suleyman S. Kozat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.00755"
  },
  {
    "id": "arXiv:2208.02369",
    "title": "Deep VULMAN: A Deep Reinforcement Learning-Enabled Cyber Vulnerability  Management Framework",
    "abstract": "Comments: 12 pages, 3 figures",
    "descriptor": "\nComments: 12 pages, 3 figures\n",
    "authors": [
      "Soumyadeep Hore",
      "Ankit Shah",
      "Nathaniel D. Bastian"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2208.02369"
  },
  {
    "id": "arXiv:2208.02496",
    "title": "Modelling the Rise and Fall of Two-Sided Mobility Markets with  Microsimulation",
    "abstract": "Modelling the Rise and Fall of Two-Sided Mobility Markets with  Microsimulation",
    "descriptor": "",
    "authors": [
      "Farnoud Ghasemi",
      "Rafa\u0142 Kucharski"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2208.02496"
  },
  {
    "id": "arXiv:2208.04171",
    "title": "Object Detection Using Sim2Real Domain Randomization for Robotic  Applications",
    "abstract": "Comments: Published in IEEE Transactions on Robotics (T-RO)",
    "descriptor": "\nComments: Published in IEEE Transactions on Robotics (T-RO)\n",
    "authors": [
      "D\u00e1niel Horv\u00e1th",
      "G\u00e1bor Erd\u0151s",
      "Zolt\u00e1n Istenes",
      "Tom\u00e1\u0161 Horv\u00e1th",
      "S\u00e1ndor F\u00f6ldi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.04171"
  },
  {
    "id": "arXiv:2208.04506",
    "title": "Second Order Ensemble Langevin Method for Sampling and Inverse Problems",
    "abstract": "Second Order Ensemble Langevin Method for Sampling and Inverse Problems",
    "descriptor": "",
    "authors": [
      "Ziming Liu",
      "Andrew M. Stuart",
      "Yixuan Wang"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2208.04506"
  },
  {
    "id": "arXiv:2208.05140",
    "title": "Self-supervised Co-learning of Uncurated Images and Reports Enables  Oversight AI in Radiology",
    "abstract": "Self-supervised Co-learning of Uncurated Images and Reports Enables  Oversight AI in Radiology",
    "descriptor": "",
    "authors": [
      "Sangjoon Park",
      "Eun Sun Lee",
      "Kyung Sook Shin",
      "Jeong Eun Lee",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.05140"
  },
  {
    "id": "arXiv:2208.07559",
    "title": "A Graph-Based Modelling of Epidemics: Properties, Simulation, and  Continuum Limit",
    "abstract": "A Graph-Based Modelling of Epidemics: Properties, Simulation, and  Continuum Limit",
    "descriptor": "",
    "authors": [
      "Giovanni Naldi",
      "Giuseppe Patane'"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2208.07559"
  },
  {
    "id": "arXiv:2208.08220",
    "title": "Towards an Error-free Deep Occupancy Detector for Smart Camera Parking  System",
    "abstract": "Comments: Paper got accepted to Oral ECCV workshop (CVCIE)",
    "descriptor": "\nComments: Paper got accepted to Oral ECCV workshop (CVCIE)\n",
    "authors": [
      "Tung-Lam Duong",
      "Van-Duc Le",
      "Tien-Cuong Bui",
      "Hai-Thien To"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.08220"
  },
  {
    "id": "arXiv:2208.08401",
    "title": "Conformal Inference for Online Prediction with Arbitrary Distribution  Shifts",
    "abstract": "Comments: 29 pages, 10 figures",
    "descriptor": "\nComments: 29 pages, 10 figures\n",
    "authors": [
      "Isaac Gibbs",
      "Emmanuel Cand\u00e8s"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.08401"
  },
  {
    "id": "arXiv:2208.08547",
    "title": "Better Than Worst-Case Decoding for Quantum Error Correction",
    "abstract": "Comments: To appear at the 28th Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2023)",
    "descriptor": "\nComments: To appear at the 28th Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2023)\n",
    "authors": [
      "Gokul Subramanian Ravi",
      "Jonathan M. Baker",
      "Arash Fayyazi",
      "Sophia Fuhui Lin",
      "Ali Javadi-Abhari",
      "Massoud Pedram",
      "Frederic T. Chong"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2208.08547"
  },
  {
    "id": "arXiv:2208.11407",
    "title": "A Multi-Bennett 8R Mechanism Obtained From Factorization of Bivariate  Motion Polynomials",
    "abstract": "A Multi-Bennett 8R Mechanism Obtained From Factorization of Bivariate  Motion Polynomials",
    "descriptor": "",
    "authors": [
      "Johanna Frischauf",
      "Martin Pfurner",
      "Daniel F. Scharler",
      "Hans-Peter Schr\u00f6cker"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Rings and Algebras (math.RA)"
    ],
    "url": "https://arxiv.org/abs/2208.11407"
  },
  {
    "id": "arXiv:2208.12812",
    "title": "Speech Emotion Recognition using Supervised Deep Recurrent System for  Mental Health Monitoring",
    "abstract": "Comments: 6 pages, 5 figures, 3 tables, accepted in the IEEE WFIoT2022",
    "descriptor": "\nComments: 6 pages, 5 figures, 3 tables, accepted in the IEEE WFIoT2022\n",
    "authors": [
      "Nelly Elsayed",
      "Zag ElSayed",
      "Navid Asadizanjani",
      "Murat Ozer",
      "Ahmed Abdelgawad",
      "Magdy Bayoumi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2208.12812"
  },
  {
    "id": "arXiv:2208.13092",
    "title": "Lottery Aware Sparsity Hunting: Enabling Federated Learning on  Resource-Limited Edge",
    "abstract": "Comments: 18 pages, 13 figures, 7 tables",
    "descriptor": "\nComments: 18 pages, 13 figures, 7 tables\n",
    "authors": [
      "Sara Babakniya",
      "Souvik Kundu",
      "Saurav Prakash",
      "Yue Niu",
      "Salman Avestimehr"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.13092"
  },
  {
    "id": "arXiv:2208.13471",
    "title": "How to Extend the Abstraction Refinement Model for Systems with Emergent  Behavior ?",
    "abstract": "How to Extend the Abstraction Refinement Model for Systems with Emergent  Behavior ?",
    "descriptor": "",
    "authors": [
      "Mohamed Toufik Ailane",
      "Christoph knieke",
      "Andreas Rausch"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2208.13471"
  },
  {
    "id": "arXiv:2209.01403",
    "title": "Explainability via Short Formulas: the Case of Propositional Logic with  Implementation",
    "abstract": "Comments: 16 pages, 1 figure. A variant of a RCRA 2022 paper. Changes to version one: typos fixed in Section 3.1",
    "descriptor": "\nComments: 16 pages, 1 figure. A variant of a RCRA 2022 paper. Changes to version one: typos fixed in Section 3.1\n",
    "authors": [
      "Reijo Jaakkola",
      "Tomi Janhunen",
      "Antti Kuusisto",
      "Masood Feyzbakhsh Rankooh",
      "Miikka Vilander"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2209.01403"
  },
  {
    "id": "arXiv:2209.02250",
    "title": "Spatio-Temporal Action Detection Under Large Motion",
    "abstract": "Comments: 10 pages, 5 figures, 5 tables",
    "descriptor": "\nComments: 10 pages, 5 figures, 5 tables\n",
    "authors": [
      "Gurkirt Singh",
      "Vasileios Choutas",
      "Suman Saha",
      "Fisher Yu",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.02250"
  },
  {
    "id": "arXiv:2209.04280",
    "title": "F-coref: Fast, Accurate and Easy to Use Coreference Resolution",
    "abstract": "Comments: AACL 2022",
    "descriptor": "\nComments: AACL 2022\n",
    "authors": [
      "Shon Otmazgin",
      "Arie Cattan",
      "Yoav Goldberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.04280"
  },
  {
    "id": "arXiv:2209.04484",
    "title": "Logic and Reduction Operation based Hardware Trojans in Digital Design",
    "abstract": "Comments: 2 pages, 2 figures, accepted in ISOCC 2022",
    "descriptor": "\nComments: 2 pages, 2 figures, accepted in ISOCC 2022\n",
    "authors": [
      "Mayukhmali Das",
      "Sounak Dutta",
      "Sayan Chatterjee"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2209.04484"
  },
  {
    "id": "arXiv:2209.05598",
    "title": "Learning Causal Discovery",
    "abstract": "Comments: 15 main pages, 9 figures. Will be submitted to TMLR",
    "descriptor": "\nComments: 15 main pages, 9 figures. Will be submitted to TMLR\n",
    "authors": [
      "Xinyue Wang",
      "Konrad Paul Kording"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2209.05598"
  },
  {
    "id": "arXiv:2209.06330",
    "title": "An Improved Lower Bound for Maximin Share Allocations of Goods",
    "abstract": "Comments: It has come to the attention of the author that the same result has already been published. Please refer to Feige, Uriel, Ariel Sapir, and Laliv Tauber. \"A tight negative example for MMS fair allocations.\" International Conference on Web and Internet Economics. Springer, Cham, 2021",
    "descriptor": "\nComments: It has come to the attention of the author that the same result has already been published. Please refer to Feige, Uriel, Ariel Sapir, and Laliv Tauber. \"A tight negative example for MMS fair allocations.\" International Conference on Web and Internet Economics. Springer, Cham, 2021\n",
    "authors": [
      "Kevin Hsu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2209.06330"
  },
  {
    "id": "arXiv:2209.07259",
    "title": "Design of a Strong-Arm Dynamic-Latch based comparator with high speed,  low power and low offset for SAR-ADC",
    "abstract": "Comments: 5 pages, 3 figures",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Sounak Dutta"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.07259"
  },
  {
    "id": "arXiv:2209.07417",
    "title": "Examining Large Pre-Trained Language Models for Machine Translation:  What You Don't Know About It",
    "abstract": "Comments: System paper Accepted to WMT2022: BiomedicalMT Track (ClinSpEn2022)",
    "descriptor": "\nComments: System paper Accepted to WMT2022: BiomedicalMT Track (ClinSpEn2022)\n",
    "authors": [
      "Lifeng Han",
      "Gleb Erofeev",
      "Irina Sorokina",
      "Serge Gladkoff",
      "Goran Nenadic"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.07417"
  },
  {
    "id": "arXiv:2209.11451",
    "title": "FIAT: Fine-grained Information Audit for Trustless Transborder Data Flow",
    "abstract": "Comments: 10 pages, 6 figures, 1 table",
    "descriptor": "\nComments: 10 pages, 6 figures, 1 table\n",
    "authors": [
      "Shuhao Zheng",
      "Yanxi Lin",
      "Yang Yu",
      "Ye Yuan",
      "Yongzheng Jia",
      "Xue Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.11451"
  },
  {
    "id": "arXiv:2209.11820",
    "title": "Expanding the Deployment Envelope of Behavior Prediction via Adaptive  Meta-Learning",
    "abstract": "Comments: 12 pages, 13 figures, 2 tables. Fixed links and references to the appendix",
    "descriptor": "\nComments: 12 pages, 13 figures, 2 tables. Fixed links and references to the appendix\n",
    "authors": [
      "Boris Ivanovic",
      "James Harrison",
      "Marco Pavone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.11820"
  },
  {
    "id": "arXiv:2209.13020",
    "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial  Intelligence with Humans",
    "abstract": "Comments: Forthcoming in Northwestern Journal of Technology and Intellectual Property, Volume 20",
    "descriptor": "\nComments: Forthcoming in Northwestern Journal of Technology and Intellectual Property, Volume 20\n",
    "authors": [
      "John J. Nay"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13020"
  },
  {
    "id": "arXiv:2210.00597",
    "title": "Composition of Differential Privacy & Privacy Amplification by  Subsampling",
    "abstract": "Composition of Differential Privacy & Privacy Amplification by  Subsampling",
    "descriptor": "",
    "authors": [
      "Thomas Steinke"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.00597"
  },
  {
    "id": "arXiv:2210.00705",
    "title": "SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language  Model",
    "abstract": "Comments: Accepted to IEEE SLT 2022",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Yi-Jen Shih",
      "Hsuan-Fu Wang",
      "Heng-Jui Chang",
      "Layne Berry",
      "Hung-yi Lee",
      "David Harwath"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.00705"
  },
  {
    "id": "arXiv:2210.01461",
    "title": "In the realm of hybrid Brain: Human Brain and AI",
    "abstract": "Comments: 41 Pages, 10 Figures,",
    "descriptor": "\nComments: 41 Pages, 10 Figures,\n",
    "authors": [
      "Hoda Fares",
      "Margherita Ronchini",
      "Milad Zamani",
      "Hooman Farkhani",
      "Farshad Moradi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.01461"
  },
  {
    "id": "arXiv:2210.01585",
    "title": "How Image Generation Helps Visible-to-Infrared Person Re-Identification?",
    "abstract": "Comments: Submitted to IEEE Transactions on Image Processing",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Image Processing\n",
    "authors": [
      "Honghu Pan",
      "Yongyong Chen",
      "Yunqi He",
      "Xin Li",
      "Zhenyu He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.01585"
  },
  {
    "id": "arXiv:2210.01787",
    "title": "Rethinking Lipschitz Neural Networks and Certified Robustness: A Boolean  Function Perspective",
    "abstract": "Comments: 37 pages; to appear in NeurIPS 2022 (Oral)",
    "descriptor": "\nComments: 37 pages; to appear in NeurIPS 2022 (Oral)\n",
    "authors": [
      "Bohang Zhang",
      "Du Jiang",
      "Di He",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01787"
  },
  {
    "id": "arXiv:2210.02377",
    "title": "Goal Recognition as a Deep Learning Task: the GRNet Approach",
    "abstract": "Goal Recognition as a Deep Learning Task: the GRNet Approach",
    "descriptor": "",
    "authors": [
      "Mattia Chiari",
      "Alfonso E. Gerevini",
      "Luca Putelli",
      "Francesco Percassi",
      "Ivan Serina"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02377"
  },
  {
    "id": "arXiv:2210.02835",
    "title": "Sequentially Swapping Tokens: Further on Graph Classes",
    "abstract": "Comments: 28 pages, 15 figures, SOFSEM 2023",
    "descriptor": "\nComments: 28 pages, 15 figures, SOFSEM 2023\n",
    "authors": [
      "Hironori Kiya",
      "Yuto Okada",
      "Hirotaka Ono",
      "Yota Otachi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.02835"
  },
  {
    "id": "arXiv:2210.02980",
    "title": "Deep Learning of Near Field Beam Focusing in Terahertz Wideband Massive  MIMO Systems",
    "abstract": "Comments: The code files will be available on the DeepMIMO website this https URL",
    "descriptor": "\nComments: The code files will be available on the DeepMIMO website this https URL\n",
    "authors": [
      "Yu Zhang",
      "Ahmed Alkhateeb"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.02980"
  },
  {
    "id": "arXiv:2210.04123",
    "title": "DIMES: A Differentiable Meta Solver for Combinatorial Optimization  Problems",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Ruizhong Qiu",
      "Zhiqing Sun",
      "Yiming Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.04123"
  },
  {
    "id": "arXiv:2210.04185",
    "title": "Controllable Dialogue Simulation with In-Context Learning",
    "abstract": "Comments: EMNLP 2022 Findings, code and data are available at this https URL",
    "descriptor": "\nComments: EMNLP 2022 Findings, code and data are available at this https URL\n",
    "authors": [
      "Zekun Li",
      "Wenhu Chen",
      "Shiyang Li",
      "Hong Wang",
      "Jing Qian",
      "Xifeng Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04185"
  },
  {
    "id": "arXiv:2210.04709",
    "title": "Error analysis of a backward Euler positive preserving stabilized scheme  for a Chemotaxis system",
    "abstract": "Error analysis of a backward Euler positive preserving stabilized scheme  for a Chemotaxis system",
    "descriptor": "",
    "authors": [
      "Panagiotis Chatzipantelidis",
      "Christos Pervolianakis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.04709"
  },
  {
    "id": "arXiv:2210.05189",
    "title": "Neural Networks are Decision Trees",
    "abstract": "Neural Networks are Decision Trees",
    "descriptor": "",
    "authors": [
      "Caglar Aytekin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05189"
  },
  {
    "id": "arXiv:2210.05857",
    "title": "FlowDrone: Wind Estimation and Gust Rejection on UAVs Using  Fast-Response Hot-Wire Flow Sensors",
    "abstract": "Comments: Submitted to ICRA 2023. See supplementary video at this https URL",
    "descriptor": "\nComments: Submitted to ICRA 2023. See supplementary video at this https URL\n",
    "authors": [
      "Nathaniel Simon",
      "Allen Z. Ren",
      "Alexander Piqu\u00e9",
      "David Snyder",
      "Daphne Barretto",
      "Marcus Hultmark",
      "Anirudha Majumdar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05857"
  },
  {
    "id": "arXiv:2210.06175",
    "title": "Exploring Efficient-tuning Methods in Self-supervised Speech Models",
    "abstract": "Comments: SLT 2022",
    "descriptor": "\nComments: SLT 2022\n",
    "authors": [
      "Zih-Ching Chen",
      "Chin-Lun Fu",
      "Chih-Ying Liu",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.06175"
  },
  {
    "id": "arXiv:2210.07189",
    "title": "On Compressing Sequences for Self-Supervised Speech Models",
    "abstract": "Comments: Accepted to IEEE SLT 2022",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Yen Meng",
      "Hsuan-Jui Chen",
      "Jiatong Shi",
      "Shinji Watanabe",
      "Paola Garcia",
      "Hung-yi Lee",
      "Hao Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.07189"
  },
  {
    "id": "arXiv:2210.07681",
    "title": "Quo Vadis: Is Trajectory Forecasting the Key Towards Long-Term  Multi-Object Tracking?",
    "abstract": "Comments: Accepted at NeurIPS 2022; fixed small typo",
    "descriptor": "\nComments: Accepted at NeurIPS 2022; fixed small typo\n",
    "authors": [
      "Patrick Dendorfer",
      "Vladimir Yugay",
      "Aljo\u0161a O\u0161ep",
      "Laura Leal-Taix\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07681"
  },
  {
    "id": "arXiv:2210.07773",
    "title": "Diversified Recommendations for Agents with Adaptive Preferences",
    "abstract": "Comments: 34 pages, forthcoming in NeurIPS 2022",
    "descriptor": "\nComments: 34 pages, forthcoming in NeurIPS 2022\n",
    "authors": [
      "Arpit Agarwal",
      "William Brown"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07773"
  },
  {
    "id": "arXiv:2210.07800",
    "title": "Multiple Choice Hard Thresholding Pursuit (MCHTP) for Simultaneous  Sparse Recovery and Sparsity Order Estimation",
    "abstract": "Comments: 9 pages, 4 figures, tech-report",
    "descriptor": "\nComments: 9 pages, 4 figures, tech-report\n",
    "authors": [
      "Samrat Mukhopadhyay",
      "Himanshu Bhusan Mishra"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.07800"
  },
  {
    "id": "arXiv:2210.08128",
    "title": "On the Computation of Distributed Knowledge as the Greatest Lower Bound  of Knowledge",
    "abstract": "On the Computation of Distributed Knowledge as the Greatest Lower Bound  of Knowledge",
    "descriptor": "",
    "authors": [
      "Santiago Quintero",
      "Carlos Pinz\u00f3n",
      "Sergio Ram\u00edrez",
      "Frank Valencia"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.08128"
  },
  {
    "id": "arXiv:2210.08187",
    "title": "Optimal Controller Tuning Technique for a First-Order Process with Time  Delay",
    "abstract": "Comments: 6 pages, 7 figures, and 7 tables. Submitted to IFAC World Congress 2023",
    "descriptor": "\nComments: 6 pages, 7 figures, and 7 tables. Submitted to IFAC World Congress 2023\n",
    "authors": [
      "Clinton Enwerem",
      "Ihechiluru Okoro"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.08187"
  },
  {
    "id": "arXiv:2210.08268",
    "title": "Product Ranking for Revenue Maximization with Multiple Purchases",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Renzhe Xu",
      "Xingxuan Zhang",
      "Bo Li",
      "Yafeng Zhang",
      "Xiaolong Chen",
      "Peng Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.08268"
  },
  {
    "id": "arXiv:2210.08870",
    "title": "Differential Evolution based Dual Adversarial Camouflage: Fooling Human  Eyes and Object Detectors",
    "abstract": "Differential Evolution based Dual Adversarial Camouflage: Fooling Human  Eyes and Object Detectors",
    "descriptor": "",
    "authors": [
      "Jialiang Sun",
      "Tingsong Jiang",
      "Wen Yao",
      "Donghua Wang",
      "Xiaoqian Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.08870"
  },
  {
    "id": "arXiv:2210.08993",
    "title": "When Digital Economy Meets Web 3.0: Applications and Challenges",
    "abstract": "Comments: 14 pages, 5 figures",
    "descriptor": "\nComments: 14 pages, 5 figures\n",
    "authors": [
      "Chuan Chen",
      "Lei Zhang",
      "Yihao Li",
      "Tianchi Liao",
      "Siran Zhao",
      "Zibin Zheng",
      "Huawei Huang",
      "Jiajing Wu"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.08993"
  },
  {
    "id": "arXiv:2210.09887",
    "title": "MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving  Camera Videos",
    "abstract": "MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving  Camera Videos",
    "descriptor": "",
    "authors": [
      "Mathias Parger",
      "Chengcheng Tang",
      "Thomas Neff",
      "Christopher D. Twigg",
      "Cem Keskin",
      "Robert Wang",
      "Markus Steinberger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09887"
  },
  {
    "id": "arXiv:2210.10664",
    "title": "Deep Multi-Representation Model for Click-Through Rate Prediction",
    "abstract": "Deep Multi-Representation Model for Click-Through Rate Prediction",
    "descriptor": "",
    "authors": [
      "Shereen Elsayed",
      "Lars Schmidt-Thieme"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.10664"
  },
  {
    "id": "arXiv:2210.10879",
    "title": "G-Augment: Searching for the Meta-Structure of Data Augmentation  Policies for ASR",
    "abstract": "Comments: 6 pages, accepted at SLT 2022. Updated with copyright",
    "descriptor": "\nComments: 6 pages, accepted at SLT 2022. Updated with copyright\n",
    "authors": [
      "Gary Wang",
      "Ekin D.Cubuk",
      "Andrew Rosenberg",
      "Shuyang Cheng",
      "Ron J. Weiss",
      "Bhuvana Ramabhadran",
      "Pedro J. Moreno",
      "Quoc V. Le",
      "Daniel S. Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.10879"
  },
  {
    "id": "arXiv:2210.11248",
    "title": "OCR-VQGAN: Taming Text-within-Image Generation",
    "abstract": "Comments: Paper accepted at WACV 2023",
    "descriptor": "\nComments: Paper accepted at WACV 2023\n",
    "authors": [
      "Juan A. Rodriguez",
      "David Vazquez",
      "Issam Laradji",
      "Marco Pedersoli",
      "Pau Rodriguez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.11248"
  },
  {
    "id": "arXiv:2210.11262",
    "title": "RMBench: Benchmarking Deep Reinforcement Learning for Robotic  Manipulator Control",
    "abstract": "Comments: 8 pages, 2 figures, 2 tables; update code's link",
    "descriptor": "\nComments: 8 pages, 2 figures, 2 tables; update code's link\n",
    "authors": [
      "Yanfei Xiang",
      "Xin Wang",
      "Shu Hu",
      "Bin Zhu",
      "Xiaomeng Huang",
      "Xi Wu",
      "Siwei Lyu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.11262"
  },
  {
    "id": "arXiv:2210.11344",
    "title": "Towards Evology: a Market Ecology Agent-Based Model of US Equity Mutual  Funds",
    "abstract": "Towards Evology: a Market Ecology Agent-Based Model of US Equity Mutual  Funds",
    "descriptor": "",
    "authors": [
      "Aymeric Vie",
      "Maarten Scholl",
      "Alissa M. Kleinnijenhuis",
      "J. Doyne Farmer"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.11344"
  },
  {
    "id": "arXiv:2210.11498",
    "title": "Balanced Adversarial Training: Balancing Tradeoffs between Fickleness  and Obstinacy in NLP Models",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Hannah Chen",
      "Yangfeng Ji",
      "David Evans"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.11498"
  },
  {
    "id": "arXiv:2210.11610",
    "title": "Large Language Models Can Self-Improve",
    "abstract": "Large Language Models Can Self-Improve",
    "descriptor": "",
    "authors": [
      "Jiaxin Huang",
      "Shixiang Shane Gu",
      "Le Hou",
      "Yuexin Wu",
      "Xuezhi Wang",
      "Hongkun Yu",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.11610"
  },
  {
    "id": "arXiv:2210.11744",
    "title": "AfroLID: A Neural Language Identification Tool for African Languages",
    "abstract": "Comments: To appear at EMNLP 2022 Main conference",
    "descriptor": "\nComments: To appear at EMNLP 2022 Main conference\n",
    "authors": [
      "Ife Adebara",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed",
      "Alcides Alcoba Inciarte"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.11744"
  },
  {
    "id": "arXiv:2210.12034",
    "title": "Proceedings of the Dialogue Robot Competition 2022",
    "abstract": "Comments: Proceedings of the Dialogue Robot Competition 2022",
    "descriptor": "\nComments: Proceedings of the Dialogue Robot Competition 2022\n",
    "authors": [
      "Ryuichiro Higashinaka",
      "Takashi Minato",
      "Hiromitsu Nishizaki",
      "Takayuki Nagai"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12034"
  },
  {
    "id": "arXiv:2210.12154",
    "title": "Use of BNNM for interference wave solutions of the gBS-like equation and  comparison with PINNs",
    "abstract": "Comments: Mistakes in paper",
    "descriptor": "\nComments: Mistakes in paper\n",
    "authors": [
      "Shashank Reddy Vadyala",
      "Sai Nethra Betgeri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12154"
  },
  {
    "id": "arXiv:2210.12160",
    "title": "On the connection between Bregman divergence and value in regularized  Markov decision processes",
    "abstract": "On the connection between Bregman divergence and value in regularized  Markov decision processes",
    "descriptor": "",
    "authors": [
      "Brendan O'Donoghue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12160"
  },
  {
    "id": "arXiv:2210.12229",
    "title": "Deep Reinforcement Learning for Stabilization of Large-scale  Probabilistic Boolean Networks",
    "abstract": "Deep Reinforcement Learning for Stabilization of Large-scale  Probabilistic Boolean Networks",
    "descriptor": "",
    "authors": [
      "Sotiris Moschoyiannis",
      "Evangelos Chatzaroulas",
      "Vytenis Sliogeris",
      "Yuhu Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12229"
  },
  {
    "id": "arXiv:2210.12277",
    "title": "The Stochastic Proximal Distance Algorithm",
    "abstract": "The Stochastic Proximal Distance Algorithm",
    "descriptor": "",
    "authors": [
      "Haoyu Jiang",
      "Jason Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12277"
  },
  {
    "id": "arXiv:2210.12310",
    "title": "Tools for Extracting Spatio-Temporal Patterns in Meteorological Image  Sequences: From Feature Engineering to Attention-Based Neural Networks",
    "abstract": "Comments: The paper is submitted for review to the EDS Journal",
    "descriptor": "\nComments: The paper is submitted for review to the EDS Journal\n",
    "authors": [
      "Akansha Singh Bansal",
      "Yoonjin Lee",
      "Kyle Hilburn",
      "Imme Ebert-Uphoff"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12310"
  },
  {
    "id": "arXiv:2210.12403",
    "title": "PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models",
    "abstract": "Comments: Accepted by EMNLP 2022 main conference",
    "descriptor": "\nComments: Accepted by EMNLP 2022 main conference\n",
    "authors": [
      "Yupeng Zhang",
      "Hongzhi Zhang",
      "Sirui Wang",
      "Wei Wu",
      "Zhoujun Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12403"
  },
  {
    "id": "arXiv:2210.12415",
    "title": "ALT: Breaking the Wall between Graph and Operator Level Optimizations  for Deep Learning Compilation",
    "abstract": "ALT: Breaking the Wall between Graph and Operator Level Optimizations  for Deep Learning Compilation",
    "descriptor": "",
    "authors": [
      "Zhiying Xu",
      "Jiafan Xu",
      "Hongding Peng",
      "Wei Wang",
      "Xiaoliang Wang",
      "Haoran Wan",
      "Haipeng Dai",
      "Yixu Xu",
      "Hao Cheng",
      "Kun Wang",
      "Guihai Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12415"
  },
  {
    "id": "arXiv:2210.12432",
    "title": "Structure-Unified M-Tree Coding Solver for MathWord Problem",
    "abstract": "Comments: Accepted by EMNLP2022",
    "descriptor": "\nComments: Accepted by EMNLP2022\n",
    "authors": [
      "Bin Wang",
      "Jiangzhou Ju",
      "Yang Fan",
      "Xinyu Dai",
      "Shujian Huang",
      "Jiajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12432"
  },
  {
    "id": "arXiv:2210.12496",
    "title": "Bayesian Optimization with Conformal Coverage Guarantees",
    "abstract": "Comments: For code, see this https URL",
    "descriptor": "\nComments: For code, see this https URL\n",
    "authors": [
      "Samuel Stanton",
      "Wesley Maddox",
      "Andrew Gordon Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12496"
  },
  {
    "id": "arXiv:2210.12515",
    "title": "SpectraNet: Multivariate Forecasting and Imputation under Distribution  Shifts and Missing Data",
    "abstract": "SpectraNet: Multivariate Forecasting and Imputation under Distribution  Shifts and Missing Data",
    "descriptor": "",
    "authors": [
      "Cristian Challu",
      "Peihong Jiang",
      "Ying Nian Wu",
      "Laurent Callot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12515"
  },
  {
    "id": "arXiv:2210.12673",
    "title": "Lexical Generalization Improves with Larger Models and Longer Training",
    "abstract": "Comments: Accepted to EMNLP 2022 as Findings Paper, Presented at BlackboxNLP 2022",
    "descriptor": "\nComments: Accepted to EMNLP 2022 as Findings Paper, Presented at BlackboxNLP 2022\n",
    "authors": [
      "Elron Bandel",
      "Yoav Goldberg",
      "Yanai Elazar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12673"
  },
  {
    "id": "arXiv:2210.12803",
    "title": "LQGNet: Hybrid Model-Based and Data-Driven Linear Quadratic Stochastic  Control",
    "abstract": "Comments: Submitted to ICASSP23",
    "descriptor": "\nComments: Submitted to ICASSP23\n",
    "authors": [
      "Solomon Goldgraber Casspi",
      "Oliver Husser",
      "Guy Revach",
      "Nir Shlezinger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12803"
  },
  {
    "id": "arXiv:2210.12809",
    "title": "Automated Essay Scoring using Transformers",
    "abstract": "Comments: Working on its improvement",
    "descriptor": "\nComments: Working on its improvement\n",
    "authors": [
      "Kshitij Gupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12809"
  },
  {
    "id": "arXiv:2210.12889",
    "title": "DALL-E 2 Fails to Reliably Capture Common Syntactic Processes",
    "abstract": "DALL-E 2 Fails to Reliably Capture Common Syntactic Processes",
    "descriptor": "",
    "authors": [
      "Evelina Leivada",
      "Elliot Murphy",
      "Gary Marcus"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12889"
  },
  {
    "id": "arXiv:2210.12931",
    "title": "Removing Radio Frequency Interference from Auroral Kilometric Radiation  with Stacked Autoencoders",
    "abstract": "Comments: 5 pages, 3 figures",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Allen Chang",
      "Mary Knapp",
      "James LaBelle",
      "John Swoboda",
      "Ryan Volz",
      "Philip J. Erickson"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12931"
  },
  {
    "id": "arXiv:2210.13029",
    "title": "Multilingual Auxiliary Tasks Training: Bridging the Gap between  Languages for Zero-Shot Transfer of Hate Speech Detection Models",
    "abstract": "Comments: Accepted to Findings of AACL-IJCNLP 2022",
    "descriptor": "\nComments: Accepted to Findings of AACL-IJCNLP 2022\n",
    "authors": [
      "Syrielle Montariol",
      "Arij Riabi",
      "Djam\u00e9 Seddah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13029"
  },
  {
    "id": "arXiv:2210.13088",
    "title": "Your Router is My Prober: Measuring IPv6 Networks via ICMP Rate Limiting  Side Channels",
    "abstract": "Your Router is My Prober: Measuring IPv6 Networks via ICMP Rate Limiting  Side Channels",
    "descriptor": "",
    "authors": [
      "Long Pan",
      "Jiahai Yang",
      "Lin He",
      "Zhiliang Wang",
      "Leyao Nie",
      "Guanglei Song",
      "Yaozhong Liu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13088"
  },
  {
    "id": "arXiv:2210.13148",
    "title": "DAGformer: Directed Acyclic Graph Transformer",
    "abstract": "DAGformer: Directed Acyclic Graph Transformer",
    "descriptor": "",
    "authors": [
      "Yuankai Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13148"
  },
  {
    "id": "arXiv:2210.13382",
    "title": "Emergent World Representations: Exploring a Sequence Model Trained on a  Synthetic Task",
    "abstract": "Comments: code: this https URL",
    "descriptor": "\nComments: code: this https URL\n",
    "authors": [
      "Kenneth Li",
      "Aspen K. Hopkins",
      "David Bau",
      "Fernanda Vi\u00e9gas",
      "Hanspeter Pfister",
      "Martin Wattenberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13382"
  },
  {
    "id": "arXiv:2210.13416",
    "title": "A Continuous Convolutional Trainable Filter for Modelling Unstructured  Data",
    "abstract": "A Continuous Convolutional Trainable Filter for Modelling Unstructured  Data",
    "descriptor": "",
    "authors": [
      "Dario Coscia",
      "Laura Meneghetti",
      "Nicola Demo",
      "Giovanni Stabile",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13416"
  },
  {
    "id": "arXiv:2210.13430",
    "title": "Data-Driven Stabilizing and Robust Control of Discrete-Time Linear  Systems with Error in Variables",
    "abstract": "Comments: 27 pages, 1 figure, 9 tables",
    "descriptor": "\nComments: 27 pages, 1 figure, 9 tables\n",
    "authors": [
      "Jared Miller",
      "Tianyu Dai",
      "Mario Sznaier"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13430"
  }
]