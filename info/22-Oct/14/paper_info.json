[
  {
    "id": "arXiv:2210.06467",
    "title": "Against Interaction Design",
    "abstract": "Against Interaction Design is a short manifesto that distils a position\nthat's emerged through a decade of creating interactive art.\nI intend it here as a provocation and a speculation on an alternative future\nrelationship between people and machines.",
    "descriptor": "",
    "authors": [
      "Tim Murray-Browne"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06467"
  },
  {
    "id": "arXiv:2210.06468",
    "title": "Emergence of Shared Sensory-motor Graphical Language from Visual Input",
    "abstract": "The framework of Language Games studies the emergence of languages in\npopulations of agents. Recent contributions relying on deep learning methods\nfocused on agents communicating via an idealized communication channel, where\nutterances produced by a speaker are directly perceived by a listener. This\ncomes in contrast with human communication, which instead relies on a\nsensory-motor channel, where motor commands produced by the speaker (e.g. vocal\nor gestural articulators) result in sensory effects perceived by the listener\n(e.g. audio or visual). Here, we investigate if agents can evolve a shared\nlanguage when they are equipped with a continuous sensory-motor system to\nproduce and perceive signs, e.g. drawings. To this end, we introduce the\nGraphical Referential Game (GREG) where a speaker must produce a graphical\nutterance to name a visual referent object consisting of combinations of MNIST\ndigits while a listener has to select the corresponding object among distractor\nreferents, given the produced message. The utterances are drawing images\nproduced using dynamical motor primitives combined with a sketching library. To\ntackle GREG we present CURVES: a multimodal contrastive deep learning mechanism\nthat represents the energy (alignment) between named referents and utterances\ngenerated through gradient ascent on the learned energy landscape. We, then,\npresent a set of experiments and metrics based on a systematic compositional\ndataset to evaluate the resulting language. We show that our method allows the\nemergence of a shared, graphical language with compositional properties.",
    "descriptor": "",
    "authors": [
      "Yoann Lemesle",
      "Tristan Karch",
      "Romain Laroche",
      "Cl\u00e9ment Moulin-Frier",
      "Pierre-Yves Oudeyer"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06468"
  },
  {
    "id": "arXiv:2210.06469",
    "title": "Participatory Design for Mental Health Data Visualization on a Social  Robot",
    "abstract": "The intersection of data visualization and human-robot interaction (HRI) is a\nburgeoning field. Understanding, communicating, and processing different kinds\nof data for creating versatile visualizations can benefit HRI. Conversely,\nexpressing different kinds of data generated from HRI through effective\nvisualizations can provide interesting insights. Our work adds to the\nliterature of this growing domain. In this paper, we present our exploratory\nwork on visualizing mental health data on a social robot. Particularly, we\ndiscuss development of mental health data visualizations using a participatory\ndesign (PD) approach. As a first step with mental health data visualization on\na social robot, this work paves the way for relevant further work and using\nsocial robots as data visualization tools.",
    "descriptor": "\nComments: Accepted to workshop on participatory design (PD) in human-robot interaction in RO-MAN 2022\n",
    "authors": [
      "Raida Karim",
      "Edgar Lopez",
      "Elin A. Bj\u00f6rling",
      "Maya Cakmak"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06469"
  },
  {
    "id": "arXiv:2210.06470",
    "title": "Who Wrote this? How Smart Replies Impact Language and Agency in the  Workplace",
    "abstract": "AI-mediated communication is designed to help us do our work more quickly and\nefficiently. But does it come at a cost? This study uses smart replies (SRs) to\nshow how AI influences humans without any intent on the part of the developer -\nthe very use of AI is sufficient. I propose a loss of agency theory as a viable\napproach for studying the impact of AI on human agency. I use mixed methods\ninvolving a crowdsourced experiment to test the theory and qualitative\ninterviews to elucidate non-use of AI. My quantitative results reveal that\nmachine agency affects the content we author and the behavior we generate. But\nit is a non-zero-sum game.",
    "descriptor": "\nComments: 35 pages, 5 figures, 5 tables\n",
    "authors": [
      "Kilian Wenker"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06470"
  },
  {
    "id": "arXiv:2210.06472",
    "title": "Inner speech recognition through electroencephalographic signals",
    "abstract": "This work focuses on inner speech recognition starting from EEG signals.\nInner speech recognition is defined as the internalized process in which the\nperson thinks in pure meanings, generally associated with an auditory imagery\nof own inner \"voice\". The decoding of the EEG into text should be understood as\nthe classification of a limited number of words (commands) or the presence of\nphonemes (units of sound that make up words). Speech-related BCIs provide\neffective vocal communication strategies for controlling devices through speech\ncommands interpreted from brain signals, improving the quality of life of\npeople who have lost the capability to speak, by restoring communication with\ntheir environment. Two public inner speech datasets are analysed. Using this\ndata, some classification models are studied and implemented starting from\nbasic methods such as Support Vector Machines, to ensemble methods such as the\neXtreme Gradient Boosting classifier up to the use of neural networks such as\nLong Short Term Memory (LSTM) and Bidirectional Long Short Term Memory\n(BiLSTM). With the LSTM and BiLSTM models, generally not used in the literature\nof inner speech recognition, results in line with or superior to those present\nin the stateof-the-art are obtained.",
    "descriptor": "\nComments: Submitted to the Italian Workshop on Artificial Intelligence for Human Machine Interaction (AIxHMI 2022), December 02, 2022, Udine, Italy\n",
    "authors": [
      "Francesca Gasparini",
      "Elisa Cazzaniga",
      "Aurora Saibene"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06472"
  },
  {
    "id": "arXiv:2210.06474",
    "title": "VR-SFT: Reproducing Swinging Flashlight Test in Virtual Reality to  Detect Relative Afferent Pupillary Defect",
    "abstract": "The relative afferent asymmetry between two eyes can be diagnosed using\nswinging flashlight test, also known as the alternating light test. This\nremains one of the most used clinical tests to this day. Despite the swinging\nflashlight test's straightforward approach, a number of factors can add\nvariability into the clinical methodology and reduce the measurement's validity\nand reliability. This includes small and poorly responsive pupils, dark iris,\nanisocoria, uneven illumination in both eyes. Due to these limitations, the\ntrue condition of relative afferent asymmetry may create confusion and various\nobservers may quantify the relative afferent pupillary defect differently.\nConsequently, the results of the swinging flashlight test are subjective and\nambiguous. In order to eliminate the limitations of traditional swinging\nflashlight test and introduce objectivity, we propose a novel approach to the\nswinging flashlight exam, VR-SFT, by making use of virtual reality (VR). We\nsuggest that the clinical records of the subjects and the results of VR-SFT are\ncomparable. In this paper, we describe how we exploit the features of immersive\nVR experience to create a reliable and objective swinging flashlight test.",
    "descriptor": "\nComments: International Symposium on Visual Computing\n",
    "authors": [
      "Prithul Sarker",
      "Nasif Zaman",
      "Alireza Tavakkoli"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06474"
  },
  {
    "id": "arXiv:2210.06475",
    "title": "Equi-Tuning: Group Equivariant Fine-Tuning of Pretrained Models",
    "abstract": "We introduce equi-tuning, a novel fine-tuning method that transforms\n(potentially non-equivariant) pretrained models into group equivariant models\nwhile incurring minimum $L_2$ loss between the feature representations of the\npretrained and the equivariant models. Large pretrained models can be\nequi-tuned for different groups to satisfy the needs of various downstream\ntasks. Equi-tuned models benefit from both group equivariance as an inductive\nbias and semantic priors from pretrained models. We provide applications of\nequi-tuning on three different tasks: image classification, compositional\ngeneralization in language, and fairness in natural language generation (NLG).\nWe also provide a novel group-theoretic definition for fairness in NLG. The\neffectiveness of this definition is shown by testing it against a standard\nempirical method of fairness in NLG. We provide experimental results for\nequi-tuning using a variety of pretrained models: Alexnet, Resnet, VGG, and\nDensenet for image classification; RNNs, GRUs, and LSTMs for compositional\ngeneralization; and GPT2 for fairness in NLG. We test these models on benchmark\ndatasets across all considered tasks to show the generality and effectiveness\nof the proposed method.",
    "descriptor": "",
    "authors": [
      "Sourya Basu",
      "Prasanna Sattigeri",
      "Karthikeyan Natesan Ramamurthy",
      "Vijil Chenthamarakshan",
      "Kush R. Varshney",
      "Lav R. Varshney",
      "Payel Das"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06475"
  },
  {
    "id": "arXiv:2210.06479",
    "title": "Real World Offline Reinforcement Learning with Realistic Data Source",
    "abstract": "Offline reinforcement learning (ORL) holds great promise for robot learning\ndue to its ability to learn from arbitrary pre-generated experience. However,\ncurrent ORL benchmarks are almost entirely in simulation and utilize contrived\ndatasets like replay buffers of online RL agents or sub-optimal trajectories,\nand thus hold limited relevance for real-world robotics. In this work\n(Real-ORL), we posit that data collected from safe operations of closely\nrelated tasks are more practical data sources for real-world robot learning.\nUnder these settings, we perform an extensive (6500+ trajectories collected\nover 800+ robot hours and 270+ human labor hour) empirical study evaluating\ngeneralization and transfer capabilities of representative ORL methods on four\nreal-world tabletop manipulation tasks. Our study finds that ORL and imitation\nlearning prefer different action spaces, and that ORL algorithms can generalize\nfrom leveraging offline heterogeneous data sources and outperform imitation\nlearning. We release our dataset and implementations at URL:\nhttps://sites.google.com/view/real-orl",
    "descriptor": "\nComments: Project website: this https URL\n",
    "authors": [
      "Gaoyue Zhou",
      "Liyiming Ke",
      "Siddhartha Srinivasa",
      "Abhinav Gupta",
      "Aravind Rajeswaran",
      "Vikash Kumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06479"
  },
  {
    "id": "arXiv:2210.06496",
    "title": "SUMBot: Summarizing Context in Open-Domain Dialogue Systems",
    "abstract": "In this paper, we investigate the problem of including relevant information\nas context in open-domain dialogue systems. Most models struggle to identify\nand incorporate important knowledge from dialogues and simply use the entire\nturns as context, which increases the size of the input fed to the model with\nunnecessary information. Additionally, due to the input size limitation of a\nfew hundred tokens of large pre-trained models, regions of the history are not\nincluded and informative parts from the dialogue may be omitted. In order to\nsurpass this problem, we introduce a simple method that substitutes part of the\ncontext with a summary instead of the whole history, which increases the\nability of models to keep track of all the previous relevant information. We\nshow that the inclusion of a summary may improve the answer generation task and\ndiscuss some examples to further understand the system's weaknesses.",
    "descriptor": "\nComments: 4 pages, 3 figures, accepted at IberSPEECH 2022\n",
    "authors": [
      "Rui Ribeiro",
      "Lu\u00edsa Coheur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06496"
  },
  {
    "id": "arXiv:2210.06501",
    "title": "Robust Action Segmentation from Timestamp Supervision",
    "abstract": "Action segmentation is the task of predicting an action label for each frame\nof an untrimmed video. As obtaining annotations to train an approach for action\nsegmentation in a fully supervised way is expensive, various approaches have\nbeen proposed to train action segmentation models using different forms of weak\nsupervision, e.g., action transcripts, action sets, or more recently\ntimestamps. Timestamp supervision is a promising type of weak supervision as\nobtaining one timestamp per action is less expensive than annotating all\nframes, but it provides more information than other forms of weak supervision.\nHowever, previous works assume that every action instance is annotated with a\ntimestamp, which is a restrictive assumption since it assumes that annotators\ndo not miss any action. In this work, we relax this restrictive assumption and\ntake missing annotations for some action instances into account. We show that\nour approach is more robust to missing annotations compared to other approaches\nand various baselines.",
    "descriptor": "\nComments: BMVC 2022\n",
    "authors": [
      "Yaser Souri",
      "Yazan Abu Farha",
      "Emad Bahrami",
      "Gianpiero Francesca",
      "Juergen Gall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06501"
  },
  {
    "id": "arXiv:2210.06505",
    "title": "About some generalizations trigonometric splines",
    "abstract": "Methods of constructing trigonometric fundamental splines with constant sign\nand sign-changing convergence factors are given. An example and graphics\nillustrating the concepts of convergence and interpolation grids are given.\nSome methods of constructing constant-sign and sign-changing coefficients of\nconvergence of trigonometric splines are considered.",
    "descriptor": "",
    "authors": [
      "V. Denysiuk"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06505"
  },
  {
    "id": "arXiv:2210.06507",
    "title": "Better Approximation for Interdependent SOS Valuations",
    "abstract": "Submodular over signal (SOS) defines a family of interesting functions for\nwhich there exist truthful mechanisms with constant approximation to the social\nwelfare for agents with interdependent valuations. The best-known truthful\nauction is of $4$-approximation and a lower bound of 2 was proved. We propose a\nnew and simple truthful mechanism to achieve an approximation ratio of 3.315.",
    "descriptor": "",
    "authors": [
      "Pinyan Lu",
      "Enze Sun",
      "Chenghan Zhou"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.06507"
  },
  {
    "id": "arXiv:2210.06509",
    "title": "Understanding Impacts of Task Similarity on Backdoor Attack and  Detection",
    "abstract": "With extensive studies on backdoor attack and detection, still fundamental\nquestions are left unanswered regarding the limits in the adversary's\ncapability to attack and the defender's capability to detect. We believe that\nanswers to these questions can be found through an in-depth understanding of\nthe relations between the primary task that a benign model is supposed to\naccomplish and the backdoor task that a backdoored model actually performs. For\nthis purpose, we leverage similarity metrics in multi-task learning to formally\ndefine the backdoor distance (similarity) between the primary task and the\nbackdoor task, and analyze existing stealthy backdoor attacks, revealing that\nmost of them fail to effectively reduce the backdoor distance and even for\nthose that do, still much room is left to further improve their stealthiness.\nSo we further design a new method, called TSA attack, to automatically generate\na backdoor model under a given distance constraint, and demonstrate that our\nnew attack indeed outperforms existing attacks, making a step closer to\nunderstanding the attacker's limits. Most importantly, we provide both\ntheoretic results and experimental evidence on various datasets for the\npositive correlation between the backdoor distance and backdoor detectability,\ndemonstrating that indeed our task similarity analysis help us better\nunderstand backdoor risks and has the potential to identify more effective\nmitigations.",
    "descriptor": "",
    "authors": [
      "Di Tang",
      "Rui Zhu",
      "XiaoFeng Wang",
      "Haixu Tang",
      "Yi Chen"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06509"
  },
  {
    "id": "arXiv:2210.06510",
    "title": "Unscented Kalman Filtering on Manifolds for AUV Navigation --  Experimental Results",
    "abstract": "In this work, we present an aided inertial navigation system for an\nautonomous underwater vehicle (AUV) using an unscented Kalman filter on\nmanifolds (UKF-M). The inertial navigation estimate is aided by a Doppler\nvelocity log (DVL), depth sensor, acoustic range and, while on the surface,\nGPS. The sensor model for each navigation sensor on the AUV is explicitly\ndescribed, including compensation for lever arm offsets between the IMU and\neach sensor. Additionally, an outlier rejection step is proposed to reject\nmeasurement outliers that would degrade navigation performance. The UKF-M for\nAUV navigation is implemented for real-time navigation on the Virginia Tech 690\nAUV and validated in the field. Finally, by post-processing the navigation\nsensor data, we show experimentally that the UKF-M is able to converge to the\ncorrect heading in the presence of arbitrarily large initial heading error.",
    "descriptor": "\nComments: Published in IEEE OCEANS 2022\n",
    "authors": [
      "Stephen T. Krauss",
      "Daniel J. Stilwell"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06510"
  },
  {
    "id": "arXiv:2210.06511",
    "title": "Evaluated CMI Bounds for Meta Learning: Tightness and Expressiveness",
    "abstract": "Recent work has established that the conditional mutual information (CMI)\nframework of Steinke and Zakynthinou (2020) is expressive enough to capture\ngeneralization guarantees in terms of algorithmic stability, VC dimension, and\nrelated complexity measures for conventional learning (Harutyunyan et al.,\n2021, Haghifam et al., 2021). Hence, it provides a unified method for\nestablishing generalization bounds. In meta learning, there has so far been a\ndivide between information-theoretic results and results from classical\nlearning theory. In this work, we take a first step toward bridging this\ndivide. Specifically, we present novel generalization bounds for meta learning\nin terms of the evaluated CMI (e-CMI). To demonstrate the expressiveness of the\ne-CMI framework, we apply our bounds to a representation learning setting, with\n$n$ samples from $\\hat n$ tasks parameterized by functions of the form $f_i\n\\circ h$. Here, each $f_i \\in \\mathcal F$ is a task-specific function, and $h\n\\in \\mathcal H$ is the shared representation. For this setup, we show that the\ne-CMI framework yields a bound that scales as $\\sqrt{ \\mathcal C(\\mathcal\nH)/(n\\hat n) + \\mathcal C(\\mathcal F)/n} $, where $\\mathcal C(\\cdot)$ denotes a\ncomplexity measure of the hypothesis class. This scaling behavior coincides\nwith the one reported in Tripuraneni et al. (2020) using Gaussian complexity.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Fredrik Hellstr\u00f6m",
      "Giuseppe Durisi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06511"
  },
  {
    "id": "arXiv:2210.06516",
    "title": "How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?",
    "abstract": "Given the volume of data needed to train modern machine learning models,\nexternal suppliers are increasingly used. However, incorporating external data\nposes data poisoning risks, wherein attackers manipulate their data to degrade\nmodel utility or integrity. Most poisoning defenses presume access to a set of\nclean data (or base set). While this assumption has been taken for granted,\ngiven the fast-growing research on stealthy poisoning attacks, a question\narises: can defenders really identify a clean subset within a contaminated\ndataset to support defenses?\nThis paper starts by examining the impact of poisoned samples on defenses\nwhen they are mistakenly mixed into the base set. We analyze five defenses and\nfind that their performance deteriorates dramatically with less than 1%\npoisoned points in the base set. These findings suggest that sifting out a base\nset with high precision is key to these defenses' performance. Motivated by\nthese observations, we study how precise existing automated tools and human\ninspection are at identifying clean data in the presence of data poisoning.\nUnfortunately, neither effort achieves the precision needed. Worse yet, many of\nthe outcomes are worse than random selection.\nIn addition to uncovering the challenge, we propose a practical\ncountermeasure, Meta-Sift. Our method is based on the insight that existing\nattacks' poisoned samples shifts from clean data distributions. Hence, training\non the clean portion of a dataset and testing on the corrupted portion will\nresult in high prediction loss. Leveraging the insight, we formulate a bilevel\noptimization to identify clean data and further introduce a suite of techniques\nto improve efficiency and precision. Our evaluation shows that Meta-Sift can\nsift a clean base set with 100% precision under a wide range of poisoning\nattacks. The selected base set is large enough to give rise to successful\ndefenses.",
    "descriptor": "\nComments: 13 pages of the main text\n",
    "authors": [
      "Yi Zeng",
      "Minzhou Pan",
      "Himanshu Jahagirdar",
      "Ming Jin",
      "Lingjuan Lyu",
      "Ruoxi Jia"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06516"
  },
  {
    "id": "arXiv:2210.06518",
    "title": "Semi-Supervised Offline Reinforcement Learning with Action-Free  Trajectories",
    "abstract": "Natural agents can effectively learn from multiple data sources that differ\nin size, quality, and types of measurements. We study this heterogeneity in the\ncontext of offline reinforcement learning (RL) by introducing a new,\npractically motivated semi-supervised setting. Here, an agent has access to two\nsets of trajectories: labelled trajectories containing state, action, reward\ntriplets at every timestep, along with unlabelled trajectories that contain\nonly state and reward information. For this setting, we develop a simple\nmeta-algorithmic pipeline that learns an inverse-dynamics model on the labelled\ndata to obtain proxy-labels for the unlabelled data, followed by the use of any\noffline RL algorithm on the true and proxy-labelled trajectories. Empirically,\nwe find this simple pipeline to be highly successful -- on several D4RL\nbenchmarks \\cite{fu2020d4rl}, certain offline RL algorithms can match the\nperformance of variants trained on a fully labeled dataset even when we label\nonly 10\\% trajectories from the low return regime. Finally, we perform a\nlarge-scale controlled empirical study investigating the interplay of\ndata-centric properties of the labelled and unlabelled datasets, with\nalgorithmic design choices (e.g., inverse dynamics, offline RL algorithm) to\nidentify general trends and best practices for training RL agents on\nsemi-supervised offline datasets.",
    "descriptor": "",
    "authors": [
      "Qinqing Zheng",
      "Mikael Henaff",
      "Brandon Amos",
      "Aditya Grover"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06518"
  },
  {
    "id": "arXiv:2210.06525",
    "title": "Subword Segmental Language Modelling for Nguni Languages",
    "abstract": "Subwords have become the standard units of text in NLP, enabling efficient\nopen-vocabulary models. With algorithms like byte-pair encoding (BPE), subword\nsegmentation is viewed as a preprocessing step applied to the corpus before\ntraining. This can lead to sub-optimal segmentations for low-resource languages\nwith complex morphologies. We propose a subword segmental language model (SSLM)\nthat learns how to segment words while being trained for autoregressive\nlanguage modelling. By unifying subword segmentation and language modelling,\nour model learns subwords that optimise LM performance. We train our model on\nthe 4 Nguni languages of South Africa. These are low-resource agglutinative\nlanguages, so subword information is critical. As an LM, SSLM outperforms\nexisting approaches such as BPE-based models on average across the 4 languages.\nFurthermore, it outperforms standard subword segmenters on unsupervised\nmorphological segmentation. We also train our model as a word-level sequence\nmodel, resulting in an unsupervised morphological segmenter that outperforms\nexisting methods by a large margin for all 4 languages. Our results show that\nlearning subword segmentation is an effective alternative to existing subword\nsegmenters, enabling the model to discover morpheme-like subwords that improve\nits LM capabilities.",
    "descriptor": "",
    "authors": [
      "Francois Meyer",
      "Jan Buys"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06525"
  },
  {
    "id": "arXiv:2210.06527",
    "title": "Multilingual textual data: an approach through multiple factor analysis",
    "abstract": "This paper focuses on the analysis of open-ended questions answered in\ndifferent languages. Closed-ended questions, called contextual variables, are\nasked to all respondents in order to understand the relationships between the\nfree and the closed responses among the different samples since the latter\nassumably affect the word choices. We have developed \"Multiple Factor Analysis\non Generalized Aggregated Lexical Tables\" (MFA-GALT) to jointly study the\nopen-ended responses in different languages through the relationships between\nthe choice of words and the variables that drive this choice. MFA-GALT studies\nif variability among words is structured in the same way by variability among\nvariables, and inversely, from one sample to another. An application on an\ninternational satisfaction survey shows the easy-to-interpret results that are\nproposed.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Kostov Blechin",
      "Alvarez-Esteban Ram\u00f3n",
      "B\u00e9cue-Bertaut M\u00f3nica",
      "Husson Fran\u00e7ois"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.06527"
  },
  {
    "id": "arXiv:2210.06528",
    "title": "Parallel Domain Decomposition techniques applied to Multivariate  Functional Approximation of discrete data",
    "abstract": "Compactly expressing large-scale datasets through Multivariate Functional\nApproximations (MFA) can be critically important for analysis and visualization\nto drive scientific discovery. Tackling such problems requires scalable data\npartitioning approaches to compute MFA representations in amenable wall clock\ntimes. We introduce a fully parallel scheme to reduce the total work per task\nin combination with an overlapping additive Schwarz-based iterative scheme to\ncompute MFA with a tensor expansion of B-spline bases, while preserving full\ndegree continuity across subdomain boundaries. While previous work on MFA has\nbeen successfully proven to be effective, the computational complexity of\nencoding large datasets on a single process can be severely prohibitive.\nParallel algorithms for generating reconstructions from the MFA have had to\nrely on post-processing techniques to blend discontinuities across subdomain\nboundaries. In contrast, a robust constrained minimization infrastructure to\nimpose higher-order continuity directly on the MFA representation is presented\nhere. We demonstrate the effectiveness of the parallel approach with domain\ndecomposition solvers, to minimize the subdomain error residuals of the decoded\nMFA, and more specifically to recover continuity across non-matching boundaries\nat scale. The analysis of the presented scheme for analytical and scientific\ndatasets in 1-, 2- and 3-dimensions are presented. Extensive strong and weak\nscalability performances are also demonstrated for large-scale datasets to\nevaluate the parallel speedup of the MPI-based algorithm implementation on\nleadership computing machines.",
    "descriptor": "\nComments: Submitted to SIAM Journal of Scientific Computing\n",
    "authors": [
      "Vijay S. Mahadevan",
      "David Lenz",
      "Iulian Grindeanu",
      "Thomas Peterka"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06528"
  },
  {
    "id": "arXiv:2210.06529",
    "title": "Prepended Domain Transformer: Heterogeneous Face Recognition without  Bells and Whistles",
    "abstract": "Heterogeneous Face Recognition (HFR) refers to matching face images captured\nin different domains, such as thermal to visible images (VIS), sketches to\nvisible images, near-infrared to visible, and so on. This is particularly\nuseful in matching visible spectrum images to images captured from other\nmodalities. Though highly useful, HFR is challenging because of the domain gap\nbetween the source and target domain. Often, large-scale paired heterogeneous\nface image datasets are absent, preventing training models specifically for the\nheterogeneous task. In this work, we propose a surprisingly simple, yet, very\neffective method for matching face images across different sensing modalities.\nThe core idea of the proposed approach is to add a novel neural network block\ncalled Prepended Domain Transformer (PDT) in front of a pre-trained face\nrecognition (FR) model to address the domain gap. Retraining this new block\nwith few paired samples in a contrastive learning setup was enough to achieve\nstate-of-the-art performance in many HFR benchmarks. The PDT blocks can be\nretrained for several source-target combinations using the proposed general\nframework. The proposed approach is architecture agnostic, meaning they can be\nadded to any pre-trained FR models. Further, the approach is modular and the\nnew block can be trained with a minimal set of paired samples, making it much\neasier for practical deployment. The source code and protocols will be made\navailable publicly.",
    "descriptor": "\nComments: 16 pages. Accepted for publication in IEEE TIFS\n",
    "authors": [
      "Anjith George",
      "Amir Mohammadi",
      "Sebastien Marcel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06529"
  },
  {
    "id": "arXiv:2210.06535",
    "title": "Development of a Simulation Environment for Evaluation of a Forward  Looking Sonar System for Small AUVs",
    "abstract": "This paper describes a high-fidelity sonar model and a simulation environment\nthat implements the model. The model and simulation environment have been\ndeveloped to aid in the design of a forward looking sonar for autonomous\nunderwater vehicles (AUVs). The simulator achieves real-time visualization\nthrough ray tracing and approximation. The simulator facilitates the assessment\nof sonar design choices, such as beam pattern and beam location, and assessment\nof obstacle detection and tracking algorithms. An obstacle detection model is\nproposed for which the null hypothesis is estimated from the environmental\nmodel. Sonar data is generated from the simulator and compared to the expected\nresults from the detection model demonstrating the benefits and limitations of\nthe proposed approach.",
    "descriptor": "\nComments: 9 Pages, OCEANS 2019\n",
    "authors": [
      "Christopher Morency",
      "Daniel J. Stilwell",
      "Sebastian Hess"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06535"
  },
  {
    "id": "arXiv:2210.06537",
    "title": "Evaluating the Benefit of Using Multiple Low-Cost Forward-Looking Sonar  Beams for Collision Avoidance in Small AUVs",
    "abstract": "We seek to rigorously evaluate the benefit of using a few beams rather than a\nsingle beam for a low-cost obstacle avoidance sonar for small AUVs. For a small\nlow-cost AUV, the complexity, cost, and volume required for a multi-beam\nforward looking sonar are prohibitive. In contrast, a single-beam system is\nrelatively easy to integrate into a small AUV, but does not provide the\nperformance of a multi-beam solution. To better understand this trade-off, we\nseek to rigorously quantify the improvement with respect to obstacle avoidance\nperformance of adding just a few beams to a single-beam forward looking sonar\nrelative to the performance of the single-beam system. Our work fundamentally\nsupports the goal of using small low-cost AUV systems in cluttered and\nunstructured environments. Specifically, we investigate the benefit of\nincorporating a port and starboard beam to a single-beam sonar system for\ncollision avoidance. A methodology for collision avoidance is developed to\nobtain a fair comparison between a single-beam and multi-beam system,\nexplicitly incorporating the geometry of the beam patterns from forward-looking\nsonars with large beam angles, and simulated using a high-fidelity\nrepresentation of acoustic signal propagation.",
    "descriptor": "\nComments: 7 pages, IROS 2022\n",
    "authors": [
      "Christopher Morency",
      "Daniel J. Stilwell"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06537"
  },
  {
    "id": "arXiv:2210.06540",
    "title": "Blockchain for Unmanned Underwater Drones: Research Issues, Challenges,  Trends and Future Directions",
    "abstract": "Underwater drones have found a place in oceanography, oceanic research,\nbathymetric surveys, military, surveillance, monitoring, undersea exploration,\nmining, commercial diving, photography and several other activities. Drones\nhoused with several sensors and complex propulsion systems help oceanographic\nscientists and undersea explorers to map the seabed, study waves, view dead\nzones, analyze fish counts, predict tidal wave behaviors, aid in finding\nshipwrecks, building windfarms, examine oil platforms located in deep seas and\ninspect nuclear reactors in the ship vessels. While drones can be explicitly\nprogrammed for specific missions, data security and privacy are crucial issues\nof serious concern. Blockchain has emerged as a key enabling technology,\namongst other disruptive technological enablers, to address security, data\nsharing, storage, process tracking, collaboration and resource management. This\nstudy presents a comprehensive review on the utilization of Blockchain in\ndifferent underwater applications, discussing use cases and detailing benefits.\nPotential challenges of underwater applications addressed by Blockchain have\nbeen detailed. This work identifies knowledge gaps between theoretical research\nand real-time Blockchain integration in realistic underwater drone\napplications. The key limitations for effective integration of Blockchain in\nreal-time integration in UUD applications, along with directions for future\nresearch have been presented.",
    "descriptor": "",
    "authors": [
      "Neelu Jyoti Ahuja",
      "Adarsh Kumar",
      "Monika Thapliyal",
      "Sarthika Dutt",
      "Tanesh Kumar",
      "Diego Augusto De Jesus Pacheco",
      "Charalambos Konstantinou",
      "Kim-Kwang Raymond Choo"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06540"
  },
  {
    "id": "arXiv:2210.06541",
    "title": "MicroLib: A library of 3D microstructures generated from 2D micrographs  using SliceGAN",
    "abstract": "3D microstructural datasets are commonly used to define the geometrical\ndomains used in finite element modelling. This has proven a useful tool for\nunderstanding how complex material systems behave under applied stresses,\ntemperatures and chemical conditions. However, 3D imaging of materials is\nchallenging for a number of reasons, including limited field of view, low\nresolution and difficult sample preparation. Recently, a machine learning\nmethod, SliceGAN, was developed to statistically generate 3D microstructural\ndatasets of arbitrary size using a single 2D input slice as training data. In\nthis paper, we present the results from applying SliceGAN to 87 different\nmicrostructures, ranging from biological materials to high-strength steels. To\ndemonstrate the accuracy of the synthetic volumes created by SliceGAN, we\ncompare three microstructural properties between the 2D training data and 3D\ngenerations, which show good agreement. This new microstructure library both\nprovides valuable 3D microstructures that can be used in models, and also\ndemonstrates the broad applicability of the SliceGAN algorithm.",
    "descriptor": "\nComments: 10 pages, 4 figures\n",
    "authors": [
      "Steve Kench",
      "Isaac Squires",
      "Amir Dahari",
      "Samuel J Cooper"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.06541"
  },
  {
    "id": "arXiv:2210.06545",
    "title": "GULP: a prediction-based metric between representations",
    "abstract": "Comparing the representations learned by different neural networks has\nrecently emerged as a key tool to understand various architectures and\nultimately optimize them. In this work, we introduce GULP, a family of distance\nmeasures between representations that is explicitly motivated by downstream\npredictive tasks. By construction, GULP provides uniform control over the\ndifference in prediction performance between two representations, with respect\nto regularized linear prediction tasks. Moreover, it satisfies several\ndesirable structural properties, such as the triangle inequality and invariance\nunder orthogonal transformations, and thus lends itself to data embedding and\nvisualization. We extensively evaluate GULP relative to other methods, and\ndemonstrate that it correctly differentiates between architecture families,\nconverges over the course of training, and captures generalization performance\non downstream linear tasks.",
    "descriptor": "\nComments: 34 pages, 24 figures, to appear in NeurIPS'22\n",
    "authors": [
      "Enric Boix-Adsera",
      "Hannah Lawrence",
      "George Stepaniants",
      "Philippe Rigollet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06545"
  },
  {
    "id": "arXiv:2210.06546",
    "title": "Auto-Encoding Goodness of Fit",
    "abstract": "For generative autoencoders to learn a meaningful latent representation for\ndata generation, a careful balance must be achieved between reconstruction\nerror and how close the distribution in the latent space is to the prior.\nHowever, this balance is challenging to achieve due to a lack of criteria that\nwork both at the mini-batch (local) and aggregated posterior (global) level.\nGoodness of fit (GoF) hypothesis tests provide a measure of statistical\nindistinguishability between the latent distribution and a target distribution\nclass. In this work, we develop the Goodness of Fit Autoencoder (GoFAE), which\nincorporates hypothesis tests at two levels. At the mini-batch level, it uses\nGoF test statistics as regularization objectives. At a more global level, it\nselects a regularization coefficient based on higher criticism, i.e., a test on\nthe uniformity of the local GoF p-values. We justify the use of GoF tests by\nproviding a relaxed $L_2$-Wasserstein bound on the distance between the latent\ndistribution and target prior. We propose to use GoF tests and prove that\noptimization based on these tests can be done with stochastic gradient (SGD)\ndescent on a compact Riemannian manifold. Empirically, we show that our higher\ncriticism parameter selection procedure balances reconstruction and generation\nusing mutual information and uniformity of p-values respectively. Finally, we\nshow that GoFAE achieves comparable FID scores and mean squared errors with\ncompeting deep generative models while retaining statistical\nindistinguishability from Gaussian in the latent space based on a variety of\nhypothesis tests.",
    "descriptor": "",
    "authors": [
      "Aaron Palmer",
      "Zhiyi Chi",
      "Derek Aguiar",
      "Jinbo Bi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06546"
  },
  {
    "id": "arXiv:2210.06551",
    "title": "MotionBERT: Unified Pretraining for Human Motion Analysis",
    "abstract": "We present MotionBERT, a unified pretraining framework, to tackle different\nsub-tasks of human motion analysis including 3D pose estimation, skeleton-based\naction recognition, and mesh recovery. The proposed framework is capable of\nutilizing all kinds of human motion data resources, including motion capture\ndata and in-the-wild videos. During pretraining, the pretext task requires the\nmotion encoder to recover the underlying 3D motion from noisy partial 2D\nobservations. The pretrained motion representation thus acquires geometric,\nkinematic, and physical knowledge about human motion and therefore can be\neasily transferred to multiple downstream tasks. We implement the motion\nencoder with a novel Dual-stream Spatio-temporal Transformer (DSTformer) neural\nnetwork. It could capture long-range spatio-temporal relationships among the\nskeletal joints comprehensively and adaptively, exemplified by the lowest 3D\npose estimation error so far when trained from scratch. More importantly, the\nproposed framework achieves state-of-the-art performance on all three\ndownstream tasks by simply finetuning the pretrained motion encoder with 1-2\nlinear layers, which demonstrates the versatility of the learned motion\nrepresentations.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Wentao Zhu",
      "Xiaoxuan Ma",
      "Zhaoyang Liu",
      "Libin Liu",
      "Wayne Wu",
      "Yizhou Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06551"
  },
  {
    "id": "arXiv:2210.06553",
    "title": "Scenario-based Evaluation of Prediction Models for Automated Vehicles",
    "abstract": "To operate safely, an automated vehicle (AV) must anticipate how the\nenvironment around it will evolve. For that purpose, it is important to know\nwhich prediction models are most appropriate for every situation. Currently,\nassessment of prediction models is often performed over a set of trajectories\nwithout distinction of the type of movement they capture, resulting in the\ninability to determine the suitability of each model for different situations.\nIn this work we illustrate how standardized evaluation methods result in wrong\nconclusions regarding a model's predictive capabilities, preventing a clear\nassessment of prediction models and potentially leading to dangerous on-road\nsituations. We argue that following evaluation practices in safety assessment\nfor AVs, assessment of prediction models should be performed in a\nscenario-based fashion. To encourage scenario-based assessment of prediction\nmodels and illustrate the dangers of improper assessment, we categorize\ntrajectories of the Waymo Open Motion dataset according to the type of movement\nthey capture. Next, three different models are thoroughly evaluated for\ndifferent trajectory types and prediction horizons. Results show that common\nevaluation methods are insufficient and the assessment should be performed\ndepending on the application in which the model will operate.",
    "descriptor": "\nComments: To be published in IEEE Intelligent Transportation Systems Conference 2022\n",
    "authors": [
      "Manuel Mu\u00f1oz S\u00e1nchez",
      "Jos Elfring",
      "Emilia Silvas",
      "Ren\u00e9 van de Molengraft"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06553"
  },
  {
    "id": "arXiv:2210.06554",
    "title": "Toward the application of XAI methods in EEG-based systems",
    "abstract": "An interesting case of the well-known Dataset Shift Problem is the\nclassification of Electroencephalogram (EEG) signals in the context of\nBrain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to\npoor generalisation performance in BCI classification systems used in different\nsessions, also from the same subject. In this paper, we start from the\nhypothesis that the Dataset Shift problem can be alleviated by exploiting\nsuitable eXplainable Artificial Intelligence (XAI) methods to locate and\ntransform the relevant characteristics of the input for the goal of\nclassification. In particular, we focus on an experimental analysis of\nexplanations produced by several XAI methods on an ML system trained on a\ntypical EEG dataset for emotion recognition. Results show that many relevant\ncomponents found by XAI methods are shared across the sessions and can be used\nto build a system able to generalise better. However, relevant components of\nthe input signal also appear to be highly dependent on the input itself.",
    "descriptor": "\nComments: under review\n",
    "authors": [
      "Andrea Apicella",
      "Francesco Isgr\u00f2",
      "Andrea Pollastro",
      "Roberto Prevete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06554"
  },
  {
    "id": "arXiv:2210.06559",
    "title": "Exact and approximation algorithms for sensor placement against DDoS  attacks",
    "abstract": "In a DDoS attack (Distributed Denial of Service), an attacker gains control\nof many network users through a virus. Then the controlled users send many\nrequests to a victim, leading to its resources being depleted. DDoS attacks are\nhard to defend because of their distributed nature, large scale and various\nattack techniques. One possible mode of defense is to place sensors in a\nnetwork that can detect and stop an unwanted request. However, such sensors are\nexpensive so there is a natural question as to the minimum number of sensors\nand the optimal placement required to get the necessary level of safety.\nPresented below are two mixed integer models for optimal sensor placement\nagainst DDoS attacks. Both models lead to a trade-off between the number of\ndeployed sensors and the volume of uncontrolled flow. Since the above placement\nproblems are NP-hard, two efficient heuristics are designed, implemented and\ncompared experimentally with exact mixed integer linear programming solvers.",
    "descriptor": "",
    "authors": [
      "Konstanty Junosza-Szaniawski",
      "Dariusz Nogalski",
      "Pawe\u0142 Rz\u0105\u017cewski"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06559"
  },
  {
    "id": "arXiv:2210.06562",
    "title": "Scalability in Visualization",
    "abstract": "We introduce a conceptual model for scalability designed for visualization\nresearch. With this model, we systematically analyze over 120 visualization\npublications from 1990-2020 to characterize the different notions of\nscalability in these works. While many papers have addressed scalability\nissues, our survey identifies a lack of consistency in the use of the term in\nthe visualization research community. We address this issue by introducing a\nconsistent terminology meant to help visualization researchers better\ncharacterize the scalability aspects in their research. It also helps in\nproviding multiple methods for supporting the claim that a work is \"scalable\".\nOur model is centered around an effort function with inputs and outputs. The\ninputs are the problem size and resources, whereas the outputs are the actual\nefforts, for instance, in terms of computational run time or visual clutter. We\nselect representative examples to illustrate different approaches and facets of\nwhat scalability can mean in visualization literature. Finally, targeting the\ndiverse crowd of visualization researchers without a scalability tradition, we\nprovide a set of recommendations for how scalability can be presented in a\nclear and consistent way to improve fair comparison between visualization\ntechniques and systems and foster reproducibility.",
    "descriptor": "",
    "authors": [
      "Ga\u00eblle Richer",
      "Alexis Pister",
      "Moataz Abdelaal",
      "Jean-Daniel Fekete",
      "Michael Sedlmair",
      "Daniel Weiskopf"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06562"
  },
  {
    "id": "arXiv:2210.06565",
    "title": "That's the Wrong Lung! Evaluating and Improving the Interpretability of  Unsupervised Multimodal Encoders for Medical Data",
    "abstract": "Pretraining multimodal models on Electronic Health Records (EHRs) provides a\nmeans of learning representations that can transfer to downstream tasks with\nminimal supervision. Recent multimodal models induce soft local alignments\nbetween image regions and sentences. This is of particular interest in the\nmedical domain, where alignments might highlight regions in an image relevant\nto specific phenomena described in free-text. While past work has suggested\nthat attention \"heatmaps\" can be interpreted in this manner, there has been\nlittle evaluation of such alignments. We compare alignments from a\nstate-of-the-art multimodal (image and text) model for EHR with human\nannotations that link image regions to sentences. Our main finding is that the\ntext has an often weak or unintuitive influence on attention; alignments do not\nconsistently reflect basic anatomical information. Moreover, synthetic\nmodifications -- such as substituting \"left\" for \"right\" -- do not\nsubstantially influence highlights. Simple techniques such as allowing the\nmodel to opt out of attending to the image and few-shot finetuning show promise\nin terms of their ability to improve alignments with very little or no\nsupervision.",
    "descriptor": "",
    "authors": [
      "Denis Jered McInerney",
      "Geoffrey Young",
      "Jan-Willem van de Meent",
      "Byron Wallace"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.06565"
  },
  {
    "id": "arXiv:2210.06566",
    "title": "Developing a general-purpose clinical language inference model from a  large corpus of clinical notes",
    "abstract": "Several biomedical language models have already been developed for clinical\nlanguage inference. However, these models typically utilize general\nvocabularies and are trained on relatively small clinical corpora. We sought to\nevaluate the impact of using a domain-specific vocabulary and a large clinical\ntraining corpus on the performance of these language models in clinical\nlanguage inference. We trained a Bidirectional Encoder Decoder from\nTransformers (BERT) model using a diverse, deidentified corpus of 75 million\ndeidentified clinical notes authored at the University of California, San\nFrancisco (UCSF). We evaluated this model on several clinical language\ninference benchmark tasks: clinical and temporal concept recognition, relation\nextraction and medical language inference. We also evaluated our model on two\ntasks using discharge summaries from UCSF: diagnostic code assignment and\ntherapeutic class inference. Our model performs at par with the best publicly\navailable biomedical language models of comparable sizes on the public\nbenchmark tasks, and is significantly better than these models in a\nwithin-system evaluation on the two tasks using UCSF data. The use of in-domain\nvocabulary appears to improve the encoding of longer documents. The use of\nlarge clinical corpora appears to enhance document encoding and inferential\naccuracy. However, further research is needed to improve abbreviation\nresolution, and numerical, temporal, and implicitly causal inference.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Madhumita Sushil",
      "Dana Ludwig",
      "Atul J. Butte",
      "Vivek A. Rudrapatna"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06566"
  },
  {
    "id": "arXiv:2210.06570",
    "title": "Flare7K: A Phenomenological Nighttime Flare Removal Dataset",
    "abstract": "Artificial lights commonly leave strong lens flare artifacts on images\ncaptured at night. Nighttime flare not only affects the visual quality but also\ndegrades the performance of vision algorithms. Existing flare removal methods\nmainly focus on removing daytime flares and fail in nighttime. Nighttime flare\nremoval is challenging because of the unique luminance and spectrum of\nartificial lights and the diverse patterns and image degradation of the flares\ncaptured at night. The scarcity of nighttime flare removal datasets limits the\nresearch on this crucial task. In this paper, we introduce, Flare7K, the first\nnighttime flare removal dataset, which is generated based on the observation\nand statistics of real-world nighttime lens flares. It offers 5,000 scattering\nand 2,000 reflective flare images, consisting of 25 types of scattering flares\nand 10 types of reflective flares. The 7,000 flare patterns can be randomly\nadded to flare-free images, forming the flare-corrupted and flare-free image\npairs. With the paired data, we can train deep models to restore\nflare-corrupted images taken in the real world effectively. Apart from abundant\nflare patterns, we also provide rich annotations, including the labeling of\nlight source, glare with shimmer, reflective flare, and streak, which are\ncommonly absent from existing datasets. Hence, our dataset can facilitate new\nwork in nighttime flare removal and more fine-grained analysis of flare\npatterns. Extensive experiments show that our dataset adds diversity to\nexisting flare datasets and pushes the frontier of nighttime flare removal.",
    "descriptor": "\nComments: Camera-ready version for NeurIPS 2022 Track Datasets and Benchmarks\n",
    "authors": [
      "Yuekun Dai",
      "Chongyi Li",
      "Shangchen Zhou",
      "Ruicheng Feng",
      "Chen Change Loy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06570"
  },
  {
    "id": "arXiv:2210.06575",
    "title": "GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and  Specular Objects Using Generalizable NeRF",
    "abstract": "In this work, we tackle 6-DoF grasp detection for transparent and specular\nobjects, which is an important yet challenging problem in vision-based robotic\nsystems, due to the failure of depth cameras in sensing their geometry. We, for\nthe first time, propose a multiview RGB-based 6-DoF grasp detection network,\nGraspNeRF, that leverages the generalizable neural radiance field (NeRF) to\nachieve material-agnostic object grasping in clutter. Compared to the existing\nNeRF-based 3-DoF grasp detection methods that rely on densely captured input\nimages and time-consuming per-scene optimization, our system can perform\nzero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF\ngrasps, both in real-time. The proposed framework jointly learns generalizable\nNeRF and grasp detection in an end-to-end manner, optimizing the scene\nrepresentation construction for the grasping. For training data, we generate a\nlarge-scale photorealistic domain-randomized synthetic dataset of grasping in\ncluttered tabletop scenes that enables direct transfer to the real world. Our\nextensive experiments in synthetic and real-world environments demonstrate that\nour method significantly outperforms all the baselines in all the experiments\nwhile remaining in real-time.",
    "descriptor": "",
    "authors": [
      "Qiyu Dai",
      "Yan Zhu",
      "Yiran Geng",
      "Ciyu Ruan",
      "Jiazhao Zhang",
      "He Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06575"
  },
  {
    "id": "arXiv:2210.06576",
    "title": "DATScore: Evaluating Translation with Data Augmented Translations",
    "abstract": "The rapid development of large pretrained language models has revolutionized\nnot only the field of Natural Language Generation (NLG) but also its\nevaluation. Inspired by the recent work of BARTScore: a metric leveraging the\nBART language model to evaluate the quality of generated text from various\naspects, we introduce DATScore. DATScore uses data augmentation techniques to\nimprove the evaluation of machine translation. Our main finding is that\nintroducing data augmented translations of the source and reference texts is\ngreatly helpful in evaluating the quality of the generated translation. We also\npropose two novel score averaging and term weighting strategies to improve the\noriginal score computing process of BARTScore. Experimental results on WMT show\nthat DATScore correlates better with human meta-evaluations than the other\nrecent state-of-the-art metrics, especially for low-resource languages.\nAblation studies demonstrate the value added by our new scoring strategies.\nMoreover, we report in our extended experiments the performance of DATScore on\n3 NLG tasks other than translation.",
    "descriptor": "",
    "authors": [
      "Moussa Kamal Eddine",
      "Guokan Shang",
      "Michalis Vazirgiannis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06576"
  },
  {
    "id": "arXiv:2210.06578",
    "title": "FASTER-CE: Fast, Sparse, Transparent, and Robust Counterfactual  Explanations",
    "abstract": "Counterfactual explanations have substantially increased in popularity in the\npast few years as a useful human-centric way of understanding individual\nblack-box model predictions. While several properties desired of high-quality\ncounterfactuals have been identified in the literature, three crucial concerns:\nthe speed of explanation generation, robustness/sensitivity and succinctness of\nexplanations (sparsity) have been relatively unexplored. In this paper, we\npresent FASTER-CE: a novel set of algorithms to generate fast, sparse, and\nrobust counterfactual explanations. The key idea is to efficiently find\npromising search directions for counterfactuals in a latent space that is\nspecified via an autoencoder. These directions are determined based on\ngradients with respect to each of the original input features as well as of the\ntarget, as estimated in the latent space. The ability to quickly examine\ncombinations of the most promising gradient directions as well as to\nincorporate additional user-defined constraints allows us to generate multiple\ncounterfactual explanations that are sparse, realistic, and robust to input\nmanipulations. Through experiments on three datasets of varied complexities, we\nshow that FASTER-CE is not only much faster than other state of the art methods\nfor generating multiple explanations but also is significantly superior when\nconsidering a larger set of desirable (and often conflicting) properties.\nSpecifically we present results across multiple performance metrics: sparsity,\nproximity, validity, speed of generation, and the robustness of explanations,\nto highlight the capabilities of the FASTER-CE family.",
    "descriptor": "",
    "authors": [
      "Shubham Sharma",
      "Alan H. Gee",
      "Jette Henderson",
      "Joydeep Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06578"
  },
  {
    "id": "arXiv:2210.06579",
    "title": "Task-Free Continual Learning via Online Discrepancy Distance Learning",
    "abstract": "Learning from non-stationary data streams, also called Task-Free Continual\nLearning (TFCL) remains challenging due to the absence of explicit task\ninformation. Although recently some methods have been proposed for TFCL, they\nlack theoretical guarantees. Moreover, forgetting analysis during TFCL was not\nstudied theoretically before. This paper develops a new theoretical analysis\nframework which provides generalization bounds based on the discrepancy\ndistance between the visited samples and the entire information made available\nfor training the model. This analysis gives new insights into the forgetting\nbehaviour in classification tasks. Inspired by this theoretical model, we\npropose a new approach enabled by the dynamic component expansion mechanism for\na mixture model, namely the Online Discrepancy Distance Learning (ODDL). ODDL\nestimates the discrepancy between the probabilistic representation of the\ncurrent memory buffer and the already accumulated knowledge and uses it as the\nexpansion signal to ensure a compact network architecture with optimal\nperformance. We then propose a new sample selection approach that selectively\nstores the most relevant samples into the memory buffer through the\ndiscrepancy-based measure, further improving the performance. We perform\nseveral TFCL experiments with the proposed methodology, which demonstrate that\nthe proposed approach achieves the state of the art performance.",
    "descriptor": "\nComments: Accepted at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06579"
  },
  {
    "id": "arXiv:2210.06583",
    "title": "S4ND: Modeling Images and Videos as Multidimensional Signals Using State  Spaces",
    "abstract": "Visual data such as images and videos are typically modeled as\ndiscretizations of inherently continuous, multidimensional signals. Existing\ncontinuous-signal models attempt to exploit this fact by modeling the\nunderlying signals of visual (e.g., image) data directly. However, these models\nhave not yet been able to achieve competitive performance on practical vision\ntasks such as large-scale image and video classification. Building on a recent\nline of work on deep state space models (SSMs), we propose \\method, a new\nmultidimensional SSM layer that extends the continuous-signal modeling ability\nof SSMs to multidimensional data including images and videos. We show that S4ND\ncan model large-scale visual data in $1$D, $2$D, and $3$D as continuous\nmultidimensional signals and demonstrates strong performance by simply swapping\nConv2D and self-attention layers with \\method\\ layers in existing\nstate-of-the-art models. On ImageNet-1k, \\method\\ exceeds the performance of a\nVision Transformer baseline by $1.5\\%$ when training with a $1$D sequence of\npatches, and matches ConvNeXt when modeling images in $2$D. For videos, S4ND\nimproves on an inflated $3$D ConvNeXt in activity classification on HMDB-51 by\n$4\\%$. S4ND implicitly learns global, continuous convolutional kernels that are\nresolution invariant by construction, providing an inductive bias that enables\ngeneralization across multiple resolutions. By developing a simple bandlimiting\nmodification to S4 to overcome aliasing, S4ND achieves strong zero-shot (unseen\nat training time) resolution performance, outperforming a baseline Conv2D by\n$40\\%$ on CIFAR-10 when trained on $8 \\times 8$ and tested on $32 \\times 32$\nimages. When trained with progressive resizing, S4ND comes within $\\sim 1\\%$ of\na high-resolution model while training $22\\%$ faster.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Eric Nguyen",
      "Karan Goel",
      "Albert Gu",
      "Gordon W. Downs",
      "Preey Shah",
      "Tri Dao",
      "Stephen A. Baccus",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.06583"
  },
  {
    "id": "arXiv:2210.06585",
    "title": "Towards an Efficient ML System: Unveiling a Trade-off between Task  Accuracy and Engineering Efficiency in a Large-scale Car Sharing Platform",
    "abstract": "Upon the significant performance of the supervised deep neural networks,\nconventional procedures of developing ML system are \\textit{task-centric},\nwhich aims to maximize the task accuracy. However, we scrutinized this\n\\textit{task-centric} ML system lacks in engineering efficiency when the ML\npractitioners solve multiple tasks in their domain. To resolve this problem, we\npropose an \\textit{efficiency-centric} ML system that concatenates numerous\ndatasets, classifiers, out-of-distribution detectors, and prediction tables\nexisting in the practitioners' domain into a single ML pipeline. Under various\nimage recognition tasks in the real world car-sharing platform, our study\nillustrates how we established the proposed system and lessons learned from\nthis journey as follows. First, the proposed ML system accomplishes supreme\nengineering efficiency while achieving a competitive task accuracy. Moreover,\ncompared to the \\textit{task-centric} paradigm, we discovered that the\n\\textit{efficiency-centric} ML system yields satisfactory prediction results on\nmulti-labelable samples, which frequently exist in the real world. We analyze\nthese benefits derived from the representation power, which learned broader\nlabel spaces from the concatenated dataset. Last but not least, our study\nelaborated how we deployed this \\textit{efficiency-centric} ML system is\ndeployed in the real world live cloud environment. Based on the proposed\nanalogies, we highly expect that ML practitioners can utilize our study to\nelevate engineering efficiency in their domain.",
    "descriptor": "",
    "authors": [
      "Kyung Ho Park",
      "Hyunhee Chung",
      "Soonwoo Kwon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06585"
  },
  {
    "id": "arXiv:2210.06586",
    "title": "Automatic Real-time Vehicle Classification by Image Colour Component  Based Template Matching",
    "abstract": "Selection of appropriate template matching algorithms to run effectively on\nreal-time low-cost systems is always major issue. This is due to unpredictable\nchanges in image scene which often necessitate more sophisticated real-time\nalgorithms to retain image consistency. Inefficiency of low cost auxiliary\nhardware and time limitations are the major constraints in using these sorts of\nalgorithms. The real-time system introduced here copes with these problems\nutilising a fast running template matching algorithm, which makes use of best\ncolour band selection. The system uses fast running real-time algorithms to\nachieve template matching and vehicle classification at about 4 frames /sec. on\nlow-cost hardware. The colour image sequences have been taken by a fixed CCTV\ncamera overlooking a busy multi-lane road",
    "descriptor": "",
    "authors": [
      "Ahmet Orun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06586"
  },
  {
    "id": "arXiv:2210.06587",
    "title": "BLADERUNNER: Rapid Countermeasure for Synthetic (AI-Generated) StyleGAN  Faces",
    "abstract": "StyleGAN is the open-sourced TensorFlow implementation made by NVIDIA. It has\nrevolutionized high quality facial image generation. However, this\ndemocratization of Artificial Intelligence / Machine Learning (AI/ML)\nalgorithms has enabled hostile threat actors to establish cyber personas or\nsock-puppet accounts in social media platforms. These ultra-realistic synthetic\nfaces. This report surveys the relevance of AI/ML with respect to Cyber &\nInformation Operations. The proliferation of AI/ML algorithms has led to a rise\nin DeepFakes and inauthentic social media accounts. Threats are analyzed within\nthe Strategic and Operational Environments. Existing methods of identifying\nsynthetic faces exists, but they rely on human beings to visually scrutinize\neach photo for inconsistencies. However, through use of the DLIB 68-landmark\npre-trained file, it is possible to analyze and detect synthetic faces by\nexploiting repetitive behaviors in StyleGAN images. Project Blade Runner\nencompasses two scripts necessary to counter StyleGAN images. Through\nPapersPlease.py acting as the analyzer, it is possible to derive\nindicators-of-attack (IOA) from scraped image samples. These IOAs can be fed\nback into among_us.py acting as the detector to identify synthetic faces from\nlive operational samples. The opensource copy of Blade Runner may lack\nadditional unit tests and some functionality, but the open-source copy is a\nredacted version, far leaner, better optimized, and a proof-of-concept for the\ninformation security community. The desired end-state will be to incrementally\nadd automation to stay on-par with its closed-source predecessor.",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Adam Dorian Wong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06587"
  },
  {
    "id": "arXiv:2210.06588",
    "title": "Efficient Deep Unfolding for SISO-OFDM Channel Estimation",
    "abstract": "In modern communication systems, channel state information is of paramount\nimportance to achieve capacity. It is then crucial to accurately estimate the\nchannel. It is possible to perform SISO-OFDM channel estimation using sparse\nrecovery techniques. However, this approach relies on the use of a physical\nwave propagation model to build a dictionary, which requires perfect knowledge\nof the system's parameters. In this paper, an unfolded neural network is used\nto lighten this constraint. Its architecture, based on a sparse recovery\nalgorithm, allows SISO-OFDM channel estimation even if the system's parameters\nare not perfectly known. Indeed, its unsupervised online learning allows to\nlearn the system's imperfections in order to enhance the estimation\nperformance. The practicality of the proposed method is improved with respect\nto the state of the art in two aspects: constrained dictionaries are introduced\nin order to reduce sample complexity and hierarchical search within\ndictionaries is proposed in order to reduce time complexity. Finally, the\nperformance of the proposed unfolded network is evaluated and compared to\nseveral baselines using realistic channel data, showing the great potential of\nthe approach.",
    "descriptor": "",
    "authors": [
      "Baptiste Chatelier",
      "Luc Le Magoarou",
      "Getachew Redieteab"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06588"
  },
  {
    "id": "arXiv:2210.06589",
    "title": "Adversarial Attack Against Image-Based Localization Neural Networks",
    "abstract": "In this paper, we present a proof of concept for adversarially attacking the\nimage-based localization module of an autonomous vehicle. This attack aims to\ncause the vehicle to perform a wrong navigational decisions and prevent it from\nreaching a desired predefined destination in a simulated urban environment. A\ndatabase of rendered images allowed us to train a deep neural network that\nperforms a localization task and implement, develop and assess the adversarial\npattern. Our tests show that using this adversarial attack we can prevent the\nvehicle from turning at a given intersection. This is done by manipulating the\nvehicle's navigational module to falsely estimate its current position and thus\nfail to initialize the turning procedure until the vehicle misses the last\nopportunity to perform a safe turn in a given intersection.",
    "descriptor": "\nComments: 13 pages, 10 figures\n",
    "authors": [
      "Meir Brand",
      "Itay Naeh",
      "Daniel Teitelman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06589"
  },
  {
    "id": "arXiv:2210.06592",
    "title": "Can Calibration Improve Sample Prioritization?",
    "abstract": "Calibration can reduce overconfident predictions of deep neural networks, but\ncan calibration also accelerate training by selecting the right samples? In\nthis paper, we show that it can. We study the effect of popular calibration\ntechniques in selecting better subsets of samples during training (also called\nsample prioritization) and observe that calibration can improve the quality of\nsubsets, reduce the number of examples per epoch (by at least 70%), and can\nthereby speed up the overall training process. We further study the effect of\nusing calibrated pre-trained models coupled with calibration during training to\nguide sample prioritization, which again seems to improve the quality of\nsamples selected.",
    "descriptor": "",
    "authors": [
      "Ganesh Tata",
      "Gautham Krishna Gudur",
      "Gopinath Chennupati",
      "Mohammad Emtiyaz Khan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06592"
  },
  {
    "id": "arXiv:2210.06593",
    "title": "Differentially Private Online-to-Batch for Smooth Losses",
    "abstract": "We develop a new reduction that converts any online convex optimization\nalgorithm suffering $O(\\sqrt{T})$ regret into an $\\epsilon$-differentially\nprivate stochastic convex optimization algorithm with the optimal convergence\nrate $\\tilde O(1/\\sqrt{T} + \\sqrt{d}/\\epsilon T)$ on smooth losses in linear\ntime, forming a direct analogy to the classical non-private \"online-to-batch\"\nconversion. By applying our techniques to more advanced adaptive online\nalgorithms, we produce adaptive differentially private counterparts whose\nconvergence rates depend on apriori unknown variances or parameter norms.",
    "descriptor": "",
    "authors": [
      "Qinzi Zhang",
      "Hoang Tran",
      "Ashok Cutkosky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06593"
  },
  {
    "id": "arXiv:2210.06594",
    "title": "Sample Constrained Treatment Effect Estimation",
    "abstract": "Treatment effect estimation is a fundamental problem in causal inference. We\nfocus on designing efficient randomized controlled trials, to accurately\nestimate the effect of some treatment on a population of $n$ individuals. In\nparticular, we study sample-constrained treatment effect estimation, where we\nmust select a subset of $s \\ll n$ individuals from the population to experiment\non. This subset must be further partitioned into treatment and control groups.\nAlgorithms for partitioning the entire population into treatment and control\ngroups, or for choosing a single representative subset, have been well-studied.\nThe key challenge in our setting is jointly choosing a representative subset\nand a partition for that set.\nWe focus on both individual and average treatment effect estimation, under a\nlinear effects model. We give provably efficient experimental designs and\ncorresponding estimators, by identifying connections to discrepancy\nminimization and leverage-score-based sampling used in randomized numerical\nlinear algebra. Our theoretical results obtain a smooth transition to known\nguarantees when $s$ equals the population size. We also empirically demonstrate\nthe performance of our algorithms.",
    "descriptor": "\nComments: Conference on Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Raghavendra Addanki",
      "David Arbour",
      "Tung Mai",
      "Cameron Musco",
      "Anup Rao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Econometrics (econ.EM)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.06594"
  },
  {
    "id": "arXiv:2210.06596",
    "title": "Reducing The Mismatch Between Marginal and Learned Distributions in  Neural Video Compression",
    "abstract": "During the last four years, we have witnessed the success of end-to-end\ntrainable models for image compression. Compared to decades of incremental\nwork, these machine learning (ML) techniques learn all the components of the\ncompression technique, which explains their actual superiority. However,\nend-to-end ML models have not yet reached the performance of traditional video\ncodecs such as VVC. Possible explanations can be put forward: lack of data to\naccount for the temporal redundancy, or inefficiency of latent's density\nestimation in the neural model. The latter problem can be defined by the\ndiscrepancy between the latent's marginal distribution and the learned prior\ndistribution. This mismatch, known as amortization gap of entropy model,\nenlarges the file size of compressed data. In this paper, we propose to\nevaluate the amortization gap for three state-of-the-art ML video compression\nmethods. Second, we propose an efficient and generic method to solve the\namortization gap and show that it leads to an improvement between $2\\%$ to\n$5\\%$ without impacting reconstruction quality.",
    "descriptor": "\nComments: VCIP2022\n",
    "authors": [
      "Muhammet Balcilar",
      "Bharath Bhushan Damodaran",
      "Pierre Hellier"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06596"
  },
  {
    "id": "arXiv:2210.06597",
    "title": "Find Your Friends: Personalized Federated Learning with the Right  Collaborators",
    "abstract": "In the traditional federated learning setting, a central server coordinates a\nnetwork of clients to train one global model. However, the global model may\nserve many clients poorly due to data heterogeneity. Moreover, there may not\nexist a trusted central party that can coordinate the clients to ensure that\neach of them can benefit from others. To address these concerns, we present a\nnovel decentralized framework, FedeRiCo, where each client can learn as much or\nas little from other clients as is optimal for its local data distribution.\nBased on expectation-maximization, FedeRiCo estimates the utilities of other\nparticipants' models on each client's data so that everyone can select the\nright collaborators for learning. As a result, our algorithm outperforms other\nfederated, personalized, and/or decentralized approaches on several benchmark\ndatasets, being the only approach that consistently performs better than\ntraining with local data only.",
    "descriptor": "",
    "authors": [
      "Yi Sui",
      "Junfeng Wen",
      "Yenson Lau",
      "Brendan Leigh Ross",
      "Jesse C. Cresswell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06597"
  },
  {
    "id": "arXiv:2210.06599",
    "title": "Improving Question Answering with Generation of NQ-like Questions",
    "abstract": "Question Answering (QA) systems require a large amount of annotated data\nwhich is costly and time-consuming to gather. Converting datasets of existing\nQA benchmarks are challenging due to different formats and complexities. To\naddress these issues, we propose an algorithm to automatically generate shorter\nquestions resembling day-to-day human communication in the Natural Questions\n(NQ) dataset from longer trivia questions in Quizbowl (QB) dataset by\nleveraging conversion in style among the datasets. This provides an automated\nway to generate more data for our QA systems. To ensure quality as well as\nquantity of data, we detect and remove ill-formed questions using a neural\nclassifier. We demonstrate that in a low resource setting, using the generated\ndata improves the QA performance over the baseline system on both NQ and QB\ndata. Our algorithm improves the scalability of training data while maintaining\nquality of data for QA systems.",
    "descriptor": "",
    "authors": [
      "Saptarashmi Bandyopadhyay",
      "Shraman Pal",
      "Hao Zou",
      "Abhranil Chandra",
      "Jordan Boyd-Graber"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06599"
  },
  {
    "id": "arXiv:2210.06600",
    "title": "Iterative Document-level Information Extraction via Imitation Learning",
    "abstract": "We present a novel iterative extraction (IterX) model for extracting complex\nrelations, or templates, i.e., N-tuples representing a mapping from named slots\nto spans of text contained within a document. Documents may support zero or\nmore instances of a template of any particular type, leading to the tasks of\nidentifying the templates in a document, and extracting each template's slot\nvalues. Our imitation learning approach relieves the need to use predefined\ntemplate orders to train an extractor and leads to state-of-the-art results on\ntwo established benchmarks -- 4-ary relation extraction on SciREX and template\nextraction on MUC-4 -- as well as a strong baseline on the new BETTER Granular\ntask.",
    "descriptor": "",
    "authors": [
      "Yunmo Chen",
      "William Gantt",
      "Weiwei Gu",
      "Tongfei Chen",
      "Aaron Steven White",
      "Benjamin Van Durme"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06600"
  },
  {
    "id": "arXiv:2210.06601",
    "title": "Generalization with Lossy Affordances: Leveraging Broad Offline Data for  Learning Visuomotor Tasks",
    "abstract": "The utilization of broad datasets has proven to be crucial for generalization\nfor a wide range of fields. However, how to effectively make use of diverse\nmulti-task data for novel downstream tasks still remains a grand challenge in\nrobotics. To tackle this challenge, we introduce a framework that acquires\ngoal-conditioned policies for unseen temporally extended tasks via offline\nreinforcement learning on broad data, in combination with online fine-tuning\nguided by subgoals in learned lossy representation space. When faced with a\nnovel task goal, the framework uses an affordance model to plan a sequence of\nlossy representations as subgoals that decomposes the original task into easier\nproblems. Learned from the broad data, the lossy representation emphasizes\ntask-relevant information about states and goals while abstracting away\nredundant contexts that hinder generalization. It thus enables subgoal planning\nfor unseen tasks, provides a compact input to the policy, and facilitates\nreward shaping during fine-tuning. We show that our framework can be\npre-trained on large-scale datasets of robot experiences from prior work and\nefficiently fine-tuned for novel tasks, entirely from visual inputs without any\nmanual reward engineering.",
    "descriptor": "\nComments: CoRL 2022\n",
    "authors": [
      "Kuan Fang",
      "Patrick Yin",
      "Ashvin Nair",
      "Homer Walke",
      "Gengchen Yan",
      "Sergey Levine"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06601"
  },
  {
    "id": "arXiv:2210.06605",
    "title": "RAMP: A Risk-Aware Mapping and Planning Pipeline for Fast Off-Road  Ground Robot Navigation",
    "abstract": "A key challenge in fast ground robot navigation in 3D terrain is balancing\nrobot speed and safety. Recent work has shown that 2.5D maps (2D\nrepresentations with additional 3D information) are ideal for real-time safe\nand fast planning. However, raytracing as a prevalent method of generating\noccupancy grid as the base 2D representation makes the generated map unsafe to\nplan in, due to inaccurate representation of unknown space. Additionally,\nexisting planners such as MPPI do not reason about speeds in known free and\nunknown space separately, leading to slow plans. This work therefore first\npresents ground point inflation as a way to generate accurate occupancy grid\nmaps from classified pointclouds. Then we present an MPPI-based planner with\nembedded variability in horizon, to maximize speed in known free space while\nretaining cautionary penetration into unknown space. Finally, we integrate this\nmapping and planning pipeline with risk constraints arising from 3D terrain,\nand verify that it enables fast and safe navigation using simulations and a\nhardware demonstration.",
    "descriptor": "\nComments: 7 pages submitted to ICRA 2023\n",
    "authors": [
      "Lakshay Sharma",
      "Michael Everett",
      "Donggun Lee",
      "Xiaoyi Cai",
      "Philip Osteen",
      "Jonathan P. How"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06605"
  },
  {
    "id": "arXiv:2210.06609",
    "title": "TrafficGen: Learning to Generate Diverse and Realistic Traffic Scenarios",
    "abstract": "Diverse and realistic traffic scenarios are crucial for evaluating the AI\nsafety of autonomous driving systems in simulation. This work introduces a\ndata-driven method called TrafficGen for traffic scenario generation. It learns\nfrom the fragmented human driving data collected in the real world and then can\ngenerate realistic traffic scenarios. TrafficGen is an autoregressive\ngenerative model with an encoder-decoder architecture. In each autoregressive\niteration, it first encodes the current traffic context with the attention\nmechanism and then decodes a vehicle's initial state followed by generating its\nlong trajectory. We evaluate the trained model in terms of vehicle placement\nand trajectories and show substantial improvements over baselines. TrafficGen\ncan be also used to augment existing traffic scenarios, by adding new vehicles\nand extending the fragmented trajectories. We further demonstrate that\nimporting the generated scenarios into a simulator as interactive training\nenvironments improves the performance and the safety of driving policy learned\nfrom reinforcement learning. More project resource is available at\nhttps://metadriverse.github.io/trafficgen",
    "descriptor": "",
    "authors": [
      "Lan Feng",
      "Quanyi Li",
      "Zhenghao Peng",
      "Shuhan Tan",
      "Bolei Zhou"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06609"
  },
  {
    "id": "arXiv:2210.06610",
    "title": "A Neural Mean Embedding Approach for Back-door and Front-door Adjustment",
    "abstract": "We consider the estimation of average and counterfactual treatment effects,\nunder two settings: back-door adjustment and front-door adjustment. The goal in\nboth cases is to recover the treatment effect without having an access to a\nhidden confounder. This objective is attained by first estimating the\nconditional mean of the desired outcome variable given relevant covariates (the\n\"first stage\" regression), and then taking the (conditional) expectation of\nthis function as a \"second stage\" procedure. We propose to compute these\nconditional expectations directly using a regression function to the learned\ninput features of the first stage, thus avoiding the need for sampling or\ndensity estimation. All functions and features (and in particular, the output\nfeatures in the second stage) are neural networks learned adaptively from data,\nwith the sole requirement that the final layer of the first stage should be\nlinear. The proposed method is shown to converge to the true causal parameter,\nand outperforms the recent state-of-the-art methods on challenging causal\nbenchmarks, including settings involving high-dimensional image data.",
    "descriptor": "",
    "authors": [
      "Liyuan Xu",
      "Arthur Gretton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.06610"
  },
  {
    "id": "arXiv:2210.06614",
    "title": "Anomaly Detection via Federated Learning",
    "abstract": "Machine learning has helped advance the field of anomaly detection by\nincorporating classifiers and autoencoders to decipher between normal and\nanomalous behavior. Additionally, federated learning has provided a way for a\nglobal model to be trained with multiple clients' data without requiring the\nclient to directly share their data. This paper proposes a novel anomaly\ndetector via federated learning to detect malicious network activity on a\nclient's server. In our experiments, we use an autoencoder with a classifier in\na federated learning framework to determine if the network activity is benign\nor malicious. By using our novel min-max scalar and sampling technique, called\nFedSam, we determined federated learning allows the global model to learn from\neach client's data and, in turn, provide a means for each client to improve\ntheir intrusion detection system's defense against cyber-attacks.",
    "descriptor": "",
    "authors": [
      "Marc Vucovich",
      "Amogh Tarcar",
      "Penjo Rebelo",
      "Narendra Gade",
      "Ruchi Porwal",
      "Abdul Rahman",
      "Christopher Redino",
      "Kevin Choi",
      "Dhruv Nandakumar",
      "Robert Schiller",
      "Edward Bowen",
      "Alex West",
      "Sanmitra Bhattacharya",
      "Balaji Veeramani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06614"
  },
  {
    "id": "arXiv:2210.06618",
    "title": "QMRNet: Quality Metric Regression for EO Image Quality Assessment and  Super-Resolution",
    "abstract": "Latest advances in Super-Resolution (SR) have been tested with general\npurpose images such as faces, landscapes and objects, mainly unused for the\ntask of super-resolving Earth Observation (EO) images. In this research paper,\nwe benchmark state-of-the-art SR algorithms for distinct EO datasets using both\nFull-Reference and No-Reference Image Quality Assessment (IQA) metrics. We also\npropose a novel Quality Metric Regression Network (QMRNet) that is able to\npredict quality (as a No-Reference metric) by training on any property of the\nimage (i.e. its resolution, its distortions...) and also able to optimize SR\nalgorithms for a specific metric objective. This work is part of the\nimplementation of the framework IQUAFLOW which has been developed for\nevaluating image quality, detection and classification of objects as well as\nimage compression in EO use cases. We integrated our experimentation and tested\nour QMRNet algorithm on predicting features like blur, sharpness, snr, rer and\nground sampling distance (GSD) and obtain validation medRs below 1.0 (out of\nN=50) and recall rates above 95\\%. Overall benchmark shows promising results\nfor LIIF, CAR and MSRN and also the potential use of QMRNet as Loss for\noptimizing SR predictions. Due to its simplicity, QMRNet could also be used for\nother use cases and image domains, as its architecture and data processing is\nfully scalable.",
    "descriptor": "\nComments: 29 pages, 13 figures, 9 tables\n",
    "authors": [
      "David Berga",
      "Pau Gall\u00e9s",
      "Katalin Tat\u00e1ks",
      "Eva Mohedano",
      "Laura Riordan-Chen",
      "Clara Garcia-Moll",
      "David Vilaseca",
      "Javier Mar\u00edn"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Image and Video Processing (eess.IV)",
      "Geophysics (physics.geo-ph)",
      "Space Physics (physics.space-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.06618"
  },
  {
    "id": "arXiv:2210.06621",
    "title": "Bounds on the Wireless MapReduce NDT-Computation Tradeoff",
    "abstract": "We consider a full-duplex wireless Distributed Computing (DC) system under\nthe MapReduce framework. New upper bound and lower bounds on the tradeoff\nbetween Normalized Delivery Time (NDT) and computation load are obtained. The\nlower bound is proved through an information-theoretic converse. The upper\nbound is based on a novel IA scheme tailored to the interference cancellation\ncapabilities of the nodes and improves over existing bounds.",
    "descriptor": "",
    "authors": [
      "Yue Bi",
      "Mich\u00e8le Wigger"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06621"
  },
  {
    "id": "arXiv:2210.06628",
    "title": "OpenCQA: Open-ended Question Answering with Charts",
    "abstract": "Charts are very popular to analyze data and convey important insights. People\noften analyze visualizations to answer open-ended questions that require\nexplanatory answers. Answering such questions are often difficult and\ntime-consuming as it requires a lot of cognitive and perceptual efforts. To\naddress this challenge, we introduce a new task called OpenCQA, where the goal\nis to answer an open-ended question about a chart with descriptive texts. We\npresent the annotation process and an in-depth analysis of our dataset. We\nimplement and evaluate a set of baselines under three practical settings. In\nthe first setting, a chart and the accompanying article is provided as input to\nthe model. The second setting provides only the relevant paragraph(s) to the\nchart instead of the entire article, whereas the third setting requires the\nmodel to generate an answer solely based on the chart. Our analysis of the\nresults show that the top performing models generally produce fluent and\ncoherent text while they struggle to perform complex logical and arithmetic\nreasoning.",
    "descriptor": "",
    "authors": [
      "Shankar Kantharaj",
      "Xuan Long Do",
      "Rixie Tiffany Ko Leong",
      "Jia Qing Tan",
      "Enamul Hoque",
      "Shafiq Joty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06628"
  },
  {
    "id": "arXiv:2210.06629",
    "title": "Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis",
    "abstract": "Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis\ntask which involves four elements from user-generated texts: aspect term,\naspect category, opinion term, and sentiment polarity. Most computational\napproaches focus on some of the ABSA sub-tasks such as tuple (aspect term,\nsentiment polarity) or triplet (aspect term, opinion term, sentiment polarity)\nextraction using either pipeline or joint modeling approaches. Recently,\ngenerative approaches have been proposed to extract all four elements as (one\nor more) quadruplets from text as a single task. In this work, we take a step\nfurther and propose a unified framework for solving ABSA, and the associated\nsub-tasks to improve the performance in few-shot scenarios. To this end, we\nfine-tune a T5 model with instructional prompts in a multi-task learning\nfashion covering all the sub-tasks, as well as the entire quadruple prediction\ntask. In experiments with multiple benchmark data sets, we show that the\nproposed multi-task prompting approach brings performance boost (by absolute\n$6.75$ F1) in the few-shot learning setting.",
    "descriptor": "",
    "authors": [
      "Siddharth Varia",
      "Shuai Wang",
      "Kishaloy Halder",
      "Robert Vacareanu",
      "Miguel Ballesteros",
      "Yassine Benajiba",
      "Neha Anna John",
      "Rishita Anubhai",
      "Smaranda Muresan",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06629"
  },
  {
    "id": "arXiv:2210.06630",
    "title": "Fairness via Adversarial Attribute Neighbourhood Robust Learning",
    "abstract": "Improving fairness between privileged and less-privileged sensitive attribute\ngroups (e.g, {race, gender}) has attracted lots of attention. To enhance the\nmodel performs uniformly well in different sensitive attributes, we propose a\nprincipled \\underline{R}obust \\underline{A}dversarial \\underline{A}ttribute\n\\underline{N}eighbourhood (RAAN) loss to debias the classification head and\npromote a fairer representation distribution across different sensitive\nattribute groups. The key idea of RAAN is to mitigate the differences of biased\nrepresentations between different sensitive attribute groups by assigning each\nsample an adversarial robust weight, which is defined on the representations of\nadversarial attribute neighbors, i.e, the samples from different protected\ngroups. To provide efficient optimization algorithms, we cast the RAAN into a\nsum of coupled compositional functions and propose a stochastic adaptive\n(Adam-style) and non-adaptive (SGD-style) algorithm framework SCRAAN with\nprovable theoretical guarantee. Extensive empirical studies on fairness-related\nbenchmark datasets verify the effectiveness of the proposed method.",
    "descriptor": "\nComments: 25pages, 7 figures\n",
    "authors": [
      "Qi Qi",
      "Shervin Ardeshir",
      "Yi Xu",
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06630"
  },
  {
    "id": "arXiv:2210.06632",
    "title": "Transferable Learning on Analog Hardware",
    "abstract": "While analog neural network (NN) accelerators promise massive energy and time\nsavings, an important challenge is to make them robust to static fabrication\nerror. Present-day training methods for programmable photonic interferometer\ncircuits, a leading analog NN platform, do not produce networks that perform\nwell in the presence of static hardware errors. Moreover, existing hardware\nerror correction techniques either require individual re-training of every\nanalog NN (which is impractical in an edge setting with millions of devices),\nplace stringent demands on component quality, or introduce hardware overhead.\nWe solve all three problems by introducing one-time error-aware training\ntechniques that produce robust NNs that match the performance of ideal hardware\nand can be exactly transferred to arbitrary highly faulty photonic NNs with\nhardware errors up to 5x larger than present-day fabrication tolerances.",
    "descriptor": "",
    "authors": [
      "Sri Krishna Vadlamani",
      "Dirk Englund",
      "Ryan Hamerly"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2210.06632"
  },
  {
    "id": "arXiv:2210.06633",
    "title": "Language Agnostic Multilingual Information Retrieval with Contrastive  Learning",
    "abstract": "Multilingual information retrieval is challenging due to the lack of training\ndatasets for many low-resource languages. We present an effective method by\nleveraging parallel and non-parallel corpora to improve the pretrained\nmultilingual language models' cross-lingual transfer ability for information\nretrieval. We design the semantic contrastive loss as regular contrastive\nlearning to improve the cross-lingual alignment of parallel sentence pairs, and\nwe propose a new contrastive loss, the language contrastive loss, to leverage\nboth parallel corpora and non-parallel corpora to further improve multilingual\nrepresentation learning. We train our model on an English information retrieval\ndataset, and test its zero-shot transfer ability to other languages. Our\nexperiment results show that our method brings significant improvement to prior\nwork on retrieval performance, while it requires much less computational\neffort. Our model can work well even with a small number of parallel corpora.\nAnd it can be used as an add-on module to any backbone and other tasks. Our\ncode is available at: https://github.com/xiyanghu/multilingualIR.",
    "descriptor": "",
    "authors": [
      "Xiyang Hu",
      "Xinchi Chen",
      "Peng Qi",
      "Deguang Kong",
      "Kunlun Liu",
      "William Yang Wang",
      "Zhiheng Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06633"
  },
  {
    "id": "arXiv:2210.06637",
    "title": "Output Feedback Adaptive Optimal Control of Affine Nonlinear systems  with a Linear Measurement Model",
    "abstract": "Real-world control applications in complex and uncertain environments require\nadaptability to model uncertainties and robustness to disturbances. This paper\npresents an online output-feedback critic-only model-based reinforcement\nlearning architecture to simultaneously learn and implement an optimal\ncontroller while maintaining stability during the learning phase. Using\nmultiplier matrices, a convenient way to search for observer gains is designed.\nalong with a controller that learns from simulated experience to ensure\nstability and convergence of trajectories of the closed-loop system to a\nneighborhood of the origin. Local uniform ultimate boundedness of the\ntrajectories, under mild excitation conditions, is established using a\nLyapunov-based analysis and demonstrated via simulation results.",
    "descriptor": "\nComments: 16 pages, 5 figures, submitted to 2023 American Control Conference (ACC)\n",
    "authors": [
      "Tochukwu Elijah Ogri",
      "S M Nahid Mahmud",
      "Zachary I. Bell",
      "Rushikesh Kamalapurkar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06637"
  },
  {
    "id": "arXiv:2210.06640",
    "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities",
    "abstract": "Although deep learning has made great progress in recent years, the exploding\neconomic and environmental costs of training neural networks are becoming\nunsustainable. To address this problem, there has been a great deal of research\non *algorithmically-efficient deep learning*, which seeks to reduce training\ncosts not at the hardware or implementation level, but through changes in the\nsemantics of the training program. In this paper, we present a structured and\ncomprehensive overview of the research in this field. First, we formalize the\n*algorithmic speedup* problem, then we use fundamental building blocks of\nalgorithmically efficient training to develop a taxonomy. Our taxonomy\nhighlights commonalities of seemingly disparate methods and reveals current\nresearch gaps. Next, we present evaluation best practices to enable\ncomprehensive, fair, and reliable comparisons of speedup techniques. To further\naid research and applications, we discuss common bottlenecks in the training\npipeline (illustrated via experiments) and offer taxonomic mitigation\nstrategies for them. Finally, we highlight some unsolved research challenges\nand present promising future directions.",
    "descriptor": "",
    "authors": [
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Davis Blalock"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06640"
  },
  {
    "id": "arXiv:2210.06642",
    "title": "What's in a Decade? Transforming Faces Through Time",
    "abstract": "How can one visually characterize people in a decade? In this work, we\nassemble the Faces Through Time dataset, which contains over a thousand\nportrait images from each decade, spanning the 1880s to the present day. Using\nour new dataset, we present a framework for resynthesizing portrait images\nacross time, imagining how a portrait taken during a particular decade might\nhave looked like, had it been taken in other decades. Our framework optimizes a\nfamily of per-decade generators that reveal subtle changes that differentiate\ndecade--such as different hairstyles or makeup--while maintaining the identity\nof the input portrait. Experiments show that our method is more effective in\nresynthesizing portraits across time compared to state-of-the-art\nimage-to-image translation methods, as well as attribute-based and\nlanguage-guided portrait editing models. Our code and data will be available at\nhttps://facesthroughtime.github.io",
    "descriptor": "\nComments: Project Page: this https URL\n",
    "authors": [
      "Eric Ming Chen",
      "Jin Sun",
      "Apoorv Khandelwal",
      "Dani Lischinski",
      "Noah Snavely",
      "Hadar Averbuch-Elor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06642"
  },
  {
    "id": "arXiv:2210.06644",
    "title": "The COVID That Wasn't: Counterfactual Journalism Using GPT",
    "abstract": "In this paper, we explore the use of large language models to assess human\ninterpretations of real world events. To do so, we use a language model trained\nprior to 2020 to artificially generate news articles concerning COVID-19 given\nthe headlines of actual articles written during the pandemic. We then compare\nstylistic qualities of our artificially generated corpus with a news corpus, in\nthis case 5,082 articles produced by CBC News between January 23 and May 5,\n2020. We find our artificially generated articles exhibits a considerably more\nnegative attitude towards COVID and a significantly lower reliance on\ngeopolitical framing. Our methods and results hold importance for researchers\nseeking to simulate large scale cultural processes via recent breakthroughs in\ntext generation.",
    "descriptor": "\nComments: To appear in the proceedings of the 6th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature\n",
    "authors": [
      "Sil Hamilton",
      "Andrew Piper"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06644"
  },
  {
    "id": "arXiv:2210.06646",
    "title": "ditlab system for Dialogue Robot Competition 2022",
    "abstract": "We developed a dialogue system for Dialogue Robot Competition 2022. Our\nsystem is composed of three parts. First part investigates participants'\ndemographic information by rule-based interview. Second part recommends a point\nof interest (POI) based on the collected demographic information. Third part\nanswers participants' question based on the combination of rule-based answering\nand deep-learning-based answering with nearby POI search.",
    "descriptor": "\nComments: This paper is part of the proceedings of the Dialogue Robot Competition 2022\n",
    "authors": [
      "Yuuki Tachioka"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06646"
  },
  {
    "id": "arXiv:2210.06649",
    "title": "Neuro-symbolic Explainable Artificial Intelligence Twin for Zero-touch  IoE in Wireless Network",
    "abstract": "Explainable artificial intelligence (XAI) twin systems will be a fundamental\nenabler of zero-touch network and service management (ZSM) for sixth-generation\n(6G) wireless networks. A reliable XAI twin system for ZSM requires two\ncomposites: an extreme analytical ability for discretizing the physical\nbehavior of the Internet of Everything (IoE) and rigorous methods for\ncharacterizing the reasoning of such behavior. In this paper, a novel\nneuro-symbolic explainable artificial intelligence twin framework is proposed\nto enable trustworthy ZSM for a wireless IoE. The physical space of the XAI\ntwin executes a neural-network-driven multivariate regression to capture the\ntime-dependent wireless IoE environment while determining unconscious decisions\nof IoE service aggregation. Subsequently, the virtual space of the XAI twin\nconstructs a directed acyclic graph (DAG)-based Bayesian network that can infer\na symbolic reasoning score over unconscious decisions through a first-order\nprobabilistic language model. Furthermore, a Bayesian multi-arm bandits-based\nlearning problem is proposed for reducing the gap between the expected\nexplained score and the current obtained score of the proposed neuro-symbolic\nXAI twin. To address the challenges of extensible, modular, and stateless\nmanagement functions in ZSM, the proposed neuro-symbolic XAI twin framework\nconsists of two learning systems: 1) an implicit learner that acts as an\nunconscious learner in physical space, and 2) an explicit leaner that can\nexploit symbolic reasoning based on implicit learner decisions and prior\nevidence. Experimental results show that the proposed neuro-symbolic XAI twin\ncan achieve around 96.26% accuracy while guaranteeing from 18% to 44% more\ntrust score in terms of reasoning and closed-loop automation.",
    "descriptor": "\nComments: Submitted to a journal for peer review\n",
    "authors": [
      "Md. Shirajum Munir",
      "Ki Tae Kim",
      "Apurba Adhikary",
      "Walid Saad",
      "Sachin Shetty",
      "Seong-Bae Park",
      "Choong Seon Hong"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06649"
  },
  {
    "id": "arXiv:2210.06650",
    "title": "Interpreting Neural Policies with Disentangled Tree Representations",
    "abstract": "Compact neural networks used in policy learning and closed-loop end-to-end\ncontrol learn representations from data that encapsulate agent dynamics and\npotentially the agent-environment's factors of variation. A formal and\nquantitative understanding and interpretation of these explanatory factors in\nneural representations is difficult to achieve due to the complex and\nintertwined correspondence of neural activities with emergent behaviors. In\nthis paper, we design a new algorithm that programmatically extracts tree\nrepresentations from compact neural policies, in the form of a set of logic\nprograms grounded by the world state. To assess how well networks uncover the\ndynamics of the task and their factors of variation, we introduce\ninterpretability metrics that measure the disentanglement of learned neural\ndynamics from a concentration of decisions, mutual information, and modularity\nperspectives. Moreover, our method allows us to quantify how accurate the\nextracted decision paths (explanations) are and computes cross-neuron logic\nconflict. We demonstrate the effectiveness of our approach with several types\nof compact network architectures on a series of end-to-end learning to control\ntasks.",
    "descriptor": "",
    "authors": [
      "Tsun-Hsuan Wang",
      "Wei Xiao",
      "Tim Seyde",
      "Ramin Hasani",
      "Daniela Rus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Robotics (cs.RO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06650"
  },
  {
    "id": "arXiv:2210.06651",
    "title": "Asymptotic expansion regularization for inverse source problems in  two-dimensional singularly perturbed nonlinear parabolic PDEs",
    "abstract": "In this paper, we develop an asymptotic expansion-regularization (AER) method\nfor inverse source problems in two-dimensional nonlinear and nonstationary\nsingularly perturbed partial differential equations (PDEs). The key idea of\nthis approach is the use of the asymptotic-expansion theory, which allows us to\ndetermine the conditions for the existence and uniqueness of a solution to a\ngiven PDE with a sharp transition layer. As a by-product, we derive a simpler\nlink equation between the source function and first-order asymptotic\napproximation of the measurable quantities, and based on that equation we\npropose an efficient inversion algorithm, AER, for inverse source problems. We\nprove that this simplification will not decrease the accuracy of the inversion\nresult, especially for inverse problems with noisy data. Various numerical\nexamples are provided to demonstrate the efficiency of our new approach.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2210.05220; text overlap with arXiv:2106.15249\n",
    "authors": [
      "Dmitrii Chaikovskii",
      "Aleksei Liubavin",
      "Ye Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06651"
  },
  {
    "id": "arXiv:2210.06654",
    "title": "The Inventory is Dark and Full of Misinformation: Understanding the  Abuse of Ad Inventory Pooling in the Ad-Tech Supply Chain",
    "abstract": "Ad-tech enables publishers to programmatically sell their ad inventory to\nmillions of demand partners through a complex supply chain. Bogus or low\nquality publishers can exploit the opaque nature of the ad-tech to deceptively\nmonetize their ad inventory. In this paper, we investigate for the first time\nhow misinformation sites subvert the ad-tech transparency standards and pool\ntheir ad inventory with unrelated sites to circumvent brand safety protections.\nWe find that a few major ad exchanges are disproportionately responsible for\nthe dark pools that are exploited by misinformation websites. We further find\nevidence that dark pooling allows misinformation sites to deceptively sell\ntheir ad inventory to reputable brands. We conclude with a discussion of\npotential countermeasures such as better vetting of ad exchange partners,\nadoption of new ad-tech transparency standards that enable end-to-end\nvalidation of the ad-tech supply chain, as well as widespread deployment of\nindependent audits like ours.",
    "descriptor": "",
    "authors": [
      "Yash Vekaria",
      "Rishab Nithyanand",
      "Zubair Shafiq"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Networking and Internet Architecture (cs.NI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.06654"
  },
  {
    "id": "arXiv:2210.06656",
    "title": "Knowledge-grounded Dialog State Tracking",
    "abstract": "Knowledge (including structured knowledge such as schema and ontology, and\nunstructured knowledge such as web corpus) is a critical part of dialog\nunderstanding, especially for unseen tasks and domains. Traditionally, such\ndomain-specific knowledge is encoded implicitly into model parameters for the\nexecution of downstream tasks, which makes training inefficient. In addition,\nsuch models are not easily transferable to new tasks with different schemas. In\nthis work, we propose to perform dialog state tracking grounded on knowledge\nencoded externally. We query relevant knowledge of various forms based on the\ndialog context where such information can ground the prediction of dialog\nstates. We demonstrate superior performance of our proposed method over strong\nbaselines, especially in the few-shot learning setting.",
    "descriptor": "\nComments: EMNLP 2022 Findings\n",
    "authors": [
      "Dian Yu",
      "Mingqiu Wang",
      "Yuan Cao",
      "Izhak Shafran",
      "Laurent El Shafey",
      "Hagen Soltau"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06656"
  },
  {
    "id": "arXiv:2210.06658",
    "title": "Nonvolatile Electrochemical Random-Access Memory Under Short Circuit",
    "abstract": "Electrochemical random-access memory (ECRAM) is a recently developed and\nhighly promising analog resistive memory element for in-memory computing. One\nlongstanding challenge of ECRAM is attaining retention time beyond a few hours.\nThis short retention has precluded ECRAM from being considered for inference\nclassification in deep neural networks, which is likely the largest opportunity\nfor in-memory computing. In this work, we develop an ECRAM cell with orders of\nmagnitude longer retention than previously achieved, and which we anticipate to\nexceed 10 years at 85C. We hypothesize that the origin of this exceptional\nretention is phase separation, which enables the formation of multiple\neffectively equilibrium resistance states. This work highlights the promises\nand opportunities to use phase separation to yield ECRAM cells with\nexceptionally long, and potentially permanent, retention times.",
    "descriptor": "\nComments: 14 pages, 4 figures\n",
    "authors": [
      "Diana Kim",
      "Virgil Watkins",
      "Laszlo Cline",
      "Jingxian Li",
      "Kai Sun",
      "Joshua D. Sugar",
      "Elliot J. Fuller",
      "A. Alec Talin",
      "Yiyang Li"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.06658"
  },
  {
    "id": "arXiv:2210.06659",
    "title": "Structural Pruning via Latency-Saliency Knapsack",
    "abstract": "Structural pruning can simplify network architecture and improve inference\nspeed. We propose Hardware-Aware Latency Pruning (HALP) that formulates\nstructural pruning as a global resource allocation optimization problem, aiming\nat maximizing the accuracy while constraining latency under a predefined budget\non targeting device. For filter importance ranking, HALP leverages latency\nlookup table to track latency reduction potential and global saliency score to\ngauge accuracy drop. Both metrics can be evaluated very efficiently during\npruning, allowing us to reformulate global structural pruning under a reward\nmaximization problem given target constraint. This makes the problem solvable\nvia our augmented knapsack solver, enabling HALP to surpass prior work in\npruning efficacy and accuracy-efficiency trade-off. We examine HALP on both\nclassification and detection tasks, over varying networks, on ImageNet and VOC\ndatasets, on different platforms. In particular, for ResNet-50/-101 pruning on\nImageNet, HALP improves network throughput by $1.60\\times$/$1.90\\times$ with\n$+0.3\\%$/$-0.2\\%$ top-1 accuracy changes, respectively. For SSD pruning on VOC,\nHALP improves throughput by $1.94\\times$ with only a $0.56$ mAP drop. HALP\nconsistently outperforms prior art, sometimes by large margins. Project page at\nhttps://halp-neurips.github.io/.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. arXiv admin note: substantial text overlap with arXiv:2110.10811\n",
    "authors": [
      "Maying Shen",
      "Hongxu Yin",
      "Pavlo Molchanov",
      "Lei Mao",
      "Jianna Liu",
      "Jose M. Alvarez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06659"
  },
  {
    "id": "arXiv:2210.06662",
    "title": "Action Matching: A Variational Method for Learning Stochastic Dynamics  from Samples",
    "abstract": "Stochastic dynamics are ubiquitous in many fields of science, from the\nevolution of quantum systems in physics to diffusion-based models in machine\nlearning. Existing methods such as score matching can be used to simulate these\nphysical processes by assuming that the dynamics is a diffusion, which is not\nalways the case. In this work, we propose a method called \"Action Matching\"\nthat enables us to learn a much broader family of stochastic dynamics. Our\nmethod requires access only to samples from different time-steps, makes no\nexplicit assumptions about the underlying dynamics, and can be applied even\nwhen samples are uncorrelated (i.e., are not part of a trajectory). Action\nMatching directly learns an underlying mechanism to move samples in time\nwithout modeling the distributions at each time-step. In this work, we showcase\nhow Action Matching can be used for several computer vision tasks such as\ngenerative modeling, super-resolution, colorization, and inpainting; and\nfurther discuss potential applications in other areas of science.",
    "descriptor": "",
    "authors": [
      "Kirill Neklyudov",
      "Daniel Severo",
      "Alireza Makhzani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06662"
  },
  {
    "id": "arXiv:2210.06663",
    "title": "A Logical Framework with Higher-Order Rational (Circular) Terms",
    "abstract": "Logical frameworks provide natural and direct ways of specifying and\nreasoning within deductive systems. The logical framework LF and subsequent\ndevelopments focus on finitary proof systems, making the formalization of\ncircular proof systems in such logical frameworks a cumbersome and awkward\ntask. To address this issue, we propose CoLF, a conservative extension of LF\nwith higher-order rational terms and mixed inductive and coinductive\ndefinitions. In this framework, two terms are equal if they unfold to the same\ninfinite regular B\\\"ohm tree. Both term equality and type checking are\ndecidable in CoLF. We illustrate the elegance and expressive power of the\nframework with several small case studies.",
    "descriptor": "",
    "authors": [
      "Zhibo Chen",
      "Frank Pfenning"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.06663"
  },
  {
    "id": "arXiv:2210.06667",
    "title": "Understanding the Effect of Smartphone Cameras on Estimating Munsell  Soil Colors from Imagery",
    "abstract": "The Munsell soil color chart (MSCC) is a in laboratories under controlled\nconditions. To support an appbased solution, this paper explores three research\nareas including: (i) identifying the most effective color space, (ii)\nestablishing then important reference for many professionals in the area of\nsoil color analysis. Currently, the functionality to identify Munsell soil\ncolors (MSCs) automatically from an image is only feasible color difference\ncalculation method with the highest accuracy and (iii) evaluating the effects\nof smartphone cameras on estimating the MSCs. The existing methods that we have\nanalysed have returned promising results and will help inform other researchers\nto better understand and develop informed solutions. This study provides both\nresearchers and developers with an insight into the best methods for\nautomatically predicting MSCs. Future research is needed to improve the\nreliability of results under differing environmental conditions.",
    "descriptor": "\nComments: 8 pages, 4 figures, accepted to publish in DICTA 2022 conference proceedings\n",
    "authors": [
      "Ricky Sinclair",
      "Muhammad Ashad Kabir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06667"
  },
  {
    "id": "arXiv:2210.06670",
    "title": "A Game Theoretical vulnerability analysis of Adversarial Attack",
    "abstract": "In recent times deep learning has been widely used for automating various\nsecurity tasks in Cyber Domains. However, adversaries manipulate data in many\nsituations and diminish the deployed deep learning model's accuracy. One\nnotable example is fooling CAPTCHA data to access the CAPTCHA-based Classifier\nleading to the critical system being vulnerable to cybersecurity attacks. To\nalleviate this, we propose a computational framework of game theory to analyze\nthe CAPTCHA-based Classifier's vulnerability, strategy, and outcomes by forming\na simultaneous two-player game. We apply the Fast Gradient Symbol Method (FGSM)\nand One Pixel Attack on CAPTCHA Data to imitate real-life scenarios of possible\ncyber-attack. Subsequently, to interpret this scenario from a Game theoretical\nperspective, we represent the interaction in the Stackelberg Game in Kuhn tree\nto study players' possible behaviors and actions by applying our Classifier's\nactual predicted values. Thus, we interpret potential attacks in deep learning\napplications while representing viable defense strategies in the game theory\nprospect.",
    "descriptor": "\nComments: Accepted in 17th International Symposium on Visual Computing,2022\n",
    "authors": [
      "Khondker Fariha Hossain",
      "Alireza Tavakkoli",
      "Shamik Sengupta"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.06670"
  },
  {
    "id": "arXiv:2210.06671",
    "title": "Wasserstein Barycenter-based Model Fusion and Linear Mode Connectivity  of Neural Networks",
    "abstract": "Based on the concepts of Wasserstein barycenter (WB) and Gromov-Wasserstein\nbarycenter (GWB), we propose a unified mathematical framework for neural\nnetwork (NN) model fusion and utilize it to reveal new insights about the\nlinear mode connectivity of SGD solutions. In our framework, the fusion occurs\nin a layer-wise manner and builds on an interpretation of a node in a network\nas a function of the layer preceding it. The versatility of our mathematical\nframework allows us to talk about model fusion and linear mode connectivity for\na broad class of NNs, including fully connected NN, CNN, ResNet, RNN, and LSTM,\nin each case exploiting the specific structure of the network architecture. We\npresent extensive numerical experiments to: 1) illustrate the strengths of our\napproach in relation to other model fusion methodologies and 2) from a certain\nperspective, provide new empirical evidence for recent conjectures which say\nthat two local minima found by gradient-based methods end up lying on the same\nbasin of the loss landscape after a proper permutation of weights is applied to\none of the models.",
    "descriptor": "",
    "authors": [
      "Aditya Kumar Akash",
      "Sixu Li",
      "Nicol\u00e1s Garc\u00eda Trillos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06671"
  },
  {
    "id": "arXiv:2210.06676",
    "title": "A Tagging Solution to Discover IoT Devices in Apartments",
    "abstract": "The number of IoT devices in smart homes is increasing. This broad adoption\nfacilitates users' lives, but it also brings problems. One such issue is that\nsome IoT devices may invade users' privacy. Some reasons for this invasion can\nstem from obscure data collection practices or hidden devices. Specific IoT\ndevices can exist out of sight and still collect user data to send to third\nparties via the Internet. Owners can easily forget the location or even the\nexistence of these devices, especially if the owner is a landlord who manages\nseveral properties. The landlord-owner scenario creates multi-user problems as\ndesigners build machines for single users. We developed tags that use wireless\nprotocols, buzzers, and LED lighting to lead users to solve the issue of device\ndiscovery in shared spaces and accommodate multi-user scenarios. They are\nattached to IoT devices inside a unit during their installation to be later\ndiscovered by a tenant. These tags have similar functionalities as the popular\nTile models or Airtag, but our tags have different features based on our\nprivacy use case. Our tags do not require pairing; multiple users can interact\nwith them through our Android application. Although researchers developed\nseveral other tools, such as thermal cameras or virtual reality (VR), for\ndiscovering devices in environments, they have not used wireless protocols as a\nsolution. We measured specific performance metrics of our tags to analyze their\nfeasibility for this problem. We also conducted a user study to measure the\nparticipants' comfort levels while finding objects with our tags attached. Our\nresults indicate that wireless tags can be viable for device tracking in\nresidential properties.",
    "descriptor": "",
    "authors": [
      "Berkay Kaplan",
      "Jingyu Qian",
      "Israel J Lopez-Toledo",
      "Carl A. Gunter"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06676"
  },
  {
    "id": "arXiv:2210.06679",
    "title": "A Survey on UAV-enabled Edge Computing: Resource Management Perspective",
    "abstract": "Edge computing facilitates low-latency services at the network's edge by\ndistributing computation, communication, and storage resources within the\ngeographic proximity of mobile and Internet-of-Things (IoT) devices. The recent\nadvancement in Unmanned Aerial Vehicles (UAVs) technologies has opened new\nopportunities for edge computing in military operations, disaster response, or\nremote areas where traditional terrestrial networks are limited or unavailable.\nIn such environments, UAVs can be deployed as aerial edge servers or relays to\nfacilitate edge computing services. This form of computing is also known as\nUAV-enabled Edge Computing (UEC), which offers several unique benefits such as\nmobility, line-of-sight, flexibility, computational capability, and\ncost-efficiency. However, the resources on UAVs, edge servers, and IoT devices\nare typically very limited in the context of UEC. Efficient resource management\nis, therefore, a critical research challenge in UEC. In this article, we\npresent a survey on the existing research in UEC from the resource management\nperspective. We identify a conceptual architecture, different types of\ncollaborations, wireless communication models, research directions, key\ntechniques and performance indicators for resource management in UEC. We also\npresent a taxonomy of resource management in UEC. Finally, we identify and\ndiscuss some open research challenges that can stimulate future research\ndirections for resource management in UEC.",
    "descriptor": "\nComments: 36 pages, submitted to ACM CSUR for Review\n",
    "authors": [
      "Xiaoyu Xia",
      "Sheik Mohammad Mostakim Fattah",
      "Muhammad Ali Babar"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06679"
  },
  {
    "id": "arXiv:2210.06680",
    "title": "Walk a Mile in Their Shoes: a New Fairness Criterion for Machine  Learning",
    "abstract": "The old empathetic adage, ``Walk a mile in their shoes,'' asks that one\nimagine the difficulties others may face. This suggests a new ML counterfactual\nfairness criterion, based on a \\textit{group} level: How would members of a\nnonprotected group fare if their group were subject to conditions in some\nprotected group? Instead of asking what sentence would a particular Caucasian\nconvict receive if he were Black, take that notion to entire groups; e.g. how\nwould the average sentence for all White convicts change if they were Black,\nbut with their same White characteristics, e.g. same number of prior\nconvictions? We frame the problem and study it empirically, for different\ndatasets. Our approach also is a solution to the problem of covariate\ncorrelation with sensitive attributes.",
    "descriptor": "",
    "authors": [
      "Norman Matloff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.06680"
  },
  {
    "id": "arXiv:2210.06681",
    "title": "Brain Network Transformer",
    "abstract": "Human brains are commonly modeled as networks of Regions of Interest (ROIs)\nand their connections for the understanding of brain functions and mental\ndisorders. Recently, Transformer-based models have been studied over different\ntypes of data, including graphs, shown to bring performance gains widely. In\nthis work, we study Transformer-based models for brain network analysis. Driven\nby the unique properties of data, we model brain networks as graphs with nodes\nof fixed size and order, which allows us to (1) use connection profiles as node\nfeatures to provide natural and low-cost positional information and (2) learn\npair-wise connection strengths among ROIs with efficient attention weights\nacross individuals that are predictive towards downstream analysis tasks.\nMoreover, we propose an Orthonormal Clustering Readout operation based on\nself-supervised soft clustering and orthonormal projection. This design\naccounts for the underlying functional modules that determine similar behaviors\namong groups of ROIs, leading to distinguishable cluster-aware node embeddings\nand informative graph embeddings. Finally, we re-standardize the evaluation\npipeline on the only one publicly available large-scale brain network dataset\nof ABIDE, to enable meaningful comparison of different models. Experiment\nresults show clear improvements of our proposed Brain Network Transformer on\nboth the public ABIDE and our restricted ABCD datasets. The implementation is\navailable at https://github.com/Wayfear/BrainNetworkTransformer.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022. The previous version is accepted for Workshop ICML-IMLH 2022 (Oral, no proceedings)\n",
    "authors": [
      "Xuan Kan",
      "Wei Dai",
      "Hejie Cui",
      "Zilong Zhang",
      "Ying Guo",
      "Carl Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06681"
  },
  {
    "id": "arXiv:2210.06682",
    "title": "Application-Driven AI Paradigm for Hand-Held Action Detection",
    "abstract": "In practical applications especially with safety requirement, some hand-held\nactions need to be monitored closely, including smoking cigarettes, dialing,\neating, etc. Taking smoking cigarettes as example, existing smoke detection\nalgorithms usually detect the cigarette or cigarette with hand as the target\nobject only, which leads to low accuracy. In this paper, we propose an\napplication-driven AI paradigm for hand-held action detection based on\nhierarchical object detection. It is a coarse-to-fine hierarchical detection\nframework composed of two modules. The first one is a coarse detection module\nwith the human pose consisting of the whole hand, cigarette and head as target\nobject. The followed second one is a fine detection module with the fingers\nholding cigarette, mouth area and the whole cigarette as target. Some\nexperiments are done with the dataset collected from real-world scenarios, and\nthe results show that the proposed framework achieve higher detection rate with\ngood adaptation and robustness in complex environments.",
    "descriptor": "",
    "authors": [
      "Kohou Wang",
      "Zhaoxiang Liu",
      "Shiguo Lian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06682"
  },
  {
    "id": "arXiv:2210.06683",
    "title": "Augmenting Flight Training with AI to Efficiently Train Pilots",
    "abstract": "We propose an AI-based pilot trainer to help students learn how to fly\naircraft. First, an AI agent uses behavioral cloning to learn flying maneuvers\nfrom qualified flight instructors. Later, the system uses the agent's decisions\nto detect errors made by students and provide feedback to help students correct\ntheir errors. This paper presents an instantiation of the pilot trainer. We\nfocus on teaching straight and level flying maneuvers by automatically\nproviding formative feedback to the human student.",
    "descriptor": "\nComments: 3 pages, 3 figures, submitted to AAAI-23 Demonstration Program\n",
    "authors": [
      "Michael Guevarra",
      "Srijita Das",
      "Christabel Wayllace",
      "Carrie Demmans Epp",
      "Matthew E. Taylor",
      "Alan Tay"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06683"
  },
  {
    "id": "arXiv:2210.06684",
    "title": "Connectivity-Aware Pheromone Mobility Model for Autonomous UAV Networks",
    "abstract": "UAV networks consisting of reduced size, weight, and power (low SWaP)\nfixed-wing UAVs are used for civilian and military applications such as search\nand rescue, surveillance, and tracking. To carry out these operations\nefficiently, there is a need to develop scalable, decentralized autonomous UAV\nnetwork architectures with high network connectivity. However, the area\ncoverage and the network connectivity requirements exhibit a fundamental\ntrade-off. In this paper, a connectivity-aware pheromone mobility (CAP) model\nis designed for search and rescue operations, which is capable of maintaining\nconnectivity among UAVs in the network. We use stigmergy-based digital\npheromone maps along with distance-based local connectivity information to\nautonomously coordinate the UAV movements, in order to improve its map coverage\nefficiency while maintaining high network connectivity.",
    "descriptor": "",
    "authors": [
      "Shreyas Devaraju",
      "Alexander Ihler",
      "Sunil Kumar"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06684"
  },
  {
    "id": "arXiv:2210.06686",
    "title": "Real Spike: Learning Real-valued Spikes for Spiking Neural Networks",
    "abstract": "Brain-inspired spiking neural networks (SNNs) have recently drawn more and\nmore attention due to their event-driven and energy-efficient characteristics.\nThe integration of storage and computation paradigm on neuromorphic hardwares\nmakes SNNs much different from Deep Neural Networks (DNNs). In this paper, we\nargue that SNNs may not benefit from the weight-sharing mechanism, which can\neffectively reduce parameters and improve inference efficiency in DNNs, in some\nhardwares, and assume that an SNN with unshared convolution kernels could\nperform better. Motivated by this assumption, a training-inference decoupling\nmethod for SNNs named as Real Spike is proposed, which not only enjoys both\nunshared convolution kernels and binary spikes in inference-time but also\nmaintains both shared convolution kernels and Real-valued Spikes during\ntraining. This decoupling mechanism of SNN is realized by a re-parameterization\ntechnique. Furthermore, based on the training-inference-decoupled idea, a\nseries of different forms for implementing Real Spike on different levels are\npresented, which also enjoy shared convolutions in the inference and are\nfriendly to both neuromorphic and non-neuromorphic hardware platforms. A\ntheoretical proof is given to clarify that the Real Spike-based SNN network is\nsuperior to its vanilla counterpart. Experimental results show that all\ndifferent Real Spike versions can consistently improve the SNN performance.\nMoreover, the proposed method outperforms the state-of-the-art models on both\nnon-spiking static and neuromorphic datasets.",
    "descriptor": "\nComments: Accepted by ECCV2022\n",
    "authors": [
      "Yufei Guo",
      "Liwen Zhang",
      "Yuanpei Chen",
      "Xinyi Tong",
      "Xiaode Liu",
      "YingLei Wang",
      "Xuhui Huang",
      "Zhe Ma"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06686"
  },
  {
    "id": "arXiv:2210.06688",
    "title": "Overlooked Video Classification in Weakly Supervised Video Anomaly  Detection",
    "abstract": "Current weakly supervised video anomaly detection algorithms mostly use\nmultiple instance learning (MIL) or their varieties. Almost all recent\napproaches focus on how to select the correct snippets for training to improve\nthe performance. They overlook or do not realize the power of video\nclassification in boosting the performance of anomaly detection. In this paper,\nwe study explicitly the power of video classification supervision using a BERT\nor LSTM. With this BERT or LSTM, CNN features of all snippets of a video can be\naggregated into a single feature which can be used for video classification.\nThis simple yet powerful video classification supervision, combined into the\nMIL framework, brings extraordinary performance improvement on all three major\nvideo anomaly detection datasets. Particularly it improves the mean average\nprecision (mAP) on the XD-Violence from SOTA 78.84\\% to new 82.10\\%. The source\ncode is available at\nhttps://github.com/wjtan99/BERT_Anomaly_Video_Classification.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2101.10030 by other authors\n",
    "authors": [
      "Weijun Tan",
      "Qi Yao",
      "Jingfeng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06688"
  },
  {
    "id": "arXiv:2210.06691",
    "title": "Bifurcation Analysis Reveals Solution Structures of Phase Field Models",
    "abstract": "Phase field method is playing an increasingly important role in understanding\nand predicting morphological evolution in materials and biological systems.\nHere, we develop a new analytical approach based on bifurcation analysis to\nexplore the mathematical solution structure of phase field models. Revealing\nsuch solution structures not only is of great mathematical interest but also\nmay provide guidance to experimentally or computationally uncover new\nmorphological evolution phenomena in materials undergoing electronic and\nstructural phase transitions. To elucidate the idea, we apply this analytical\napproach to three representative phase field equations: Allen-Cahn equation,\nCahn-Hilliard equation, and Allen-Cahn-Ohta-Kawasaki system. The solution\nstructures of these three phase field equations are also verified numerically\nby the homotopy continuation method.",
    "descriptor": "",
    "authors": [
      "Xinyue Evelyn Zhao",
      "Long-Qing Chen",
      "Wenrui Hao",
      "Yanxiang Zhao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06691"
  },
  {
    "id": "arXiv:2210.06692",
    "title": "Model-Based Offline Reinforcement Learning with Pessimism-Modulated  Dynamics Belief",
    "abstract": "Model-based offline reinforcement learning (RL) aims to find highly rewarding\npolicy, by leveraging a previously collected static dataset and a dynamics\nmodel. While learned through reuse of static dataset, the dynamics model's\ngeneralization ability hopefully promotes policy learning if properly utilized.\nTo that end, several works propose to quantify the uncertainty of predicted\ndynamics, and explicitly apply it to penalize reward. However, as the dynamics\nand the reward are intrinsically different factors in context of MDP,\ncharacterizing the impact of dynamics uncertainty through reward penalty may\nincur unexpected tradeoff between model utilization and risk avoidance. In this\nwork, we instead maintain a belief distribution over dynamics, and\nevaluate/optimize policy through biased sampling from the belief. The sampling\nprocedure, biased towards pessimism, is derived based on an alternating Markov\ngame formulation of offline RL. We formally show that the biased sampling\nnaturally induces an updated dynamics belief with policy-dependent reweighting\nfactor, termed Pessimism-Modulated Dynamics Belief. To improve policy, we\ndevise an iterative regularized policy optimization algorithm for the game,\nwith guarantee of monotonous improvement under certain condition. To make\npractical, we further devise an offline RL algorithm to approximately find the\nsolution. Empirical results show that the proposed approach achieves\nstate-of-the-art performance on a wide range of benchmark tasks.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Kaiyang Guo",
      "Yunfeng Shao",
      "Yanhui Geng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06692"
  },
  {
    "id": "arXiv:2210.06694",
    "title": "SubeventWriter: Iterative Sub-event Sequence Generation with Coherence  Controller",
    "abstract": "In this paper, we propose a new task of sub-event generation for an unseen\nprocess to evaluate the understanding of the coherence of sub-event actions and\nobjects. To solve the problem, we design SubeventWriter, a sub-event sequence\ngeneration framework with a coherence controller. Given an unseen process, the\nframework can iteratively construct the sub-event sequence by generating one\nsub-event at each iteration. We also design a very effective coherence\ncontroller to decode more coherent sub-events. As our extensive experiments and\nanalysis indicate, SubeventWriter can generate more reliable and meaningful\nsub-event sequences for unseen processes.",
    "descriptor": "\nComments: Accepted to the main conference of EMNLP 2022\n",
    "authors": [
      "Zhaowei Wang",
      "Hongming Zhang",
      "Tianqing Fang",
      "Yangqiu Song",
      "Ginny Y. Wong",
      "Simon See"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06694"
  },
  {
    "id": "arXiv:2210.06696",
    "title": "CPSAA: Accelerating Sparse Attention using Crossbar-based  Processing-In-Memory Architecture",
    "abstract": "The attention mechanism requires huge computational efforts to process\nunnecessary calculations, significantly limiting the system's performance.\nResearchers propose sparse attention to convert some DDMM operations to SDDMM\nand SpMM operations. However, current sparse attention solutions introduce\nmassive off-chip random memory access. We propose CPSAA, a novel crossbar-based\nPIM-featured sparse attention accelerator. First, we present a novel attention\ncalculation mode. Second, we design a novel PIM-based sparsity pruning\narchitecture. Finally, we present novel crossbar-based methods. Experimental\nresults show that CPSAA has an average of 89.6X, 32.2X, 17.8X, 3.39X, and 3.84X\nperformance improvement and 755.6X, 55.3X, 21.3X, 5.7X, and 4.9X energy-saving\nwhen compare with GPU, FPGA, SANGER, ReBERT, and ReTransformer.",
    "descriptor": "\nComments: 14 pages, 19 figures\n",
    "authors": [
      "Huize Li",
      "Hai Jin",
      "Long Zheng",
      "Yu Huang",
      "Xiaofei Liao",
      "Dan Chen",
      "Zhuohui Duan",
      "Cong Liu",
      "Jiahong Xu",
      "Chuanyi Gui"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06696"
  },
  {
    "id": "arXiv:2210.06697",
    "title": "Size Does Matter: An Experimental Study of Anxiety in Virtual Reality",
    "abstract": "The emotional response of users induced by VR scenarios has become a topic of\ninterest, however, whether changing the size of objects in VR scenes induces\ndifferent levels of anxiety remains a question to be studied. In this study, we\nconducted an experiment to initially reveal how the size of a large object in a\nVR environment affects changes in participants' (N = 38) anxiety level and\nheart rate. To holistically quantify the size of large objects in the VR visual\nfield, we used the omnidirectional field of view occupancy (OFVO) criterion for\nthe first time to represent the dimension of the object in the participant's\nentire field of view. The results showed that the participants' heartbeat and\nanxiety while viewing the large objects were positively and significantly\ncorrelated to OFVO. These study reveals that the increase of object size in VR\nenvironments is accompanied by a higher degree of user's anxiety.",
    "descriptor": "\nComments: to appear in VRST 2022\n",
    "authors": [
      "Junyi Shen",
      "Itaru Kitahara",
      "Shinichi Koyama",
      "Qiaoge Li"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06697"
  },
  {
    "id": "arXiv:2210.06698",
    "title": "A Near-Sensor Processing Accelerator for Approximate Local Binary  Pattern Networks",
    "abstract": "In this work, a high-speed and energy-efficient comparator-based Near-Sensor\nLocal Binary Pattern accelerator architecture (NS-LBP) is proposed to execute a\nnovel local binary pattern deep neural network. First, inspired by recent LBP\nnetworks, we design an approximate, hardware-oriented, and multiply-accumulate\n(MAC)-free network named Ap-LBP for efficient feature extraction, further\nreducing the computation complexity. Then, we develop NS-LBP as a\nprocessing-in-SRAM unit and a parallel in-memory LBP algorithm to process\nimages near the sensor in a cache, remarkably reducing the power consumption of\ndata transmission to an off-chip processor. Our circuit-to-application\nco-simulation results on MNIST and SVHN data-sets demonstrate minor accuracy\ndegradation compared to baseline CNN and LBP-network models, while NS-LBP\nachieves 1.25 GHz and energy-efficiency of 37.4 TOPS/W. NS-LBP reduces energy\nconsumption by 2.2x and execution time by a factor of 4x compared to the best\nrecent LBP-based networks.",
    "descriptor": "\nComments: 10 pages, 11 figures, 4 tables\n",
    "authors": [
      "Shaahin Angizi",
      "Mehrdad Morsali",
      "Sepehr Tabrizchi",
      "Arman Roohi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.06698"
  },
  {
    "id": "arXiv:2210.06699",
    "title": "Parameter-Efficient Masking Networks",
    "abstract": "A deeper network structure generally handles more complicated non-linearity\nand performs more competitively. Nowadays, advanced network designs often\ncontain a large number of repetitive structures (e.g., Transformer). They\nempower the network capacity to a new level but also increase the model size\ninevitably, which is unfriendly to either model restoring or transferring. In\nthis study, we are the first to investigate the representative potential of\nfixed random weights with limited unique values by learning diverse masks and\nintroduce the Parameter-Efficient Masking Networks (PEMN). It also naturally\nleads to a new paradigm for model compression to diminish the model size.\nConcretely, motivated by the repetitive structures in modern neural networks,\nwe utilize one random initialized layer, accompanied with different masks, to\nconvey different feature mappings and represent repetitive network modules.\nTherefore, the model can be expressed as \\textit{one-layer} with a bunch of\nmasks, which significantly reduce the model storage cost. Furthermore, we\nenhance our strategy by learning masks for a model filled by padding a given\nrandom weights vector. In this way, our method can further lower the space\ncomplexity, especially for models without many repetitive architectures. We\nvalidate the potential of PEMN learning masks on random weights with limited\nunique values and test its effectiveness for a new compression paradigm based\non different network architectures. Code is available at\nhttps://github.com/yueb17/PEMN",
    "descriptor": "",
    "authors": [
      "Yue Bai",
      "Huan Wang",
      "Xu Ma",
      "Yitian Zhang",
      "Zhiqiang Tao",
      "Yun Fu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06699"
  },
  {
    "id": "arXiv:2210.06701",
    "title": "Empirical Evaluation of Data Augmentations for Biobehavioral Time Series  Data with Deep Learning",
    "abstract": "Deep learning has performed remarkably well on many tasks recently. However,\nthe superior performance of deep models relies heavily on the availability of a\nlarge number of training data, which limits the wide adaptation of deep models\non various clinical and affective computing tasks, as the labeled data are\nusually very limited. As an effective technique to increase the data\nvariability and thus train deep models with better generalization, data\naugmentation (DA) is a critical step for the success of deep learning models on\nbiobehavioral time series data. However, the effectiveness of various DAs for\ndifferent datasets with different tasks and deep models is understudied for\nbiobehavioral time series data. In this paper, we first systematically review\neight basic DA methods for biobehavioral time series data, and evaluate the\neffects on seven datasets with three backbones. Next, we explore adapting more\nrecent DA techniques (i.e., automatic augmentation, random augmentation) to\nbiobehavioral time series data by designing a new policy architecture\napplicable to time series data. Last, we try to answer the question of why a DA\nis effective (or not) by first summarizing two desired attributes for\naugmentations (challenging and faithful), and then utilizing two metrics to\nquantitatively measure the corresponding attributes, which can guide us in the\nsearch for more effective DA for biobehavioral time series data by designing\nmore challenging but still faithful transformations. Our code and results are\navailable at Link.",
    "descriptor": "\nComments: 8 pages, 5 figures, 2 tables, 3 pages appendix\n",
    "authors": [
      "Huiyuan Yang",
      "Han Yu",
      "Akane Sano"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06701"
  },
  {
    "id": "arXiv:2210.06702",
    "title": "A Mixture of Surprises for Unsupervised Reinforcement Learning",
    "abstract": "Unsupervised reinforcement learning aims at learning a generalist policy in a\nreward-free manner for fast adaptation to downstream tasks. Most of the\nexisting methods propose to provide an intrinsic reward based on surprise.\nMaximizing or minimizing surprise drives the agent to either explore or gain\ncontrol over its environment. However, both strategies rely on a strong\nassumption: the entropy of the environment's dynamics is either high or low.\nThis assumption may not always hold in real-world scenarios, where the entropy\nof the environment's dynamics may be unknown. Hence, choosing between the two\nobjectives is a dilemma. We propose a novel yet simple mixture of policies to\naddress this concern, allowing us to optimize an objective that simultaneously\nmaximizes and minimizes the surprise. Concretely, we train one mixture\ncomponent whose objective is to maximize the surprise and another whose\nobjective is to minimize the surprise. Hence, our method does not make\nassumptions about the entropy of the environment's dynamics. We call our method\na $\\textbf{M}\\text{ixture }\\textbf{O}\\text{f\n}\\textbf{S}\\text{urprise}\\textbf{S}$ (MOSS) for unsupervised reinforcement\nlearning. Experimental results show that our simple method achieves\nstate-of-the-art performance on the URLB benchmark, outperforming previous pure\nsurprise maximization-based objectives. Our code is available at:\nhttps://github.com/LeapLabTHU/MOSS.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Andrew Zhao",
      "Matthieu Gaetan Lin",
      "Yangguang Li",
      "Yong-Jin Liu",
      "Gao Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06702"
  },
  {
    "id": "arXiv:2210.06703",
    "title": "On the Minimum Cycle Cover problem on graphs with bounded co-degeneracy",
    "abstract": "In 2021, Duarte, Oliveira, and Souza [MFCS 2021] showed some problems that\nare FPT when parameterized by the treewidth of the complement graph (called\nco-treewidth). Since the degeneracy of a graph is at most its treewidth, they\nalso introduced the study of co-degeneracy (the degeneracy of the complement\ngraph) as a parameter. In 1976, Bondy and Chv\\'{a}tal [DM 1976] introduced the\nnotion of closure of a graph: let $\\ell$ be an integer; the $(n+\\ell)$-closure,\n$\\operatorname{cl}_{n+\\ell}(G)$, of a graph $G$ with $n$ vertices is obtained\nfrom $G$ by recursively adding an edge between pairs of nonadjacent vertices\nwhose degree sum is at least $n+\\ell$ until no such pair remains. A graph\nproperty $\\Upsilon$ defined on all graphs of order $n$ is said to be\n$(n+\\ell)$-stable if for any graph $G$ of order $n$ that does not satisfy\n$\\Upsilon$, the fact that $uv$ is not an edge of $G$ and that $G+uv$ satisfies\n$\\Upsilon$ implies $d(u)+d(v)< n+\\ell$. Duarte et al. [MFCS 2021] developed an\nalgorithmic framework for co-degeneracy parameterization based on the notion of\nclosures for solving problems that are $(n+\\ell)$-stable for some $\\ell$\nbounded by a function of the co-degeneracy. In this paper, we first determine\nthe stability of the property of having a bounded cycle cover. After that,\ncombining the framework of Duarte et al. [MFCS 2021] with some results of\nJansen, Kozma, and Nederlof [WG 2019], we obtain a $2^{\\mathcal{O}(k)}\\cdot\nn^{\\mathcal{O}(1)}$-time algorithm for Minimum Cycle Cover on graphs with\nco-degeneracy at most $k$, which generalizes Duarte et al. [MFCS 2021] and\nJansen et al. [WG 2019] results concerning the Hamiltonian Cycle problem.",
    "descriptor": "",
    "authors": [
      "Gabriel L. Duarte",
      "U\u00e9verton S. Souza"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.06703"
  },
  {
    "id": "arXiv:2210.06704",
    "title": "COLLIDER: A Robust Training Framework for Backdoor Data",
    "abstract": "Deep neural network (DNN) classifiers are vulnerable to backdoor attacks. An\nadversary poisons some of the training data in such attacks by installing a\ntrigger. The goal is to make the trained DNN output the attacker's desired\nclass whenever the trigger is activated while performing as usual for clean\ndata. Various approaches have recently been proposed to detect malicious\nbackdoored DNNs. However, a robust, end-to-end training approach, like\nadversarial training, is yet to be discovered for backdoor poisoned data. In\nthis paper, we take the first step toward such methods by developing a robust\ntraining framework, COLLIDER, that selects the most prominent samples by\nexploiting the underlying geometric structures of the data. Specifically, we\neffectively filter out candidate poisoned data at each training epoch by\nsolving a geometrical coreset selection objective. We first argue how clean\ndata samples exhibit (1) gradients similar to the clean majority of data and\n(2) low local intrinsic dimensionality (LID). Based on these criteria, we\ndefine a novel coreset selection objective to find such samples, which are used\nfor training a DNN. We show the effectiveness of the proposed method for robust\ntraining of DNNs on various poisoned datasets, reducing the backdoor success\nrate significantly.",
    "descriptor": "\nComments: Accepted to the 16th Asian Conference on Computer Vision (ACCV 2022)\n",
    "authors": [
      "Hadi M. Dolatabadi",
      "Sarah Erfani",
      "Christopher Leckie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06704"
  },
  {
    "id": "arXiv:2210.06705",
    "title": "From Gradient Flow on Population Loss to Learning with Stochastic  Gradient Descent",
    "abstract": "Stochastic Gradient Descent (SGD) has been the method of choice for learning\nlarge-scale non-convex models. While a general analysis of when SGD works has\nbeen elusive, there has been a lot of recent progress in understanding the\nconvergence of Gradient Flow (GF) on the population loss, partly due to the\nsimplicity that a continuous-time analysis buys us. An overarching theme of our\npaper is providing general conditions under which SGD converges, assuming that\nGF on the population loss converges. Our main tool to establish this connection\nis a general converse Lyapunov like theorem, which implies the existence of a\nLyapunov potential under mild assumptions on the rates of convergence of GF. In\nfact, using these potentials, we show a one-to-one correspondence between rates\nof convergence of GF and geometrical properties of the underlying objective.\nWhen these potentials further satisfy certain self-bounding properties, we show\nthat they can be used to provide a convergence guarantee for Gradient Descent\n(GD) and SGD (even when the paths of GF and GD/SGD are quite far apart). It\nturns out that these self-bounding assumptions are in a sense also necessary\nfor GD/SGD to work. Using our framework, we provide a unified analysis for\nGD/SGD not only for classical settings like convex losses, or objectives that\nsatisfy PL / KL properties, but also for more complex problems including Phase\nRetrieval and Matrix sq-root, and extending the results in the recent work of\nChatterjee 2022.",
    "descriptor": "",
    "authors": [
      "Satyen Kale",
      "Jason D. Lee",
      "Chris De Sa",
      "Ayush Sekhari",
      "Karthik Sridharan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.06705"
  },
  {
    "id": "arXiv:2210.06706",
    "title": "Jointly Reinforced User Simulator and Task-oriented Dialog System with  Simplified Generative Architecture",
    "abstract": "Recently, there has been progress in supervised funetuning pretrained GPT-2\nto build end-to-end task-oriented dialog (TOD) systems. However, online\nreinforcement learning of a GPT-2 based dialog system (DS), together with a\nend-to-end user simulator (US), has not ever been explored. Moreover, a\ndrawback with existing GPT-2 based TOD systems is that they mostly employ the\nwhole dialog history as input, which brings inefficiencies in memory and\ncompute. In this paper, we first propose Simplified Generative Architectures\n(SGA) for DS and US respectively, both based on GPT-2 but using shortened\nhistory. Then, we successfully develop Jointly Reinforced US and DS, called\nSGA-JRUD. Our DS with the proposed SGA, when only supervised trained, achieves\nstate-of-the-art performance on MultiWOZ2.1 and is more compute-efficient in\nboth training and generation. Extensive experiments on MultiWOZ2.1 further show\nthe superiority of SGA-JRUD in both offline and online evaluations.",
    "descriptor": "\nComments: An early version of Markovian Generative Architectures (MGA) and Generative User Simulator (GUS)\n",
    "authors": [
      "Hong Liu",
      "Zhijian Ou",
      "Yi Huang",
      "Junlan Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06706"
  },
  {
    "id": "arXiv:2210.06707",
    "title": "Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer",
    "abstract": "The large pre-trained vision transformers (ViTs) have demonstrated remarkable\nperformance on various visual tasks, but suffer from expensive computational\nand memory cost problems when deployed on resource-constrained devices. Among\nthe powerful compression approaches, quantization extremely reduces the\ncomputation and memory consumption by low-bit parameters and bit-wise\noperations. However, low-bit ViTs remain largely unexplored and usually suffer\nfrom a significant performance drop compared with the real-valued counterparts.\nIn this work, through extensive empirical analysis, we first identify the\nbottleneck for severe performance drop comes from the information distortion of\nthe low-bit quantized self-attention map. We then develop an information\nrectification module (IRM) and a distribution guided distillation (DGD) scheme\nfor fully quantized vision transformers (Q-ViT) to effectively eliminate such\ndistortion, leading to a fully quantized ViTs. We evaluate our methods on\npopular DeiT and Swin backbones. Extensive experimental results show that our\nmethod achieves a much better performance than the prior arts. For example, our\nQ-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9%\nTop-1 accuracy, even surpassing the full-precision counterpart by 1.0% on\nImageNet dataset. Our codes and models are attached on\nhttps://github.com/YanjingLi0202/Q-ViT",
    "descriptor": "\nComments: Accepted by NeurIPS2022\n",
    "authors": [
      "Yanjing Li",
      "Sheng Xu",
      "Baochang Zhang",
      "Xianbin Cao",
      "Peng Gao",
      "Guodong Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06707"
  },
  {
    "id": "arXiv:2210.06709",
    "title": "Categorizing Semantic Representations for Neural Machine Translation",
    "abstract": "Modern neural machine translation (NMT) models have achieved competitive\nperformance in standard benchmarks. However, they have recently been shown to\nsuffer limitation in compositional generalization, failing to effectively learn\nthe translation of atoms (e.g., words) and their semantic composition (e.g.,\nmodification) from seen compounds (e.g., phrases), and thus suffering from\nsignificantly weakened translation performance on unseen compounds during\ninference. We address this issue by introducing categorization to the source\ncontextualized representations. The main idea is to enhance generalization by\nreducing sparsity and overfitting, which is achieved by finding prototypes of\ntoken representations over the training set and integrating their embeddings\ninto the source encoding. Experiments on a dedicated MT dataset (i.e.,\nCoGnition) show that our method reduces compositional generalization error\nrates by 24\\% error reduction. In addition, our conceptually simple method\ngives consistently better results than the Transformer baseline on a range of\ngeneral MT datasets.",
    "descriptor": "\nComments: COLING 2022\n",
    "authors": [
      "Yongjing Yin",
      "Yafu Li",
      "Fandong Meng",
      "Jie Zhou",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06709"
  },
  {
    "id": "arXiv:2210.06710",
    "title": "Large Language Models are few(1)-shot Table Reasoners",
    "abstract": "Recent literature has shown that large language models (LLMs) are generally\nexcellent few-shot reasoners to solve text reasoning tasks. However, the\ncapability of LLMs on table reasoning tasks is yet to be explored. In this\npaper, we aim at understanding how well LLMs can perform on these table tasks\nwith few-shot in-context learning. Specifically, we evaluate LLMs on popular\ntable QA and fact verification datasets like WikiTableQuestion, FetaQA,\nTabFact, and FEVEROUS and found that LLMs are really competent at complex\nreasoning over table structures. When combined with `chain of thoughts'\nprompting, GPT-3 is able to achieve very strong performance with only a 1-shot\ndemonstration. We further manually study the reasoning chains elicited from\nLLMs and found that these reasoning chains are highly consistent with the\n`ground truth' semantic form. We believe that our study opens new possibilities\nto employ LLMs on different table-based reasoning tasks under few-shot\nscenario.",
    "descriptor": "",
    "authors": [
      "Wenhu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06710"
  },
  {
    "id": "arXiv:2210.06711",
    "title": "Weighted Distillation with Unlabeled Examples",
    "abstract": "Distillation with unlabeled examples is a popular and powerful method for\ntraining deep neural networks in settings where the amount of labeled data is\nlimited: A large ''teacher'' neural network is trained on the labeled data\navailable, and then it is used to generate labels on an unlabeled dataset\n(typically much larger in size). These labels are then utilized to train the\nsmaller ''student'' model which will actually be deployed. Naturally, the\nsuccess of the approach depends on the quality of the teacher's labels, since\nthe student could be confused if trained on inaccurate data. This paper\nproposes a principled approach for addressing this issue based on a\n''debiasing'' reweighting of the student's loss function tailored to the\ndistillation training paradigm. Our method is hyper-parameter free,\ndata-agnostic, and simple to implement. We demonstrate significant improvements\non popular academic datasets and we accompany our results with a theoretical\nanalysis which rigorously justifies the performance of our method in certain\nsettings.",
    "descriptor": "\nComments: To appear in NeurIPS 2022\n",
    "authors": [
      "Fotis Iliopoulos",
      "Vasilis Kontonis",
      "Cenk Baykal",
      "Gaurav Menghani",
      "Khoa Trinh",
      "Erik Vee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06711"
  },
  {
    "id": "arXiv:2210.06714",
    "title": "Perfect matching cuts partitioning a graph into complementary subgraphs",
    "abstract": "In Partition Into Complementary Subgraphs (Comp-Sub) we are given a graph\n$G=(V,E)$, and an edge set property $\\Pi$, and asked whether $G$ can be\ndecomposed into two graphs, $H$ and its complement $\\overline{H}$, for some\ngraph $H$, in such a way that the edge cut $[V(H),V(\\overline{H})]$ satisfies\nthe property $\\Pi$. Motivated by previous work, we consider Comp-Sub($\\Pi$)\nwhen the property $\\Pi=\\mathcal{PM}$ specifies that the edge cut of the\ndecomposition is a perfect matching. We prove that Comp-Sub($\\mathcal{PM}$) is\nGI-hard when the graph $G$ is $\\{C_{k\\geq 7}, \\overline{C}_{k\\geq 7} \\}$-free.\nOn the other hand, we show that Comp-Sub($\\mathcal{PM}$) is polynomial-time\nsolvable on $hole$-free graphs and on $P_5$-free graphs. Furthermore, we\npresent characterizations of Comp-Sub($\\mathcal{PM}$) on chordal,\ndistance-hereditary, and extended $P_4$-laden graphs.",
    "descriptor": "",
    "authors": [
      "Diane Castonguay",
      "Erika M. M. Coelho",
      "Hebert Coelho",
      "Julliano R. Nascimento",
      "U\u00e9verton S. Souza"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.06714"
  },
  {
    "id": "arXiv:2210.06716",
    "title": "Low-resource Neural Machine Translation with Cross-modal Alignment",
    "abstract": "How to achieve neural machine translation with limited parallel data?\nExisting techniques often rely on large-scale monolingual corpora, which is\nimpractical for some low-resource languages. In this paper, we turn to connect\nseveral low-resource languages to a particular high-resource one by additional\nvisual modality. Specifically, we propose a cross-modal contrastive learning\nmethod to learn a shared space for all languages, where both a coarse-grained\nsentence-level objective and a fine-grained token-level one are introduced.\nExperimental results and further analysis show that our method can effectively\nlearn the cross-modal and cross-lingual alignment with a small amount of\nimage-text pairs and achieves significant improvements over the text-only\nbaseline under both zero-shot and few-shot scenarios.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Zhe Yang",
      "Qingkai Fang",
      "Yang Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06716"
  },
  {
    "id": "arXiv:2210.06718",
    "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient",
    "abstract": "We consider a hybrid reinforcement learning setting (Hybrid RL), in which an\nagent has access to an offline dataset and the ability to collect experience\nvia real-world online interaction. The framework mitigates the challenges that\narise in both pure offline and online RL settings, allowing for the design of\nsimple and highly effective algorithms, in both theory and practice. We\ndemonstrate these advantages by adapting the classical Q learning/iteration\nalgorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In\nour theoretical results, we prove that the algorithm is both computationally\nand statistically efficient whenever the offline dataset supports a\nhigh-quality policy and the environment has bounded bilinear rank. Notably, we\nrequire no assumptions on the coverage provided by the initial distribution, in\ncontrast with guarantees for policy gradient/iteration methods. In our\nexperimental results, we show that Hy-Q with neural network function\napproximation outperforms state-of-the-art online, offline, and hybrid RL\nbaselines on challenging benchmarks, including Montezuma's Revenge.",
    "descriptor": "\nComments: 40 pages, 6 figures. Code available at this https URL\n",
    "authors": [
      "Yuda Song",
      "Yifei Zhou",
      "Ayush Sekhari",
      "J. Andrew Bagnell",
      "Akshay Krishnamurthy",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06718"
  },
  {
    "id": "arXiv:2210.06719",
    "title": "Partial Information as Full: Reward Imputation with Sketching in Bandits",
    "abstract": "We focus on the setting of contextual batched bandit (CBB), where a batch of\nrewards is observed from the environment in each episode. But the rewards of\nthe non-executed actions are unobserved (i.e., partial-information feedbacks).\nExisting approaches for CBB usually ignore the rewards of the non-executed\nactions, resulting in feedback information being underutilized. In this paper,\nwe propose an efficient reward imputation approach using sketching for CBB,\nwhich completes the unobserved rewards with the imputed rewards approximating\nthe full-information feedbacks. Specifically, we formulate the reward\nimputation as a problem of imputation regularized ridge regression, which\ncaptures the feedback mechanisms of both the non-executed and executed actions.\nTo reduce the time complexity of reward imputation, we solve the regression\nproblem using randomized sketching. We prove that our reward imputation\napproach obtains a relative-error bound for sketching approximation, achieves\nan instantaneous regret with a controllable bias and a smaller variance than\nthat without reward imputation, and enjoys a sublinear regret bound against the\noptimal policy. Moreover, we present two extensions of our approach, including\nthe rate-scheduled version and the version for nonlinear rewards, making our\napproach more feasible. Experimental results demonstrated that our approach can\noutperform the state-of-the-art baselines on synthetic and real-world datasets.",
    "descriptor": "\nComments: Contextual batch bandits\n",
    "authors": [
      "Xiao Zhang",
      "Ninglu Shao",
      "Zihua Si",
      "Jun Xu",
      "Wenha Wang",
      "Hanjing Su",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06719"
  },
  {
    "id": "arXiv:2210.06720",
    "title": "LIME: Weakly-Supervised Text Classification Without Seeds",
    "abstract": "In weakly-supervised text classification, only label names act as sources of\nsupervision. Predominant approaches to weakly-supervised text classification\nutilize a two-phase framework, where test samples are first assigned\npseudo-labels and are then used to train a neural text classifier. In most\nprevious work, the pseudo-labeling step is dependent on obtaining seed words\nthat best capture the relevance of each class label. We present LIME, a\nframework for weakly-supervised text classification that entirely replaces the\nbrittle seed-word generation process with entailment-based\npseudo-classification. We find that combining weakly-supervised classification\nand textual entailment mitigates shortcomings of both, resulting in a more\nstreamlined and effective classification pipeline. With just an off-the-shelf\ntextual entailment model, LIME outperforms recent baselines in\nweakly-supervised text classification and achieves state-of-the-art in 4\nbenchmarks. We open source our code at https://github.com/seongminp/LIME.",
    "descriptor": "\nComments: COLING 2022. Code at this https URL\n",
    "authors": [
      "Seongmin Park",
      "Jihwa Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06720"
  },
  {
    "id": "arXiv:2210.06722",
    "title": "Few-shot Relational Reasoning via Connection Subgraph Pretraining",
    "abstract": "Few-shot knowledge graph (KG) completion task aims to perform inductive\nreasoning over the KG: given only a few support triplets of a new relation\n$\\bowtie$ (e.g., (chop,$\\bowtie$,kitchen), (read,$\\bowtie$,library), the goal\nis to predict the query triplets of the same unseen relation $\\bowtie$, e.g.,\n(sleep,$\\bowtie$,?). Current approaches cast the problem in a meta-learning\nframework, where the model needs to be first jointly trained over many training\nfew-shot tasks, each being defined by its own relation, so that\nlearning/prediction on the target few-shot task can be effective. However, in\nreal-world KGs, curating many training tasks is a challenging ad hoc process.\nHere we propose Connection Subgraph Reasoner (CSR), which can make predictions\nfor the target few-shot task directly without the need for pre-training on the\nhuman curated set of training tasks. The key to CSR is that we explicitly model\na shared connection subgraph between support and query triplets, as inspired by\nthe principle of eliminative induction. To adapt to specific KG, we design a\ncorresponding self-supervised pretraining scheme with the objective of\nreconstructing automatically sampled connection subgraphs. Our pretrained model\ncan then be directly applied to target few-shot tasks on without the need for\ntraining few-shot tasks. Extensive experiments on real KGs, including NELL,\nFB15K-237, and ConceptNet, demonstrate the effectiveness of our framework: we\nshow that even a learning-free implementation of CSR can already perform\ncompetitively to existing methods on target few-shot tasks; with pretraining,\nCSR can achieve significant gains of up to 52% on the more challenging\ninductive few-shot tasks where the entities are also unseen during\n(pre)training.",
    "descriptor": "\nComments: NeurIPS 2022. Open source implementation at this https URL\n",
    "authors": [
      "Qian Huang",
      "Hongyu Ren",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06722"
  },
  {
    "id": "arXiv:2210.06725",
    "title": "Assessing Out-of-Domain Language Model Performance from Few Examples",
    "abstract": "While pretrained language models have exhibited impressive generalization\ncapabilities, they still behave unpredictably under certain domain shifts. In\nparticular, a model may learn a reasoning process on in-domain training data\nthat does not hold for out-of-domain test data. We address the task of\npredicting out-of-domain (OOD) performance in a few-shot fashion: given a few\ntarget-domain examples and a set of models with similar training performance,\ncan we understand how these models will perform on OOD test data? We benchmark\nthe performance on this task when looking at model accuracy on the few-shot\nexamples, then investigate how to incorporate analysis of the models' behavior\nusing feature attributions to better tackle this problem. Specifically, we\nexplore a set of \"factors\" designed to reveal model agreement with certain\npathological heuristics that may indicate worse generalization capabilities. On\ntextual entailment, paraphrase recognition, and a synthetic classification\ntask, we show that attribution-based factors can help rank relative model OOD\nperformance. However, accuracy on a few-shot test set is a surprisingly strong\nbaseline, particularly when the system designer does not have in-depth prior\nknowledge about the domain shift.",
    "descriptor": "",
    "authors": [
      "Prasann Singhal",
      "Jarad Forristal",
      "Xi Ye",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06725"
  },
  {
    "id": "arXiv:2210.06726",
    "title": "Explanations from Large Language Models Make Small Reasoners Better",
    "abstract": "Integrating free-text explanations to in-context learning of large language\nmodels (LLM) is shown to elicit strong reasoning capabilities along with\nreasonable explanations. In this paper, we consider the problem of leveraging\nthe explanations generated by LLM to improve the training of small reasoners,\nwhich are more favorable in real-production deployment due to their low cost.\nWe systematically explore three explanation generation approaches from LLM and\nutilize a multi-task learning framework to facilitate small models to acquire\nstrong reasoning power together with explanation generation capabilities.\nExperiments on multiple reasoning tasks show that our method can consistently\nand significantly outperform finetuning baselines across different settings,\nand even perform better than finetuning/prompting a 60x larger GPT-3 (175B)\nmodel by up to 9.5% in accuracy. As a side benefit, human evaluation further\nshows that our method can generate high-quality explanations to justify its\npredictions, moving towards the goal of explainable AI.",
    "descriptor": "",
    "authors": [
      "Shiyang Li",
      "Jianshu Chen",
      "Yelong Shen",
      "Zhiyu Chen",
      "Xinlu Zhang",
      "Zekun Li",
      "Hong Wang",
      "Jing Qian",
      "Baolin Peng",
      "Yi Mao",
      "Wenhu Chen",
      "Xifeng Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06726"
  },
  {
    "id": "arXiv:2210.06729",
    "title": "A Stream Learning Approach for Real-Time Identification of False Data  Injection Attacks in Cyber-Physical Power Systems",
    "abstract": "This paper presents a novel data-driven framework to aid in system state\nestimation when the power system is under unobservable false data injection\nattacks. The proposed framework dynamically detects and classifies false data\ninjection attacks. Then, it retrieves the control signal using the acquired\ninformation. This process is accomplished in three main modules, with novel\ndesigns, for detection, classification, and control signal retrieval. The\ndetection module monitors historical changes in phasor measurements and\ncaptures any deviation pattern caused by an attack on a complex plane. This\napproach can help to reveal characteristics of the attacks including the\ndirection, magnitude, and ratio of the injected false data. Using this\ninformation, the signal retrieval module can easily recover the original\ncontrol signal and remove the injected false data. Further information\nregarding the attack type can be obtained through the classifier module. The\nproposed ensemble learner is compatible with harsh learning conditions\nincluding the lack of labeled data, concept drift, concept evolution, recurring\nclasses, and independence from external updates. The proposed novel classifier\ncan dynamically learn from data and classify attacks under all these harsh\nlearning conditions. The introduced framework is evaluated w.r.t. real-world\ndata captured from the Central New York Power System. The obtained results\nindicate the efficacy and stability of the proposed framework.",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Information Forensics and Security\n",
    "authors": [
      "Ehsan Hallaji",
      "Roozbeh Razavi-Far",
      "Meng Wang",
      "Mehrdad Saif",
      "Bruce Fardanesh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06729"
  },
  {
    "id": "arXiv:2210.06732",
    "title": "Equal Improvability: A New Fairness Notion Considering the Long-term  Impact",
    "abstract": "Devising a fair classifier that does not discriminate against different\ngroups is an important problem in machine learning. Although researchers have\nproposed various ways of defining group fairness, most of them only focused on\nthe immediate fairness, ignoring the long-term impact of a fair classifier\nunder the dynamic scenario where each individual can improve its feature over\ntime. Such dynamic scenarios happen in real world, e.g., college admission and\ncredit loaning, where each rejected sample makes effort to change its features\nto get accepted afterwards. In this dynamic setting, the long-term fairness\nshould equalize the samples' feature distribution across different groups after\nthe rejected samples make some effort to improve. In order to promote long-term\nfairness, we propose a new fairness notion called Equal Improvability (EI),\nwhich equalizes the potential acceptance rate of the rejected samples across\ndifferent groups assuming a bounded level of effort will be spent by each\nrejected sample. We analyze the properties of EI and its connections with\nexisting fairness notions. To find a classifier that satisfies the EI\nrequirement, we propose and study three different approaches that solve\nEI-regularized optimization problems. Through experiments on both synthetic and\nreal datasets, we demonstrate that the proposed EI-regularized algorithms\nencourage us to find a fair classifier in terms of EI. Finally, we provide\nexperimental results on dynamic scenarios which highlight the advantages of our\nEI metric in achieving the long-term fairness. Codes are available in a GitHub\nrepository, see https://github.com/guldoganozgur/ei_fairness.",
    "descriptor": "\nComments: Codes are available in a GitHub repository, see this https URL 19 pages, 5 figures, 4 tables\n",
    "authors": [
      "Ozgur Guldogan",
      "Yuchen Zeng",
      "Jy-yong Sohn",
      "Ramtin Pedarsani",
      "Kangwook Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.06732"
  },
  {
    "id": "arXiv:2210.06733",
    "title": "Codes from incidence matrices of hypergraphs",
    "abstract": "Binary codes are constructed from incidence matrices of hypergraphs. A\ncombinatroial description is given for the minimum distances of such codes via\na combinatorial tool called ``eonv\". This combinatorial approach provides a\nfaster alternative method of finding the minimum distance, which is known to be\na hard problem. This is demonstrated on several classes of codes from\nhypergraphs. Moreover, self-duality and self-orthogonality conditions are also\nstudied through hypergraphs.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Sudipta Mallik",
      "Bahattin Yildiz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06733"
  },
  {
    "id": "arXiv:2210.06734",
    "title": "Optimal Control of Material Micro-Structures",
    "abstract": "In this paper, we consider the optimal control of material micro-structures.\nSuch material micro-structures are modeled by the so-called phase field model.\nWe study the underlying physical structure of the model and propose a data\nbased approach for its optimal control, along with a comparison to the control\nusing a state of the art Reinforcement Learning (RL) algorithm. Simulation\nresults show the feasibility of optimally controlling such micro-structures to\nattain desired material properties and complex target micro-structures.",
    "descriptor": "",
    "authors": [
      "Aayushman Sharma",
      "Zirui Mao",
      "Haiying Yang",
      "Suman Chakravorty",
      "Michael J Demkowicz",
      "Dileep Kalathil"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06734"
  },
  {
    "id": "arXiv:2210.06738",
    "title": "PUPoW: A framework for designing blockchains with  practically-useful-proof-of-work & vanitycoin",
    "abstract": "Bitcoin is the first of its kind, a truly decentralized and anonymous\ncryptocurrency. To realize it, it has developed blockchain technology using the\nconcept of `Proof of Work' (PoW). The miners, nodes responsible for writing\ntransaction databases, solve a cryptographic puzzle to claim the right to write\nto the database. Though bitcoin and many other relevant cryptocurrencies, such\nas ether use revolutionary ideas, the main criticism involves computing\nresources and energy consumption to solve puzzles that have otherwise no use.\nThere are attempts to use the PoW to do something useful, commonly referred to\nas Proof-of-Useful-Work (PoUW). In this paper, we attempt to (i) make PoUW more\nusable -- describe how a central problem setter can crowdsource their work as\nPoUW and (ii) in the true spirit of blockchains, decentralize the role of\nproblem setter, whom we call puzzlers. We propose a formal framework to do so,\nnamely PUPoW. PUPoW has an inbuilt provision of payments from the puzzler to\nthe miner who solves its puzzle. Additionally, miners have the option to not\nrely on a continuous feed of the puzzles and instead use original PoW puzzles.\nWe also propose a way to use PUPOW for solving TOR vanity URL generation and\nbitcoin vanity address generation problems. We call this PUPoW blockchain\nsolving vanity address generation problems as VanityCoin. Both problems require\ngenerating public keys from private keys such that resultant addresses are of\ninterest. Such key pairs are found only by a brute-force search. However, there\nare privacy concerns that miners would know the private keys of the puzzlers.\nWe resolve this by splitting the private keys, and the miners would know only\none part of it. In summary, we are proposing how PoW can be made practically\nhelpful, and we believe such an approach is needed for PoW blockchains to\nsurvive.",
    "descriptor": "",
    "authors": [
      "Yash Chaurasia",
      "Visvesh Subramanian",
      "Sujit Gujar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06738"
  },
  {
    "id": "arXiv:2210.06739",
    "title": "Computing the Best Case Energy Complexity of Satisfying Assignments in  Monotone Circuits",
    "abstract": "Measures of circuit complexity are usually analyzed to ensure the computation\nof Boolean functions with economy and efficiency. One of these measures is\nenergy complexity, which is related to the number of gates that output true in\na circuit for an assignment. The idea behind energy complexity comes from the\ncounting of `firing' neurons in a natural neural network. The initial model is\nbased on threshold circuits, but recent works also have analyzed the energy\ncomplexity of traditional Boolean circuits. In this work, we discuss the time\ncomplexity needed to compute the best-case energy complexity among satisfying\nassignments of a monotone Boolean circuit, and we call such a problem as\nMinEC$^+_M$. In the MinEC$^+_M$ problem, we are given a monotone Boolean\ncircuit $C$, a positive integer $k$ and asked to determine whether there is a\nsatisfying assignment $X$ for $C$ such that $EC(C,X) \\leq k$, where $EC(C,X)$\nis the number of gates that output true in $C$ according to the assignment $X$.\nWe prove that MinEC$^+_M$ is NP-complete even when the input monotone circuit\nis planar. Besides, we show that the problem is W[1]-hard but in XP when\nparameterized by the size of the solution. In contrast, we show that when the\nsize of the solution and the genus of the input circuit are aggregated\nparameters, the MinEC$^+_M$ problem becomes fixed-parameter tractable.",
    "descriptor": "",
    "authors": [
      "Janio Carlos Nascimento Silva",
      "U\u00e9verton S. Souza"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06739"
  },
  {
    "id": "arXiv:2210.06741",
    "title": "Why self-attention is Natural for Sequence-to-Sequence Problems? A  Perspective from Symmetries",
    "abstract": "In this paper, we show that structures similar to self-attention are natural\nto learn many sequence-to-sequence problems from the perspective of symmetry.\nInspired by language processing applications, we study the orthogonal\nequivariance of seq2seq functions with knowledge, which are functions taking\ntwo inputs -- an input sequence and a ``knowledge'' -- and outputting another\nsequence. The knowledge consists of a set of vectors in the same embedding\nspace as the input sequence, containing the information of the language used to\nprocess the input sequence. We show that orthogonal equivariance in the\nembedding space is natural for seq2seq functions with knowledge, and under such\nequivariance the function must take the form close to the self-attention. This\nshows that network structures similar to self-attention are the right\nstructures to represent the target function of many seq2seq problems. The\nrepresentation can be further refined if a ``finite information principle'' is\nconsidered, or a permutation equivariance holds for the elements of the input\nsequence.",
    "descriptor": "",
    "authors": [
      "Chao Ma",
      "Lexing Ying"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06741"
  },
  {
    "id": "arXiv:2210.06742",
    "title": "H2RBox: Horizonal Box Annotation is All You Need for Oriented Object  Detection",
    "abstract": "Oriented object detection emerges in many applications from aerial images to\nautonomous driving, while many existing detection benchmarks are annotated with\nhorizontal bounding box only which is also less costive than fine-grained\nrotated box, leading to a gap between the readily available training corpus and\nthe rising demand for oriented object detection. This paper proposes a simple\nyet effective oriented object detection approach called H2RBox merely using\nhorizontal box annotation for weakly-supervised training, which closes the\nabove gap and shows competitive performance even against those trained with\nrotated boxes. The cores of our method are weakly- and self-supervised\nlearning, which predicts the angle of the object by learning the consistency of\ntwo different views. To our best knowledge, H2RBox is the first horizontal box\nannotation-based oriented object detector. Compared to an alternative i.e.\nhorizontal box-supervised instance segmentation with our post adaption to\noriented object detection, our approach is not susceptible to the prediction\nquality of mask and can perform more robustly in complex scenes containing a\nlarge number of dense objects and outliers. Experimental results show that\nH2RBox has significant performance and speed advantages over horizontal\nbox-supervised instance segmentation methods, as well as lower memory\nrequirements. While compared to rotated box-supervised oriented object\ndetectors, our method shows very close performance and speed, and even\nsurpasses them in some cases. The source code is available at\nhttps://github.com/yangxue0827/h2rbox-mmrotate.",
    "descriptor": "\nComments: 14 pages, 6 figures, 6 tables, the source code is available at this https URL\n",
    "authors": [
      "Xue Yang",
      "Gefan Zhang",
      "Wentong Li",
      "Xuehui Wang",
      "Yue Zhou",
      "Junchi Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06742"
  },
  {
    "id": "arXiv:2210.06744",
    "title": "Visualizing Multispecies Coalescent Trees: Drawing Gene Trees Inside  Species Trees",
    "abstract": "We consider the problem of drawing multiple gene trees inside a single\nspecies tree in order to visualize multispecies coalescent trees. Specifically,\nthe drawing of the species tree fills a rectangle in which each of its edges is\nrepresented by a smaller rectangle, and the gene trees are drawn as rectangular\ncladograms (that is, orthogonally and downward, with one bend per edge) inside\nthe drawing of the species tree. As an alternative, we also consider a style\nwhere the widths of the edges of the species tree are proportional to given\neffective population sizes.\nIn order to obtain readable visualizations, our aim is to minimize the number\nof crossings between edges of the gene trees in such drawings. We show that\nplanar instances can be recognized in linear time and that the general problem\nis NP-hard. Therefore, we introduce two heuristics and give an integer linear\nprogramming (ILP) formulation that provides us with exact solutions in\nexponential time. We use the ILP to measure the quality of the heuristics on\nreal-world instances. The heuristics yield surprisingly good solutions, and the\nILP runs surprisingly fast.",
    "descriptor": "\nComments: Appears in the Proceedings of SOFSEM 2023\n",
    "authors": [
      "Jonathan Klawitter",
      "Felix Klesen",
      "Moritz Niederer",
      "Alexander Wolff"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.06744"
  },
  {
    "id": "arXiv:2210.06746",
    "title": "PoliGraph: Automated Privacy Policy Analysis using Knowledge Graphs",
    "abstract": "Privacy policies disclose how an organization collects and handles personal\ninformation. Recent work has made progress in leveraging natural language\nprocessing (NLP) to automate privacy policy analysis and extract collection\nstatements from different sentences, considered in isolation from each other.\nIn this paper, we view and analyze, for the first time, the entire text of a\nprivacy policy in an integrated way. In terms of methodology: (1) we define\nPoliGraph, a type of knowledge graph that captures different relations between\ndifferent parts of the text in a privacy policy; and (2) we develop an\nNLP-based tool, PoliGraph-er, to automatically extract PoliGraph from the text.\nIn addition, (3) we revisit the notion of ontologies, previously defined in\nheuristic ways, to capture subsumption relations between terms. We make a clear\ndistinction between local and global ontologies to capture the context of\nindividual privacy policies, application domains, and privacy laws. Using a\npublic dataset for evaluation, we show that PoliGraph-er identifies 61% more\ncollection statements than prior state-of-the-art, with over 90% precision. In\nterms of applications, PoliGraph enables automated analysis of a corpus of\nprivacy policies and allows us to: (1) reveal common patterns in the texts\nacross different privacy policies, and (2) assess the correctness of the terms\nas defined within a privacy policy. We also apply PoliGraph to: (3) detect\ncontradictions in a privacy policy-we show false positives by prior work, and\n(4) analyze the consistency of privacy policies and network traffic, where we\nidentify significantly more clear disclosures than prior work.",
    "descriptor": "\nComments: 20 pages, 12 figures. Under submission\n",
    "authors": [
      "Hao Cui",
      "Rahmadi Trimananda",
      "Athina Markopoulou",
      "Scott Jordan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06746"
  },
  {
    "id": "arXiv:2210.06748",
    "title": "Shortcomings of Question Answering Based Factuality Frameworks for Error  Localization",
    "abstract": "Despite recent progress in abstractive summarization, models often generate\nsummaries with factual errors. Numerous approaches to detect these errors have\nbeen proposed, the most popular of which are question answering (QA)-based\nfactuality metrics. These have been shown to work well at predicting\nsummary-level factuality and have potential to localize errors within\nsummaries, but this latter capability has not been systematically evaluated in\npast research. In this paper, we conduct the first such analysis and find that,\ncontrary to our expectations, QA-based frameworks fail to correctly identify\nerror spans in generated summaries and are outperformed by trivial exact match\nbaselines. Our analysis reveals a major reason for such poor localization:\nquestions generated by the QG module often inherit errors from non-factual\nsummaries which are then propagated further into downstream modules. Moreover,\neven human-in-the-loop question generation cannot easily offset these problems.\nOur experiments conclusively show that there exist fundamental issues with\nlocalization using the QA framework which cannot be fixed solely by stronger QA\nand QG models.",
    "descriptor": "",
    "authors": [
      "Ryo Kamoi",
      "Tanya Goyal",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06748"
  },
  {
    "id": "arXiv:2210.06749",
    "title": "Reducing Annotation Effort by Identifying and Labeling Contextually  Diverse Classes for Semantic Segmentation Under Domain Shift",
    "abstract": "In Active Domain Adaptation (ADA), one uses Active Learning (AL) to select a\nsubset of images from the target domain, which are then annotated and used for\nsupervised domain adaptation (DA). Given the large performance gap between\nsupervised and unsupervised DA techniques, ADA allows for an excellent\ntrade-off between annotation cost and performance. Prior art makes use of\nmeasures of uncertainty or disagreement of models to identify `regions' to be\nannotated by the human oracle. However, these regions frequently comprise of\npixels at object boundaries which are hard and tedious to annotate. Hence, even\nif the fraction of image pixels annotated reduces, the overall annotation time\nand the resulting cost still remain high. In this work, we propose an ADA\nstrategy, which given a frame, identifies a set of classes that are hardest for\nthe model to predict accurately, thereby recommending semantically meaningful\nregions to be annotated in a selected frame. We show that these set of `hard'\nclasses are context-dependent and typically vary across frames, and when\nannotated help the model generalize better. We propose two ADA techniques: the\nAnchor-based and Augmentation-based approaches to select complementary and\ndiverse regions in the context of the current training set. Our approach\nachieves 66.6 mIoU on GTA to Cityscapes dataset with an annotation budget of\n4.7% in comparison to 64.9 mIoU by MADA using 5% of annotations. Our technique\ncan also be used as a decorator for any existing frame-based AL technique,\ne.g., we report 1.5% performance improvement for CDAL on Cityscapes using our\napproach.",
    "descriptor": "\nComments: Accepted WACV2023\n",
    "authors": [
      "Sharat Agarwal",
      "Saket Anand",
      "Chetan Arora"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06749"
  },
  {
    "id": "arXiv:2210.06751",
    "title": "On the Reliability Function for a BSC with Noiseless Feedback at Zero  Rate",
    "abstract": "We consider the transmission of nonexponentially many messages through a\nbinary symmetric channel with noiseless feedback. We obtain an upper bound for\nthe best decoding error exponent. Combined with the corresponding known lower\nbound, this allows to find the reliability function for this channel at zero\nrate.",
    "descriptor": "",
    "authors": [
      "Marat V. Burnashev"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06751"
  },
  {
    "id": "arXiv:2210.06755",
    "title": "Long-Memory Message-Passing for Spatially Coupled Systems",
    "abstract": "This paper addresses the reconstruction of sparse signals from spatially\ncoupled, linear, and noisy measurements. A unified framework of rigorous state\nevolution is established for developing long-memory message-passing (LM-MP) in\nspatially coupled systems. LM-MP utilizes all previous messages to compute the\ncurrent message while conventional MP only uses the latest messages. The\nunified framework is utilized to propose orthogonal approximate message-passing\n(OAMP) for spatially coupled systems. The framework for LM-MP is used as a\ntechnical tool to prove the convergence of state evolution for OAMP. Numerical\nresults show that OAMP for spatially coupled systems is superior to that for\nsystems without spatial coupling in the so-called waterfall region.",
    "descriptor": "",
    "authors": [
      "Keigo Takeuchi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06755"
  },
  {
    "id": "arXiv:2210.06756",
    "title": "Decoding Visual Neural Representations by Multimodal Learning of  Brain-Visual-Linguistic Features",
    "abstract": "Decoding human visual neural representations is a challenging task with great\nscientific significance in revealing vision-processing mechanisms and\ndeveloping brain-like intelligent machines. Most existing methods are difficult\nto generalize to novel categories that have no corresponding neural data for\ntraining. The two main reasons are 1) the under-exploitation of the multimodal\nsemantic knowledge underlying the neural data and 2) the small number of paired\n(stimuli-responses) training data. To overcome these limitations, this paper\npresents a generic neural decoding method called BraVL that uses multimodal\nlearning of brain-visual-linguistic features. We focus on modeling the\nrelationships between brain, visual and linguistic features via multimodal deep\ngenerative models. Specifically, we leverage the mixture-of-product-of-experts\nformulation to infer a latent code that enables a coherent joint generation of\nall three modalities. To learn a more consistent joint representation and\nimprove the data efficiency in the case of limited brain activity data, we\nexploit both intra- and inter-modality mutual information maximization\nregularization terms. In particular, our BraVL model can be trained under\nvarious semi-supervised scenarios to incorporate the visual and textual\nfeatures obtained from the extra categories. Finally, we construct three\ntrimodal matching datasets, and the extensive experiments lead to some\ninteresting conclusions and cognitive insights: 1) decoding novel visual\ncategories from human brain activity is practically possible with good\naccuracy; 2) decoding models using the combination of visual and linguistic\nfeatures perform much better than those using either of them alone; 3) visual\nperception may be accompanied by linguistic influences to represent the\nsemantics of visual stimuli. Code and data: https://github.com/ChangdeDu/BraVL.",
    "descriptor": "\nComments: Generic neural decoding, brain-visual-linguistic embedding, multimodal Learning\n",
    "authors": [
      "Changde Du",
      "Kaicheng Fu",
      "Jinpeng Li",
      "Huiguang He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06756"
  },
  {
    "id": "arXiv:2210.06758",
    "title": "Learning Driving Policies for End-to-End Autonomous Driving",
    "abstract": "Humans tend to drive vehicles efficiently by relying on contextual and\nspatial information through the sensory organs. Inspired by this, most of the\nresearch is focused on how to learn robust and efficient driving policies.\nThese works are mostly categorized as making modular or end-to-end systems for\nlearning driving policies. However, the former approach has limitations due to\nthe manual supervision of specific modules that hinder the scalability of these\nsystems. In this work, we focus on the latter approach to formalize a framework\nfor learning driving policies for end-to-end autonomous driving. In order to\ntake inspiration from human driving, we have proposed a framework that\nincorporates three RGB cameras (left, right, and center) to mimic the human\nfield of view and top-down semantic information for contextual representation\nin predicting the driving policies for autonomous driving. The sensor\ninformation is fused and encoded by the self-attention mechanism and followed\nby the auto-regressive waypoint prediction module. The proposed method's\nefficacy is experimentally evaluated using the CARLA simulator and outperforms\nthe state-of-the-art methods by achieving the highest driving score at the\nevaluation time.",
    "descriptor": "",
    "authors": [
      "Shoaib Azam",
      "Farzeen Munir",
      "Moongu Jeon"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06758"
  },
  {
    "id": "arXiv:2210.06759",
    "title": "Outlier-Robust Group Inference via Gradient Space Clustering",
    "abstract": "Traditional machine learning models focus on achieving good performance on\nthe overall training distribution, but they often underperform on minority\ngroups. Existing methods can improve the worst-group performance, but they can\nhave several limitations: (i) they require group annotations, which are often\nexpensive and sometimes infeasible to obtain, and/or (ii) they are sensitive to\noutliers. Most related works fail to solve these two issues simultaneously as\nthey focus on conflicting perspectives of minority groups and outliers. We\naddress the problem of learning group annotations in the presence of outliers\nby clustering the data in the space of gradients of the model parameters. We\nshow that data in the gradient space has a simpler structure while preserving\ninformation about minority groups and outliers, making it suitable for standard\nclustering methods like DBSCAN. Extensive experiments demonstrate that our\nmethod significantly outperforms state-of-the-art both in terms of group\nidentification and downstream worst-group performance.",
    "descriptor": "\nComments: 17 pages, 6 tables, 8 figures\n",
    "authors": [
      "Yuchen Zeng",
      "Kristjan Greenewald",
      "Kangwook Lee",
      "Justin Solomon",
      "Mikhail Yurochkin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06759"
  },
  {
    "id": "arXiv:2210.06766",
    "title": "Policy Gradient With Serial Markov Chain Reasoning",
    "abstract": "We introduce a new framework that performs decision-making in reinforcement\nlearning (RL) as an iterative reasoning process. We model agent behavior as the\nsteady-state distribution of a parameterized reasoning Markov chain (RMC),\noptimized with a new tractable estimate of the policy gradient. We perform\naction selection by simulating the RMC for enough reasoning steps to approach\nits steady-state distribution. We show our framework has several useful\nproperties that are inherently missing from traditional RL. For instance, it\nallows agent behavior to approximate any continuous distribution over actions\nby parameterizing the RMC with a simple Gaussian transition function. Moreover,\nthe number of reasoning steps to reach convergence can scale adaptively with\nthe difficulty of each action selection decision and can be accelerated by\nre-using past solutions. Our resulting algorithm achieves state-of-the-art\nperformance in popular Mujoco and DeepMind Control benchmarks, both for\nproprioceptive and pixel-based tasks.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Edoardo Cetin",
      "Oya Celiktutan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06766"
  },
  {
    "id": "arXiv:2210.06771",
    "title": "Feature Reconstruction Attacks and Countermeasures of DNN training in  Vertical Federated Learning",
    "abstract": "Federated learning (FL) has increasingly been deployed, in its vertical form,\namong organizations to facilitate secure collaborative training over siloed\ndata. In vertical FL (VFL), participants hold disjoint features of the same set\nof sample instances. Among them, only one has labels. This participant, known\nas the active party, initiates the training and interacts with the other\nparticipants, known as the passive parties. Despite the increasing adoption of\nVFL, it remains largely unknown if and how the active party can extract feature\ndata from the passive party, especially when training deep neural network (DNN)\nmodels.\nThis paper makes the first attempt to study the feature security problem of\nDNN training in VFL. We consider a DNN model partitioned between active and\npassive parties, where the latter only holds a subset of the input layer and\nexhibits some categorical features of binary values. Using a reduction from the\nExact Cover problem, we prove that reconstructing those binary features is\nNP-hard. Through analysis, we demonstrate that, unless the feature dimension is\nexceedingly large, it remains feasible, both theoretically and practically, to\nlaunch a reconstruction attack with an efficient search-based algorithm that\nprevails over current feature protection techniques. To address this problem,\nwe develop a novel feature protection scheme against the reconstruction attack\nthat effectively misleads the search to some pre-specified random values. With\nan extensive set of experiments, we show that our protection scheme sustains\nthe feature reconstruction attack in various VFL applications at no expense of\naccuracy loss.",
    "descriptor": "",
    "authors": [
      "Peng Ye",
      "Zhifeng Jiang",
      "Wei Wang",
      "Bo Li",
      "Baochun Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06771"
  },
  {
    "id": "arXiv:2210.06772",
    "title": "Mitigating Unintended Memorization in Language Models via Alternating  Teaching",
    "abstract": "Recent research has shown that language models have a tendency to memorize\nrare or unique sequences in the training corpora which can thus leak sensitive\nattributes of user data. We employ a teacher-student framework and propose a\nnovel approach called alternating teaching to mitigate unintended memorization\nin sequential modeling. In our method, multiple teachers are trained on\ndisjoint training sets whose privacy one wishes to protect, and teachers'\npredictions supervise the training of a student model in an alternating manner\nat each time step. Experiments on LibriSpeech datasets show that the proposed\nmethod achieves superior privacy-preserving results than other counterparts. In\ncomparison with no prevention for unintended memorization, the overall utility\nloss is small when training records are sufficient.",
    "descriptor": "",
    "authors": [
      "Zhe Liu",
      "Xuedong Zhang",
      "Fuchun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06772"
  },
  {
    "id": "arXiv:2210.06773",
    "title": "An Additive Autoencoder for Dimension Estimation",
    "abstract": "An additive autoencoder for dimension reduction, which is composed of a\nserially performed bias estimation, linear trend estimation, and nonlinear\nresidual estimation, is proposed and analyzed. Computational experiments\nconfirm that an autoencoder of this form, with only a shallow network to\nencapsulate the nonlinear behavior, is able to identify an intrinsic dimension\nof a dataset with a low autoencoding error. This observation leads to an\ninvestigation in which shallow and deep network structures, and how they are\ntrained, are compared. We conclude that the deeper network structures obtain\nlower autoencoding errors during the identification of the intrinsic dimension.\nHowever, the detected dimension does not change compared to a shallow network.",
    "descriptor": "\nComments: Manuscript submitted to review\n",
    "authors": [
      "Tommi K\u00e4rkk\u00e4inen",
      "Jan H\u00e4nninen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06773"
  },
  {
    "id": "arXiv:2210.06774",
    "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
    "abstract": "We consider the problem of automatically generating longer stories of over\ntwo thousand words. Compared to prior work on shorter stories, long-range plot\ncoherence and relevance are more central challenges here. We propose the\nRecursive Reprompting and Revision framework (Re3) to address these challenges\nby (a) prompting a general-purpose language model to construct a structured\noverarching plan, and (b) generating story passages by repeatedly injecting\ncontextual information from both the plan and current story state into a\nlanguage model prompt. We then revise by (c) reranking different continuations\nfor plot coherence and premise relevance, and finally (d) editing the best\ncontinuation for factual consistency. Compared to similar-length stories\ngenerated directly from the same base model, human evaluators judged\nsubstantially more of Re3's stories as having a coherent overarching plot (by\n14% absolute increase), and relevant to the given initial premise (by 20%).",
    "descriptor": "\nComments: To appear at EMNLP 2022\n",
    "authors": [
      "Kevin Yang",
      "Nanyun Peng",
      "Yuandong Tian",
      "Dan Klein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06774"
  },
  {
    "id": "arXiv:2210.06776",
    "title": "Improving the Reliability for Confidence Estimation",
    "abstract": "Confidence estimation, a task that aims to evaluate the trustworthiness of\nthe model's prediction output during deployment, has received lots of research\nattention recently, due to its importance for the safe deployment of deep\nmodels. Previous works have outlined two important qualities that a reliable\nconfidence estimation model should possess, i.e., the ability to perform well\nunder label imbalance and the ability to handle various out-of-distribution\ndata inputs. In this work, we propose a meta-learning framework that can\nsimultaneously improve upon both qualities in a confidence estimation model.\nSpecifically, we first construct virtual training and testing sets with some\nintentionally designed distribution differences between them. Our framework\nthen uses the constructed sets to train the confidence estimation model through\na virtual training and testing scheme leading it to learn knowledge that\ngeneralizes to diverse distributions. We show the effectiveness of our\nframework on both monocular depth estimation and image classification.",
    "descriptor": "\nComments: Accepted by ECCV 2022\n",
    "authors": [
      "Haoxuan Qu",
      "Yanchao Li",
      "Lin Geng Foo",
      "Jason Kuen",
      "Jiuxiang Gu",
      "Jun Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06776"
  },
  {
    "id": "arXiv:2210.06778",
    "title": "X-Align: Cross-Modal Cross-View Alignment for Bird's-Eye-View  Segmentation",
    "abstract": "Bird's-eye-view (BEV) grid is a common representation for the perception of\nroad components, e.g., drivable area, in autonomous driving. Most existing\napproaches rely on cameras only to perform segmentation in BEV space, which is\nfundamentally constrained by the absence of reliable depth information. Latest\nworks leverage both camera and LiDAR modalities, but sub-optimally fuse their\nfeatures using simple, concatenation-based mechanisms.\nIn this paper, we address these problems by enhancing the alignment of the\nunimodal features in order to aid feature fusion, as well as enhancing the\nalignment between the cameras' perspective view (PV) and BEV representations.\nWe propose X-Align, a novel end-to-end cross-modal and cross-view learning\nframework for BEV segmentation consisting of the following components: (i) a\nnovel Cross-Modal Feature Alignment (X-FA) loss, (ii) an attention-based\nCross-Modal Feature Fusion (X-FF) module to align multi-modal BEV features\nimplicitly, and (iii) an auxiliary PV segmentation branch with Cross-View\nSegmentation Alignment (X-SA) losses to improve the PV-to-BEV transformation.\nWe evaluate our proposed method across two commonly used benchmark datasets,\ni.e., nuScenes and KITTI-360. Notably, X-Align significantly outperforms the\nstate-of-the-art by 3 absolute mIoU points on nuScenes. We also provide\nextensive ablation studies to demonstrate the effectiveness of the individual\ncomponents.",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Shubhankar Borse",
      "Marvin Klingner",
      "Varun Ravi Kumar",
      "Hong Cai",
      "Abdulaziz Almuzairee",
      "Senthil Yogamani",
      "Fatih Porikli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06778"
  },
  {
    "id": "arXiv:2210.06779",
    "title": "Generalized Inter-class Loss for Gait Recognition",
    "abstract": "Gait recognition is a unique biometric technique that can be performed at a\nlong distance non-cooperatively and has broad applications in public safety and\nintelligent traffic systems. Previous gait works focus more on minimizing the\nintra-class variance while ignoring the significance in constraining\ninter-class variance. To this end, we propose a generalized inter-class loss\nwhich resolves the inter-class variance from both sample-level feature\ndistribution and class-level feature distribution. Instead of equal penalty\nstrength on pair scores, the proposed loss optimizes sample-level inter-class\nfeature distribution by dynamically adjusting the pairwise weight. Further, in\nclass-level distribution, generalized inter-class loss adds a constraint on the\nuniformity of inter-class feature distribution, which forces the feature\nrepresentations to approximate a hypersphere and keep maximal inter-class\nvariance. In addition, the proposed method automatically adjusts the margin\nbetween classes which enables the inter-class feature distribution to be more\nflexible. The proposed method can be generalized to different gait recognition\nnetworks and achieves significant improvements. We conduct a series of\nexperiments on CASIA-B and OUMVLP, and the experimental results show that the\nproposed loss can significantly improve the performance and achieves the\nstate-of-the-art performances.",
    "descriptor": "\nComments: to be published in ACMMM 2022\n",
    "authors": [
      "Weichen Yu",
      "Hongyuan Yu",
      "Yan Huang",
      "Liang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06779"
  },
  {
    "id": "arXiv:2210.06780",
    "title": "Intermediate Prototype Mining Transformer for Few-Shot Semantic  Segmentation",
    "abstract": "Few-shot semantic segmentation aims to segment the target objects in query\nunder the condition of a few annotated support images. Most previous works\nstrive to mine more effective category information from the support to match\nwith the corresponding objects in query. However, they all ignored the category\ninformation gap between query and support images. If the objects in them show\nlarge intra-class diversity, forcibly migrating the category information from\nthe support to the query is ineffective. To solve this problem, we are the\nfirst to introduce an intermediate prototype for mining both deterministic\ncategory information from the support and adaptive category knowledge from the\nquery. Specifically, we design an Intermediate Prototype Mining Transformer\n(IPMT) to learn the prototype in an iterative way. In each IPMT layer, we\npropagate the object information in both support and query features to the\nprototype and then use it to activate the query feature map. By conducting this\nprocess iteratively, both the intermediate prototype and the query feature can\nbe progressively improved. At last, the final query feature is used to yield\nprecise segmentation prediction. Extensive experiments on both PASCAL-5i and\nCOCO-20i datasets clearly verify the effectiveness of our IPMT and show that it\noutperforms previous state-of-the-art methods by a large margin. Code is\navailable at https://github.com/LIUYUANWEI98/IPMT",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Yuanwei Liu",
      "Nian Liu",
      "Xiwen Yao",
      "Junwei Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06780"
  },
  {
    "id": "arXiv:2210.06781",
    "title": "Closed-book Question Generation via Contrastive Learning",
    "abstract": "Question Generation (QG) is a fundamental NLP task for many downstream\napplications. Recent studies on open-book QG, where supportive question-context\npairs are provided to models, have achieved promising progress. However,\ngenerating natural questions under a more practical closed-book setting that\nlacks these supporting documents still remains a challenge. In this work, to\nlearn better representations from semantic information hidden in\nquestion-answer pairs under the closed-book setting, we propose a new QG model\nempowered by a contrastive learning module and an answer reconstruction module.\nWe present a new closed-book QA dataset -- WikiCQA involving abstractive long\nanswers collected from a wiki-style website. In the experiments, we validate\nthe proposed QG model on both public datasets and the new WikiCQA dataset.\nEmpirical results show that the proposed QG model outperforms baselines in both\nautomatic evaluation and human evaluation. In addition, we show how to leverage\nthe proposed model to improve existing closed-book QA systems. We observe that\nby pre-training a closed-book QA model on our generated synthetic QA pairs,\nsignificant QA improvement can be achieved on both seen and unseen datasets,\nwhich further demonstrates the effectiveness of our QG model for enhancing\nunsupervised and semi-supervised QA.",
    "descriptor": "",
    "authors": [
      "Xiangjue Dong",
      "Jiaying Lu",
      "Jianling Wang",
      "James Caverlee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06781"
  },
  {
    "id": "arXiv:2210.06784",
    "title": "Efficient circuit implementation for coined quantum walks on binary  trees and application to reinforcement learning",
    "abstract": "Quantum walks on binary trees are used in many quantum algorithms to achieve\nimportant speedup over classical algorithms. The formulation of this kind of\nalgorithms as quantum circuit present the advantage of being easily readable,\nexecutable on circuit based quantum computers and simulators and optimal on the\nusage of resources. We propose a strategy to compose quantum circuit that\nperforms quantum walk on binary trees following universal gate model quantum\ncomputation principles. We give a particular attention to NAND formula\nevaluation algorithm as it could have many applications in game theory and\nreinforcement learning. We therefore propose an application of this algorithm\nand show how it can be used to train a quantum reinforcement learning agent in\na two player game environment.",
    "descriptor": "",
    "authors": [
      "Thomas Mullor",
      "David Vigouroux",
      "Louis Bethune"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.06784"
  },
  {
    "id": "arXiv:2210.06787",
    "title": "Observed Adversaries in Deep Reinforcement Learning",
    "abstract": "In this work, we point out the problem of observed adversaries for deep\npolicies. Specifically, recent work has shown that deep reinforcement learning\nis susceptible to adversarial attacks where an observed adversary acts under\nenvironmental constraints to invoke natural but adversarial observations. This\nsetting is particularly relevant for HRI since HRI-related robots are expected\nto perform their tasks around and with other agents. In this work, we\ndemonstrate that this effect persists even with low-dimensional observations.\nWe further show that these adversarial attacks transfer across victims, which\npotentially allows malicious attackers to train an adversary without access to\nthe target victim.",
    "descriptor": "",
    "authors": [
      "Eugene Lim",
      "Harold Soh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06787"
  },
  {
    "id": "arXiv:2210.06788",
    "title": "TiDAL: Learning Training Dynamics for Active Learning",
    "abstract": "Active learning (AL) aims to select the most useful data samples from an\nunlabeled data pool and annotate them to expand the labeled dataset under a\nlimited budget. Especially, uncertainty-based methods choose the most uncertain\nsamples, which are known to be effective in improving model performance.\nHowever, AL literature often overlooks training dynamics (TD), defined as the\never-changing model behavior during optimization via stochastic gradient\ndescent, even though other areas of literature have empirically shown that TD\nprovides important clues for measuring the sample uncertainty. In this paper,\nwe propose a novel AL method, Training Dynamics for Active Learning (TiDAL),\nwhich leverages the TD to quantify uncertainties of unlabeled data. Since\ntracking the TD of all the large-scale unlabeled data is impractical, TiDAL\nutilizes an additional prediction module that learns the TD of labeled data. To\nfurther justify the design of TiDAL, we provide theoretical and empirical\nevidence to argue the usefulness of leveraging TD for AL. Experimental results\nshow that our TiDAL achieves better or comparable performance on both balanced\nand imbalanced benchmark datasets compared to state-of-the-art AL methods,\nwhich estimate data uncertainty using only static information after model\ntraining.",
    "descriptor": "",
    "authors": [
      "Seong Min Kye",
      "Kwanghee Choi",
      "Buru Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06788"
  },
  {
    "id": "arXiv:2210.06789",
    "title": "Large-Scale Open-Set Classification Protocols for ImageNet",
    "abstract": "Open-Set Classification (OSC) intends to adapt closed-set classification\nmodels to real-world scenarios, where the classifier must correctly label\nsamples of known classes while rejecting previously unseen unknown samples.\nOnly recently, research started to investigate on algorithms that are able to\nhandle these unknown samples correctly. Some of these approaches address OSC by\nincluding into the training set negative samples that a classifier learns to\nreject, expecting that these data increase the robustness of the classifier on\nunknown classes. Most of these approaches are evaluated on small-scale and\nlow-resolution image datasets like MNIST, SVHN or CIFAR, which makes it\ndifficult to assess their applicability to the real world, and to compare them\namong each other. We propose three open-set protocols that provide rich\ndatasets of natural images with different levels of similarity between known\nand unknown classes. The protocols consist of subsets of ImageNet classes\nselected to provide training and testing data closer to real-world scenarios.\nAdditionally, we propose a new validation metric that can be employed to assess\nwhether the training of deep learning models addresses both the classification\nof known samples and the rejection of unknown samples. We use the protocols to\ncompare the performance of two baseline open-set algorithms to the standard\nSoftMax baseline and find that the algorithms work well on negative samples\nthat have been seen during training, and partially on out-of-distribution\ndetection tasks, but drop performance in the presence of samples from\npreviously unseen unknown classes.",
    "descriptor": "\nComments: This is a pre-print of the original paper accepted at the Winter Conference on Applications of Computer Vision (WACV) 2023\n",
    "authors": [
      "Jesus Andres Palechor Anacona",
      "Annesha Bhoumik",
      "Manuel G\u00fcnther"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06789"
  },
  {
    "id": "arXiv:2210.06790",
    "title": "Deep Gesture Generation for Social Robots Using Type-Specific Libraries",
    "abstract": "Body language such as conversational gesture is a powerful way to ease\ncommunication. Conversational gestures do not only make a speech more lively\nbut also contain semantic meaning that helps to stress important information in\nthe discussion. In the field of robotics, giving conversational agents\n(humanoid robots or virtual avatars) the ability to properly use gestures is\ncritical, yet remain a task of extraordinary difficulty. This is because given\nonly a text as input, there are many possibilities and ambiguities to generate\nan appropriate gesture. Different to previous works we propose a new method\nthat explicitly takes into account the gesture types to reduce these\nambiguities and generate human-like conversational gestures. Key to our\nproposed system is a new gesture database built on the TED dataset that allows\nus to map a word to one of three types of gestures: \"Imagistic\" gestures, which\nexpress the content of the speech, \"Beat\" gestures, which emphasize words, and\n\"No gestures.\" We propose a system that first maps the words in the input text\nto their corresponding gesture type, generate type-specific gestures and\ncombine the generated gestures into one final smooth gesture. In our\ncomparative experiments, the effectiveness of the proposed method was confirmed\nin user studies for both avatar and humanoid robot.",
    "descriptor": "",
    "authors": [
      "Hitoshi Teshima",
      "Naoki Wake",
      "Diego Thomas",
      "Yuta Nakashima",
      "Hiroshi Kawasaki",
      "Katsushi Ikeuchi"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.06790"
  },
  {
    "id": "arXiv:2210.06791",
    "title": "SDW-ASL: A Dynamic System to Generate Large Scale Dataset for Continuous  American Sign Language",
    "abstract": "Despite tremendous progress in natural language processing using deep\nlearning techniques in recent years, sign language production and comprehension\nhas advanced very little. One critical barrier is the lack of largescale\ndatasets available to the public due to the unbearable cost of labeled data\ngeneration. Efforts to provide public data for American Sign Language (ASL)\ncomprehension have yielded two datasets, comprising more than thousand video\nclips. These datasets are large enough to enable a meaningful start to deep\nlearning research on sign languages but are far too small to lead to any\nsolution that can be practically deployed. So far, there is still no suitable\ndataset for ASL production. We proposed a system that can generate large scale\nASL datasets for continuous ASL. It is suitable for general ASL processing and\nis particularly useful for ASL production. The continuous ASL dataset contains\nEnglish labeled human articulations in condensed body pose data formats. To\nbetter serve the research community, we are releasing the first version of our\nASL dataset, which contains 30k sentences, 416k words, a vocabulary of 18k\nwords, in a total of 104 hours. This is the largest continuous sign language\ndataset published to date in terms of video duration. We also describe a system\nthat can evolve and expand the dataset to incorporate better data processing\ntechniques and more contents when available. It is our hope that the release of\nthis ASL dataset and the sustainable dataset generation system to the public\nwill propel better deep-learning research in ASL natural language processing.",
    "descriptor": "\nComments: There are 9 pages, 3 figures and 1 table. The ASL dataset viewing system is at this https URL\n",
    "authors": [
      "Yehong Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06791"
  },
  {
    "id": "arXiv:2210.06792",
    "title": "SoK: How `Not' to Architect Your Next-Generation TEE Malware?",
    "abstract": "Besides Intel's SGX technology, there are long-running discussions on how\ntrusted computing technologies can be used to cloak malware. Past research\nshowed example methods of malicious activities utilising Flicker, Trusted\nPlatform Module, and recently integrating with enclaves. There is, however, an\nambiguity over the core SGX ecosystem helps to cloak malware, or whether the\nadditional engineering work outside SGX's ecosystem forcefully attaches\n(overfits) malware-behaviour into the enclave. We examine what malware aims to\ndo in real-world scenarios and state-of-art techniques in malware evasion. The\nrising disadvantages of maintaining the malware and protecting it from\nanti-malware mechanisms make SGX enclaves a bad choice for achieving a\nsuccessful malware campaign. We systematise twelve points outlining how an\noverfit-malware using SGX weakens malware's existing abilities. By making a\ncomparison with a non-SGX malware (i.e., malware in the wild in our paper), we\nconclude that the use of hardware enclaves does not increase the preexisting\nattack surface, provides no new infection point, and does not contribute any\nnew methods to the stealthiness of malware.",
    "descriptor": "\nComments: Accepted at Hardware and Architectural Support for Security and Privacy (HASP) 2022. this https URL\n",
    "authors": [
      "Kubilay Ahmet K\u00fc\u00e7\u00fck",
      "Steve Moyle",
      "Andrew Martin",
      "Alexandru Mereacre",
      "Nicholas Allott"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06792"
  },
  {
    "id": "arXiv:2210.06794",
    "title": "Towards Holographic Video Communications: A Promising AI-driven Solution",
    "abstract": "Real-time holographic video communications enable immersive experiences for\nnext-generation video services in the future metaverse era. However,\nhigh-fidelity holographic videos require high bandwidth and significant\ncomputation resources, which exceed the transferring and computing capacity of\n5G networks. This article reviews state-of-the-art holographic point cloud\nvideo (PCV) transmission techniques and highlights the critical challenges of\ndelivering such immersive services. We further implement a preliminary\nprototype of an AI-driven holographic video communication system and present\ncritical experimental results to evaluate its performance. Finally, we identify\nfuture research directions and discuss potential solutions for providing\nreal-time and high-quality holographic experiences.",
    "descriptor": "",
    "authors": [
      "Yakun Huang",
      "Yuanwei Zhu",
      "Xiuquan Qiao",
      "Xiang Su",
      "Schahram Dustdar",
      "Ping Zhang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06794"
  },
  {
    "id": "arXiv:2210.06795",
    "title": "Subspace-Contrastive Multi-View Clustering",
    "abstract": "Most multi-view clustering methods are limited by shallow models without\nsound nonlinear information perception capability, or fail to effectively\nexploit complementary information hidden in different views. To tackle these\nissues, we propose a novel Subspace-Contrastive Multi-View Clustering (SCMC)\napproach. Specifically, SCMC utilizes view-specific auto-encoders to map the\noriginal multi-view data into compact features perceiving its nonlinear\nstructures. Considering the large semantic gap of data from different\nmodalities, we employ subspace learning to unify the multi-view data into a\njoint semantic space, namely the embedded compact features are passed through\nmultiple self-expression layers to learn the subspace representations,\nrespectively. In order to enhance the discriminability and efficiently excavate\nthe complementarity of various subspace representations, we use the contrastive\nstrategy to maximize the similarity between positive pairs while differentiate\nnegative pairs. Thus, a weighted fusion scheme is developed to initially learn\na consistent affinity matrix. Furthermore, we employ the graph regularization\nto encode the local geometric structure within varying subspaces for further\nfine-tuning the appropriate affinities between instances. To demonstrate the\neffectiveness of the proposed model, we conduct a large number of comparative\nexperiments on eight challenge datasets, the experimental results show that\nSCMC outperforms existing shallow and deep multi-view clustering methods.",
    "descriptor": "",
    "authors": [
      "Fu Lele",
      "Zhang Lei",
      "Yang Jinghua",
      "Chen Chuan",
      "Zhang Chuanfu",
      "Zheng Zibin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06795"
  },
  {
    "id": "arXiv:2210.06798",
    "title": "Analysis of publications by authors of Ukrainian institutes in  Scopus-delisted titles",
    "abstract": "In Ukraine, Scopus data are used to evaluate academics. Existing shortcomings\nin the Ukrainian evaluation system allow them to publish in titles that have\nbeen delisted from Scopus, and continue to use those papers as credible\nresearch output for evaluation. The purpose of this study was to analyse the\npublishing activity of Ukrainian institutions in Scopus-delisted titles (as of\nSeptember 2021) in different fields between 2011 and 2020 and to attempt to\nappreciate how common this practice is among Ukrainian authors. Scopus was\nsourced to collect bibliographic and citations-related data, while SciVal was\nused to analyse these data. The findings suggest that for 17 Ukrainian\ninstitutions, papers from titles that have been delisted from Scopus still play\nan important part of the publication achievement of their employees. In\nparticular, in the field of economics, econometrics and finance, 46.92% of\nUkrainian papers were published in a title that was excluded from Scopus.\nMoreover, the analysis indicated that in two Ukrainian institutions, the level\nof citation of such papers significantly exceeds the average number of\ncitations to Scopus-indexed papers in the same year and in the same field.\nGiven that bibliometric indicators are also used for research assessment in\nother Eastern European countries, the results of this paper are applicable to a\nwider geographic context.",
    "descriptor": "",
    "authors": [
      "Serhii Nazarovets"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.06798"
  },
  {
    "id": "arXiv:2210.06799",
    "title": "Benchmarking Long-tail Generalization with Likelihood Splits",
    "abstract": "In order to reliably process natural language, NLP systems must generalize to\nthe long tail of rare utterances. We propose a method to create challenging\nbenchmarks that require generalizing to the tail of the distribution by\nre-splitting existing datasets. We create 'Likelihood splits' where examples\nthat are assigned lower likelihood by a pre-trained language model (LM) are\nplaced in the test set, and more likely examples are in the training set. This\nsimple approach can be customized to construct meaningful train-test splits for\na wide range of tasks. Likelihood splits are more challenging than random\nsplits: relative error rates of state-of-the-art models on our splits increase\nby 59% for semantic parsing on Spider, 77% for natural language inference on\nSNLI, and 38% for yes/no question answering on BoolQ compared with the\ncorresponding random splits. Moreover, Likelihood splits create fairer\nbenchmarks than adversarial filtering; when the LM used to create the splits is\nused as the task model, our splits do not adversely penalize the LM.",
    "descriptor": "",
    "authors": [
      "Ameya Godbole",
      "Robin Jia"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06799"
  },
  {
    "id": "arXiv:2210.06801",
    "title": "Robust offset-free nonlinear model predictive control for systems  learned by neural nonlinear autoregressive exogenous models",
    "abstract": "This paper presents a robust Model Predictive Control (MPC) scheme that\nprovides offset-free setpoint tracking for systems described by Neural\nNonlinear AutoRegressive eXogenous (NNARX) models. The NNARX model learns the\ndynamics of the plant from input-output data, and during the training the\nIncremental Input-to-State Stability (${\\delta}$ISS) property is forced to\nguarantee stability. The trained NNARX model is then augmented with an explicit\nintegral action on the output tracking error, which allows the control scheme\nto enjoy offset-free tracking ability. A tube-based MPC is finally designed,\nleveraging the unique structure of the model, to ensure robust stability and\nrobust asymptotic zero error regulation for constant reference signals in the\npresence of model-plant mismatch or unknown disturbances. Numerical simulations\non a water heating system show the effectiveness of the proposed control\nalgorithm.",
    "descriptor": "\nComments: This work has been submitted to Wiley for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Jing Xie",
      "Fabio Bonassi",
      "Marcello Farina",
      "Riccardo Scattolini"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06801"
  },
  {
    "id": "arXiv:2210.06802",
    "title": "Multi-Player Immersive Communications and Interactions in Metaverse:  Challenges, Architecture, and Future Directions",
    "abstract": "The metaverse has awakened users' expectations of an immersive interaction\nthat fuses the virtual digital world and the physical world across space and\ntime. However, the metaverse is still in its infancy, typically expanding\nmulti-player applications (e.g., multi-player games) to implement a prototype\nwith the help of 5G/Beyond 5G, Artificial Intelligence, digital twin, and other\nenabling technologies. This article reviews the characteristics, key enabling\ntechnologies, and driving applications of the state-of-the-art metaverse. We\nfocus on the immersive interactions perspective of the metaverse from the\ntasks, inputs, and feedback across the users, digital world, and physical world\nand reveal the key challenges. Afterwards, we present a multi-player\ninteraction prototype platform based on a cloud-edge-device collaborative\nframework. Also, we evaluate it with centralized and device-to-device (D2D)\napproaches to verify the efficiency and flexibility of interactions. Finally,\nwe point out future research approaches and discuss potential solutions to\nenable more stable and higher quality multi-player interactions for metaverse\nservices.",
    "descriptor": "",
    "authors": [
      "Yakun Huang",
      "Xiuquan Qiao",
      "Haowen Wang",
      "Xiang Su",
      "Schahram Dustdar",
      "Ping Zhang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.06802"
  },
  {
    "id": "arXiv:2210.06803",
    "title": "Trajectory Optimization for Quadruped Mobile Manipulators that Carry  Heavy Payload",
    "abstract": "This paper presents a simplified model-based trajectory optimization (TO)\nformulation for motion planning on quadruped mobile manipulators that carry\nheavy payload of known mass. The proposed payload-aware formulation\nsimultaneously plans locomotion, payload manipulation and considers both robot\nand payload model dynamics while remaining computationally efficient. At the\npresence of heavy payload, the approach exhibits reduced leg outstretching\n(thus increased manipulability) in kinematically demanding motions due to the\ncontribution of payload manipulation in the optimization. The framework's\ncomputational efficiency and performance is validated through a number of\nsimulation and experimental studies with the bi-manual quadruped CENTAURO robot\ncarrying on its arms a payload that exceeds 15 % of its mass and traversing\nnon-flat terrain.",
    "descriptor": "\nComments: 8 pages, 9 figures, 2 tables, accepted article at the 2022 IEEE-RAS International Conference on Humanoid Robots (Humanoids 2022)\n",
    "authors": [
      "Ioannis Dadiotis",
      "Arturo Laurenzi",
      "Nikos Tsagarakis"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06803"
  },
  {
    "id": "arXiv:2210.06806",
    "title": "OOOE: Only-One-Object-Exists Assumption to Find Very Small Objects in  Chest Radiographs",
    "abstract": "The accurate localization of inserted medical tubes and parts of human\nanatomy is a common problem when analyzing chest radiographs and something deep\nneural networks could potentially automate. However, many foreign objects like\ntubes and various anatomical structures are small in comparison to the entire\nchest X-ray, which leads to severely unbalanced data and makes training deep\nneural networks difficult. In this paper, we present a simple yet effective\n`Only-One-Object-Exists' (OOOE) assumption to improve the deep network's\nability to localize small landmarks in chest radiographs. The OOOE enables us\nto recast the localization problem as a classification problem and we can\nreplace commonly used continuous regression techniques with a multi-class\ndiscrete objective. We validate our approach using a large scale proprietary\ndataset of over 100K radiographs as well as publicly available RANZCR-CLiP\nKaggle Challenge dataset and show that our method consistently outperforms\ncommonly used regression-based detection models as well as commonly used\npixel-wise classification methods. Additionally, we find that the method using\nthe OOOE assumption generalizes to multiple detection problems in chest X-rays\nand the resulting model shows state-of-the-art performance on detecting various\ntube tips inserted to the patient as well as patient anatomy.",
    "descriptor": "\nComments: AMAI Workshop, MICCAI 2022\n",
    "authors": [
      "Gunhee Nam",
      "Taesoo Kim",
      "Sanghyup Lee",
      "Thijs Kooi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06806"
  },
  {
    "id": "arXiv:2210.06807",
    "title": "Improving Out-of-Distribution Generalization by Adversarial Training  with Structured Priors",
    "abstract": "Deep models often fail to generalize well in test domains when the data\ndistribution differs from that in the training domain. Among numerous\napproaches to address this Out-of-Distribution (OOD) generalization problem,\nthere has been a growing surge of interest in exploiting Adversarial Training\n(AT) to improve OOD performance. Recent works have revealed that the robust\nmodel obtained by conducting sample-wise AT also retains transferability to\nbiased test domains. In this paper, we empirically show that sample-wise AT has\nlimited improvement on OOD performance. Specifically, we find that AT can only\nmaintain performance at smaller scales of perturbation while Universal AT (UAT)\nis more robust to larger-scale perturbations. This provides us with clues that\nadversarial perturbations with universal (low dimensional) structures can\nenhance the robustness against large data distribution shifts that are common\nin OOD scenarios. Inspired by this, we propose two AT variants with low-rank\nstructures to train OOD-robust models. Extensive experiments on DomainBed\nbenchmark show that our proposed approaches outperform Empirical Risk\nMinimization (ERM) and sample-wise AT. Our code is available at\nhttps://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD.",
    "descriptor": "",
    "authors": [
      "Qixun Wang",
      "Yifei Wang",
      "Hong Zhu",
      "Yisen Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06807"
  },
  {
    "id": "arXiv:2210.06808",
    "title": "ISCom: Interest-aware Semantic Communication Scheme for Point Cloud  Video Streaming",
    "abstract": "The provisioning of immersive point cloud video (PCV) streaming on pervasive\nmobile devices is a cornerstone for enabling immersive communication and\ninteractions in the future 6G metaverse era. However, most streaming techniques\nare dedicated to efficient PCV compression and codec extending from traditional\n3-DoF video services. Some emerging AI-enabled approaches are still in their\ninfancy phase and are constrained by intensive computational and adaptive flow\ntechniques. In this paper, we present ISCom, an Interest-aware Semantic\nCommunication Scheme for PCV, consisting of a region-of-interest (ROI)\nselection module, a lightweight PCV streaming module, and an intelligent\nscheduler. First, we propose a two-stage efficient ROI selection method for\nproviding interest-aware PCV streaming, which significantly reduces the data\nvolume. Second, we design a lightweight PCV encoder-decoder network for\nresource-constrained devices, adapting to the heterogeneous computing\ncapabilities of terminals. Third, we train a deep reinforcement learning\n(DRL)-based scheduler to adapt an optimal encoder-decoder network for various\ndevices, considering the dynamic network environments and computing\ncapabilities of different devices. Extensive experiments show that ISCom\noutperforms baselines on mobile devices at least 10 FPS and up to 22 FPS.",
    "descriptor": "",
    "authors": [
      "Yakun Huang",
      "Boyuan Bai",
      "Yuanwei Zhu",
      "Xiuquan Qiao",
      "Xiang Su",
      "Ping Zhang"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.06808"
  },
  {
    "id": "arXiv:2210.06811",
    "title": "On the calibration of underrepresented classes in LiDAR-based semantic  segmentation",
    "abstract": "The calibration of deep learning-based perception models plays a crucial role\nin their reliability. Our work focuses on a class-wise evaluation of several\nmodel's confidence performance for LiDAR-based semantic segmentation with the\naim of providing insights into the calibration of underrepresented classes.\nThose classes often include VRUs and are thus of particular interest for safety\nreasons. With the help of a metric based on sparsification curves we compare\nthe calibration abilities of three semantic segmentation models with different\narchitectural concepts, each in a in deterministic and a probabilistic version.\nBy identifying and describing the dependency between the predictive performance\nof a class and the respective calibration quality we aim to facilitate the\nmodel selection and refinement for safety-critical applications.",
    "descriptor": "",
    "authors": [
      "Mariella Dreissig",
      "Florian Piewak",
      "Joschka Boedecker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06811"
  },
  {
    "id": "arXiv:2210.06812",
    "title": "Utilizing supervised models to infer consensus labels and their quality  from data with multiple annotators",
    "abstract": "Real-world data for classification is often labeled by multiple annotators.\nFor analyzing such data, we introduce CROWDLAB, a straightforward approach to\nestimate: (1) A consensus label for each example that aggregates the individual\nannotations (more accurately than aggregation via majority-vote or other\nalgorithms used in crowdsourcing); (2) A confidence score for how likely each\nconsensus label is correct (via well-calibrated estimates that account for the\nnumber of annotations for each example and their agreement,\nprediction-confidence from a trained classifier, and trustworthiness of each\nannotator vs. the classifier); (3) A rating for each annotator quantifying the\noverall correctness of their labels. While many algorithms have been proposed\nto estimate related quantities in crowdsourcing, these often rely on\nsophisticated generative models with iterative inference schemes, whereas\nCROWDLAB is based on simple weighted ensembling. Many algorithms also rely\nsolely on annotator statistics, ignoring the features of the examples from\nwhich the annotations derive. CROWDLAB in contrast utilizes any classifier\nmodel trained on these features, which can generalize between examples with\nsimilar features. In evaluations on real-world multi-annotator image data, our\nproposed method provides superior estimates for (1)-(3) than many alternative\nalgorithms.",
    "descriptor": "",
    "authors": [
      "Hui Wen Goh",
      "Ulyana Tkachenko",
      "Jonas Mueller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06812"
  },
  {
    "id": "arXiv:2210.06814",
    "title": "Accelerating the Evolutionary Algorithms by Gaussian Process Regression  with $\u03b5$-greedy acquisition function",
    "abstract": "In this paper, we propose a novel method to estimate the elite individual to\naccelerate the convergence of optimization. Inspired by the Bayesian\nOptimization Algorithm (BOA), the Gaussian Process Regression (GPR) is applied\nto approximate the fitness landscape of original problems based on every\ngeneration of optimization. And simple but efficient $\\epsilon$-greedy\nacquisition function is employed to find a promising solution in the surrogate\nmodel. Proximity Optimal Principle (POP) states that well-performed solutions\nhave a similar structure, and there is a high probability of better solutions\nexisting around the elite individual. Based on this hypothesis, in each\ngeneration of optimization, we replace the worst individual in Evolutionary\nAlgorithms (EAs) with the elite individual to participate in the evolution\nprocess. To illustrate the scalability of our proposal, we combine our proposal\nwith the Genetic Algorithm (GA), Differential Evolution (DE), and CMA-ES.\nExperimental results in CEC2013 benchmark functions show our proposal has a\nbroad prospect to estimate the elite individual and accelerate the convergence\nof optimization.",
    "descriptor": "",
    "authors": [
      "Rui Zhong",
      "Enzhi Zhang",
      "Masaharu Munetomo"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06814"
  },
  {
    "id": "arXiv:2210.06816",
    "title": "ALIFE: Adaptive Logit Regularizer and Feature Replay for Incremental  Semantic Segmentation",
    "abstract": "We address the problem of incremental semantic segmentation (ISS) recognizing\nnovel object/stuff categories continually without forgetting previous ones that\nhave been learned. The catastrophic forgetting problem is particularly severe\nin ISS, since pixel-level ground-truth labels are available only for the novel\ncategories at training time. To address the problem, regularization-based\nmethods exploit probability calibration techniques to learn semantic\ninformation from unlabeled pixels. While such techniques are effective, there\nis still a lack of theoretical understanding of them. Replay-based methods\npropose to memorize a small set of images for previous categories. They achieve\nstate-of-the-art performance at the cost of large memory footprint. We propose\nin this paper a novel ISS method, dubbed ALIFE, that provides a better\ncompromise between accuracy and efficiency. To this end, we first show an\nin-depth analysis on the calibration techniques to better understand the\neffects on ISS. Based on this, we then introduce an adaptive logit regularizer\n(ALI) that enables our model to better learn new categories, while retaining\nknowledge for previous ones. We also present a feature replay scheme that\nmemorizes features, instead of images directly, in order to reduce memory\nrequirements significantly. Since a feature extractor is changed continually,\nmemorized features should also be updated at every incremental stage. To handle\nthis, we introduce category-specific rotation matrices updating the features\nfor each category separately. We demonstrate the effectiveness of our approach\nwith extensive experiments on standard ISS benchmarks, and show that our method\nachieves a better trade-off in terms of accuracy and efficiency.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Youngmin Oh",
      "Donghyeon Baek",
      "Bumsub Ham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06816"
  },
  {
    "id": "arXiv:2210.06819",
    "title": "Mean-field analysis for heavy ball methods: Dropout-stability,  connectivity, and global convergence",
    "abstract": "The stochastic heavy ball method (SHB), also known as stochastic gradient\ndescent (SGD) with Polyak's momentum, is widely used in training neural\nnetworks. However, despite the remarkable success of such algorithm in\npractice, its theoretical characterization remains limited. In this paper, we\nfocus on neural networks with two and three layers and provide a rigorous\nunderstanding of the properties of the solutions found by SHB: \\emph{(i)}\nstability after dropping out part of the neurons, \\emph{(ii)} connectivity\nalong a low-loss path, and \\emph{(iii)} convergence to the global optimum. To\nachieve this goal, we take a mean-field view and relate the SHB dynamics to a\ncertain partial differential equation in the limit of large network widths.\nThis mean-field perspective has inspired a recent line of work focusing on SGD\nwhile, in contrast, our paper considers an algorithm with momentum. More\nspecifically, after proving existence and uniqueness of the limit differential\nequations, we show convergence to the global optimum and give a quantitative\nbound between the mean-field limit and the SHB dynamics of a finite-width\nnetwork. Armed with this last bound, we are able to establish the\ndropout-stability and connectivity of SHB solutions.",
    "descriptor": "\nComments: 49 pages\n",
    "authors": [
      "Diyuan Wu",
      "Vyacheslav Kungurtsev",
      "Marco Mondelli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06819"
  },
  {
    "id": "arXiv:2210.06820",
    "title": "Personalized Federated Hypernetworks for Privacy Preservation in  Multi-Task Reinforcement Learning",
    "abstract": "Multi-Agent Reinforcement Learning currently focuses on implementations where\nall data and training can be centralized to one machine. But what if local\nagents are split across multiple tasks, and need to keep data private between\neach? We develop the first application of Personalized Federated Hypernetworks\n(PFH) to Reinforcement Learning (RL). We then present a novel application of\nPFH to few-shot transfer, and demonstrate significant initial increases in\nlearning. PFH has never been demonstrated beyond supervised learning\nbenchmarks, so we apply PFH to an important domain: RL price-setting for energy\ndemand response. We consider a general case across where agents are split\nacross multiple microgrids, wherein energy consumption data must be kept\nprivate within each microgrid. Together, our work explores how the fields of\npersonalized federated learning and RL can come together to make learning\nefficient across multiple tasks while keeping data secure.",
    "descriptor": "",
    "authors": [
      "Doseok Jang",
      "Larry Yan",
      "Lucas Spangher",
      "Costas J. Spanos",
      "Selvaprabu Nadarajah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06820"
  },
  {
    "id": "arXiv:2210.06821",
    "title": "Conflict-free Charging and Real-time Control for an Electric Bus Network",
    "abstract": "The rapid adoption of electric buses by transit agencies around the world is\nleading to new challenges in the planning and operation of bus networks. In\nparticular, the limited driving range of electric vehicles imposes operational\nconstraints such as the need to charge buses during service. Research on this\ntopic has mostly focused on the strategic and tactical planning aspects until\nnow, and very little work has been done on the real-time operational aspect. To\nremedy this, we propose integrating the charging scheduling problem with a\nreal-time speed control strategy in this paper. The control problem is\nformulated as a mixed-integer linear program and solved to optimality with the\nbranch-and-bound method. Simulations are carried out by repeatedly solving the\ncontrol problem in a receding horizon fashion over a full day of operation. The\nresults show that the proposed controller manages to anticipate and avoid\npotential conflicts where the charging demand exceeds the charger capacity.\nThis feature makes the controller achieve lower operational costs, both in\nterms of service regularity and energy consumption, compared to a standard\nfirst-come, first-served charging scheme.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "R\u00e9mi Lacombe",
      "Nikolce Murgovski",
      "S\u00e9bastien Gros",
      "Bal\u00e1zs Kulcs\u00e1r"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06821"
  },
  {
    "id": "arXiv:2210.06823",
    "title": "Scalable Neural Video Representations with Learnable Positional Features",
    "abstract": "Succinct representation of complex signals using coordinate-based neural\nrepresentations (CNRs) has seen great progress, and several recent efforts\nfocus on extending them for handling videos. Here, the main challenge is how to\n(a) alleviate a compute-inefficiency in training CNRs to (b) achieve\nhigh-quality video encoding while (c) maintaining the parameter-efficiency. To\nmeet all requirements (a), (b), and (c) simultaneously, we propose neural video\nrepresentations with learnable positional features (NVP), a novel CNR by\nintroducing \"learnable positional features\" that effectively amortize a video\nas latent codes. Specifically, we first present a CNR architecture based on\ndesigning 2D latent keyframes to learn the common video contents across each\nspatio-temporal axis, which dramatically improves all of those three\nrequirements. Then, we propose to utilize existing powerful image and video\ncodecs as a compute-/memory-efficient compression procedure of latent codes. We\ndemonstrate the superiority of NVP on the popular UVG benchmark; compared with\nprior arts, NVP not only trains 2 times faster (less than 5 minutes) but also\nexceeds their encoding quality as 34.07$\\rightarrow$34.57 (measured with the\nPSNR metric), even using $>$8 times fewer parameters. We also show intriguing\nproperties of NVP, e.g., video inpainting, video frame interpolation, etc.",
    "descriptor": "\nComments: NeurIPS 2022. Project page with videos and code: this https URL\n",
    "authors": [
      "Subin Kim",
      "Sihyun Yu",
      "Jaeho Lee",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06823"
  },
  {
    "id": "arXiv:2210.06824",
    "title": "An Empirical Study on Finding Spans",
    "abstract": "We present an empirical study on methods for span finding, the selection of\nconsecutive tokens in text for some downstream tasks. We focus on approaches\nthat can be employed in training end-to-end information extraction systems. We\nrecognize there is no silver bullet that can simply solve all downstream tasks\nwell without considering task properties and provide our observations to help\nwith design choices in the future: 1) tagging method usually yields a higher\nprecision while span enumeration and boundary prediction prefer a higher\nrecall; 2) span type information can benefit boundary prediction approach; 3)\nadditional contextualization does not help span finding in most cases.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Weiwei Gu",
      "Boyuan Zheng",
      "Yunmo Chen",
      "Tongfei Chen",
      "Benjamin Van Durme"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06824"
  },
  {
    "id": "arXiv:2210.06825",
    "title": "Fast Optimization of Weighted Sparse Decision Trees for use in Optimal  Treatment Regimes and Optimal Policy Design",
    "abstract": "Sparse decision trees are one of the most common forms of interpretable\nmodels. While recent advances have produced algorithms that fully optimize\nsparse decision trees for prediction, that work does not address policy design,\nbecause the algorithms cannot handle weighted data samples. Specifically, they\nrely on the discreteness of the loss function, which means that real-valued\nweights cannot be directly used. For example, none of the existing techniques\nproduce policies that incorporate inverse propensity weighting on individual\ndata points. We present three algorithms for efficient sparse weighted decision\ntree optimization. The first approach directly optimizes the weighted loss\nfunction; however, it tends to be computationally inefficient for large\ndatasets. Our second approach, which scales more efficiently, transforms\nweights to integer values and uses data duplication to transform the weighted\ndecision tree optimization problem into an unweighted (but larger) counterpart.\nOur third algorithm, which scales to much larger datasets, uses a randomized\nprocedure that samples each data point with a probability proportional to its\nweight. We present theoretical bounds on the error of the two fast methods and\nshow experimentally that these methods can be two orders of magnitude faster\nthan the direct optimization of the weighted loss, without losing significant\naccuracy.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.00798\n",
    "authors": [
      "Ali Behrouz",
      "Mathias Lecuyer",
      "Cynthia Rudin",
      "Margo Seltzer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06825"
  },
  {
    "id": "arXiv:2210.06827",
    "title": "Processing Particle Data Flows with SmartNICs",
    "abstract": "Many distributed applications implement complex data flows and need a\nflexible mechanism for routing data between producers and consumers. Recent\nadvances in programmable network interface cards, or SmartNICs, represent an\nopportunity to offload data-flow tasks into the network fabric, thereby freeing\nthe hosts to perform other work. System architects in this space face multiple\nquestions about the best way to leverage SmartNICs as processing elements in\ndata flows. In this paper, we advocate the use of Apache Arrow as a foundation\nfor implementing data-flow tasks on SmartNICs. We report on our experiences\nadapting a partitioning algorithm for particle data to Apache Arrow and measure\nthe on-card processing performance for the BlueField-2 SmartNIC. Our\nexperiments confirm that the BlueField-2's (de)compression hardware can have a\nsignificant impact on in-transit workflows where data must be unpacked,\nprocessed, and repacked.",
    "descriptor": "\nComments: This is an expansion of the paper with the same name published in HPEC'22\n",
    "authors": [
      "Jianshen Liu",
      "Carlos Maltzahn",
      "Matthew L. Curry",
      "Craig Ulmer"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2210.06827"
  },
  {
    "id": "arXiv:2210.06828",
    "title": "Rethinking Annotation: Can Language Learners Contribute?",
    "abstract": "Researchers have traditionally recruited native speakers to provide\nannotations for the widely used benchmark datasets. But there are languages for\nwhich recruiting native speakers is difficult, and it would help to get\nlearners of those languages to annotate the data. In this paper, we investigate\nwhether language learners can contribute annotations to the benchmark datasets.\nIn a carefully controlled annotation experiment, we recruit 36 language\nlearners, provide two types of additional resources (dictionaries and\nmachine-translated sentences), and perform mini-tests to measure their language\nproficiency. We target three languages, English, Korean, and Indonesian, and\nfour NLP tasks, sentiment analysis, natural language inference, named entity\nrecognition, and machine reading comprehension. We find that language learners,\nespecially those with intermediate or advanced language proficiency, are able\nto provide fairly accurate labels with the help of additional resources.\nMoreover, we show that data annotation improves learners' language proficiency\nin terms of vocabulary and grammar. The implication of our findings is that\nbroadening the annotation task to include language learners can open up the\nopportunity to build benchmark datasets for languages for which it is difficult\nto recruit native speakers.",
    "descriptor": "",
    "authors": [
      "Haneul Yoo",
      "Rifki Afina Putri",
      "Changyoon Lee",
      "Youngin Lee",
      "So-Yeon Ahn",
      "Dongyeop Kang",
      "Alice Oh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06828"
  },
  {
    "id": "arXiv:2210.06829",
    "title": "Ensemble Creation via Anchored Regularization for Unsupervised Aspect  Extraction",
    "abstract": "Aspect Based Sentiment Analysis is the most granular form of sentiment\nanalysis that can be performed on the documents / sentences. Besides delivering\nthe most insights at a finer grain, it also poses equally daunting challenges.\nOne of them being the shortage of labelled data. To bring in value right out of\nthe box for the text data being generated at a very fast pace in today's world,\nunsupervised aspect-based sentiment analysis allows us to generate insights\nwithout investing time or money in generating labels. From topic modelling\napproaches to recent deep learning-based aspect extraction models, this domain\nhas seen a lot of development. One of the models that we improve upon is ABAE\nthat reconstructs the sentences as a linear combination of aspect terms present\nin it, In this research we explore how we can use information from another\nunsupervised model to regularize ABAE, leading to better performance. We\ncontrast it with baseline rule based ensemble and show that the ensemble\nmethods work better than the individual models and the regularization based\nensemble performs better than the rule-based one.",
    "descriptor": "",
    "authors": [
      "Pulah Dhandekar",
      "Manu Joseph"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06829"
  },
  {
    "id": "arXiv:2210.06831",
    "title": "Multi-Target XGBoostLSS Regression",
    "abstract": "Current implementations of Gradient Boosting Machines are mostly designed for\nsingle-target regression tasks and commonly assume independence between\nresponses when used in multivariate settings. As such, these models are not\nwell suited if non-negligible dependencies exist between targets. To overcome\nthis limitation, we present an extension of XGBoostLSS that models multiple\ntargets and their dependencies in a probabilistic regression setting. Empirical\nresults show that our approach outperforms existing GBMs with respect to\nruntime and compares well in terms of accuracy.",
    "descriptor": "\nComments: Compositional Data Analysis; Multi-Target Distributional Regression; Probabilistic Modelling; XGBoostLSS\n",
    "authors": [
      "Alexander M\u00e4rz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06831"
  },
  {
    "id": "arXiv:2210.06832",
    "title": "Soft isogeometric analysis of the Bound States of a Quantum Three-Body  Problem in 1D",
    "abstract": "The study of quantum three-body problems has been centered on low-energy\nstates that rely on accurate numerical approximation. Recently, isogeometric\nanalysis (IGA) has been adopted to solve the problem as an alternative but more\nrobust (with respect to atom mass ratios) method that outperforms the classical\nBorn-Oppenheimer (BO) approximation. In this paper, we focus on the performance\nof IGA and apply the recently-developed softIGA to reduce the spectral errors\nof the low-energy bound states. The main idea is to add high-order\nderivative-jump terms with a penalty parameter to the IGA bilinear forms. With\nan optimal choice of the penalty parameter, we observe eigenvalue error\nsuperconvergence. We focus on linear (finite elements) and quadratic elements\nand demonstrate the outperformance of softIGA over IGA through a variety of\nexamples including both two- and three-body problems in 1D.",
    "descriptor": "",
    "authors": [
      "Danyang Li",
      "Quanling Deng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.06832"
  },
  {
    "id": "arXiv:2210.06833",
    "title": "Exploiting Mixed Unlabeled Data for Detecting Samples of Seen and Unseen  Out-of-Distribution Classes",
    "abstract": "Out-of-Distribution (OOD) detection is essential in real-world applications,\nwhich has attracted increasing attention in recent years. However, most\nexisting OOD detection methods require many labeled In-Distribution (ID) data,\ncausing a heavy labeling cost. In this paper, we focus on the more realistic\nscenario, where limited labeled data and abundant unlabeled data are available,\nand these unlabeled data are mixed with ID and OOD samples. We propose the\nAdaptive In-Out-aware Learning (AIOL) method, in which we employ the\nappropriate temperature to adaptively select potential ID and OOD samples from\nthe mixed unlabeled data and consider the entropy over them for OOD detection.\nMoreover, since the test data in realistic applications may contain OOD samples\nwhose classes are not in the mixed unlabeled data (we call them unseen OOD\nclasses), data augmentation techniques are brought into the method to further\nimprove the performance. The experiments are conducted on various benchmark\ndatasets, which demonstrate the superiority of our method.",
    "descriptor": "\nComments: Published in AAAI 2022. arXiv admin note: text overlap with arXiv:2209.09616 by other authors\n",
    "authors": [
      "Yi-Xuan Sun",
      "Wei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06833"
  },
  {
    "id": "arXiv:2210.06835",
    "title": "Multi-agent Dynamic Algorithm Configuration",
    "abstract": "Automated algorithm configuration relieves users from tedious,\ntrial-and-error tuning tasks. A popular algorithm configuration tuning paradigm\nis dynamic algorithm configuration (DAC), in which an agent learns dynamic\nconfiguration policies across instances by reinforcement learning (RL).\nHowever, in many complex algorithms, there may exist different types of\nconfiguration hyperparameters, and such heterogeneity may bring difficulties\nfor classic DAC which uses a single-agent RL policy. In this paper, we aim to\naddress this issue and propose multi-agent DAC (MA-DAC), with one agent working\nfor one type of configuration hyperparameter. MA-DAC formulates the dynamic\nconfiguration of a complex algorithm with multiple types of hyperparameters as\na contextual multi-agent Markov decision process and solves it by a cooperative\nmulti-agent RL (MARL) algorithm. To instantiate, we apply MA-DAC to a\nwell-known optimization algorithm for multi-objective optimization problems.\nExperimental results show the effectiveness of MA-DAC in not only achieving\nsuperior performance compared with other configuration tuning approaches based\non heuristic rules, multi-armed bandits, and single-agent RL, but also being\ncapable of generalizing to different problem classes. Furthermore, we release\nthe environments in this paper as a benchmark for testing MARL algorithms, with\nthe hope of facilitating the application of MARL.",
    "descriptor": "\nComments: NeurIPS 2022 Accept\n",
    "authors": [
      "Ke Xue",
      "Jiacheng Xu",
      "Lei Yuan",
      "Miqing Li",
      "Chao Qian",
      "Zongzhang Zhang",
      "Yang Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06835"
  },
  {
    "id": "arXiv:2210.06839",
    "title": "Inverse and forward sparse-grids-based uncertainty quantification  analysis of laser-based powder bed fusion of metals",
    "abstract": "The present paper aims at applying uncertainty quantification methodologies\nto process simulations of Laser-based Powder Bed Fusion of Metal. In\nparticular, for a part-scale thermomechanical model of an Inconel 625\nsuper-alloy beam, we study the uncertainties of three process parameters,\nnamely the activation temperature, the powder convection coefficient and the\ngas convection coefficient. First, we perform a variance-based Global\nSensitivity Analysis to study how each uncertain parameter contributes to the\nvariability of the beam displacements. The results allow us to conclude that\nthe gas convection coefficient has little impact and can therefore be fixed to\na constant value for subsequent studies. Then, we conduct an inverse\nuncertainty quantification analysis, based on a Bayesian approach on synthetic\ndisplacements data, to quantify the uncertainties of the activation temperature\nand the powder convection coefficient. Finally, we use the results of the\ninverse uncertainty quantification analysis to perform a data-informed forward\nuncertainty quantification analysis of the residual strains. Crucially, we make\nuse of surrogate models based on sparse grids to reduce the computational\nburden.",
    "descriptor": "",
    "authors": [
      "Mihaela Chiappetta",
      "Chiara Piazzola",
      "Massimo Carraturo",
      "Lorenzo Tamellini",
      "Alessandro Reali",
      "Ferdinando Auricchio"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.06839"
  },
  {
    "id": "arXiv:2210.06840",
    "title": "Forensic-Ready Risk Management Concepts",
    "abstract": "Currently, numerous approaches exist supporting the implementation of\nforensic readiness and, indirectly, forensic-ready software systems. However,\nthe terminology used in the approaches and their focus tends to vary. To\nfacilitate the design of forensic-ready software systems, the clarity of the\nunderlying concepts needs to be established so that their requirements can be\nunambiguously formulated and assessed. This is especially important when\nconsidering forensic readiness as an add-on to information security. In this\npaper, the concepts relevant to forensic readiness are derived and aligned\nbased on six existing approaches. The results then serve as a stepping stone\nfor enhancing Information Systems Security Risk Management (ISSRM) with\nforensic readiness.",
    "descriptor": "",
    "authors": [
      "Lukas Daubner",
      "Martin Macak",
      "Raimundas Matulevi\u010dius",
      "Barbora Buhnova",
      "Sofija Maksovi\u0107",
      "Tomas Pitner"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.06840"
  },
  {
    "id": "arXiv:2210.06843",
    "title": "Neighborhood Structure Configuration Models",
    "abstract": "We develop a new method to efficiently sample synthetic networks that\npreserve the d-hop neighborhood structure of a given network for any given d.\nThe proposed algorithm trades off the diversity in network samples against the\ndepth of the neighborhood structure that is preserved. Our key innovation is to\nemploy a colored Configuration Model with colors derived from iterations of the\nso-called Color Refinement algorithm. We prove that with increasing iterations\nthe preserved structural information increases: the generated synthetic\nnetworks and the original network become more and more similar, and are\neventually indistinguishable in terms of centrality measures such as PageRank,\nHITS, Katz centrality and eigenvector centrality. Our work enables to\nefficiently generate samples with a precisely controlled similarity to the\noriginal network, especially for large networks.",
    "descriptor": "",
    "authors": [
      "Felix I. Stamm",
      "Michael Scholkemper",
      "Markus Strohmaier",
      "Michael T. Schaub"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.06843"
  },
  {
    "id": "arXiv:2210.06845",
    "title": "The Fine-Grained Complexity of Graph Homomorphism Parameterized by  Clique-Width",
    "abstract": "The generic homomorphism problem, which asks whether an input graph $G$\nadmits a homomorphism into a fixed target graph $H$, has been widely studied in\nthe literature. In this article, we provide a fine-grained complexity\nclassification of the running time of the homomorphism problem with respect to\nthe clique-width of $G$ (denoted $\\operatorname{cw}$) for virtually all choices\nof $H$ under the Strong Exponential Time Hypothesis. In particular, we identify\na property of $H$ called the signature number $s(H)$ and show that for each\n$H$, the homomorphism problem can be solved in time\n$\\mathcal{O}^*(s(H)^{\\operatorname{cw}})$. Crucially, we then show that this\nalgorithm can be used to obtain essentially tight upper bounds. Specifically,\nwe provide a reduction that yields matching lower bounds for each $H$ that is\neither a projective core or a graph admitting a factorization with additional\nproperties -- allowing us to cover all possible target graphs under\nlong-standing conjectures.",
    "descriptor": "\nComments: 21 pages, 2 figures. arXiv admin note: text overlap with arXiv:1804.07975 by other authors\n",
    "authors": [
      "Robert Ganian",
      "Thekla Hamm",
      "Viktoriia Korchemna",
      "Karolina Okrasa",
      "Kirill Simonov"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.06845"
  },
  {
    "id": "arXiv:2210.06846",
    "title": "An $\u03b1$-regret analysis of Adversarial Bilateral Trade",
    "abstract": "We study sequential bilateral trade where sellers and buyers valuations are\ncompletely arbitrary (i.e., determined by an adversary). Sellers and buyers are\nstrategic agents with private valuations for the good and the goal is to design\na mechanism that maximizes efficiency (or gain from trade) while being\nincentive compatible, individually rational and budget balanced. In this paper\nwe consider gain from trade which is harder to approximate than social welfare.\nWe consider a variety of feedback scenarios and distinguish the cases where\nthe mechanism posts one price and when it can post different prices for buyer\nand seller. We show several surprising results about the separation between the\ndifferent scenarios. In particular we show that (a) it is impossible to achieve\nsublinear $\\alpha$-regret for any $\\alpha<2$, (b) but with full feedback\nsublinear $2$-regret is achievable (c) with a single price and partial feedback\none cannot get sublinear $\\alpha$ regret for any constant $\\alpha$ (d)\nnevertheless, posting two prices even with one-bit feedback achieves sublinear\n$2$-regret, and (e) there is a provable separation in the $2$-regret bounds\nbetween full and partial feedback.",
    "descriptor": "",
    "authors": [
      "Yossi Azar",
      "Amos Fiat",
      "Federico Fusco"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06846"
  },
  {
    "id": "arXiv:2210.06849",
    "title": "Retrospectives on the Embodied AI Workshop",
    "abstract": "We present a retrospective on the state of Embodied AI research. Our analysis\nfocuses on 13 challenges presented at the Embodied AI Workshop at CVPR. These\nchallenges are grouped into three themes: (1) visual navigation, (2)\nrearrangement, and (3) embodied vision-and-language. We discuss the dominant\ndatasets within each theme, evaluation metrics for the challenges, and the\nperformance of state-of-the-art models. We highlight commonalities between top\napproaches to the challenges and identify potential future directions for\nEmbodied AI research.",
    "descriptor": "",
    "authors": [
      "Matt Deitke",
      "Dhruv Batra",
      "Yonatan Bisk",
      "Tommaso Campari",
      "Angel X. Chang",
      "Devendra Singh Chaplot",
      "Changan Chen",
      "Claudia P\u00e9rez D'Arpino",
      "Kiana Ehsani",
      "Ali Farhadi",
      "Li Fei-Fei",
      "Anthony Francis",
      "Chuang Gan",
      "Kristen Grauman",
      "David Hall",
      "Winson Han",
      "Unnat Jain",
      "Aniruddha Kembhavi",
      "Jacob Krantz",
      "Stefan Lee",
      "Chengshu Li",
      "Sagnik Majumder",
      "Oleksandr Maksymets",
      "Roberto Mart\u00edn-Mart\u00edn",
      "Roozbeh Mottaghi",
      "Sonia Raychaudhuri",
      "Mike Roberts",
      "Silvio Savarese",
      "Manolis Savva",
      "Mohit Shridhar",
      "Niko S\u00fcnderhauf",
      "Andrew Szot",
      "Ben Talbot",
      "Joshua B. Tenenbaum",
      "Jesse Thomason",
      "Alexander Toshev",
      "Joanne Truong",
      "Luca Weihs",
      "Jiajun Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06849"
  },
  {
    "id": "arXiv:2210.06850",
    "title": "Sample-Then-Optimize Batch Neural Thompson Sampling",
    "abstract": "Bayesian optimization (BO), which uses a Gaussian process (GP) as a surrogate\nto model its objective function, is popular for black-box optimization.\nHowever, due to the limitations of GPs, BO underperforms in some problems such\nas those with categorical, high-dimensional or image inputs. To this end,\nrecent works have used the highly expressive neural networks (NNs) as the\nsurrogate model and derived theoretical guarantees using the theory of neural\ntangent kernel (NTK). However, these works suffer from the limitations of the\nrequirement to invert an extremely large parameter matrix and the restriction\nto the sequential (rather than batch) setting. To overcome these limitations,\nwe introduce two algorithms based on the Thompson sampling (TS) policy named\nSample-Then-Optimize Batch Neural TS (STO-BNTS) and STO-BNTS-Linear. To choose\nan input query, we only need to train an NN (resp. a linear model) and then\nchoose the query by maximizing the trained NN (resp. linear model), which is\nequivalently sampled from the GP posterior with the NTK as the kernel function.\nAs a result, our algorithms sidestep the need to invert the large parameter\nmatrix yet still preserve the validity of the TS policy. Next, we derive regret\nupper bounds for our algorithms with batch evaluations, and use insights from\nbatch BO and NTK to show that they are asymptotically no-regret under certain\nconditions. Finally, we verify their empirical effectiveness using practical\nAutoML and reinforcement learning experiments.",
    "descriptor": "\nComments: Accepted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022), Extended version with proofs and additional experimental details and results, 30 pages\n",
    "authors": [
      "Zhongxiang Dai",
      "Yao Shu",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06850"
  },
  {
    "id": "arXiv:2210.06852",
    "title": "Overview of BioASQ 2022: The tenth BioASQ challenge on Large-Scale  Biomedical Semantic Indexing and Question Answering",
    "abstract": "This paper presents an overview of the tenth edition of the BioASQ challenge\nin the context of the Conference and Labs of the Evaluation Forum (CLEF) 2022.\nBioASQ is an ongoing series of challenges that promotes advances in the domain\nof large-scale biomedical semantic indexing and question answering. In this\nedition, the challenge was composed of the three established tasks a, b, and\nSynergy, and a new task named DisTEMIST for automatic semantic annotation and\ngrounding of diseases from clinical content in Spanish, a key concept for\nsemantic indexing and search engines of literature and clinical records. This\nyear, BioASQ received more than 170 distinct systems from 38 teams in total for\nthe four different tasks of the challenge. As in previous years, the majority\nof the competing systems outperformed the strong baselines, indicating the\ncontinuous advancement of the state-of-the-art in this domain.",
    "descriptor": "\nComments: 25 pages, 14 tables, 4 figures. arXiv admin note: substantial text overlap with arXiv:2106.14885\n",
    "authors": [
      "Anastasios Nentidis",
      "Georgios Katsimpras",
      "Eirini Vandorou",
      "Anastasia Krithara",
      "Antonio Miranda-Escalada",
      "Luis Gasco",
      "Martin Krallinger",
      "Georgios Paliouras"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.06852"
  },
  {
    "id": "arXiv:2210.06853",
    "title": "NeuralRoom: Geometry-Constrained Neural Implicit Surfaces for Indoor  Scene Reconstruction",
    "abstract": "We present a novel neural surface reconstruction method called NeuralRoom for\nreconstructing room-sized indoor scenes directly from a set of 2D images.\nRecently, implicit neural representations have become a promising way to\nreconstruct surfaces from multiview images due to their high-quality results\nand simplicity. However, implicit neural representations usually cannot\nreconstruct indoor scenes well because they suffer severe shape-radiance\nambiguity. We assume that the indoor scene consists of texture-rich and flat\ntexture-less regions. In texture-rich regions, the multiview stereo can obtain\naccurate results. In the flat area, normal estimation networks usually obtain a\ngood normal estimation. Based on the above observations, we reduce the possible\nspatial variation range of implicit neural surfaces by reliable geometric\npriors to alleviate shape-radiance ambiguity. Specifically, we use multiview\nstereo results to limit the NeuralRoom optimization space and then use reliable\ngeometric priors to guide NeuralRoom training. Then the NeuralRoom would\nproduce a neural scene representation that can render an image consistent with\nthe input training images. In addition, we propose a smoothing method called\nperturbation-residual restrictions to improve the accuracy and completeness of\nthe flat region, which assumes that the sampling points in a local surface\nshould have the same normal and similar distance to the observation center.\nExperiments on the ScanNet dataset show that our method can reconstruct the\ntexture-less area of indoor scenes while maintaining the accuracy of detail. We\nalso apply NeuralRoom to more advanced multiview reconstruction algorithms and\nsignificantly improve their reconstruction quality.",
    "descriptor": "",
    "authors": [
      "Yusen Wang",
      "Zongcheng Li",
      "Yu Jiang",
      "Kaixuan Zhou",
      "Tuo Cao",
      "Yanping Fu",
      "Chunxia Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06853"
  },
  {
    "id": "arXiv:2210.06856",
    "title": "Federated Learning for Tabular Data: Exploring Potential Risk to Privacy",
    "abstract": "Federated Learning (FL) has emerged as a potentially powerful\nprivacy-preserving machine learning methodology, since it avoids exchanging\ndata between participants, but instead exchanges model parameters. FL has\ntraditionally been applied to image, voice and similar data, but recently it\nhas started to draw attention from domains including financial services where\nthe data is predominantly tabular. However, the work on tabular data has not\nyet considered potential attacks, in particular attacks using Generative\nAdversarial Networks (GANs), which have been successfully applied to FL for\nnon-tabular data. This paper is the first to explore leakage of private data in\nFederated Learning systems that process tabular data. We design a Generative\nAdversarial Networks (GANs)-based attack model which can be deployed on a\nmalicious client to reconstruct data and its properties from other\nparticipants. As a side-effect of considering tabular data, we are able to\nstatistically assess the efficacy of the attack (without relying on human\nobservation such as done for FL for images). We implement our attack model in a\nrecently developed generic FL software framework for tabular data processing.\nThe experimental results demonstrate the effectiveness of the proposed attack\nmodel, thus suggesting that further research is required to counter GAN-based\nprivacy attacks.",
    "descriptor": "\nComments: In the proceedings of The 33rd IEEE International Symposium on Software Reliability Engineering (ISSRE), November 2022\n",
    "authors": [
      "Han Wu",
      "Zilong Zhao",
      "Lydia Y. Chen",
      "Aad van Moorsel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.06856"
  },
  {
    "id": "arXiv:2210.06858",
    "title": "Adopting Microservices and DevOps in the Cyber-Physical Systems Domain:  A Rapid Review and Case Study",
    "abstract": "The domain of cyber-physical systems (CPS) has recently seen strong growth,\ne.g., due to the rise of the Internet of Things (IoT) in industrial domains,\ncommonly referred to as \"Industry 4.0\". However, CPS challenges like the strong\nhardware focus can impact modern software development practices, especially in\nthe context of modernizing legacy systems. While microservices and DevOps have\nbeen widely studied for enterprise applications, there is insufficient coverage\nfor the CPS domain. Our goal is therefore to analyze the peculiarities of such\nsystems regarding challenges and practices for using and migrating towards\nmicroservices and DevOps. We conducted a rapid review based on 146 scientific\npapers, and subsequently validated our findings in an interview-based case\nstudy with 9 CPS professionals in different business units at Siemens AG. The\ncombined results picture the specifics of microservices and DevOps in the CPS\ndomain. While several differences were revealed that may require adapted\nmethods, many challenges and practices are shared with typical enterprise\napplications. Our study supports CPS researchers and practitioners with a\nsummary of challenges, practices to address them, and research opportunities.",
    "descriptor": "\nComments: 10 pages, 8 figures, accepted for publication at \"Software: Practice and Experience - Wiley Online Library\"\n",
    "authors": [
      "Jonas Fritzsch",
      "Justus Bogner",
      "Markus Haug",
      "Ana Cristina Franco da Silva",
      "Carolin Rubner",
      "Matthias Saft",
      "Horst Sauer",
      "Stefan Wagner"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.06858"
  },
  {
    "id": "arXiv:2210.06866",
    "title": "Competition among Parallel Contests",
    "abstract": "We investigate the model of multiple contests held in parallel, where each\ncontestant selects one contest to join and each contest designer decides the\nprize structure to compete for the participation of contestants. We first\nanalyze the strategic behaviors of contestants and completely characterize the\nsymmetric Bayesian Nash equilibrium. As for the strategies of contest\ndesigners, when other designers' strategies are known, we show that computing\nthe best response is NP-hard and propose a fully polynomial time approximation\nscheme (FPTAS) to output the $\\epsilon$-approximate best response. When other\ndesigners' strategies are unknown, we provide a worst case analysis on one\ndesigner's strategy. We give an upper bound on the utility of any strategy and\npropose a method to construct a strategy whose utility can guarantee a constant\nratio of this upper bound in the worst case.",
    "descriptor": "\nComments: Accepted by the 18th Conference on Web and Internet Economics (WINE 2022)\n",
    "authors": [
      "Xiaotie Deng",
      "Ningyuan Li",
      "Weian Li",
      "Qi Qi"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.06866"
  },
  {
    "id": "arXiv:2210.06871",
    "title": "Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face  Recognition",
    "abstract": "Deep learning models have shown their vulnerability when dealing with\nadversarial attacks. Existing attacks almost perform on low-level instances,\nsuch as pixels and super-pixels, and rarely exploit semantic clues. For face\nrecognition attacks, existing methods typically generate the l_p-norm\nperturbations on pixels, however, resulting in low attack transferability and\nhigh vulnerability to denoising defense models. In this work, instead of\nperforming perturbations on the low-level pixels, we propose to generate\nattacks through perturbing on the high-level semantics to improve attack\ntransferability. Specifically, a unified flexible framework, Adversarial\nAttributes (Adv-Attribute), is designed to generate inconspicuous and\ntransferable attacks on face recognition, which crafts the adversarial noise\nand adds it into different attributes based on the guidance of the difference\nin face recognition features from the target. Moreover, the importance-aware\nattribute selection and the multi-objective optimization strategy are\nintroduced to further ensure the balance of stealthiness and attacking\nstrength. Extensive experiments on the FFHQ and CelebA-HQ datasets show that\nthe proposed Adv-Attribute method achieves the state-of-the-art attacking\nsuccess rates while maintaining better visual effects against recent attack\nmethods.",
    "descriptor": "\nComments: Accepted by NeurIPS2022\n",
    "authors": [
      "Shuai Jia",
      "Bangjie Yin",
      "Taiping Yao",
      "Shouhong Ding",
      "Chunhua Shen",
      "Xiaokang Yang",
      "Chao Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06871"
  },
  {
    "id": "arXiv:2210.06873",
    "title": "Data augmentation on-the-fly and active learning in data stream  classification",
    "abstract": "There is an emerging need for predictive models to be trained on-the-fly,\nsince in numerous machine learning applications data are arriving in an online\nfashion. A critical challenge encountered is that of limited availability of\nground truth information (e.g., labels in classification tasks) as new data are\nobserved one-by-one online, while another significant challenge is that of\nclass imbalance. This work introduces the novel Augmented Queues method, which\naddresses the dual-problem by combining in a synergistic manner online active\nlearning, data augmentation, and a multi-queue memory to maintain separate and\nbalanced queues for each class. We perform an extensive experimental study\nusing image and time-series augmentations, in which we examine the roles of the\nactive learning budget, memory size, imbalance level, and neural network type.\nWe demonstrate two major advantages of Augmented Queues. First, it does not\nreserve additional memory space as the generation of synthetic data occurs only\nat training times. Second, learning models have access to more labelled data\nwithout the need to increase the active learning budget and / or the original\nmemory size. Learning on-the-fly poses major challenges which, typically,\nhinder the deployment of learning models. Augmented Queues significantly\nimproves the performance in terms of learning quality and speed. Our code is\nmade publicly available.",
    "descriptor": "\nComments: Keywords: incremental learning, active learning, data streams, class imbalance, neural networks\n",
    "authors": [
      "Kleanthis Malialis",
      "Dimitris Papatheodoulou",
      "Stylianos Filippou",
      "Christos G. Panayiotou",
      "Marios M. Polycarpou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06873"
  },
  {
    "id": "arXiv:2210.06876",
    "title": "Learning Physical Dynamics with Subequivariant Graph Neural Networks",
    "abstract": "Graph Neural Networks (GNNs) have become a prevailing tool for learning\nphysical dynamics. However, they still encounter several challenges: 1)\nPhysical laws abide by symmetry, which is a vital inductive bias accounting for\nmodel generalization and should be incorporated into the model design. Existing\nsimulators either consider insufficient symmetry, or enforce excessive\nequivariance in practice when symmetry is partially broken by gravity. 2)\nObjects in the physical world possess diverse shapes, sizes, and properties,\nwhich should be appropriately processed by the model. To tackle these\ndifficulties, we propose a novel backbone, Subequivariant Graph Neural Network,\nwhich 1) relaxes equivariance to subequivariance by considering external fields\nlike gravity, where the universal approximation ability holds theoretically; 2)\nintroduces a new subequivariant object-aware message passing for learning\nphysical interactions between multiple objects of various shapes in the\nparticle-based representation; 3) operates in a hierarchical fashion, allowing\nfor modeling long-range and complex interactions. Our model achieves on average\nover 3% enhancement in contact prediction accuracy across 8 scenarios on\nPhysion and 2X lower rollout MSE on RigidFall compared with state-of-the-art\nGNN simulators, while exhibiting strong generalization and data efficiency.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Jiaqi Han",
      "Wenbing Huang",
      "Hengbo Ma",
      "Jiachen Li",
      "Joshua B. Tenenbaum",
      "Chuang Gan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06876"
  },
  {
    "id": "arXiv:2210.06877",
    "title": "Pre-Avatar: An Automatic Presentation Generation Framework Leveraging  Talking Avatar",
    "abstract": "Since the beginning of the COVID-19 pandemic, remote conferencing and\nschool-teaching have become important tools. The previous applications aim to\nsave the commuting cost with real-time interactions. However, our application\nis going to lower the production and reproduction costs when preparing the\ncommunication materials. This paper proposes a system called Pre-Avatar,\ngenerating a presentation video with a talking face of a target speaker with 1\nfront-face photo and a 3-minute voice recording. Technically, the system\nconsists of three main modules, user experience interface (UEI), talking face\nmodule and few-shot text-to-speech (TTS) module. The system firstly clones the\ntarget speaker's voice, and then generates the speech, and finally generate an\navatar with appropriate lip and head movements. Under any scenario, users only\nneed to replace slides with different notes to generate another new video. The\ndemo has been released here and will be published as free software for use.",
    "descriptor": "\nComments: Accepted by ICTAI2022. The 34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)\n",
    "authors": [
      "Aolan Sun",
      "Xulong Zhang",
      "Tiandong Ling",
      "Jianzong Wang",
      "Ning Cheng",
      "Jing Xiao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06877"
  },
  {
    "id": "arXiv:2210.06878",
    "title": "CS-Insights: A System for Analyzing Computer Science Research",
    "abstract": "This paper presents CS-Insights, an interactive web application to analyze\ncomputer science publications from DBLP through multiple perspectives. The\ndedicated interfaces allow its users to identify trends in research activity,\nproductivity, accessibility, author's productivity, venues' statistics, topics\nof interest, and the impact of computer science research on other fields.\nCS-Insightsis publicly available, and its modular architecture can be easily\nadapted to domains other than computer science.",
    "descriptor": "",
    "authors": [
      "Terry Ruas",
      "Jan Philip Wahle",
      "Lennart K\u00fcll",
      "Saif M. Mohammad",
      "Bela Gipp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.06878"
  },
  {
    "id": "arXiv:2210.06881",
    "title": "RaP: Redundancy-aware Video-language Pre-training for Text-Video  Retrieval",
    "abstract": "Video language pre-training methods have mainly adopted sparse sampling\ntechniques to alleviate the temporal redundancy of videos. Though effective,\nsparse sampling still suffers inter-modal redundancy: visual redundancy and\ntextual redundancy. Compared with highly generalized text, sparsely sampled\nframes usually contain text-independent portions, called visual redundancy.\nSparse sampling is also likely to miss important frames corresponding to some\ntext portions, resulting in textual redundancy. Inter-modal redundancy leads to\na mismatch of video and text information, hindering the model from better\nlearning the shared semantics across modalities. To alleviate it, we propose\nRedundancy-aware Video-language Pre-training. We design a redundancy\nmeasurement of video patches and text tokens by calculating the cross-modal\nminimum dis-similarity. Then, we penalize the highredundant video patches and\ntext tokens through a proposed redundancy-aware contrastive learning. We\nevaluate our method on four benchmark datasets, MSRVTT, MSVD, DiDeMo, and\nLSMDC, achieving a significant improvement over the previous stateof-the-art\nresults. Our code are available at\nhttps://github.com/caskcsg/VLP/tree/main/RaP.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Xing Wu",
      "Chaochen Gao",
      "Zijia Lin",
      "Zhongyuan Wang",
      "Jizhong Han",
      "Songlin Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06881"
  },
  {
    "id": "arXiv:2210.06882",
    "title": "Beeping Shortest Paths via Hypergraph Bipartite Decomposition",
    "abstract": "Constructing a shortest path between two network nodes is a fundamental task\nin distributed computing. This work develops schemes for the construction of\nshortest paths in randomized beeping networks between a predetermined source\nnode and an arbitrary set of destination nodes. Our first scheme constructs a\n(single) shortest path to an arbitrary destination in $O (D \\log\\log n + \\log^3\nn)$ rounds with high probability. Our second scheme constructs multiple\nshortest paths, one per each destination, in $O (D \\log^2 n + \\log^3 n)$ rounds\nwith high probability.\nThe key technique behind the aforementioned schemes is a novel decomposition\nof hypergraphs into bipartite hypergraphs. Namely, we show how to partition the\nhyperedge set of a hypergraph $H = (V_H, E_H)$ into $k = \\Theta (\\log^2 n)$\ndisjoint subsets $F_1 \\cup \\cdots \\cup F_k = E_H$ such that the\n(sub-)hypergraph $(V_H, F_i)$ is bipartite in the sense that there exists a\nvertex subset $U \\subseteq V$ such that $|U \\cap e| = 1$ for every $e \\in F_i$.\nThis decomposition turns out to be instrumental in speeding up shortest path\nconstructions under the beeping model.",
    "descriptor": "",
    "authors": [
      "Fabien Dufoulon",
      "Yuval Emek",
      "Ran Gelles"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06882"
  },
  {
    "id": "arXiv:2210.06884",
    "title": "Algorithms for Weighted Pushdown Automata",
    "abstract": "Weighted pushdown automata (WPDAs) are at the core of many natural language\nprocessing tasks, like syntax-based statistical machine translation and\ntransition-based dependency parsing. As most existing dynamic programming\nalgorithms are designed for context-free grammars (CFGs), algorithms for PDAs\noften resort to a PDA-to-CFG conversion. In this paper, we develop novel\nalgorithms that operate directly on WPDAs. Our algorithms are inspired by\nLang's algorithm, but use a more general definition of pushdown automaton and\neither reduce the space requirements by a factor of $|\\Gamma|$ (the size of the\nstack alphabet) or reduce the runtime by a factor of more than $|Q|$ (the\nnumber of states). When run on the same class of PDAs as Lang's algorithm, our\nalgorithm is both more space-efficient by a factor of $|\\Gamma|$ and more\ntime-efficient by a factor of $|Q| \\cdot |\\Gamma|$.",
    "descriptor": "\nComments: 12 pages, 7 figures. Submitted to EMNLP 2022\n",
    "authors": [
      "Alexandra Butoi",
      "Brian DuSell",
      "Tim Vieira",
      "Ryan Cotterell",
      "David Chiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06884"
  },
  {
    "id": "arXiv:2210.06885",
    "title": "Geometric Active Learning for Segmentation of Large 3D Volumes",
    "abstract": "Segmentation, i.e., the partitioning of volumetric data into components, is a\ncrucial task in many image processing applications ever since such data could\nbe generated. Most existing applications nowadays, specifically CNNs, make use\nof voxelwise classification systems which need to be trained on a large number\nof annotated training volumes. However, in many practical applications such\ndata sets are seldom available and the generation of annotations is\ntime-consuming and cumbersome. In this paper, we introduce a novel voxelwise\nsegmentation method based on active learning on geometric features. Our method\nuses interactively provided seed points to train a voxelwise classifier based\nentirely on local information. The combination of an ad hoc incorporation of\ndomain knowledge and local processing results in a flexible yet efficient\nsegmentation method that is applicable to three-dimensional volumes without\nsize restrictions. We illustrate the potential and flexibility of our approach\nby applying it to selected computed tomography scans where we perform different\nsegmentation tasks to scans from different domains and of different sizes.",
    "descriptor": "\nComments: 10 pages, 27 figures, 3 tables\n",
    "authors": [
      "Thomas Lang",
      "Tomas Sauer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06885"
  },
  {
    "id": "arXiv:2210.06886",
    "title": "ImaginaryNet: Learning Object Detectors without Real Images and  Annotations",
    "abstract": "Without the demand of training in reality, humans can easily detect a known\nconcept simply based on its language description. Empowering deep learning with\nthis ability undoubtedly enables the neural network to handle complex vision\ntasks, e.g., object detection, without collecting and annotating real images.\nTo this end, this paper introduces a novel challenging learning paradigm\nImaginary-Supervised Object Detection (ISOD), where neither real images nor\nmanual annotations are allowed for training object detectors. To resolve this\nchallenge, we propose ImaginaryNet, a framework to synthesize images by\ncombining pretrained language model and text-to-image synthesis model. Given a\nclass label, the language model is used to generate a full description of a\nscene with a target object, and the text-to-image model deployed to generate a\nphoto-realistic image. With the synthesized images and class labels, weakly\nsupervised object detection can then be leveraged to accomplish ISOD. By\ngradually introducing real images and manual annotations, ImaginaryNet can\ncollaborate with other supervision settings to further boost detection\nperformance. Experiments show that ImaginaryNet can (i) obtain about 70%\nperformance in ISOD compared with the weakly supervised counterpart of the same\nbackbone trained on real data, (ii) significantly improve the baseline while\nachieving state-of-the-art or comparable performance by incorporating\nImaginaryNet with other supervision settings.",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Minheng Ni",
      "Zitong Huang",
      "Kailai Feng",
      "Wangmeng Zuo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06886"
  },
  {
    "id": "arXiv:2210.06887",
    "title": "ROS-PyBullet Interface: A Framework for Reliable Contact Simulation and  Human-Robot Interaction",
    "abstract": "Reliable contact simulation plays a key role in the development of\n(semi-)autonomous robots, especially when dealing with contact-rich\nmanipulation scenarios, an active robotics research topic. Besides simulation,\ncomponents such as sensing, perception, data collection, robot hardware\ncontrol, human interfaces, etc. are all key enablers towards applying machine\nlearning algorithms or model-based approaches in real world systems. However,\nthere is a lack of software connecting reliable contact simulation with the\nlarger robotics ecosystem (i.e. ROS, Orocos), for a more seamless application\nof novel approaches, found in the literature, to existing robotic hardware. In\nthis paper, we present the ROS-PyBullet Interface, a framework that provides a\nbridge between the reliable contact/impact simulator PyBullet and the Robot\nOperating System (ROS). Furthermore, we provide additional utilities for\nfacilitating Human-Robot Interaction (HRI) in the simulated environment. We\nalso present several use-cases that highlight the capabilities and usefulness\nof our framework. Please check our video, source code, and examples included in\nthe supplementary material. Our full code base is open source and can be found\nat https://github.com/cmower/ros_pybullet_interface.",
    "descriptor": "",
    "authors": [
      "Christopher E. Mower",
      "Theodoros Stouraitis",
      "Jo\u00e3o Moura",
      "Christian Rauch",
      "Lei Yan",
      "Nazanin Zamani Behabadi",
      "Michael Gienger",
      "Tom Vercauteren",
      "Christos Bergeles",
      "Sethu Vijayakumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06887"
  },
  {
    "id": "arXiv:2210.06888",
    "title": "AccelAT: A Framework for Accelerating the Adversarial Training of Deep  Neural Networks through Accuracy Gradient",
    "abstract": "Adversarial training is exploited to develop a robust Deep Neural Network\n(DNN) model against the malicious altered data. These attacks may have\ncatastrophic effects on DNN models but are indistinguishable for a human being.\nFor example, an external attack can modify an image adding noises invisible for\na human eye, but a DNN model misclassified the image. A key objective for\ndeveloping robust DNN models is to use a learning algorithm that is fast but\ncan also give model that is robust against different types of adversarial\nattacks. Especially for adversarial training, enormously long training times\nare needed for obtaining high accuracy under many different types of\nadversarial samples generated using different adversarial attack techniques.\nThis paper aims at accelerating the adversarial training to enable fast\ndevelopment of robust DNN models against adversarial attacks. The general\nmethod for improving the training performance is the hyperparameters\nfine-tuning, where the learning rate is one of the most crucial\nhyperparameters. By modifying its shape (the value over time) and value during\nthe training, we can obtain a model robust to adversarial attacks faster than\nstandard training.\nFirst, we conduct experiments on two different datasets (CIFAR10, CIFAR100),\nexploring various techniques. Then, this analysis is leveraged to develop a\nnovel fast training methodology, AccelAT, which automatically adjusts the\nlearning rate for different epochs based on the accuracy gradient. The\nexperiments show comparable results with the related works, and in several\nexperiments, the adversarial training of DNNs using our AccelAT framework is\nconducted up to 2 times faster than the existing techniques. Thus, our findings\nboost the speed of adversarial training in an era in which security and\nperformance are fundamental optimization objectives in DNN-based applications.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Farzad Nikfam",
      "Alberto Marchisio",
      "Maurizio Martina",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06888"
  },
  {
    "id": "arXiv:2210.06889",
    "title": "Multi-recipient and threshold encryption based on hidden multipliers",
    "abstract": "In this paper, a scheme is proposed that allow the dealer to encrypt messages\nin such a way that only one authorized coalition of parties (which the dealer\nchooses depending on the message) can decrypt. At the setup stage, each of the\nparties involved in the process receives an individual key from the dealer. To\ndecrypt information, an authorized coalition of parties must cooperate to use\ntheir keys. Based on this scheme, we propose a threshold encryption scheme. For\na given message $f$ the dealer can choose any threshold $m = m(f).$ More\nprecisely, any set of parties of size at least $m$ can evaluate $f$; any set of\nsize less than $m$ cannot do this. Similarly, the distribution of keys among\nthe included parties can be done in such a way that authorized coalitions of\nparties will be given the opportunity to put a collective digital signature on\nany documents. This primitive can be generalized to the dynamic setting, where\nany user can dynamically join the pool $S$, as a possible recipient. In this\ncase the new user receives a key from the dealer. Also any user can leave pool\n$S$. In both cases, already distributed keys of other users do not change. The\nmain feature of the proposed schemes is that for a given $n$ the keys are\ndistributed once and can be used multiple times. This property distinguishes\nthe proposed schemes from the most of such schemes known in the literature. The\nproposed scheme based on the idea of hidden multipliers in encryption. As a\nplatform, one can use both multiplicative groups of finite fields and groups of\ninvertible elements of commutative rings, in particular, multiplicative groups\nof residue rings. We propose two versions of this scheme.",
    "descriptor": "\nComments: 14 pages. arXiv admin note: substantial text overlap with arXiv:2108.06967\n",
    "authors": [
      "Vitaly Roman'kov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Group Theory (math.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06889"
  },
  {
    "id": "arXiv:2210.06891",
    "title": "An Experiment Design Paradigm using Joint Feature Selection and Task  Optimization",
    "abstract": "This paper presents a subsampling-task paradigm for data-driven task-specific\nexperiment design (ED) and a novel method in populationwide supervised feature\nselection (FS). Optimal ED, the choice of sampling points under constraints of\nlimited acquisition-time, arises in a wide variety of scientific and\nengineering contexts. However the continuous optimization used in classical\napproaches depend on a-priori parameter choices and challenging non-convex\noptimization landscapes. This paper proposes to replace this strategy with a\nsubsampling-task paradigm, analogous to populationwide supervised FS. In\nparticular, we introduce JOFSTO, which performs JOint Feature Selection and\nTask Optimization. JOFSTO jointly optimizes two coupled networks: one for\nfeature scoring, which provides the ED, the other for execution of a downstream\ntask or process. Unlike most FS problems, e.g. selecting protein expressions\nfor classification, ED problems typically select from highly correlated\nglobally informative candidates rather than seeking a small number of highly\ninformative features among many uninformative features. JOFSTO's construction\nefficiently identifies potentially correlated, but effective subsets and\nreturns a trained task network. We demonstrate the approach using parameter\nestimation and mapping problems in quantitative MRI, where economical ED is\ncrucial for clinical application. Results from simulations and empirical data\nshow the subsampling-task paradigm strongly outperforms classical ED, and\nwithin our paradigm, JOFSTO outperforms state-of-the-art supervised FS\ntechniques. JOFSTO extends immediately to wider image-based ED problems and\nother scenarios where the design must be specified globally across large\nnumbers of acquisitions. Code will be released.",
    "descriptor": "",
    "authors": [
      "Stefano B. Blumberg",
      "Hongxiang Lin",
      "Yukun Zhou",
      "Paddy Slator",
      "Daniel C. Alexander"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.06891"
  },
  {
    "id": "arXiv:2210.06893",
    "title": "Bug Analysis in Jupyter Notebook Projects: An Empirical Study",
    "abstract": "Computational notebooks, such as Jupyter, have been widely adopted by data\nscientists to write code for analyzing and visualizing data. Despite their\ngrowing adoption and popularity, there has been no thorough study to understand\nJupyter development challenges from the practitioners' point of view. This\npaper presents a systematic study of bugs and challenges that Jupyter\npractitioners face through a large-scale empirical investigation. We mined\n14,740 commits from 105 GitHub open-source projects with Jupyter notebook code.\nNext, we analyzed 30,416 Stack Overflow posts which gave us insights into bugs\nthat practitioners face when developing Jupyter notebook projects. Finally, we\nconducted nineteen interviews with data scientists to uncover more details\nabout Jupyter bugs and to gain insights into Jupyter developers' challenges. We\npropose a bug taxonomy for Jupyter projects based on our results. We also\nhighlight bug categories, their root causes, and the challenges that Jupyter\npractitioners face.",
    "descriptor": "",
    "authors": [
      "Taijara Loiola de Santana",
      "Paulo Anselmo da Mota Silveira Neto",
      "Eduardo Santana de Almeida",
      "Iftekhar Ahmed"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.06893"
  },
  {
    "id": "arXiv:2210.06894",
    "title": "Dim-Krum: Backdoor-Resistant Federated Learning for NLP with  Dimension-wise Krum-Based Aggregation",
    "abstract": "Despite the potential of federated learning, it is known to be vulnerable to\nbackdoor attacks. Many robust federated aggregation methods are proposed to\nreduce the potential backdoor risk. However, they are mainly validated in the\nCV field. In this paper, we find that NLP backdoors are hard to defend against\nthan CV, and we provide a theoretical analysis that the malicious update\ndetection error probabilities are determined by the relative backdoor\nstrengths. NLP attacks tend to have small relative backdoor strengths, which\nmay result in the failure of robust federated aggregation methods for NLP\nattacks. Inspired by the theoretical results, we can choose some dimensions\nwith higher backdoor strengths to settle this issue. We propose a novel\nfederated aggregation algorithm, Dim-Krum, for NLP tasks, and experimental\nresults validate its effectiveness.",
    "descriptor": "\nComments: Accepted by Findings of EMNLP 2022\n",
    "authors": [
      "Zhiyuan Zhang",
      "Qi Su",
      "Xu Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06894"
  },
  {
    "id": "arXiv:2210.06895",
    "title": "GA-SAM: Gradient-Strength based Adaptive Sharpness-Aware Minimization  for Improved Generalization",
    "abstract": "Recently, Sharpness-Aware Minimization (SAM) algorithm has shown\nstate-of-the-art generalization abilities in vision tasks. It demonstrates that\nflat minima tend to imply better generalization abilities. However, it has some\ndifficulty implying SAM to some natural language tasks, especially to models\nwith drastic gradient changes, such as RNNs. In this work, we analyze the\nrelation between the flatness of the local minimum and its generalization\nability from a novel and straightforward theoretical perspective. We propose\nthat the shift of the training and test distributions can be equivalently seen\nas a virtual parameter corruption or perturbation, which can explain why flat\nminima that are robust against parameter corruptions or perturbations have\nbetter generalization performances. On its basis, we propose a\nGradient-Strength based Adaptive Sharpness-Aware Minimization (GA-SAM)\nalgorithm to help to learn algorithms find flat minima that generalize better.\nResults in various language benchmarks validate the effectiveness of the\nproposed GA-SAM algorithm on natural language tasks.",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Zhiyuan Zhang",
      "Ruixuan Luo",
      "Qi Su",
      "Xu Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06895"
  },
  {
    "id": "arXiv:2210.06901",
    "title": "Entropy Approximation by Machine Learning Regression: Application for  Irregularity Evaluation of Images in Remote Sensing",
    "abstract": "Approximation of entropies of various types using machine learning (ML)\nregression methods is shown for the first time. The ML models presented in this\nstudy defines the complexity of short time series by approximating dissimilar\nentropy techniques such as Singular value decomposition entropy (SvdEn),\nPermutation entropy (PermEn), Sample entropy (SampEn) and Neural Network\nentropy (NNetEn) and their 2D analogies. A new method for calculating SvdEn2D,\nPermEn2D and SampEn2D for 2D images was tested using the technique of circular\nkernels. Training and test datasets on the basis of Sentinel-2 images are\npresented (2 train images and 198 test images). The results of entropy\napproximation are demonstrated using the example of calculating the 2D entropy\nof Sentinel-2 images and R2 metric evaluation. Applicability of the method for\nshort time series with length from N = 5 to N = 113 elements is shown. A\ntendency for the R2 metric to decrease with an increase in the length of the\ntime series was found. For SvdEn entropy, the regression accuracy is R2 > 0.99\nfor N = 5 and R2 > 0.82 for N = 113. The best metrics are observed for the\nML_SvdEn2D and ML_NNetEn2D models. The results of the study can be used for\nfundamental research of entropy approximations of various types using ML\nregression, as well as for accelerating entropy calculations in remote sensing.",
    "descriptor": "\nComments: 23 pages, 22 figures, 4 tables\n",
    "authors": [
      "Andrei Velichko",
      "Maksim Belyaev",
      "Matthias P. Wagner",
      "Alireza Taravat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.06901"
  },
  {
    "id": "arXiv:2210.06906",
    "title": "Hierarchical and Progressive Image Matting",
    "abstract": "Most matting researches resort to advanced semantics to achieve high-quality\nalpha mattes, and direct low-level features combination is usually explored to\ncomplement alpha details. However, we argue that appearance-agnostic\nintegration can only provide biased foreground details and alpha mattes require\ndifferent-level feature aggregation for better pixel-wise opacity perception.\nIn this paper, we propose an end-to-end Hierarchical and Progressive Attention\nMatting Network (HAttMatting++), which can better predict the opacity of the\nforeground from single RGB images without additional input. Specifically, we\nutilize channel-wise attention to distill pyramidal features and employ spatial\nattention at different levels to filter appearance cues. This progressive\nattention mechanism can estimate alpha mattes from adaptive semantics and\nsemantics-indicated boundaries. We also introduce a hybrid loss function fusing\nStructural SIMilarity (SSIM), Mean Square Error (MSE), Adversarial loss, and\nsentry supervision to guide the network to further improve the overall\nforeground structure. Besides, we construct a large-scale and challenging image\nmatting dataset comprised of 59, 600 training images and 1000 test images (a\ntotal of 646 distinct foreground alpha mattes), which can further improve the\nrobustness of our hierarchical and progressive aggregation model. Extensive\nexperiments demonstrate that the proposed HAttMatting++ can capture\nsophisticated foreground structures and achieve state-of-the-art performance\nwith single RGB images as input.",
    "descriptor": "\nComments: 23 pages, 11 Figures, ACM TOMM accepted\n",
    "authors": [
      "Yu Qiao",
      "Yuhao Liu",
      "Ziqi Wei",
      "Yuxin Wang",
      "Qiang Cai",
      "Guofeng Zhang",
      "Xin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06906"
  },
  {
    "id": "arXiv:2210.06908",
    "title": "Feature-Proxy Transformer for Few-Shot Segmentation",
    "abstract": "Few-shot segmentation (FSS) aims at performing semantic segmentation on novel\nclasses given a few annotated support samples. With a rethink of recent\nadvances, we find that the current FSS framework has deviated far from the\nsupervised segmentation framework: Given the deep features, FSS methods\ntypically use an intricate decoder to perform sophisticated pixel-wise\nmatching, while the supervised segmentation methods use a simple linear\nclassification head. Due to the intricacy of the decoder and its matching\npipeline, it is not easy to follow such an FSS framework. This paper revives\nthe straightforward framework of \"feature extractor $+$ linear classification\nhead\" and proposes a novel Feature-Proxy Transformer (FPTrans) method, in which\nthe \"proxy\" is the vector representing a semantic class in the linear\nclassification head. FPTrans has two keypoints for learning discriminative\nfeatures and representative proxies: 1) To better utilize the limited support\nsamples, the feature extractor makes the query interact with the support\nfeatures from the bottom to top layers using a novel prompting strategy. 2)\nFPTrans uses multiple local background proxies (instead of a single one)\nbecause the background is not homogeneous and may contain some novel foreground\nregions. These two keypoints are easily integrated into the vision transformer\nbackbone with the prompting mechanism in the transformer. Given the learned\nfeatures and proxies, FPTrans directly compares their cosine similarity for\nsegmentation. Although the framework is straightforward, we show that FPTrans\nachieves competitive FSS accuracy on par with state-of-the-art decoder-based\nmethods.",
    "descriptor": "\nComments: NeurIPS 2022. Code available: this https URL\n",
    "authors": [
      "Jian-Wei Zhang",
      "Yifan Sun",
      "Yi Yang",
      "Wei Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06908"
  },
  {
    "id": "arXiv:2210.06909",
    "title": "HoechstGAN: Virtual Lymphocyte Staining Using Generative Adversarial  Networks",
    "abstract": "The presence and density of specific types of immune cells are important to\nunderstand a patient's immune response to cancer. However, immunofluorescence\nstaining required to identify T cell subtypes is expensive, timeconsuming, and\nrarely performed in clinical settings. We present a framework to virtually\nstain Hoechst images (which are cheap and widespread) with both CD3 and CD8 to\nidentify T cell subtypes in clear cell renal cell carcinoma using generative\nadversarial networks. Our proposed method jointly learns both staining tasks,\nincentivising the network to incorporate mutually beneficial information from\neach task. We devise a novel metric to quantify the virtual staining quality,\nand use it to evaluate our method.",
    "descriptor": "\nComments: Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023\n",
    "authors": [
      "Georg W\u00f6lflein",
      "In Hwa Um",
      "David J Harrison",
      "Ognjen Arandjelovi\u0107"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.06909"
  },
  {
    "id": "arXiv:2210.06910",
    "title": "CNTN: Cyclic Noise-tolerant Network for Gait Recognition",
    "abstract": "Gait recognition aims to identify individuals by recognizing their walking\npatterns. However, an observation is made that most of the previous gait\nrecognition methods degenerate significantly due to two memorization effects,\nnamely appearance memorization and label noise memorization. To address the\nproblem, for the first time noisy gait recognition is studied, and a cyclic\nnoise-tolerant network (CNTN) is proposed with a cyclic training algorithm,\nwhich equips the two parallel networks with explicitly different abilities,\nnamely one forgetting network and one memorizing network. The overall model\nwill not memorize the pattern unless the two different networks both memorize\nit. Further, a more refined co-teaching constraint is imposed to help the model\nlearn intrinsic patterns which are less influenced by memorization. Also, to\naddress label noise memorization, an adaptive noise detection module is\nproposed to rule out the samples with high possibility to be noisy from\nupdating the model. Experiments are conducted on the three most popular\nbenchmarks and CNTN achieves state-of-the-art performances. We also reconstruct\ntwo noisy gait recognition datasets, and CNTN gains significant improvements\n(especially 6% improvements on CL setting). CNTN is also compatible with any\noff-the-shelf backbones and improves them consistently.",
    "descriptor": "",
    "authors": [
      "Weichen Yu",
      "Hongyuan Yu",
      "Yan Huang",
      "Chunshui Cao",
      "Liang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06910"
  },
  {
    "id": "arXiv:2210.06915",
    "title": "Adapting Behaviour Based On Trust In Human-Agent Ad Hoc Teamwork",
    "abstract": "This work proposes a framework that incorporates trust in an ad hoc teamwork\nscenario with human-agent teams, where an agent must collaborate with a human\nto perform a task. During the task, the agent must infer, through interactions\nand observations, how much the human trusts it and adapt its behaviour to\nmaximize the team's performance. To achieve this, we propose collecting data\nfrom human participants in experiments to define different settings (based on\ntrust levels) and learning optimal policies for each of them. Then, we create a\nmodule to infer the current setting (depending on the amount of trust).\nFinally, we validate this framework in a real-world scenario and analyse how\nthis adaptable behaviour affects trust.",
    "descriptor": "",
    "authors": [
      "Ana Carrasco"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06915"
  },
  {
    "id": "arXiv:2210.06916",
    "title": "On the Evaluation of the Plausibility and Faithfulness of Sentiment  Analysis Explanations",
    "abstract": "Current Explainable AI (ExAI) methods, especially in the NLP field, are\nconducted on various datasets by employing different metrics to evaluate\nseveral aspects. The lack of a common evaluation framework is hindering the\nprogress tracking of such methods and their wider adoption. In this work,\ninspired by offline information retrieval, we propose different metrics and\ntechniques to evaluate the explainability of SA models from two angles. First,\nwe evaluate the strength of the extracted \"rationales\" in faithfully explaining\nthe predicted outcome. Second, we measure the agreement between ExAI methods\nand human judgment on a homegrown dataset1 to reflect on the rationales\nplausibility. Our conducted experiments comprise four dimensions: (1) the\nunderlying architectures of SA models, (2) the approach followed by the ExAI\nmethod, (3) the reasoning difficulty, and (4) the homogeneity of the\nground-truth rationales. We empirically demonstrate that anchors explanations\nare more aligned with the human judgment and can be more confident in\nextracting supporting rationales. As can be foreseen, the reasoning complexity\nof sentiment is shown to thwart ExAI methods from extracting supporting\nevidence. Moreover, a remarkable discrepancy is discerned between the results\nof different explainability methods on the various architectures suggesting the\nneed for consolidation to observe enhanced performance. Predominantly,\ntransformers are shown to exhibit better explainability than convolutional and\nrecurrent architectures. Our work paves the way towards designing more\ninterpretable NLP models and enabling a common evaluation ground for their\nrelative strengths and robustness.",
    "descriptor": "\nComments: 13 pages, 3 figures, conference (AIAI - springer)\n",
    "authors": [
      "Julia El Zini",
      "Mohamad Mansour",
      "Basel Mousi",
      "Mariette Awad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06916"
  },
  {
    "id": "arXiv:2210.06917",
    "title": "A Direct Approximation of AIXI Using Logical State Abstractions",
    "abstract": "We propose a practical integration of logical state abstraction with AIXI, a\nBayesian optimality notion for reinforcement learning agents, to significantly\nexpand the model class that AIXI agents can be approximated over to complex\nhistory-dependent and structured environments. The state representation and\nreasoning framework is based on higher-order logic, which can be used to define\nand enumerate complex features on non-Markovian and structured environments. We\naddress the problem of selecting the right subset of features to form state\nabstractions by adapting the $\\Phi$-MDP optimisation criterion from state\nabstraction theory. Exact Bayesian model learning is then achieved using a\nsuitable generalisation of Context Tree Weighting over abstract state\nsequences. The resultant architecture can be integrated with different planning\nalgorithms. Experimental results on controlling epidemics on large-scale\ncontact networks validates the agent's performance.",
    "descriptor": "",
    "authors": [
      "Samuel Yang-Zhao",
      "Tianyu Wang",
      "Kee Siong Ng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06917"
  },
  {
    "id": "arXiv:2210.06918",
    "title": "Automotive Multilingual Fault Diagnosis",
    "abstract": "Automated fault diagnosis can facilitate diagnostics assistance, speedier\ntroubleshooting, and better-organised logistics. Currently, AI-based\nprognostics and health management in the automotive industry ignore the textual\ndescriptions of the experienced problems or symptoms. With this study, however,\nwe show that a multilingual pre-trained Transformer can effectively classify\nthe textual claims from a large company with vehicle fleets, despite the task's\nchallenging nature due to the 38 languages and 1,357 classes involved. Overall,\nwe report an accuracy of more than 80% for high-frequency classes and above 60%\nfor above-low-frequency classes, bringing novel evidence that multilingual\nclassification can benefit automotive troubleshooting management.",
    "descriptor": "",
    "authors": [
      "John Pavlopoulos",
      "Alv Romell",
      "Jacob Curman",
      "Olof Steinert",
      "Tony Lindgren",
      "Markus Borg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06918"
  },
  {
    "id": "arXiv:2210.06919",
    "title": "Wider and Higher: Intensive Integration and Global Foreground Perception  for Image Matting",
    "abstract": "This paper reviews recent deep-learning-based matting research and conceives\nour wider and higher motivation for image matting. Many approaches achieve\nalpha mattes with complex encoders to extract robust semantics, then resort to\nthe U-net-like decoder to concatenate or fuse encoder features. However, image\nmatting is essentially a pixel-wise regression, and the ideal situation is to\nperceive the maximum opacity correspondence from the input image. In this\npaper, we argue that the high-resolution feature representation, perception and\ncommunication are more crucial for matting accuracy. Therefore, we propose an\nIntensive Integration and Global Foreground Perception network (I2GFP) to\nintegrate wider and higher feature streams. Wider means we combine intensive\nfeatures in each decoder stage, while higher suggests we retain high-resolution\nintermediate features and perceive large-scale foreground appearance. Our\nmotivation sacrifices model depth for a significant performance promotion. We\nperform extensive experiments to prove the proposed I2GFP model, and\nstate-of-the-art results can be achieved on different public datasets.",
    "descriptor": "\nComments: 12 pages, 9 figures, CGI 2022\n",
    "authors": [
      "Yu Qiao",
      "Ziqi Wei",
      "Yuhao Liu",
      "Yuxin Wang",
      "Dongsheng Zhou",
      "Qiang Zhang",
      "Xin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06919"
  },
  {
    "id": "arXiv:2210.06924",
    "title": "Scene Text Image Super-Resolution via Content Perceptual Loss and  Criss-Cross Transformer Blocks",
    "abstract": "Text image super-resolution is a unique and important task to enhance\nreadability of text images to humans. It is widely used as pre-processing in\nscene text recognition. However, due to the complex degradation in natural\nscenes, recovering high-resolution texts from the low-resolution inputs is\nambiguous and challenging. Existing methods mainly leverage deep neural\nnetworks trained with pixel-wise losses designed for natural image\nreconstruction, which ignore the unique character characteristics of texts. A\nfew works proposed content-based losses. However, they only focus on text\nrecognizers' accuracy, while the reconstructed images may still be ambiguous to\nhumans. Further, they often have weak generalizability to handle cross\nlanguages. To this end, we present TATSR, a Text-Aware Text Super-Resolution\nframework, which effectively learns the unique text characteristics using\nCriss-Cross Transformer Blocks (CCTBs) and a novel Content Perceptual (CP)\nLoss. The CCTB extracts vertical and horizontal content information from text\nimages by two orthogonal transformers, respectively. The CP Loss supervises the\ntext reconstruction with content semantics by multi-scale text recognition\nfeatures, which effectively incorporates content awareness into the framework.\nExtensive experiments on various language datasets demonstrate that TATSR\noutperforms state-of-the-art methods in terms of both recognition accuracy and\nhuman perception.",
    "descriptor": "",
    "authors": [
      "Rui Qin",
      "Bin Wang",
      "Yu-Wing Tai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06924"
  },
  {
    "id": "arXiv:2210.06926",
    "title": "Delta-Closure Structure for Studying Data Distribution",
    "abstract": "In this paper, we revisit pattern mining and study the distribution\nunderlying a binary dataset thanks to the closure structure which is based on\npasskeys, i.e., minimum generators in equivalence classes robust to noise. We\nintroduce $\\Delta$-closedness, a generalization of the closure operator, where\n$\\Delta$ measures how a closed set differs from its upper neighbors in the\npartial order induced by closure. A $\\Delta$-class of equivalence includes\nminimum and maximum elements and allows us to characterize the distribution\nunderlying the data. Moreover, the set of $\\Delta$-classes of equivalence can\nbe partitioned into the so-called $\\Delta$-closure structure. In particular, a\n$\\Delta$-class of equivalence with a high level demonstrates correlations among\nmany attributes, which are supported by more observations when $\\Delta$ is\nlarge. In the experiments, we study the $\\Delta$-closure structure of several\nreal-world datasets and show that this structure is very stable for large\n$\\Delta$ and does not substantially depend on the data sampling used for the\nanalysis.",
    "descriptor": "",
    "authors": [
      "Aleksey Buzmakov",
      "Tatiana Makhalova",
      "Sergei O. Kuznetsov",
      "Amedeo Napoli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06926"
  },
  {
    "id": "arXiv:2210.06928",
    "title": "Sentence Ambiguity, Grammaticality and Complexity Probes",
    "abstract": "It is unclear whether, how and where large pre-trained language models\ncapture subtle linguistic traits like ambiguity, grammaticality and sentence\ncomplexity. We present results of automatic classification of these traits and\ncompare their viability and patterns across representation types. We\ndemonstrate that template-based datasets with surface-level artifacts should\nnot be used for probing, careful comparisons with baselines should be done and\nthat t-SNE plots should not be used to determine the presence of a feature\namong dense vectors representations. We also show how features might be highly\nlocalized in the layers for these models and get lost in the upper layers.",
    "descriptor": "\nComments: Accepted at BlackboxNLP @ EMNLP 2022\n",
    "authors": [
      "Sunit Bhattacharya",
      "Vil\u00e9m Zouhar",
      "Ond\u0159ej Bojar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06928"
  },
  {
    "id": "arXiv:2210.06929",
    "title": "On the Explainability of Natural Language Processing Deep Models",
    "abstract": "While there has been a recent explosion of work on ExplainableAI ExAI on deep\nmodels that operate on imagery and tabular data, textual datasets present new\nchallenges to the ExAI community. Such challenges can be attributed to the lack\nof input structure in textual data, the use of word embeddings that add to the\nopacity of the models and the difficulty of the visualization of the inner\nworkings of deep models when they are trained on textual data.\nLately, methods have been developed to address the aforementioned challenges\nand present satisfactory explanations on Natural Language Processing (NLP)\nmodels. However, such methods are yet to be studied in a comprehensive\nframework where common challenges are properly stated and rigorous evaluation\npractices and metrics are proposed. Motivated to democratize ExAI methods in\nthe NLP field, we present in this work a survey that studies model-agnostic as\nwell as model-specific explainability methods on NLP models. Such methods can\neither develop inherently interpretable NLP models or operate on pre-trained\nmodels in a post-hoc manner. We make this distinction and we further decompose\nthe methods into three categories according to what they explain: (1) word\nembeddings (input-level), (2) inner workings of NLP models (processing-level)\nand (3) models' decisions (output-level). We also detail the different\nevaluation approaches interpretability methods in the NLP field. Finally, we\npresent a case-study on the well-known neural machine translation in an\nappendix and we propose promising future research directions for ExAI in the\nNLP field.",
    "descriptor": "",
    "authors": [
      "Julia El Zini",
      "Mariette Awad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06929"
  },
  {
    "id": "arXiv:2210.06931",
    "title": "Masonry elements strengthened through Textile-Reinforced Mortar:  application of detailed level modelling with a free open-source  Finite-Element code",
    "abstract": "The paper concerns the modelling of masonry elements strengthened through\nTextile Reinforced Mortar (TRM), a near surface system made of fiber-based\ngrids or textiles embedded in mortar layers. Recently, the author, focusing on\nthe mechanical characterization of TRM composites, developed a detailed level\nmodelling approach by using the free, open-source Finite-Element code OOFEM,\nfor the simulation of experimental tests on TRM coupons (pull-out tests,\ntensile tests, shear bond tests and in-plane shear tests). The model was\ncapable to account for the failure of single components (e.g. the fibers\ntensile failure, the mortar cracking and crushing), as well as of their\ninteractions (the debonding of the fibers from the mortar and of the mortar\nfrom the masonry substrate). In this paper, the detailed-level modelling\napproach is applied to the simulation of TRM strengthened masonry elements\nsubjected to diagonal compression, in-plane and out-of-plane bending tests,\ninvestigating on the typical failure modes of masonry. Non-linear static\nanalyses are performed, with nonlinearities of materials and interfaces deduced\nfrom experimental evidences. The comparison with some experimental results and\na parametric study allowed to evidence the reliability of the models and their\nsensitivity to the main components characteristics.",
    "descriptor": "",
    "authors": [
      "Ingrid Boem"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06931"
  },
  {
    "id": "arXiv:2210.06932",
    "title": "NoMorelization: Building Normalizer-Free Models from a Sample's  Perspective",
    "abstract": "The normalizing layer has become one of the basic configurations of deep\nlearning models, but it still suffers from computational inefficiency,\ninterpretability difficulties, and low generality. After gaining a deeper\nunderstanding of the recent normalization and normalizer-free research works\nfrom a sample's perspective, we reveal the fact that the problem lies in the\nsampling noise and the inappropriate prior assumption. In this paper, we\npropose a simple and effective alternative to normalization, which is called\n\"NoMorelization\". NoMorelization is composed of two trainable scalars and a\nzero-centered noise injector. Experimental results demonstrate that\nNoMorelization is a general component for deep learning and is suitable for\ndifferent model paradigms (e.g., convolution-based and attention-based models)\nto tackle different tasks (e.g., discriminative and generative tasks). Compared\nwith existing mainstream normalizers (e.g., BN, LN, and IN) and\nstate-of-the-art normalizer-free methods, NoMorelization shows the best\nspeed-accuracy trade-off.",
    "descriptor": "",
    "authors": [
      "Chang Liu",
      "Yuwen Yang",
      "Yue Ding",
      "Hongtao Lu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06932"
  },
  {
    "id": "arXiv:2210.06936",
    "title": "Multilingual Zero Resource Speech Recognition Base on Self-Supervise  Pre-Trained Acoustic Models",
    "abstract": "Labeled audio data is insufficient to build satisfying speech recognition\nsystems for most of the languages in the world. There have been some\nzero-resource methods trying to perform phoneme or word-level speech\nrecognition without labeled audio data of the target language, but the error\nrate of these methods is usually too high to be applied in real-world\nscenarios. Recently, the representation ability of self-supervise pre-trained\nmodels has been found to be extremely beneficial in zero-resource phoneme\nrecognition. As far as we are concerned, this paper is the first attempt to\nextend the use of pre-trained models into word-level zero-resource speech\nrecognition. This is done by fine-tuning the pre-trained models on IPA phoneme\ntranscriptions and decoding with a language model trained on extra texts.\nExperiments on Wav2vec 2.0 and HuBERT models show that this method can achieve\nless than 20% word error rate on some languages, and the average error rate on\n8 languages is 33.77%.",
    "descriptor": "\nComments: accepted by ISCSLP 2022\n",
    "authors": [
      "Haoyu Wang",
      "Wei-Qiang Zhang",
      "Hongbin Suo",
      "Yulong Wan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06936"
  },
  {
    "id": "arXiv:2210.06937",
    "title": "A hybridizable discontinuous Galerkin method for the coupled  Navier-Stokes and Darcy problem",
    "abstract": "We present and analyze a strongly conservative hybridizable discontinuous\nGalerkin finite element method for the coupled incompressible Navier-Stokes and\nDarcy problem with Beavers-Joseph-Saffman interface condition. An a priori\nerror analysis shows that the velocity error does not depend on the pressure,\nand that velocity and pressure converge with optimal rates. These results are\nconfirmed by numerical examples.",
    "descriptor": "",
    "authors": [
      "Aycil Cesmelioglu",
      "Sander Rhebergen"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06937"
  },
  {
    "id": "arXiv:2210.06943",
    "title": "Understand-Before-Talk (UBT): A Semantic Communication Approach to 6G  Networks",
    "abstract": "In Shannon theory, semantic aspects of communication were identified but\nconsidered irrelevant to the technical communication problems. Semantic\ncommunication (SC) techniques have recently attracted renewed research\ninterests in (6G) wireless because they have the capability to support an\nefficient interpretation of the significance and meaning intended by a sender\n(or accomplishment of the goal) when dealing with multi-modal data such as\nvideos, images, audio, text messages, and so on, which would be the case for\nvarious applications such as intelligent transportation systems where each\nautonomous vehicle needs to deal with real-time videos and data from a number\nof sensors including radars. A notable difficulty of existing SC frameworks\nlies in handling the discrete constraints imposed on the pursued semantic\ncoding and its interaction with the independent knowledge base, which makes\nreliable semantic extraction extremely challenging. Therefore, we develop a new\nlightweight hashing-based semantic extraction approach to the SC framework,\nwhere our learning objective is to generate one-time signatures (hash codes)\nusing supervised learning for low latency, secure and efficient management of\nthe SC dynamics. We first evaluate the proposed semantic extraction framework\nover large image data sets, extend it with domain adaptive hashing and then\ndemonstrate the effectiveness of \"semantics signature\" in bulk transmission and\nmulti-modal data.",
    "descriptor": "",
    "authors": [
      "Shiva Raj Pokhrel",
      "Jinho Choi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06943"
  },
  {
    "id": "arXiv:2210.06944",
    "title": "SageMix: Saliency-Guided Mixup for Point Clouds",
    "abstract": "Data augmentation is key to improving the generalization ability of deep\nlearning models. Mixup is a simple and widely-used data augmentation technique\nthat has proven effective in alleviating the problems of overfitting and data\nscarcity. Also, recent studies of saliency-aware Mixup in the image domain show\nthat preserving discriminative parts is beneficial to improving the\ngeneralization performance. However, these Mixup-based data augmentations are\nunderexplored in 3D vision, especially in point clouds. In this paper, we\npropose SageMix, a saliency-guided Mixup for point clouds to preserve salient\nlocal structures. Specifically, we extract salient regions from two point\nclouds and smoothly combine them into one continuous shape. With a simple\nsequential sampling by re-weighted saliency scores, SageMix preserves the local\nstructure of salient regions. Extensive experiments demonstrate that the\nproposed method consistently outperforms existing Mixup methods in various\nbenchmark point cloud datasets. With PointNet++, our method achieves an\naccuracy gain of 2.6% and 4.0% over standard training in 3D Warehouse dataset\n(MN40) and ScanObjectNN, respectively. In addition to generalization\nperformance, SageMix improves robustness and uncertainty calibration. Moreover,\nwhen adopting our method to various tasks including part segmentation and\nstandard 2D image classification, our method achieves competitive performance.",
    "descriptor": "\nComments: Neural Information Processing Systems (NeurIPS), 2022\n",
    "authors": [
      "Sanghyeok Lee",
      "Minkyu Jeon",
      "Injae Kim",
      "Yunyang Xiong",
      "Hyunwoo J. Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06944"
  },
  {
    "id": "arXiv:2210.06947",
    "title": "Decentralized State Estimation In A Dimension-Reduced Linear Regression",
    "abstract": "Decentralized state estimation in a communication constrained sensor network\nis considered. To reduce the communication load only dimension-reduced\nestimates are exchanged between the networking agents. The considered\ndimension-reduction is restricted to be a linear mapping from a\nhigher-dimensional space to a lower-dimensional space. The optimal, in the mean\nsquare error sense, linear mapping depends on the particular estimation method\nused. Several dimension-reducing algorithms are therefore proposed, where each\nalgorithm corresponds to a commonly applied decentralized estimation method.\nAll except one of the algorithms are shown to be optimal. For the remaining\nalgorithm we provide a convergence analysis where it is theoretically shown\nthat this algorithm converges to a stationary point and numerically shown that\nthe convergence rate is fast. A message encoding solution is proposed that\nallows for efficient communication when using the proposed dimension-reduction\ntechniques. Applicability of the different algorithms is illustrated by a\nnumerical evaluation.",
    "descriptor": "\nComments: 11 pages. Submitted to the IEEE Transactions on Signal Processing for possible publishing\n",
    "authors": [
      "Robin Forsling",
      "Fredrik Gustafsson",
      "Zoran Sjanic",
      "Gustaf Hendeby"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06947"
  },
  {
    "id": "arXiv:2210.06950",
    "title": "A Novel Spectrally Efficient Local Service Insertion Scheme With  Universal Frequency Reuse",
    "abstract": "Local content insertion enabled single frequency networks (SFN) for TV\nbroadcast is a topic of considerable research interest. The SFN broadcast\nsystem is not ideally suited for localized content, but it offers enormous\nspatial diversity and power gain advantages. Orthogonal Frequency Division\nMultiple Access (OFDMA) allows reserving a fraction of available subcarriers\nfor broadcasting local content. However, a user in the transition region from\none local service area (LSA) to another experiences high co-channel\ninterference (CCI). This work explores the possibility of using a `buffer zone'\nbased on universal frequency reuse schemes. In such systems, the power\nallocation to each content is varied to reduce the CCI-dominated region in the\nLSA boundary. Simulation results show that depending on the threshold\nsignal-to-interference plus noise (SINR) ratio selected, the proposed buffer\narchitectures increase the percentage of users served by the SFN compared to\nthe scheme where such a buffer is absent. Moreover, the proposed method is\nspectrally efficient compared to an orthogonal allocation of local content\nacross the LSAs.",
    "descriptor": "\nComments: Submitted for peer review in a journal\n",
    "authors": [
      "M. V. Abhay Mohan",
      "K. Giridhar"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06950"
  },
  {
    "id": "arXiv:2210.06951",
    "title": "Performance Optimization and Parameters Estimation for MIMO-OFDM  Dual-functional Communication-radar Systems",
    "abstract": "In dual-functional communication-radar systems, common radio frequency (RF)\nsignals are used for both communication and detection. For better compatibility\nwith existing communication systems, we adopt multiple-input multiple-output\n(MIMO) orthogonal frequency division multiplexing (OFDM) signals as integrated\nsignals and investigate the estimation performance of MIMO-OFDM signals. We\nfirst analyze the Cramer-Rao lower bound (CRLB) of parameters estimation. Then,\ntransmit powers over different subcarriers are optimized to achieve the best\ntradeoff between transmission rate and estimation performance. Finally, we\npropose a more accurate estimation method which utilizes canonical polyadic\ndecomposition (CPD) of three-order tensor to obtain the parameter matrices. Due\nto the characteristic of the column structure of the parameter matrices, we\njust need to use DFT / IDFT to recover the parameters of multiple targets. The\nsimulation results show that the estimation method based on tensor can achieve\nperformance close to CRLB and the estimation performance can be improved by\noptimizing the transmit powers.",
    "descriptor": "\nComments: Digital Communications and network\n",
    "authors": [
      "Chen Zhong",
      "Chunrong Gu",
      "Lan Tang",
      "Yechao Bai",
      "Mengting Lou"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2210.06951"
  },
  {
    "id": "arXiv:2210.06954",
    "title": "Darwinian Model Upgrades: Model Evolving with Selective Compatibility",
    "abstract": "The traditional model upgrading paradigm for retrieval requires recomputing\nall gallery embeddings before deploying the new model (dubbed as\n\"backfilling\"), which is quite expensive and time-consuming considering\nbillions of instances in industrial applications. BCT presents the first step\ntowards backward-compatible model upgrades to get rid of backfilling. It is\nworkable but leaves the new model in a dilemma between new feature\ndiscriminativeness and new-to-old compatibility due to the undifferentiated\ncompatibility constraints. In this work, we propose Darwinian Model Upgrades\n(DMU), which disentangle the inheritance and variation in the model evolving\nwith selective backward compatibility and forward adaptation, respectively. The\nold-to-new heritable knowledge is measured by old feature discriminativeness,\nand the gallery features, especially those of poor quality, are evolved in a\nlightweight manner to become more adaptive in the new latent space. We\ndemonstrate the superiority of DMU through comprehensive experiments on\nlarge-scale landmark retrieval and face recognition benchmarks. DMU effectively\nalleviates the new-to-new degradation and improves new-to-old compatibility,\nrendering a more proper model upgrading paradigm in large-scale retrieval\nsystems.",
    "descriptor": "",
    "authors": [
      "Binjie Zhang",
      "Shupeng Su",
      "Yixiao Ge",
      "Xuyuan Xu",
      "Yexin Wang",
      "Chun Yuan",
      "Mike Zheng Shou",
      "Ying Shan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06954"
  },
  {
    "id": "arXiv:2210.06955",
    "title": "Agent-Based Modelling for Urban Analytics: State of the Art and  Challenges",
    "abstract": "Agent-based modelling (ABM) is a facet of wider Multi-Agent Systems (MAS)\nresearch that explores the collective behaviour of individual `agents', and the\nimplications that their behaviour and interactions have for wider systemic\nbehaviour. The method has been shown to hold considerable value in exploring\nand understanding human societies, but is still largely confined to use in\nacademia. This is particularly evident in the field of Urban Analytics; one\nthat is characterised by the use of new forms of data in combination with\ncomputational approaches to gain insight into urban processes. In Urban\nAnalytics, ABM is gaining popularity as a valuable method for understanding the\nlow-level interactions that ultimately drive cities, but as yet is rarely used\nby stakeholders (planners, governments, etc.) to address real policy problems.\nThis paper presents the state-of-the-art in the application of ABM at the\ninterface of MAS and Urban Analytics by a group of ABM researchers who are\naffiliated with the Urban Analytics programme of the Alan Turing Institute in\nLondon (UK). It addresses issues around modelling behaviour, the use of new\nforms of data, the calibration of models under high uncertainty, real-time\nmodelling, the use of AI techniques, large-scale models, and the implications\nfor modelling policy. The discussion also contextualises current research in\nwider debates around Data Science, Artificial Intelligence, and MAS more\nbroadly.",
    "descriptor": "",
    "authors": [
      "Nick Malleson",
      "Mark Birkin",
      "Daniel Birks",
      "Jiaqi Ge",
      "Alison Heppenstall",
      "Ed Manley",
      "Josie McCulloch",
      "Patricia Ternes"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.06955"
  },
  {
    "id": "arXiv:2210.06956",
    "title": "On-Premise Artificial Intelligence as a Service for Small and Medium  Size Setups",
    "abstract": "Artificial Intelligence (AI) technologies are moving from customized\ndeployments in specific domains towards generic solutions horizontally\npermeating vertical domains and industries. For instance, decisions on when to\nperform maintenance of roads or bridges or how to optimize public lighting in\nview of costs and safety in smart cities are increasingly informed by AI\nmodels. While various commercial solutions offer user friendly and easy to use\nAI as a Service (AIaaS), functionality-wise enabling the democratization of\nsuch ecosystems, open-source equivalent ecosystems are lagging behind. In this\nchapter, we discuss AIaaS functionality and corresponding technology stack and\nanalyze possible realizations using open source user friendly technologies that\nare suitable for on-premise set-ups of small and medium sized users allowing\nfull control over the data and technological platform without any third-party\ndependence or vendor lock-in.",
    "descriptor": "\nComments: 11 Pages, 8 Figures, book chapter\n",
    "authors": [
      "Carolina Fortuna",
      "Din Mu\u0161i\u0107",
      "Gregor Cerar",
      "Andrej \u010campa",
      "Panagiotis Kapsalis",
      "Mihael Mohor\u010di\u010d"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06956"
  },
  {
    "id": "arXiv:2210.06959",
    "title": "A Survey on Explainable Anomaly Detection",
    "abstract": "In the past two decades, most research on anomaly detection has focused on\nimproving the accuracy of the detection, while largely ignoring the\nexplainability of the corresponding methods and thus leaving the explanation of\noutcomes to practitioners. As anomaly detection algorithms are increasingly\nused in safety-critical domains, providing explanations for the high-stakes\ndecisions made in those domains has become an ethical and regulatory\nrequirement. Therefore, this work provides a comprehensive and structured\nsurvey on state-of-the-art explainable anomaly detection techniques. We propose\na taxonomy based on the main aspects that characterize each explainable anomaly\ndetection technique, aiming to help practitioners and researchers find the\nexplainable anomaly detection method that best suits their needs.",
    "descriptor": "\nComments: This manuscript has been submitted to ACM for possible publication\n",
    "authors": [
      "Zhong Li",
      "Yuxuan Zhu",
      "Matthijs van Leeuwen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06959"
  },
  {
    "id": "arXiv:2210.06961",
    "title": "Feature-Adaptive Interactive Thresholding of Large 3D Volumes",
    "abstract": "Thresholding is the most widely used segmentation method in volumetric image\nprocessing, and its pointwise nature makes it attractive for the fast handling\nof large three-dimensional samples. However, global thresholds often do not\nproperly extract components in the presence of artifacts, measurement noise or\ngrayscale value fluctuations. This paper introduces Feature-Adaptive\nInteractive Thresholding (FAITH), a thresholding technique that incorporates\n(geometric) features, local processing and interactive user input to overcome\nthese limitations. Given a global threshold suitable for most regions, FAITH\nuses interactively selected seed voxels to identify critical regions in which\nthat threshold will be adapted locally on the basis of features computed from\nlocal environments around these voxels. The combination of domain expert\nknowledge and a rigorous mathematical model thus enables a very exible way of\nlocal thresholding with intuitive user interaction. A qualitative analysis\nshows that the proposed model is able to overcome limitations typically\noccuring in plain thresholding while staying efficient enough to also allow the\nsegmentation of big volumes.",
    "descriptor": "\nComments: 13 pages, 3 figures, 2 tables\n",
    "authors": [
      "Thomas Lang",
      "Tomas Sauer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.06961"
  },
  {
    "id": "arXiv:2210.06964",
    "title": "Causality-driven Hierarchical Structure Discovery for Reinforcement  Learning",
    "abstract": "Hierarchical reinforcement learning (HRL) effectively improves agents'\nexploration efficiency on tasks with sparse reward, with the guide of\nhigh-quality hierarchical structures (e.g., subgoals or options). However, how\nto automatically discover high-quality hierarchical structures is still a great\nchallenge. Previous HRL methods can hardly discover the hierarchical structures\nin complex environments due to the low exploration efficiency by exploiting the\nrandomness-driven exploration paradigm. To address this issue, we propose\nCDHRL, a causality-driven hierarchical reinforcement learning framework,\nleveraging a causality-driven discovery instead of a randomness-driven\nexploration to effectively build high-quality hierarchical structures in\ncomplicated environments. The key insight is that the causalities among\nenvironment variables are naturally fit for modeling reachable subgoals and\ntheir dependencies and can perfectly guide to build high-quality hierarchical\nstructures. The results in two complex environments, 2D-Minecraft and Eden,\nshow that CDHRL significantly boosts exploration efficiency with the\ncausality-driven paradigm.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Shaohui Peng",
      "Xing Hu",
      "Rui Zhang",
      "Ke Tang",
      "Jiaming Guo",
      "Qi Yi",
      "Ruizhi Chen",
      "Xishan Zhang",
      "Zidong Du",
      "Ling Li",
      "Qi Guo",
      "Yunji Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06964"
  },
  {
    "id": "arXiv:2210.06965",
    "title": "CUF: Continuous Upsampling Filters",
    "abstract": "Neural fields have rapidly been adopted for representing 3D signals, but\ntheir application to more classical 2D image-processing has been relatively\nlimited. In this paper, we consider one of the most important operations in\nimage processing: upsampling. In deep learning, learnable upsampling layers\nhave extensively been used for single image super-resolution. We propose to\nparameterize upsampling kernels as neural fields. This parameterization leads\nto a compact architecture that obtains a 40-fold reduction in the number of\nparameters when compared with competing arbitrary-scale super-resolution\narchitectures. When upsampling images of size 256x256 we show that our\narchitecture is 2x-10x more efficient than competing arbitrary-scale\nsuper-resolution architectures, and more efficient than sub-pixel convolutions\nwhen instantiated to a single-scale model. In the general setting, these gains\ngrow polynomially with the square of the target scale. We validate our method\non standard benchmarks showing such efficiency gains can be achieved without\nsacrifices in super-resolution performance.",
    "descriptor": "",
    "authors": [
      "Cristina Vasconcelos",
      "Kevin Swersky",
      "Mark Matthews",
      "Milad Hashemi",
      "Cengiz Oztireli",
      "Andrea Tagliasacchi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06965"
  },
  {
    "id": "arXiv:2210.06968",
    "title": "Behavioral graph fraud detection in E-commerce",
    "abstract": "In e-commerce industry, graph neural network methods are the new trends for\ntransaction risk modeling.The power of graph algorithms lie in the capability\nto catch transaction linking network information, which is very hard to be\ncaptured by other algorithms.However, in most existing approaches, transaction\nor user connections are defined by hard link strategies on shared properties,\nsuch as same credit card, same device, same ip address, same shipping address,\netc. Those types of strategies will result in sparse linkages by entities with\nstrong identification characteristics (ie. device) and over-linkages by\nentities that could be widely shared (ie. ip address), making it more difficult\nto learn useful information from graph. To address aforementioned problems, we\npresent a novel behavioral biometric based method to establish transaction\nlinkings based on user behavioral similarities, then train an unsupervised GNN\nto extract embedding features for downstream fraud prediction tasks. To our\nknowledge, this is the first time similarity based soft link has been used in\ngraph embedding applications. To speed up similarity calculation, we apply an\nin-house GPU based HDBSCAN clustering method to remove highly concentrated and\nisolated nodes before graph construction. Our experiments show that embedding\nfeatures learned from similarity based behavioral graph have achieved\nsignificant performance increase to the baseline fraud detection model in\nvarious business scenarios. In new guest buyer transaction scenario, this\nsegment is a challenge for traditional method, we can make precision increase\nfrom 0.82 to 0.86 at the same recall of 0.27, which means we can decrease false\npositive rate using this method.",
    "descriptor": "\nComments: 8 pages, 8 figures, ICDM\n",
    "authors": [
      "Hang Yin",
      "Zitao Zhang",
      "Zhurong Wang",
      "Yilmazcan Ozyurt",
      "Weiming Liang",
      "Wenyu Dong",
      "Yang Zhao",
      "Yinan Shan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06968"
  },
  {
    "id": "arXiv:2210.06970",
    "title": "Differential Bias: On the Perceptibility of Stance Imbalance in  Argumentation",
    "abstract": "Most research on natural language processing treats bias as an absolute\nconcept: Based on a (probably complex) algorithmic analysis, a sentence, an\narticle, or a text is classified as biased or not. Given the fact that for\nhumans the question of whether a text is biased can be difficult to answer or\nis answered contradictory, we ask whether an \"absolute bias classification\" is\na promising goal at all. We see the problem not in the complexity of\ninterpreting language phenomena but in the diversity of sociocultural\nbackgrounds of the readers, which cannot be handled uniformly: To decide\nwhether a text has crossed the proverbial line between non-biased and biased is\nsubjective. By asking \"Is text X more [less, equally] biased than text Y?\" we\npropose to analyze a simpler problem, which, by its construction, is rather\nindependent of standpoints, views, or sociocultural aspects. In such a model,\nbias becomes a preference relation that induces a partial ordering from least\nbiased to most biased texts without requiring a decision on where to draw the\nline. A prerequisite for this kind of bias model is the ability of humans to\nperceive relative bias differences in the first place. In our research, we\nselected a specific type of bias in argumentation, the stance bias, and\ndesigned a crowdsourcing study showing that differences in stance bias are\nperceptible when (light) support is provided through training or visual aid.",
    "descriptor": "\nComments: Accepted at AACL-IJCNLP 2022, Findings Volume\n",
    "authors": [
      "Alonso Palomino",
      "Martin Potthast",
      "Khalid Al-Khatib",
      "Benno Stein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.06970"
  },
  {
    "id": "arXiv:2210.06972",
    "title": "Simulating Ride-Pooling Services with Pre-Booking and On-Demand  Customers",
    "abstract": "If private vehicle trips can be replaced, ride-pooling services can decrease\nparking space needed by higher vehicle utilization and increase traffic\nefficiency by increasing vehicle occupancy. Nevertheless, substantial benefits\ncan only be achieved if a certain market penetration is passed to find enough\nshareable rides for pooling to take place. Additionally, because of their\nhighly dynamic and stochastic nature on-demand ride-pooling services cannot\nalways guarantee that a request is served. Allowing customers to pre-book their\ntrip in advance could provide benefits for both aspects. Additional knowledge\nhelps an operator to better plan vehicle schedules to improve service\nefficiency while an accepted trip or a rejection can be communicated early on\nto the customer. This study presents a simulation framework where a\nride-pooling provider offers a service in mixed operation: Customers can either\nuse the service on-demand or pre-book trips. A graph-based batch optimization\nformulation is proposed to create offline schedules for pre-booking customers.\nUsing two rolling horizons, this offline solution is forwarded to an online\noptimization for on-demand and pre-booking customers simultaneously. The\nframework is tested in a case study for Manhattan, NYC. That the graph-based\nbatch optimization is superior to a basic insertion method in terms of solution\nquality and run-time. Due to additional knowledge, the ride-pooling operator\ncan improve the solution quality significantly by serving more customers while\npooling efficiency can be increased. Additionally, customers have shorter\nwaiting and detour times the more customers book a trip in advance.",
    "descriptor": "",
    "authors": [
      "Roman Engelhardt",
      "Florian Dandl",
      "Klaus Bogenberger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.06972"
  },
  {
    "id": "arXiv:2210.06974",
    "title": "Evaluating Data-Driven Co-Speech Gestures of Embodied Conversational  Agents through Real-Time Interaction",
    "abstract": "Embodied Conversational Agents that make use of co-speech gestures can\nenhance human-machine interactions in many ways. In recent years, data-driven\ngesture generation approaches for ECAs have attracted considerable research\nattention, and related methods have continuously improved. Real-time\ninteraction is typically used when researchers evaluate ECA systems that\ngenerate rule-based gestures. However, when evaluating the performance of ECAs\nbased on data-driven methods, participants are often required only to watch\npre-recorded videos, which cannot provide adequate information about what a\nperson perceives during the interaction. To address this limitation, we\nexplored use of real-time interaction to assess data-driven gesturing ECAs. We\nprovided a testbed framework, and investigated whether gestures could affect\nhuman perception of ECAs in the dimensions of human-likeness, animacy,\nperceived intelligence, and focused attention. Our user study required\nparticipants to interact with two ECAs - one with and one without hand\ngestures. We collected subjective data from the participants' self-report\nquestionnaires and objective data from a gaze tracker. To our knowledge, the\ncurrent study represents the first attempt to evaluate data-driven gesturing\nECAs through real-time interaction and the first experiment using gaze-tracking\nto examine the effect of ECAs' gestures.",
    "descriptor": "\nComments: Published at the International Conference on Intelligent Virtual Agents\n",
    "authors": [
      "Yuan He",
      "Andr\u00e9 Pereira",
      "Taras Kucherenko"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06974"
  },
  {
    "id": "arXiv:2210.06976",
    "title": "Parallel photonic accelerator for decision making using optical  spatiotemporal chaos",
    "abstract": "Photonic accelerators have attracted increasing attention in artificial\nintelligence applications. The multi-armed bandit problem is a fundamental\nproblem of decision making using reinforcement learning. However, the\nscalability of photonic decision making has not yet been demonstrated in\nexperiments, owing to technical difficulties in physical realization. We\npropose a parallel photonic decision-making system for solving large-scale\nmulti-armed bandit problems using optical spatiotemporal chaos. We solve a\n512-armed bandit problem online, which is much larger than previous experiments\nby two orders of magnitude. The scaling property for correct decision making is\nexamined as a function of the number of slot machines, evaluated as an exponent\nof 0.86. This exponent is smaller than that in previous work, indicating the\nsuperiority of the proposed parallel principle. This experimental demonstration\nfacilitates photonic decision making to solve large-scale multi-armed bandit\nproblems for future photonic accelerators.",
    "descriptor": "\nComments: 20 pages, 6 figures\n",
    "authors": [
      "Kensei Morijiri",
      "Kento Takehana",
      "Takatomo Mihana",
      "Kazutaka Kanno",
      "Makoto Naruse",
      "Atsushi Uchida"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Machine Learning (cs.LG)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2210.06976"
  },
  {
    "id": "arXiv:2210.06978",
    "title": "LION: Latent Point Diffusion Models for 3D Shape Generation",
    "abstract": "Denoising diffusion models (DDMs) have shown promising results in 3D point\ncloud synthesis. To advance 3D DDMs and make them useful for digital artists,\nwe require (i) high generation quality, (ii) flexibility for manipulation and\napplications such as conditional synthesis and shape interpolation, and (iii)\nthe ability to output smooth surfaces or meshes. To this end, we introduce the\nhierarchical Latent Point Diffusion Model (LION) for 3D shape generation. LION\nis set up as a variational autoencoder (VAE) with a hierarchical latent space\nthat combines a global shape latent representation with a point-structured\nlatent space. For generation, we train two hierarchical DDMs in these latent\nspaces. The hierarchical VAE approach boosts performance compared to DDMs that\noperate on point clouds directly, while the point-structured latents are still\nideally suited for DDM-based modeling. Experimentally, LION achieves\nstate-of-the-art generation performance on multiple ShapeNet benchmarks.\nFurthermore, our VAE framework allows us to easily use LION for different\nrelevant tasks: LION excels at multimodal shape denoising and voxel-conditioned\nsynthesis, and it can be adapted for text- and image-driven 3D generation. We\nalso demonstrate shape autoencoding and latent shape interpolation, and we\naugment LION with modern surface reconstruction techniques to generate smooth\n3D meshes. We hope that LION provides a powerful tool for artists working with\n3D shapes due to its high-quality generation, flexibility, and surface\nreconstruction. Project page and code: https://nv-tlabs.github.io/LION.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Xiaohui Zeng",
      "Arash Vahdat",
      "Francis Williams",
      "Zan Gojcic",
      "Or Litany",
      "Sanja Fidler",
      "Karsten Kreis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06978"
  },
  {
    "id": "arXiv:2210.06980",
    "title": "Probabilistic Integration of Object Level Annotations in Chest X-ray  Classification",
    "abstract": "Medical image datasets and their annotations are not growing as fast as their\nequivalents in the general domain. This makes translation from the newest, more\ndata-intensive methods that have made a large impact on the vision field\nincreasingly more difficult and less efficient. In this paper, we propose a new\nprobabilistic latent variable model for disease classification in chest X-ray\nimages. Specifically we consider chest X-ray datasets that contain global\ndisease labels, and for a smaller subset contain object level expert\nannotations in the form of eye gaze patterns and disease bounding boxes. We\npropose a two-stage optimization algorithm which is able to handle these\ndifferent label granularities through a single training pipeline in a two-stage\nmanner. In our pipeline global dataset features are learned in the lower level\nlayers of the model. The specific details and nuances in the fine-grained\nexpert object-level annotations are learned in the final layers of the model\nusing a knowledge distillation method inspired by conditional variational\ninference. Subsequently, model weights are frozen to guide this learning\nprocess and prevent overfitting on the smaller richly annotated data subsets.\nThe proposed method yields consistent classification improvement across\ndifferent backbones on the common benchmark datasets Chest X-ray14 and\nMIMIC-CXR. This shows how two-stage learning of labels from coarse to\nfine-grained, in particular with object level annotations, is an effective\nmethod for more optimal annotation usage.",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Tom van Sonsbeek",
      "Xiantong Zhen",
      "Dwarikanath Mahapatra",
      "Marcel Worring"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06980"
  },
  {
    "id": "arXiv:2210.06983",
    "title": "Denoising Masked AutoEncoders are Certifiable Robust Vision Learners",
    "abstract": "In this paper, we propose a new self-supervised method, which is called\nDenoising Masked AutoEncoders (DMAE), for learning certified robust classifiers\nof images. In DMAE, we corrupt each image by adding Gaussian noises to each\npixel value and randomly masking several patches. A Transformer-based\nencoder-decoder model is then trained to reconstruct the original image from\nthe corrupted one. In this learning paradigm, the encoder will learn to capture\nrelevant semantics for the downstream tasks, which is also robust to Gaussian\nadditive noises. We show that the pre-trained encoder can naturally be used as\nthe base classifier in Gaussian smoothed models, where we can analytically\ncompute the certified radius for any data point. Although the proposed method\nis simple, it yields significant performance improvement in downstream\nclassification tasks. We show that the DMAE ViT-Base model, which just uses\n1/10 parameters of the model developed in recent work arXiv:2206.10550,\nachieves competitive or better certified accuracy in various settings. The DMAE\nViT-Large model significantly surpasses all previous results, establishing a\nnew state-of-the-art on ImageNet dataset. We further demonstrate that the\npre-trained model has good transferability to the CIFAR-10 dataset, suggesting\nits wide adaptability. Models and code are available at\nhttps://github.com/quanlin-wu/dmae.",
    "descriptor": "",
    "authors": [
      "Quanlin Wu",
      "Hang Ye",
      "Yuntian Gu",
      "Huishuai Zhang",
      "Liwei Wang",
      "Di He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06983"
  },
  {
    "id": "arXiv:2210.06984",
    "title": "QDTrack: Quasi-Dense Similarity Learning for Appearance-Only Multiple  Object Tracking",
    "abstract": "Similarity learning has been recognized as a crucial step for object\ntracking. However, existing multiple object tracking methods only use sparse\nground truth matching as the training objective, while ignoring the majority of\nthe informative regions in images. In this paper, we present Quasi-Dense\nSimilarity Learning, which densely samples hundreds of object regions on a pair\nof images for contrastive learning. We combine this similarity learning with\nmultiple existing object detectors to build Quasi-Dense Tracking (QDTrack),\nwhich does not require displacement regression or motion priors. We find that\nthe resulting distinctive feature space admits a simple nearest neighbor search\nat inference time for object association. In addition, we show that our\nsimilarity learning scheme is not limited to video data, but can learn\neffective instance similarity even from static input, enabling a competitive\ntracking performance without training on videos or using tracking supervision.\nWe conduct extensive experiments on a wide variety of popular MOT benchmarks.\nWe find that, despite its simplicity, QDTrack rivals the performance of\nstate-of-the-art tracking methods on all benchmarks and sets a new\nstate-of-the-art on the large-scale BDD100K MOT benchmark, while introducing\nnegligible computational overhead to the detector.",
    "descriptor": "",
    "authors": [
      "Tobias Fischer",
      "Jiangmiao Pang",
      "Thomas E. Huang",
      "Linlu Qiu",
      "Haofeng Chen",
      "Trevor Darrell",
      "Fisher Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06984"
  },
  {
    "id": "arXiv:2210.06985",
    "title": "A Local Discontinuous Galerkin approximation for the $p$-Navier-Stokes  system, Part III: Convergence rates for the pressure",
    "abstract": "In the present paper, we prove convergence rates for the pressure of the\nLocal Discontinuous Galerkin (LDG) approximation, proposed in Part I of the\npaper, of systems of $p$-Navier-Stokes type and $p$-Stokes type with $p\\in\n(2,\\infty)$. The results are supported by numerical experiments.",
    "descriptor": "\nComments: 18 pages, 0 figures, 1 table. arXiv admin note: substantial text overlap with arXiv:2208.04107\n",
    "authors": [
      "Alex Kaltenbach",
      "Michael R\u016f\u017ei\u010dka"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06985"
  },
  {
    "id": "arXiv:2210.06986",
    "title": "Tone prediction and orthographic conversion for Basaa",
    "abstract": "In this paper, we present a seq2seq approach for transliterating missionary\nBasaa orthographies into the official orthography. Our model uses pre-trained\nBasaa missionary and official orthography corpora using BERT. Since Basaa is a\nlow-resource language, we have decided to use the mT5 model for our project.\nBefore training our model, we pre-processed our corpora by eliminating\none-to-one correspondences between spellings and unifying characters variably\ncontaining either one to two characters into single-character form. Our best\nmT5 model achieved a CER equal to 12.6747 and a WER equal to 40.1012.",
    "descriptor": "",
    "authors": [
      "Ilya Nikitin",
      "Brian O'Connor",
      "Anastasia Safonova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06986"
  },
  {
    "id": "arXiv:2210.06989",
    "title": "Multi-Task Meta Learning: learn how to adapt to unseen tasks",
    "abstract": "This work aims to integrate two learning paradigms Multi-Task Learning (MTL)\nand meta learning, to bring together the best of both worlds, i.e.,\nsimultaneous learning of multiple tasks, an element of MTL and promptly\nadapting to new tasks with fewer data, a quality of meta learning. We propose\nMulti-task Meta Learning (MTML), an approach to enhance MTL compared to single\ntask learning by employing meta learning. The fundamental idea of this work is\nto train a multi-task model, such that when an unseen task is introduced, it\ncan learn in fewer steps whilst offering a performance at least as good as\nconventional single task learning on the new task or inclusion within the MTL.\nBy conducting various experiments, we demonstrate this paradigm on two datasets\nand four tasks: NYU-v2 and the taskonomy dataset for which we perform semantic\nsegmentation, depth estimation, surface normal estimation, and edge detection.\nMTML achieves state-of-the-art results for most of the tasks, and MTL also\nperforms reasonably well for all tasks compared to single task learning.",
    "descriptor": "",
    "authors": [
      "Richa Upadhyay",
      "Prakash Chandra Chhipa",
      "Ronald Phlypo",
      "Rajkumar Saini",
      "Marcus Liwicki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06989"
  },
  {
    "id": "arXiv:2210.06990",
    "title": "Exploring Segmentation Approaches for Neural Machine Translation of  Code-Switched Egyptian Arabic-English Text",
    "abstract": "Data sparsity is one of the main challenges posed by Code-switching (CS),\nwhich is further exacerbated in the case of morphologically rich languages. For\nthe task of Machine Translation (MT), morphological segmentation has proven\nsuccessful in alleviating data sparsity in monolingual contexts; however, it\nhas not been investigated for CS settings. In this paper, we study the\neffectiveness of different segmentation approaches on MT performance, covering\nmorphology-based and frequency-based segmentation techniques. We experiment on\nMT from code-switched Arabic-English to English. We provide detailed analysis,\nexamining a variety of conditions, such as data size and sentences with\ndifferent degrees in CS. Empirical results show that morphology-aware\nsegmenters perform the best in segmentation tasks but under-perform in MT.\nNevertheless, we find that the choice of the segmentation setup to use for MT\nis highly dependent on the data size. For extreme low-resource scenarios, a\ncombination of frequency and morphology-based segmentations is shown to perform\nthe best. For more resourced settings, such a combination does not bring\nsignificant improvements over the use of frequency-based segmentation.",
    "descriptor": "",
    "authors": [
      "Marwa Gaser",
      "Manuel Mager",
      "Injy Hamed",
      "Nizar Habash",
      "Slim Abdennadher",
      "Ngoc Thang Vu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06990"
  },
  {
    "id": "arXiv:2210.06996",
    "title": "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT",
    "abstract": "Domain-specific neural machine translation (NMT) systems (e.g., in\neducational applications) are socially significant with the potential to help\nmake information accessible to a diverse set of users in multilingual\nsocieties. It is desirable that such NMT systems be lexically constrained and\ndraw from domain-specific dictionaries. Dictionaries could present multiple\ncandidate translations for a source words/phrases on account of the polysemous\nnature of words. The onus is then on the NMT model to choose the contextually\nmost appropriate candidate. Prior work has largely ignored this problem and\nfocused on the single candidate setting where the target word or phrase is\nreplaced by a single constraint. In this work we present DICTDIS, a lexically\nconstrained NMT system that disambiguates between multiple candidate\ntranslations derived from dictionaries. We achieve this by augmenting training\ndata with multiple dictionary candidates to actively encourage disambiguation\nduring training. We demonstrate the utility of DICTDIS via extensive\nexperiments on English-Hindi sentences in a variety of domains including news,\nfinance, medicine and engineering. We obtain superior disambiguation\nperformance on all domains with improved fluency in some domains of up to 4\nBLEU points, when compared with existing approaches for lexically constrained\nand unconstrained NMT.",
    "descriptor": "",
    "authors": [
      "Ayush Maheshwari",
      "Piyush Sharma",
      "Preethi Jyothi",
      "Ganesh Ramakrishnan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06996"
  },
  {
    "id": "arXiv:2210.06997",
    "title": "Two approaches to inpainting microstructure with deep convolutional  generative adversarial networks",
    "abstract": "Imaging is critical to the characterisation of materials. However, even with\ncareful sample preparation and microscope calibration, imaging techniques are\noften prone to defects and unwanted artefacts. This is particularly problematic\nfor applications where the micrograph is to be used for simulation or feature\nanalysis, as defects are likely to lead to inaccurate results. Microstructural\ninpainting is a method to alleviate this problem by replacing occluded regions\nwith synthetic microstructure with matching boundaries. In this paper we\nintroduce two methods that use generative adversarial networks to generate\ncontiguous inpainted regions of arbitrary shape and size by learning the\nmicrostructural distribution from the unoccluded data. We find that one\nbenefits from high speed and simplicity, whilst the other gives smoother\nboundaries at the inpainting border. We also outline the development of a\ngraphical user interface that allows users to utilise these machine learning\nmethods in a 'no-code' environment.",
    "descriptor": "\nComments: 16 pages, 11 figures\n",
    "authors": [
      "Isaac Squires",
      "Samuel J. Cooper",
      "Amir Dahari",
      "Steve Kench"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.06997"
  },
  {
    "id": "arXiv:2210.06998",
    "title": "DE-FAKE: Detection and Attribution of Fake Images Generated by  Text-to-Image Diffusion Models",
    "abstract": "Diffusion models emerge to establish the new state of the art in the visual\ngeneration. In particular, text-to-image diffusion models that generate images\nbased on caption descriptions have attracted increasing attention, impressed by\ntheir user controllability. Despite encouraging performance, they exaggerate\nconcerns of fake image misuse and cast new pressures on fake image detection.\nIn this work, we pioneer a systematic study of the authenticity of fake images\ngenerated by text-to-image diffusion models. In particular, we conduct\ncomprehensive studies from two perspectives unique to the text-to-image model,\nnamely, visual modality and linguistic modality. For visual modality, we\npropose universal detection that demonstrates fake images of these\ntext-to-image diffusion models share common cues, which enable us to\ndistinguish them apart from real images. We then propose source attribution\nthat reveals the uniqueness of the fingerprints held by each diffusion model,\nwhich can be used to attribute each fake image to its model source. A variety\nof ablation and analysis studies further interpret the improvements from each\nof our proposed methods. For linguistic modality, we delve deeper to\ncomprehensively analyze the impacts of text captions (called prompt analysis)\non the image authenticity of text-to-image diffusion models, and reason the\nimpacts to the detection and attribution performance of fake images. All\nfindings contribute to the community's insight into the natural properties of\ntext-to-image diffusion models, and we appeal to our community's consideration\non the counterpart solutions, like ours, against the rapidly-evolving fake\nimage generators.",
    "descriptor": "",
    "authors": [
      "Zeyang Sha",
      "Zheng Li",
      "Ning Yu",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06998"
  },
  {
    "id": "arXiv:2210.07002",
    "title": "Anonymizing Speech with Generative Adversarial Networks to Preserve  Speaker Privacy",
    "abstract": "In order to protect the privacy of speech data, speaker anonymization aims\nfor hiding the identity of a speaker by changing the voice in speech\nrecordings. This typically comes with a privacy-utility trade-off between\nprotection of individuals and usability of the data for downstream\napplications. One of the challenges in this context is to create non-existent\nvoices that sound as natural as possible.\nIn this work, we propose to tackle this issue by generating speaker\nembeddings using a generative adversarial network with Wasserstein distance as\ncost function. By incorporating these artificial embeddings into a\nspeech-to-text-to-speech pipeline, we outperform previous approaches in terms\nof privacy and utility. According to standard objective metrics and human\nevaluation, our approach generates intelligible and content-preserving yet\nprivacy-protecting versions of the original recordings.",
    "descriptor": "\nComments: IEEE Spoken Language Technology Workshop 2022\n",
    "authors": [
      "Sarina Meyer",
      "Pascal Tilli",
      "Pavel Denisov",
      "Florian Lux",
      "Julia Koch",
      "Ngoc Thang Vu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.07002"
  },
  {
    "id": "arXiv:2210.07006",
    "title": "Sustainable Online Reinforcement Learning for Auto-bidding",
    "abstract": "Recently, auto-bidding technique has become an essential tool to increase the\nrevenue of advertisers. Facing the complex and ever-changing bidding\nenvironments in the real-world advertising system (RAS), state-of-the-art\nauto-bidding policies usually leverage reinforcement learning (RL) algorithms\nto generate real-time bids on behalf of the advertisers. Due to safety\nconcerns, it was believed that the RL training process can only be carried out\nin an offline virtual advertising system (VAS) that is built based on the\nhistorical data generated in the RAS. In this paper, we argue that there exists\nsignificant gaps between the VAS and RAS, making the RL training process suffer\nfrom the problem of inconsistency between online and offline (IBOO). Firstly,\nwe formally define the IBOO and systematically analyze its causes and\ninfluences. Then, to avoid the IBOO, we propose a sustainable online RL (SORL)\nframework that trains the auto-bidding policy by directly interacting with the\nRAS, instead of learning in the VAS. Specifically, based on our proof of the\nLipschitz smooth property of the Q function, we design a safe and efficient\nonline exploration (SER) policy for continuously collecting data from the RAS.\nMeanwhile, we derive the theoretical lower bound on the safety of the SER\npolicy. We also develop a variance-suppressed conservative Q-learning (V-CQL)\nmethod to effectively and stably learn the auto-bidding policy with the\ncollected data. Finally, extensive simulated and real-world experiments\nvalidate the superiority of our approach over the state-of-the-art auto-bidding\nalgorithm.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Zhiyu Mou",
      "Yusen Huo",
      "Rongquan Bai",
      "Mingzhou Xie",
      "Chuan Yu",
      "Jian Xu",
      "Bo Zheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07006"
  },
  {
    "id": "arXiv:2210.07011",
    "title": "Variational Graph Generator for Multi-View Graph Clustering",
    "abstract": "Multi-view graph clustering (MGC) methods are increasingly being studied due\nto the rising of multi-view data with graph structural information. The\ncritical point of MGC is to better utilize the view-specific and view-common\ninformation in features and graphs of multiple views. However, existing works\nhave an inherent limitation that they are unable to concurrently utilize the\nconsensus graph information across multiple graphs and the view-specific\nfeature information. To address this issue, we propose Variational Graph\nGenerator for Multi-View Graph Clustering (VGMGC). Specifically, a novel\nvariational graph generator is proposed to infer a reliable variational\nconsensus graph based on a priori assumption over multiple graphs. Then a\nsimple yet effective graph encoder in conjunction with the multi-view\nclustering objective is presented to learn the desired graph embeddings for\nclustering, which embeds the consensus and view-specific graphs together with\nfeatures. Finally, theoretical results illustrate the rationality of VGMGC by\nanalyzing the uncertainty of the inferred consensus graph with information\nbottleneck principle. Extensive experiments demonstrate the superior\nperformance of our VGMGC over SOTAs.",
    "descriptor": "\nComments: 15pages with 6 figures,submitted to AAAI 2023\n",
    "authors": [
      "Jianpeng Chen",
      "Yawen Ling",
      "Jie Xu",
      "Yazhou Ren",
      "Shudong Huang",
      "Xiaorong Pu",
      "Lifang He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07011"
  },
  {
    "id": "arXiv:2210.07012",
    "title": "Over-the-Air Computation Based on Balanced Number Systems for Federated  Edge Learning",
    "abstract": "In this study, we propose a digital over-the-air computation (OAC) scheme for\nachieving continuous-valued (analog) aggregation for federated edge learning\n(FEEL). We show that the average of a set of real-valued parameters can be\ncalculated approximately by using the average of the corresponding numerals,\nwhere the numerals are obtained based on a balanced number system. By\nexploiting this key property, the proposed scheme encodes the local stochastic\ngradients into a set of numerals. Next, it determines the positions of the\nactivated orthogonal frequency division multiplexing (OFDM) subcarriers by\nusing the values of the numerals. To eliminate the need for precise\nsample-level time synchronization, channel estimation overhead, and channel\ninversion, the proposed scheme also uses a non-coherent receiver at the edge\nserver (ES) and does not utilize a pre-equalization at the edge devices (EDs).\nWe theoretically analyze the MSE performance of the proposed scheme and the\nconvergence rate for a non-convex loss function. To improve the test accuracy\nof FEEL with the proposed scheme, we introduce the concept of adaptive absolute\nmaximum (AAM). Our numerical results show that when the proposed scheme is used\nwith AAM for FEEL, the test accuracy can reach up to 98% for heterogeneous data\ndistribution.",
    "descriptor": "\nComments: 13 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv:2209.11004\n",
    "authors": [
      "Alphan Sahin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.07012"
  },
  {
    "id": "arXiv:2210.07013",
    "title": "Transfer Deep Reinforcement Learning-based Large-scale V2G Continuous  Charging Coordination with Renewable Energy Sources",
    "abstract": "Due to the increasing popularity of electric vehicles (EVs) and the\ntechnological advancement of EV electronics, the vehicle-to-grid (V2G)\ntechnique and large-scale scheduling algorithms have been developed to achieve\na high level of renewable energy and power grid stability. This paper proposes\na deep reinforcement learning (DRL) method for the continuous\ncharging/discharging coordination strategy in aggregating large-scale EVs in\nV2G mode with renewable energy sources (RES). The DRL coordination strategy can\nefficiently optimize the electric vehicle aggregator's (EVA's) real-time\ncharging/discharging power with the state of charge (SOC) constraints of the\nEVA and the individual EV. Compared with uncontrolled charging, the load\nvariance is reduced by 97.37$\\%$ and the charging cost by 76.56$\\%$. The DRL\ncoordination strategy further demonstrates outstanding transfer learning\nability to microgrids with RES and large-scale EVA, as well as the complicated\nweekly scheduling. The DRL coordination strategy demonstrates flexible,\nadaptable, and scalable performance for the large-scale V2G under realistic\noperating conditions.",
    "descriptor": "",
    "authors": [
      "Yubao Zhang",
      "Xin Chen",
      "Yuchen Zhang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07013"
  },
  {
    "id": "arXiv:2210.07015",
    "title": "Augmentation for Learning From Demonstration with Environmental  Constraints",
    "abstract": "We introduce a Learning from Demonstration (LfD) approach for contact-rich\nmanipulation tasks with articulated mechanisms. The extracted policy from a\nsingle human demonstration generalizes to different mechanisms of the same type\nand is robust against environmental variations. The key to achieving such\ngeneralization and robustness from a single human demonstration is to\nautonomously augment the initial demonstration to gather additional information\nthrough purposefully interacting with the environment. Our real-world\nexperiments on complex mechanisms with multi-DOF demonstrate that our approach\ncan reliably accomplish the task in a changing environment. Videos are\navailable at the: https://sites.google.com/view/rbosalfdec/home",
    "descriptor": "\nComments: Submitted to 2023 IEEE International Conference on Robotics and Automation (ICRA)\n",
    "authors": [
      "Xing Li",
      "Manuel Baum",
      "Oliver Brock"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07015"
  },
  {
    "id": "arXiv:2210.07016",
    "title": "Learning with Style: Continual Semantic Segmentation Across Tasks and  Domains",
    "abstract": "Deep learning models dealing with image understanding in real-world settings\nmust be able to adapt to a wide variety of tasks across different domains.\nDomain adaptation and class incremental learning deal with domain and task\nvariability separately, whereas their unified solution is still an open\nproblem. We tackle both facets of the problem together, taking into account the\nsemantic shift within both input and label spaces. We start by formally\nintroducing continual learning under task and domain shift. Then, we address\nthe proposed setup by using style transfer techniques to extend knowledge\nacross domains when learning incremental tasks and a robust distillation\nframework to effectively recollect task knowledge under incremental domain\nshift. The devised framework (LwS, Learning with Style) is able to generalize\nincrementally acquired task knowledge across all the domains encountered,\nproving to be robust against catastrophic forgetting. Extensive experimental\nevaluation on multiple autonomous driving datasets shows how the proposed\nmethod outperforms existing approaches, which prove to be ill-equipped to deal\nwith continual semantic segmentation under both task and domain shift.",
    "descriptor": "\nComments: 16 pages, 7 figures\n",
    "authors": [
      "Marco Toldo",
      "Umberto Michieli",
      "Pietro Zanuttigh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07016"
  },
  {
    "id": "arXiv:2210.07017",
    "title": "ComSearch: Equation Searching with Combinatorial Strategy for Solving  Math Word Problems with Weak Supervision",
    "abstract": "Previous studies have introduced a weakly-supervised paradigm for solving\nmath word problems requiring only the answer value annotation. While these\nmethods search for correct value equation candidates as pseudo labels, they\nsearch among a narrow sub-space of the enormous equation space. To address this\nproblem, we propose a novel search algorithm with combinatorial strategy\n\\textbf{ComSearch}, which can compress the search space by excluding\nmathematically equivalent equations. The compression allows the searching\nalgorithm to enumerate all possible equations and obtain high-quality data. We\ninvestigate the noise in the pseudo labels that hold wrong mathematical logic,\nwhich we refer to as the \\textit{false-matching} problem, and propose a ranking\nmodel to denoise the pseudo labels. Our approach holds a flexible framework to\nutilize two existing supervised math word problem solvers to train pseudo\nlabels, and both achieve state-of-the-art performance in the weak supervision\ntask.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Qianying Liu",
      "Wenyu Guan",
      "Jianhao Shen",
      "Fei Cheng",
      "Sadao Kurohashi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07017"
  },
  {
    "id": "arXiv:2210.07018",
    "title": "Online matching with delays and stochastic arrival times",
    "abstract": "This paper presents a new research direction for the Min-cost Perfect\nMatching with Delays (MPMD) - a problem introduced by Emek et al. (STOC'16). In\nthe original version of this problem, we are given an $n$-point metric space,\nwhere requests arrive in an online fashion. The goal is to minimise the\nmatching cost for an even number of requests. However, contrary to traditional\nonline matching problems, a request does not have to be paired immediately at\nthe time of its arrival. Instead, the decision of whether to match a request\ncan be postponed for time $t$ at a delay cost of $t$. For this reason, the goal\nof the MPMD is to minimise the overall sum of distance and delay costs.\nInterestingly, for adversarially generated requests, no online algorithm can\nachieve a competitive ratio better than $O(\\log n/\\log \\log n)$ (Ashlagi et\nal., APPROX/RANDOM'17).\nHere, we consider a stochastic version of the MPMD problem where the input\nrequests follow a Poisson arrival process. For such a problem, we show that the\nabove lower bound can be improved by presenting two deterministic online\nalgorithms, which, in expectation, are constant-competitive. The first one is a\nsimple greedy algorithm that matches any two requests once the sum of their\ndelay costs exceeds their connection cost, i.e., the distance between them. The\nsecond algorithm builds on the tools used to analyse the first one in order to\nobtain even better performance guarantees. This result is rather surprising as\nthe greedy approach for the adversarial model achieves a competitive ratio of\n$\\Omega(m^{\\log \\frac{3}{2}+\\varepsilon})$, where $m$ denotes the number of\nrequests served (Azar et al., TOCS'20). Finally, we prove that it is possible\nto obtain similar results for the general case when the delay cost follows an\narbitrary positive and non-decreasing function, as well as for the MPMD variant\nwith penalties to clear pending requests.",
    "descriptor": "\nComments: 32 pages, 7 figures\n",
    "authors": [
      "Mathieu Mari",
      "Micha\u0142 Paw\u0142owski",
      "Runtian Ren",
      "Piotr Sankowski"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.07018"
  },
  {
    "id": "arXiv:2210.07022",
    "title": "CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual  Labeled Sequence Translation",
    "abstract": "Named entity recognition (NER) suffers from the scarcity of annotated\ntraining data, especially for low-resource languages without labeled data.\nCross-lingual NER has been proposed to alleviate this issue by transferring\nknowledge from high-resource languages to low-resource languages via aligned\ncross-lingual representations or machine translation results. However, the\nperformance of cross-lingual NER methods is severely affected by the\nunsatisfactory quality of translation or label projection. To address these\nproblems, we propose a Cross-lingual Entity Projection framework (CROP) to\nenable zero-shot cross-lingual NER with the help of a multilingual labeled\nsequence translation model. Specifically, the target sequence is first\ntranslated into the source language and then tagged by a source NER model. We\nfurther adopt a labeled sequence translation model to project the tagged\nsequence back to the target language and label the target raw sentence.\nUltimately, the whole pipeline is integrated into an end-to-end model by the\nway of self-training. Experimental results on two benchmarks demonstrate that\nour method substantially outperforms the previous strong baseline by a large\nmargin of +3~7 F1 scores and achieves state-of-the-art performance.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Jian Yang",
      "Shaohan Huang",
      "Shuming Ma",
      "Yuwei Yin",
      "Li Dong",
      "Dongdong Zhang",
      "Hongcheng Guo",
      "Zhoujun Li",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07022"
  },
  {
    "id": "arXiv:2210.07024",
    "title": "Self-explaining deep models with logic rule reasoning",
    "abstract": "We present SELOR, a framework for integrating self-explaining capabilities\ninto a given deep model to achieve both high prediction performance and human\nprecision. By \"human precision\", we refer to the degree to which humans agree\nwith the reasons models provide for their predictions. Human precision affects\nuser trust and allows users to collaborate closely with the model. We\ndemonstrate that logic rule explanations naturally satisfy human precision with\nthe expressive power required for good predictive performance. We then\nillustrate how to enable a deep model to predict and explain with logic rules.\nOur method does not require predefined logic rule sets or human annotations and\ncan be learned efficiently and easily with widely-used deep learning modules in\na differentiable way. Extensive experiments show that our method gives\nexplanations closer to human decision logic than other methods while\nmaintaining the performance of deep learning models.",
    "descriptor": "\nComments: 26 pages including reference, checklist, and appendix. Accepted in NeurIPS 2022\n",
    "authors": [
      "Seungeon Lee",
      "Xiting Wang",
      "Sungwon Han",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Meeyoung Cha"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.07024"
  },
  {
    "id": "arXiv:2210.07030",
    "title": "Hard to Detect Factors of Univariate Integer Polynomials",
    "abstract": "We investigate the computational complexity of deciding whether a given\nunivariate integer polynomial p(x) has a factor q(x) satisfying specific\nadditional constraints. When the only constraint imposed on q(x) is to have a\ndegree smaller than the degree of p(x) and greater than zero, the problem is\nequivalent to testing the irreducibility of p(x) and then it is solvable in\npolynomial time. We prove that deciding whether a given monic univariate\ninteger polynomial has factors satisfying additional properties may lead to\nNP-complete problems in the strong sense. In particular, given any constant\nvalue k in Z, we prove that it is NP-complete in the strong sense to detect the\nexistence of a factor that returns a prescribed value when evaluated at x=k or\nto detect the existence of a pair of factors - whose product is equal to the\noriginal polynomial - that return the same value when evaluated at x=k.",
    "descriptor": "",
    "authors": [
      "Alberto Dennunzio",
      "Enrico Formenti",
      "Luciano Margara"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.07030"
  },
  {
    "id": "arXiv:2210.07031",
    "title": "Rebalanced Zero-shot Learning",
    "abstract": "Zero-shot learning (ZSL) aims to identify unseen classes with zero samples\nduring training. Broadly speaking, present ZSL methods usually adopt\nclass-level semantic labels and compare them with instance-level semantic\npredictions to infer unseen classes. However, we find that such existing models\nmostly produce imbalanced semantic predictions, i.e. these models could perform\nprecisely for some semantics, but may not for others. To address the drawback,\nwe aim to introduce an imbalanced learning framework into ZSL. However, we find\nthat imbalanced ZSL has two unique challenges: (1) Its imbalanced predictions\nare highly correlated with the value of semantic labels rather than the number\nof samples as typically considered in the traditional imbalanced learning; (2)\nDifferent semantics follow quite different error distributions between classes.\nTo mitigate these issues, we first formalize ZSL as an imbalanced regression\nproblem which offers theoretical foundations to interpret how semantic labels\nlead to imbalanced semantic predictions. We then propose a re-weighted loss\ntermed Re-balanced Mean-Squared Error (ReMSE), which tracks the mean and\nvariance of error distributions, thus ensuring rebalanced learning across\nclasses. As a major contribution, we conduct a series of analyses showing that\nReMSE is theoretically well established. Extensive experiments demonstrate that\nthe proposed method effectively alleviates the imbalance in semantic prediction\nand outperforms many state-of-the-art ZSL methods.",
    "descriptor": "",
    "authors": [
      "Zihan Ye",
      "Guanyu Yang",
      "Xiaobo Jin",
      "Youfa Liu",
      "Kaizhu Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07031"
  },
  {
    "id": "arXiv:2210.07032",
    "title": "Prompt-based Connective Prediction Method for Fine-grained Implicit  Discourse Relation Recognition",
    "abstract": "Due to the absence of connectives, implicit discourse relation recognition\n(IDRR) is still a challenging and crucial task in discourse analysis. Most of\nthe current work adopted multitask learning to aid IDRR through explicit\ndiscourse relation recognition (EDRR) or utilized dependencies between\ndiscourse relation labels to constrain model predictions. But these methods\nstill performed poorly on fine-grained IDRR and even utterly misidentified on\nmost of the few-shot discourse relation classes. To address these problems, we\npropose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our\nmethod instructs large-scale pre-trained models to use knowledge relevant to\ndiscourse relation and utilizes the strong correlation between connectives and\ndiscourse relation to help the model recognize implicit discourse relations.\nExperimental results show that our method surpasses the current\nstate-of-the-art model and achieves significant improvements on those\nfine-grained few-shot discourse relation. Moreover, our approach is able to be\ntransferred to EDRR and obtain acceptable results. Our code is released in\nhttps://github.com/zh-i9/PCP-for-IDRR.",
    "descriptor": "\nComments: Findings of EMNLP 2022 Accepted\n",
    "authors": [
      "Hao Zhou",
      "Man Lan",
      "Yuanbin Wu",
      "Yuefeng Chen",
      "Meirong Ma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07032"
  },
  {
    "id": "arXiv:2210.07037",
    "title": "Self-Supervised Learning of Linear Precoders under Non-Linear PA  Distortion for Energy-Efficient Massive MIMO Systems",
    "abstract": "Massive multiple input multiple output (MIMO) systems are typically designed\nunder the assumption of linear power amplifiers (PAs). However, PAs are\ntypically most energy-efficient when operating close to their saturation point,\nwhere they cause non-linear distortion. Moreover, when using conventional\nprecoders, this distortion coherently combines at the user locations, limiting\nperformance. As such, when designing an energy-efficient massive MIMO system,\nthis distortion has to be managed. In this work, we propose the use of a neural\nnetwork (NN) to learn the mapping between the channel matrix and the precoding\nmatrix, which maximizes the sum rate in the presence of this non-linear\ndistortion. This is done for a third-order polynomial PA model for both the\nsingle and multi-user case. By learning this mapping a significant increase in\nenergy efficiency is achieved as compared to conventional precoders and even as\ncompared to perfect digital pre-distortion (DPD), in the saturation regime.",
    "descriptor": "",
    "authors": [
      "Thomas Feys",
      "Xavier Mestre",
      "Fran\u00e7ois Rottenberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.07037"
  },
  {
    "id": "arXiv:2210.07039",
    "title": "Sapling Similarity outperforms other local similarity metrics in  collaborative filtering",
    "abstract": "Many bipartite networks describe systems where a link represents a relation\nbetween a user and an item. Measuring the similarity between either users or\nitems is the basis of memory-based collaborative filtering, a widely used\nmethod to build a recommender system with the purpose of proposing items to\nusers. When the edges of the network are unweighted, traditional approaches\nallow only positive similarity values, so neglecting the possibility and the\neffect of two users (or two items) being very dissimilar. Here we propose a\nmethod to compute similarity that allows also negative values, the Sapling\nSimilarity. The key idea is to look at how the information that a user is\nconnected to an item influences our prior estimation of the probability that\nanother user is connected to the same item: if it is reduced, then the\nsimilarity between the two users will be negative, otherwise it will be\npositive. Using different datasets, we show that the Sapling Similarity\noutperforms other similarity metrics when it is used to recommend new items to\nusers.",
    "descriptor": "\nComments: 17 pages, 5 figures\n",
    "authors": [
      "Giambattista Albora",
      "Lavinia Rossi Mori",
      "Andrea Zaccaria"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.07039"
  },
  {
    "id": "arXiv:2210.07040",
    "title": "Threshold Treewidth and Hypertree Width",
    "abstract": "Treewidth and hypertree width have proven to be highly successful structural\nparameters in the context of the Constraint Satisfaction Problem (CSP). When\neither of these parameters is bounded by a constant, then CSP becomes solvable\nin polynomial time. However, here the order of the polynomial in the running\ntime depends on the width, and this is known to be unavoidable; therefore, the\nproblem is not fixed-parameter tractable parameterized by either of these width\nmeasures. Here we introduce an enhancement of tree and hypertree width through\na novel notion of thresholds, allowing the associated decompositions to take\ninto account information about the computational costs associated with solving\nthe given CSP instance. Aside from introducing these notions, we obtain\nefficient theoretical as well as empirical algorithms for computing threshold\ntreewidth and hypertree width and show that these parameters give rise to\nfixed-parameter algorithms for CSP as well as other, more general problems. We\ncomplement our theoretical results with experimental evaluations in terms of\nheuristics as well as exact methods based on SAT/SMT encodings.",
    "descriptor": "\nComments: 24 pages, 4 figures. An extended abstract appeared at IJCAI 2020. A full version appeared in the Journal of Artificial Intelligence Research\n",
    "authors": [
      "Andre Schidler",
      "Robert Ganian",
      "Manuel Sorge",
      "Stefan Szeider"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07040"
  },
  {
    "id": "arXiv:2210.07041",
    "title": "Spontaneous Emerging Preference in Two-tower Language Model",
    "abstract": "The ever-growing size of the foundation language model has brought\nsignificant performance gains in various types of downstream tasks. With the\nexistence of side-effects brought about by the large size of the foundation\nlanguage model such as deployment cost, availability issues, and environmental\ncost, there is some interest in exploring other possible directions, such as a\ndivide-and-conquer scheme. In this paper, we are asking a basic question: are\nlanguage processes naturally dividable? We study this problem with a simple\ntwo-tower language model setting, where two language models with identical\nconfigurations are trained side-by-side cooperatively. With this setting, we\ndiscover the spontaneous emerging preference phenomenon, where some of the\ntokens are consistently better predicted by one tower while others by another\ntower. This phenomenon is qualitatively stable, regardless of model\nconfiguration and type, suggesting this as an intrinsic property of natural\nlanguage. This study suggests that interesting properties of natural language\nare still waiting to be discovered, which may aid the future development of\nnatural language processing techniques.",
    "descriptor": "",
    "authors": [
      "Zhengqi He",
      "Taro Toyoizumi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07041"
  },
  {
    "id": "arXiv:2210.07048",
    "title": "A New Optimality Property of Strang's Splitting",
    "abstract": "For systems of the form $\\dot q = M^{-1} p$, $\\dot p = -Aq+f(q)$, common in\nmany applications, we analyze splitting integrators based on the\n(linear/nonlinear) split systems $\\dot q = M^{-1} p$, $\\dot p = -Aq$ and $\\dot\nq = 0$, $\\dot p = f(q)$. We show that the well-known Strang splitting is\noptimally stable in the sense that, when applied to a relevant model problem,\nit has a larger stability region than alternative integrators. This generalizes\na well-known property of the common St\\\"{o}rmer/Verlet/leapfrog algorithm,\nwhich of course arises from Strang splitting based on the (kinetic/potential)\nsplit systems $\\dot q = M^{-1} p$, $\\dot p = 0$ and $\\dot q = 0$, $\\dot p =\n-Aq+f(q)$.",
    "descriptor": "",
    "authors": [
      "Fernando Casas",
      "Jes\u00fas Mar\u00eda Sanz-Serna",
      "Luke Shaw"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.07048"
  },
  {
    "id": "arXiv:2210.07049",
    "title": "Dimensionality of datasets in object detection networks",
    "abstract": "In recent years, convolutional neural networks (CNNs) are used in a large\nnumber of tasks in computer vision. One of them is object detection for\nautonomous driving. Although CNNs are used widely in many areas, what happens\ninside the network is still unexplained on many levels. Our goal is to\ndetermine the effect of Intrinsic dimension (i.e. minimum number of parameters\nrequired to represent data) in different layers on the accuracy of object\ndetection network for augmented data sets. Our investigation determines that\nthere is difference between the representation of normal and augmented data\nduring feature extraction.",
    "descriptor": "\nComments: 7 pages, 3 figures\n",
    "authors": [
      "Ajay Chawda",
      "Axel Vierling",
      "Karsten Berns"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07049"
  },
  {
    "id": "arXiv:2210.07052",
    "title": "A new matrix equation expression for the solution of non-autonomous  linear systems of ODEs",
    "abstract": "The solution of systems of non-autonomous linear ordinary differential\nequations is crucial in a variety of applications, such us nuclear magnetic\nresonance spectroscopy. A new method with spectral accuracy has been recently\nintroduced in the scalar case. The method is based on a product that\ngeneralizes the convolution. In this work, we show that it is possible to\nextend the method to solve systems of non-autonomous linear ordinary\ndifferential equations (ODEs). In this new approach, the ODE solution can be\nexpressed through a linear system that can be equivalently rewritten as a\nmatrix equation. Numerical examples illustrate the method's efficacy and the\nlow-rank property of the matrix equation solution.",
    "descriptor": "",
    "authors": [
      "Stefano Pozza",
      "Niel Van Buggenhout"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.07052"
  },
  {
    "id": "arXiv:2210.07054",
    "title": "Scaling Back-Translation with Domain Text Generation for Sign Language  Gloss Translation",
    "abstract": "Sign language gloss translation aims to translate the sign glosses into\nspoken language texts, which is challenging due to the scarcity of labeled\ngloss-text parallel data. Back translation (BT), which generates\npseudo-parallel data by translating in-domain spoken language texts into sign\nglosses, has been applied to alleviate the data scarcity problem. However, the\nlack of large-scale high-quality domain spoken language text data limits the\neffect of BT. In this paper, to overcome the limitation, we propose a Prompt\nbased domain text Generation (PGEN) approach to produce the large-scale\nin-domain spoken language text data. Specifically, PGEN randomly concatenates\nsentences from the original in-domain spoken language text data as prompts to\ninduce a pre-trained language model (i.e., GPT-2) to generate spoken language\ntexts in a similar style. Experimental results on three benchmarks of sign\nlanguage gloss translation in varied languages demonstrate that BT with spoken\nlanguage texts generated by PGEN significantly outperforms the compared\nmethods. In addition, as the scale of spoken language texts generated by PGEN\nincreases, the BT technique can achieve further improvements, demonstrating the\neffectiveness of our approach. We release the code and data for facilitating\nfuture research in this field.",
    "descriptor": "",
    "authors": [
      "Jinhui Ye",
      "Wenxiang Jiao",
      "Xing Wang",
      "Zhaopeng Tu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07054"
  },
  {
    "id": "arXiv:2210.07055",
    "title": "Sparse in Space and Time: Audio-visual Synchronisation with Trainable  Selectors",
    "abstract": "The objective of this paper is audio-visual synchronisation of general videos\n'in the wild'. For such videos, the events that may be harnessed for\nsynchronisation cues may be spatially small and may occur only infrequently\nduring a many seconds-long video clip, i.e. the synchronisation signal is\n'sparse in space and time'. This contrasts with the case of synchronising\nvideos of talking heads, where audio-visual correspondence is dense in both\ntime and space.\nWe make four contributions: (i) in order to handle longer temporal sequences\nrequired for sparse synchronisation signals, we design a multi-modal\ntransformer model that employs 'selectors' to distil the long audio and visual\nstreams into small sequences that are then used to predict the temporal offset\nbetween streams. (ii) We identify artefacts that can arise from the compression\ncodecs used for audio and video and can be used by audio-visual models in\ntraining to artificially solve the synchronisation task. (iii) We curate a\ndataset with only sparse in time and space synchronisation signals; and (iv)\nthe effectiveness of the proposed model is shown on both dense and sparse\ndatasets quantitatively and qualitatively.\nProject page: v-iashin.github.io/SparseSync",
    "descriptor": "\nComments: Accepted as a spotlight presentation for the BMVC 2022. Code: this https URL Project page: this https URL\n",
    "authors": [
      "Vladimir Iashin",
      "Weidi Xie",
      "Esa Rahtu",
      "Andrew Zisserman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.07055"
  },
  {
    "id": "arXiv:2210.07062",
    "title": "Non-Archimedean Welch Bounds and Non-Archimedean Zauner Conjecture",
    "abstract": "Let $\\mathbb{K}$ be a non-Archimedean (complete) valued field satisfying\n\\begin{align*} \\left|\\sum_{j=1}^{n}\\lambda_j^2\\right|=\\max_{1\\leq j \\leq\nn}|\\lambda_j|^2, \\quad \\forall \\lambda_j \\in \\mathbb{K}, 1\\leq j \\leq n,\n\\forall n \\in \\mathbb{N}. \\end{align*} For $d\\in \\mathbb{N}$, let\n$\\mathbb{K}^d$ be the standard $d$-dimensional non-Archimedean Hilbert space.\nLet $m \\in \\mathbb{N}$ and $\\text{Sym}^m(\\mathbb{K}^d)$ be the non-Archimedean\nHilbert space of symmetric m-tensors. We prove the following result. If\n$\\{\\tau_j\\}_{j=1}^n$ is a collection in $\\mathbb{K}^d$ satisfying $\\langle\n\\tau_j, \\tau_j\\rangle =1$ for all $1\\leq j \\leq n$ and the operator\n$\\text{Sym}^m(\\mathbb{K}^d)\\ni x \\mapsto \\sum_{j=1}^n\\langle x, \\tau_j^{\\otimes\nm}\\rangle \\tau_j^{\\otimes m} \\in \\text{Sym}^m(\\mathbb{K}^d)$ is diagonalizable,\nthen \\begin{align} (1) \\quad \\quad \\quad \\max_{1\\leq j,k \\leq n, j \\neq\nk}\\{|n|, |\\langle \\tau_j, \\tau_k\\rangle|^{2m} \\}\\geq \\frac{|n|^2}{\\left|{d+m-1\n\\choose m}\\right| }. \\end{align} We call Inequality (1) as the non-Archimedean\nversion of Welch bounds obtained by Welch [\\textit{IEEE Transactions on\nInformation Theory, 1974}]. We formulate non-Archimedean Zauner conjecture.",
    "descriptor": "\nComments: 9 Pages, 0 Figures\n",
    "authors": [
      "K. Mahesh Krishna"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Functional Analysis (math.FA)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2210.07062"
  },
  {
    "id": "arXiv:2210.07063",
    "title": "Deep Clustering With Consensus Representations",
    "abstract": "The field of deep clustering combines deep learning and clustering to learn\nrepresentations that improve both the learned representation and the\nperformance of the considered clustering method. Most existing deep clustering\nmethods are designed for a single clustering method, e.g., k-means, spectral\nclustering, or Gaussian mixture models, but it is well known that no clustering\nalgorithm works best in all circumstances. Consensus clustering tries to\nalleviate the individual weaknesses of clustering algorithms by building a\nconsensus between members of a clustering ensemble. Currently, there is no deep\nclustering method that can include multiple heterogeneous clustering algorithms\nin an ensemble to update representations and clusterings together. To close\nthis gap, we introduce the idea of a consensus representation that maximizes\nthe agreement between ensemble members. Further, we propose DECCS (Deep\nEmbedded Clustering with Consensus representationS), a deep consensus\nclustering method that learns a consensus representation by enhancing the\nembedded space to such a degree that all ensemble members agree on a common\nclustering result. Our contributions are the following: (1) We introduce the\nidea of learning consensus representations for heterogeneous clusterings, a\nnovel notion to approach consensus clustering. (2) We propose DECCS, the first\ndeep clustering method that jointly improves the representation and clustering\nresults of multiple heterogeneous clustering algorithms. (3) We show in\nexperiments that learning a consensus representation with DECCS is\noutperforming several relevant baselines from deep clustering and consensus\nclustering. Our code can be found at https://gitlab.cs.univie.ac.at/lukas/deccs",
    "descriptor": "\nComments: Accepted by the IEEE International Conference on Data Mining (ICDM) 2022\n",
    "authors": [
      "Lukas Miklautz",
      "Martin Teuffenbach",
      "Pascal Weber",
      "Rona Perjuci",
      "Walid Durani",
      "Christian B\u00f6hm",
      "Claudia Plant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07063"
  },
  {
    "id": "arXiv:2210.07065",
    "title": "Advancing the cybersecurity of the healthcare system with  self-optimising and self-adaptative artificial intelligence (part 2)",
    "abstract": "This article advances the knowledge on teaching and training new artificial\nintelligence algorithms, for securing, preparing, and adapting the healthcare\nsystem to cope with future pandemics. The core objective is to develop a\nconcept healthcare system supported by autonomous artificial intelligence that\ncan use edge health devices with real-time data. The article constructs two\ncase scenarios for applying cybersecurity with autonomous artificial\nintelligence for (1) self-optimising predictive cyber risk analytics of\nfailures in healthcare systems during a Disease X event (i.e., undefined future\npandemic), and (2) self-adaptive forecasting of medical production and supply\nchain bottlenecks during future pandemics. To construct the two testing\nscenarios, the article uses the case of Covid-19 to synthesise data for the\nalgorithms i.e., for optimising and securing digital healthcare systems in\nanticipation of disease X. The testing scenarios are built to tackle the\nlogistical challenges and disruption of complex production and supply chains\nfor vaccine distribution with optimisation algorithms.",
    "descriptor": "",
    "authors": [
      "Petar Radanliev",
      "David De Roure"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07065"
  },
  {
    "id": "arXiv:2210.07067",
    "title": "Nonlinear approximation of high-dimensional anisotropic analytic  functions",
    "abstract": "Motivated by nonlinear approximation results for classes of parametric\npartial differential equations (PDEs), we seek to better understand so-called\nlibrary approximations to analytic functions of countably infinite number of\nvariables. Rather than approximating a function of interest in a single space,\na library approximation uses a collection of spaces and the best space may be\nchosen for any point in the domain. In the setting of this paper, we use a\nspecific library which consists of local Taylor approximations on sufficiently\nsmall rectangular subdomains of the (rescaled) parameter domain,\n$Y:=[-1,1]^\\mathbb{N}$. When the function of interest is the solution of a\ncertain type of parametric PDE, recent results (Bonito et al, 2020,\narXiv:2005.02565) prove an upper bound on the number of spaces required to\nachieve a desired target accuracy. In this work, we prove a similar result for\na more general class of functions with anisotropic analyticity. In this way we\nshow both where the theory developed in (Bonito et al 2020) depends on being in\nthe setting of parametric PDEs with affine diffusion coefficients, and also\nexpand the previous result to include more general types of parametric PDEs.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Diane Guignard",
      "Peter Jantsch"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.07067"
  },
  {
    "id": "arXiv:2210.07071",
    "title": "An Open-World Lottery Ticket for Out-of-Domain Intent Classification",
    "abstract": "Most existing methods of Out-of-Domain (OOD) intent classification, which\nrely on extensive auxiliary OOD corpora or specific training paradigms, are\nunderdeveloped in the underlying principle that the models should have\ndifferentiated confidence in In- and Out-of-domain intent. In this work, we\ndemonstrate that calibrated subnetworks can be uncovered by pruning the\n(poor-calibrated) overparameterized model. Calibrated confidence provided by\nthe subnetwork can better distinguish In- and Out-of-domain. Furthermore, we\ntheoretically bring new insights into why temperature scaling can differentiate\nIn- and Out-of-Domain intent and empirically extend the Lottery Ticket\nHypothesis to the open-world setting. Extensive experiments on three real-world\ndatasets demonstrate our approach can establish consistent improvements\ncompared with a suite of competitive baselines.",
    "descriptor": "",
    "authors": [
      "Yunhua Zhou",
      "Peiju Liu",
      "Yuxin Wang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07071"
  },
  {
    "id": "arXiv:2210.07072",
    "title": "ConvTransSeg: A Multi-resolution Convolution-Transformer Network for  Medical Image Segmentation",
    "abstract": "Convolutional neural networks (CNNs) achieved the state-of-the-art\nperformance in medical image segmentation due to their ability to extract\nhighly complex feature representations. However, it is argued in recent studies\nthat traditional CNNs lack the intelligence to capture long-term dependencies\nof different image regions. Following the success of applying Transformer\nmodels on natural language processing tasks, the medical image segmentation\nfield has also witnessed growing interest in utilizing Transformers, due to\ntheir ability to capture long-range contextual information. However, unlike\nCNNs, Transformers lack the ability to learn local feature representations.\nThus, to fully utilize the advantages of both CNNs and Transformers, we propose\na hybrid encoder-decoder segmentation model (ConvTransSeg). It consists of a\nmulti-layer CNN as the encoder for feature learning and the corresponding\nmulti-level Transformer as the decoder for segmentation prediction. The encoder\nand decoder are interconnected in a multi-resolution manner. We compared our\nmethod with many other state-of-the-art hybrid CNN and Transformer segmentation\nmodels on binary and multiple class image segmentation tasks using several\npublic medical image datasets, including skin lesion, polyp, cell and brain\ntissue. The experimental results show that our method achieves overall the best\nperformance in terms of Dice coefficient and average symmetric surface distance\nmeasures with low model complexity and memory consumption. In contrast to most\nTransformer-based methods that we compared, our method does not require the use\nof pre-trained models to achieve similar or better performance. The code is\nfreely available for research purposes on Github: (the link will be added upon\nacceptance).",
    "descriptor": "\nComments: 12 pages, 5 figures, 4 tables, also submitted to IEEE-TMI\n",
    "authors": [
      "Zhendi Gong",
      "Andrew P. French",
      "Guoping Qiu",
      "Xin Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07072"
  },
  {
    "id": "arXiv:2210.07073",
    "title": "Strong form mesh-free $hp$-adaptive solution of linear elasticity  problem",
    "abstract": "We present an algorithm for $hp$-adaptive collocation-based mesh-free\nnumerical analysis of partial differential equations. Our solution procedure\nfollows a well-established iterative solve-estimate-mark-refine paradigm. The\nsolve phase relies on the Radial Basis Function-generated Finite Differences\n(RBF-FD) using point clouds generated by advancing front node positioning\nalgorithm that supports variable node density. In the estimate phase, we\nintroduce an Implicit-Explicit (IMEX) error indicator, which assumes that the\nerror relates to the difference between the implicitly obtained solution (from\nthe solve phase) and a local explicit re-evaluation of the PDE at hand using a\nhigher order approximation. Based on the IMEX error indicator, the modified\nTexas Three Step marking strategy is used to mark the computational nodes for\n$h$-, $p$- or $hp$-(de-)refinement. Finally, in the refine phase, nodes are\nrepositioned and the order of the method is locally redefined using the\nvariable order of the augmenting monomials according to the instructions from\nthe mark phase.\nThe performance of the introduced $hp$-adaptive method is first investigated\non a two-dimensional Peak problem and further applied to two- and\nthree-dimensional contact problems. We show that the proposed IMEX error\nindicator adequately captures the global behaviour of the error in all cases\nconsidered and that the proposed $hp$-adaptive solution procedure significantly\noutperforms the non-adaptive approach. The proposed $hp$-adaptive method stands\nfor another important step towards a fully autonomous numerical method capable\nof solving complex problems in realistic geometries without the user\nintervention.",
    "descriptor": "\nComments: Preprint submitted to Computer Methods in Applied Mechanics and Engineering\n",
    "authors": [
      "Mitja Jan\u010di\u010d",
      "Gregor Kosec"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.07073"
  },
  {
    "id": "arXiv:2210.07074",
    "title": "CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing",
    "abstract": "A bottleneck to developing Semantic Parsing (SP) models is the need for a\nlarge volume of human-labeled training data. Given the complexity and cost of\nhuman annotation for SP, labeled data is often scarce, particularly in\nmultilingual settings. Large Language Models (LLMs) excel at SP given only a\nfew examples, however LLMs are unsuitable for runtime systems which require low\nlatency. In this work, we propose CLASP, a simple method to improve\nlow-resource SP for moderate-sized models: we generate synthetic data from\nAlexaTM 20B to augment the training set for a model 40x smaller (500M\nparameters). We evaluate on two datasets in low-resource settings: English\nPIZZA, containing either 348 or 16 real examples, and mTOP cross-lingual\nzero-shot, where training data is available only in English, and the model must\ngeneralize to four new languages. On both datasets, we show significant\nimprovements over strong baseline methods.",
    "descriptor": "\nComments: Accepted to AACL-IJCNLP 2022: The 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, November 21-24, 2022. See this https URL\n",
    "authors": [
      "Andy Rosenbaum",
      "Saleh Soltan",
      "Wael Hamza",
      "Amir Saffari",
      "Macro Damonte",
      "Isabel Groves"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07074"
  },
  {
    "id": "arXiv:2210.07076",
    "title": "Few-Shot Visual Question Generation: A Novel Task and Benchmark Datasets",
    "abstract": "Generating natural language questions from visual scenes, known as Visual\nQuestion Generation (VQG), has been explored in the recent past where large\namounts of meticulously labeled data provide the training corpus. However, in\npractice, it is not uncommon to have only a few images with question\nannotations corresponding to a few types of answers. In this paper, we propose\na new and challenging Few-Shot Visual Question Generation (FS-VQG) task and\nprovide a comprehensive benchmark to it. Specifically, we evaluate various\nexisting VQG approaches as well as popular few-shot solutions based on\nmeta-learning and self-supervised strategies for the FS-VQG task. We conduct\nexperiments on two popular existing datasets VQG and Visual7w. In addition, we\nhave also cleaned and extended the VQG dataset for use in a few-shot scenario,\nwith additional image-question pairs as well as additional answer categories.\nWe call this new dataset VQG-23. Several important findings emerge from our\nexperiments, that shed light on the limits of current models in few-shot vision\nand language generation tasks. We find that trivially extending existing VQG\napproaches with transfer learning or meta-learning may not be enough to tackle\nthe inherent challenges in few-shot VQG. We believe that this work will\ncontribute to accelerating the progress in few-shot learning research.",
    "descriptor": "\nComments: submitted to IEEE Transactions on Image Processing\n",
    "authors": [
      "Anurag Roy",
      "David Johnson Ekka",
      "Saptarshi Ghosh",
      "Abir Das"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07076"
  },
  {
    "id": "arXiv:2210.07082",
    "title": "Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data",
    "abstract": "The implicit biases of gradient-based optimization algorithms are conjectured\nto be a major factor in the success of modern deep learning. In this work, we\ninvestigate the implicit bias of gradient flow and gradient descent in\ntwo-layer fully-connected neural networks with leaky ReLU activations when the\ntraining data are nearly-orthogonal, a common property of high-dimensional\ndata. For gradient flow, we leverage recent work on the implicit bias for\nhomogeneous neural networks to show that asymptotically, gradient flow produces\na neural network with rank at most two. Moreover, this network is an\n$\\ell_2$-max-margin solution (in parameter space), and has a linear decision\nboundary that corresponds to an approximate-max-margin linear predictor. For\ngradient descent, provided the random initialization variance is small enough,\nwe show that a single step of gradient descent suffices to drastically reduce\nthe rank of the network, and that the rank remains small throughout training.\nWe provide experiments which suggest that a small initialization scale is\nimportant for finding low-rank neural networks with gradient descent.",
    "descriptor": "\nComments: 54 pages\n",
    "authors": [
      "Spencer Frei",
      "Gal Vardi",
      "Peter L. Bartlett",
      "Nathan Srebro",
      "Wei Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.07082"
  },
  {
    "id": "arXiv:2210.07083",
    "title": "Soundness and Completeness of SPARQL Query Containment Solver SpeCS",
    "abstract": "Tool SPECS implements an efficient automated approach for reasoning about the\nSPARQL query containment problem. In this paper, we prove the correctness of\nthis approach. We give precise semantics of the core subset of SPARQL language.\nWe briefly discuss the procedure used for reducing the query containment\nproblem into a formal logical framework. We prove that such reduction is both\nsound and complete for conjunctive queries, and also for some important cases\nof non-conjunctive queries containing operator union, operator optional, and\nsubqueries. Soundness and completeness proofs are considered in both\ncontainment and subsumption forms.",
    "descriptor": "\nComments: 32 pages, 6 figures\n",
    "authors": [
      "Mirko Spasi\u0107",
      "Milena Vujo\u0161evi\u0107 Jani\u010di\u0107"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.07083"
  },
  {
    "id": "arXiv:2210.07085",
    "title": "Towards Uniform Certification in QBF",
    "abstract": "We pioneer a new technique that allows us to prove a multitude of previously\nopen simulations in QBF proof complexity. In particular, we show that extended\nQBF Frege p-simulates clausal proof systems such as IR-Calculus, IRM-Calculus,\nLong-Distance Q-Resolution, and Merge Resolution. These results are obtained by\ntaking a technique of Beyersdorff et al. (JACM 2020) that turns strategy\nextraction into simulation and combining it with new local strategy extraction\narguments.\nThis approach leads to simulations that are carried out mainly in\npropositional logic, with minimal use of the QBF rules. Our proofs therefore\nprovide a new, largely propositional interpretation of the simulated systems.\nWe argue that these results strengthen the case for uniform certification in\nQBF solving, since many QBF proof systems now fall into place underneath\nextended QBF Frege.",
    "descriptor": "",
    "authors": [
      "Leroy Chew",
      "Friedrich Slivovsky"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.07085"
  },
  {
    "id": "arXiv:2210.07089",
    "title": "Automated Search Bias Models & YouTube Gender Bias Analysis",
    "abstract": "This work first presents our attempts to establish an automated model using\nstate-of-the-art approaches for analysing bias in search results of Bing and\nGoogle. Secondly, in this paper we also aim to analyse YouTube video search\nresults in terms of perceived gender bias, i.e. narrator's gender from the\nviewer's perspective. Experimental results indicate that the current class-wise\nF1-scores of our best model are not sufficient to establish an automated model\nfor bias analysis. Thus, to evaluate YouTube video search results in terms of\nperceived gender bias, we use manual annotations.",
    "descriptor": "",
    "authors": [
      "Gizem Gezici"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.07089"
  },
  {
    "id": "arXiv:2210.07093",
    "title": "Query Expansion Using Contextual Clue Sampling with Language Models",
    "abstract": "Query expansion is an effective approach for mitigating vocabulary mismatch\nbetween queries and documents in information retrieval. One recent line of\nresearch uses language models to generate query-related contexts for expansion.\nAlong this line, we argue that expansion terms from these contexts should\nbalance two key aspects: diversity and relevance. The obvious way to increase\ndiversity is to sample multiple contexts from the language model. However, this\ncomes at the cost of relevance, because there is a well-known tendency of\nmodels to hallucinate incorrect or irrelevant contexts. To balance these two\nconsiderations, we propose a combination of an effective filtering strategy and\nfusion of the retrieved documents based on the generation probability of each\ncontext. Our lexical matching based approach achieves a similar top-5/top-20\nretrieval accuracy and higher top-100 accuracy compared with the\nwell-established dense retrieval model DPR, while reducing the index size by\nmore than 96%. For end-to-end QA, the reader model also benefits from our\nmethod and achieves the highest Exact-Match score against several competitive\nbaselines.",
    "descriptor": "",
    "authors": [
      "Linqing Liu",
      "Minghan Li",
      "Jimmy Lin",
      "Sebastian Riedel",
      "Pontus Stenetorp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07093"
  },
  {
    "id": "arXiv:2210.07095",
    "title": "Incorporating Context into Subword Vocabularies",
    "abstract": "Most current popular subword tokenizers are trained based on word frequency\nstatistics over a corpus, without considering information about co-occurrence\nor context. Nevertheless, the resulting vocabularies are used in language\nmodels' highly contextualized settings. We present SaGe, a tokenizer that\ntailors subwords for their downstream use by baking in the contextualized\nsignal at the vocabulary creation phase. We show that SaGe does a better job\nthan current widespread tokenizers in keeping token contexts cohesive, while\nnot incurring a large price in terms of encoding efficiency or domain\nrobustness. SaGe improves performance on English GLUE classification tasks as\nwell as on NER, and on Inference and NER in Turkish, demonstrating its\nrobustness to language properties such as morphological exponence and\nagglutination.",
    "descriptor": "",
    "authors": [
      "Shaked Yehezkel",
      "Yuval Pinter"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07095"
  },
  {
    "id": "arXiv:2210.07098",
    "title": "Meta-learning Based Short-Term Passenger Flow Prediction for  Newly-Operated Urban Rail Transit Stations",
    "abstract": "Accurate short-term passenger flow prediction in urban rail transit stations\nhas great benefits for reasonably allocating resources, easing congestion, and\nreducing operational risks. However, compared with data-rich stations, the\npassenger flow prediction in newly-operated stations is limited by passenger\nflow data volume, which would reduce the prediction accuracy and increase the\ndifficulty for station management and operation. Hence, how accurately\npredicting passenger flow in newly-operated stations with limited data is an\nurgent problem to be solved. Existing passenger flow prediction approaches\ngenerally depend on sufficient data, which might be unsuitable for\nnewly-operated stations. Therefore, we propose a meta-learning method named\nMeta Long Short-Term Memory Network (Meta-LSTM) to predict the passenger flow\nin newly-operated stations. The Meta-LSTM is to construct a framework that\nincreases the generalization ability of long short-term memory network (LSTM)\nto various passenger flow characteristics by learning passenger flow\ncharacteristics from multiple data-rich stations and then applying the learned\nparameter to data-scarce stations by parameter initialization. The Meta-LSTM is\napplied to the subway network of Nanning, Hangzhou, and Beijing, China. The\nexperiments on three real-world subway networks demonstrate the effectiveness\nof our proposed Meta-LSTM over several competitive baseline models. Results\nalso show that our proposed Meta-LSTM has a good generalization ability to\nvarious passenger flow characteristics, which can provide a reference for\npassenger flow prediction in the stations with limited data.",
    "descriptor": "\nComments: 37 pages, 13 figures, 3 tables\n",
    "authors": [
      "Kuo Han",
      "Jinlei Zhang",
      "Chunqi Zhu",
      "Lixing Yang",
      "Xiaoyu Huang",
      "Songsong Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.07098"
  },
  {
    "id": "arXiv:2210.07100",
    "title": "Dissipative residual layers for unsupervised implicit parameterization  of data manifolds",
    "abstract": "We propose an unsupervised technique for implicit parameterization of data\nmanifolds. In our approach, the data is assumed to belong to a lower\ndimensional manifold in a higher dimensional space, and the data points are\nviewed as the endpoints of the trajectories originating outside the manifold.\nUnder this assumption, the data manifold is an attractive manifold of a\ndynamical system to be estimated. We parameterize such a dynamical system with\na residual neural network and propose a spectral localization technique to\nensure it is locally attractive in the vicinity of data. We also present\ninitialization and additional regularization of the proposed residual layers. %\nthat we call dissipative bottlenecks. We mention the importance of the\nconsidered problem for the tasks of reinforcement learning and support our\ndiscussion with examples demonstrating the performance of the proposed layers\nin denoising and generative tasks.",
    "descriptor": "",
    "authors": [
      "Viktor Reshniak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07100"
  },
  {
    "id": "arXiv:2210.07105",
    "title": "CORL: Research-oriented Deep Offline Reinforcement Learning Library",
    "abstract": "CORL is an open-source library that provides single-file implementations of\nDeep Offline Reinforcement Learning algorithms. It emphasizes a simple\ndeveloping experience with a straightforward codebase and a modern analysis\ntracking tool. In CORL, we isolate methods implementation into distinct single\nfiles, making performance-relevant details easier to recognise. Additionally,\nan experiment tracking feature is available to help log metrics,\nhyperparameters, dependencies, and more to the cloud. Finally, we have ensured\nthe reliability of the implementations by benchmarking a commonly employed D4RL\nbenchmark. The source code can be found https://github.com/tinkoff-ai/CORL",
    "descriptor": "",
    "authors": [
      "Denis Tarasov",
      "Alexander Nikulin",
      "Dmitry Akimov",
      "Vladislav Kurenkov",
      "Sergey Kolesnikov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07105"
  },
  {
    "id": "arXiv:2210.07109",
    "title": "Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence",
    "abstract": "AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem\nto test systems on various language-related capabilities. In this paper, we\nframe D&D specifically as a dialogue system challenge, where the tasks are to\nboth generate the next conversational turn in the game and predict the state of\nthe game given the dialogue history. We create a gameplay dataset consisting of\nnearly 900 games, with a total of 7,000 players, 800,000 dialogue turns,\n500,000 dice rolls, and 58 million words. We automatically annotate the data\nwith partial state information about the game play. We train a large language\nmodel (LM) to generate the next game turn, conditioning it on different\ninformation. The LM can respond as a particular character or as the player who\nruns the game--i.e., the Dungeon Master (DM). It is trained to produce dialogue\nthat is either in-character (roleplaying in the fictional world) or\nout-of-character (discussing rules or strategy). We perform a human evaluation\nto determine what factors make the generated output plausible and interesting.\nWe further perform an automatic evaluation to determine how well the model can\npredict the game state given the history and examine how well tracking the game\nstate improves its ability to produce plausible conversational output.",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Chris Callison-Burch",
      "Gaurav Singh Tomar",
      "Lara J. Martin",
      "Daphne Ippolito",
      "Suma Bailis",
      "David Reitter"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07109"
  },
  {
    "id": "arXiv:2210.07110",
    "title": "POSE: Practical Off-chain Smart Contract Execution",
    "abstract": "Smart contracts enable users to execute payments depending on complex program\nlogic. Ethereum is the most notable example of a blockchain that supports smart\ncontracts leveraged for countless applications including games, auctions and\nfinancial products. Unfortunately, the traditional method of running contract\ncode on-chain is very expensive, for instance, on the Ethereum platform, fees\nhave dramatically increased, rendering the system unsuitable for complex\napplications. A prominent solution to address this problem is to execute code\noff-chain and only use the blockchain as a trust anchor. While there has been\nsignificant progress in developing off-chain systems over the last years,\ncurrent off-chain solutions suffer from various drawbacks including costly\nblockchain interactions, lack of data privacy, huge capital costs from locked\ncollateral, or supporting only a restricted set of applications.\nIn this paper, we present POSE -- a practical off-chain protocol for smart\ncontracts that addresses the aforementioned shortcomings of existing solutions.\nPOSE leverages a pool of Trusted Execution Environments (TEEs) to execute the\ncomputation efficiently and to swiftly recover from accidental or malicious\nfailures. We show that POSE provides strong security guarantees even if a large\nsubset of parties is corrupted. We evaluate our proof-of-concept implementation\nwith respect to its efficiency and effectiveness.",
    "descriptor": "",
    "authors": [
      "Tommaso Frassetto",
      "Patrick Jauernig",
      "David Koisser",
      "David Kretzler",
      "Benjamin Schlosser",
      "Sebastian Faust",
      "Ahmad-Reza Sadeghi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.07110"
  },
  {
    "id": "arXiv:2210.07111",
    "title": "A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained  Models",
    "abstract": "Recent work on tokenizer-free multilingual pretrained models show promising\nresults in improving cross-lingual transfer and reducing engineering overhead\n(Clark et al., 2022; Xue et al., 2022). However, these works mainly focus on\nreporting accuracy on a limited set of tasks and data settings, placing less\nemphasis on other important factors when tuning and deploying the models in\npractice, such as memory usage, inference speed, and fine-tuning data\nrobustness. We attempt to fill this gap by performing a comprehensive empirical\ncomparison of multilingual tokenizer-free and subword-based models considering\nthese various dimensions. Surprisingly, we find that subword-based models might\nstill be the most practical choice in many settings, achieving better\nperformance for lower inference latency and memory usage. Based on these\nresults, we encourage future work in tokenizer-free methods to consider these\nfactors when designing and evaluating new models.",
    "descriptor": "",
    "authors": [
      "Jimin Sun",
      "Patrick Fernandes",
      "Xinyi Wang",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07111"
  },
  {
    "id": "arXiv:2210.07113",
    "title": "Towards End-to-End Open Conversational Machine Reading",
    "abstract": "In open-retrieval conversational machine reading (OR-CMR) task, machines are\nrequired to do multi-turn question answering given dialogue history and a\ntextual knowledge base. Existing works generally utilize two independent\nmodules to approach this problem's two successive sub-tasks: first with a\nhard-label decision making and second with a question generation aided by\nvarious entailment reasoning methods. Such usual cascaded modeling is\nvulnerable to error propagation and prevents the two sub-tasks from being\nconsistently optimized. In this work, we instead model OR-CMR as a unified\ntext-to-text task in a fully end-to-end style. Experiments on the OR-ShARC\ndataset show the effectiveness of our proposed end-to-end framework on both\nsub-tasks by a large margin, achieving new state-of-the-art results. Further\nablation studies support that our framework can generalize to different\nbackbone models.",
    "descriptor": "\nComments: 10 pages, 2 figures, 10 tables\n",
    "authors": [
      "Sizhe Zhou",
      "Siru Ouyang",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07113"
  },
  {
    "id": "arXiv:2210.07117",
    "title": "Graph-based Neural Modules to Inspect Attention-based Architectures: A  Position Paper",
    "abstract": "Encoder-decoder architectures are prominent building blocks of\nstate-of-the-art solutions for tasks across multiple fields where deep learning\n(DL) or foundation models play a key role. Although there is a growing\ncommunity working on the provision of interpretation for DL models as well as\nconsiderable work in the neuro-symbolic community seeking to integrate symbolic\nrepresentations and DL, many open questions remain around the need for better\ntools for visualization of the inner workings of DL architectures. In\nparticular, encoder-decoder models offer an exciting opportunity for\nvisualization and editing by humans of the knowledge implicitly represented in\nmodel weights. In this work, we explore ways to create an abstraction for\nsegments of the network as a two-way graph-based representation. Changes to\nthis graph structure should be reflected directly in the underlying tensor\nrepresentations. Such two-way graph representation enables new neuro-symbolic\nsystems by leveraging the pattern recognition capabilities of the\nencoder-decoder along with symbolic reasoning carried out on the graphs. The\napproach is expected to produce new ways of interacting with DL models but also\nto improve performance as a result of the combination of learning and reasoning\ncapabilities.",
    "descriptor": "",
    "authors": [
      "Breno W. Carvalho",
      "Artur D'Avilla Garcez",
      "Luis C. Lamb"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07117"
  },
  {
    "id": "arXiv:2210.07122",
    "title": "Deep Idempotent Network for Efficient Single Image Blind Deblurring",
    "abstract": "Single image blind deblurring is highly ill-posed as neither the latent sharp\nimage nor the blur kernel is known. Even though considerable progress has been\nmade, several major difficulties remain for blind deblurring, including the\ntrade-off between high-performance deblurring and real-time processing.\nBesides, we observe that current single image blind deblurring networks cannot\nfurther improve or stabilize the performance but significantly degrades the\nperformance when re-deblurring is repeatedly applied. This implies the\nlimitation of these networks in modeling an ideal deblurring process. In this\nwork, we make two contributions to tackle the above difficulties: (1) We\nintroduce the idempotent constraint into the deblurring framework and present a\ndeep idempotent network to achieve improved blind non-uniform deblurring\nperformance with stable re-deblurring. (2) We propose a simple yet efficient\ndeblurring network with lightweight encoder-decoder units and a recurrent\nstructure that can deblur images in a progressive residual fashion. Extensive\nexperiments on synthetic and realistic datasets prove the superiority of our\nproposed framework. Remarkably, our proposed network is nearly 6.5X smaller and\n6.4X faster than the state-of-the-art while achieving comparable high\nperformance.",
    "descriptor": "\nComments: The first two authors contributed equally, accepted by IEEE TCSVT(this https URL)\n",
    "authors": [
      "Yuxin Mao",
      "Zhexiong Wan",
      "Yuchao Dai",
      "Xin Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07122"
  },
  {
    "id": "arXiv:2210.07124",
    "title": "RTFormer: Efficient Design for Real-Time Semantic Segmentation with  Transformer",
    "abstract": "Recently, transformer-based networks have shown impressive results in\nsemantic segmentation. Yet for real-time semantic segmentation, pure CNN-based\napproaches still dominate in this field, due to the time-consuming computation\nmechanism of transformer. We propose RTFormer, an efficient dual-resolution\ntransformer for real-time semantic segmenation, which achieves better trade-off\nbetween performance and efficiency than CNN-based models. To achieve high\ninference efficiency on GPU-like devices, our RTFormer leverages GPU-Friendly\nAttention with linear complexity and discards the multi-head mechanism.\nBesides, we find that cross-resolution attention is more efficient to gather\nglobal context information for high-resolution branch by spreading the high\nlevel knowledge learned from low-resolution branch. Extensive experiments on\nmainstream benchmarks demonstrate the effectiveness of our proposed RTFormer,\nit achieves state-of-the-art on Cityscapes, CamVid and COCOStuff, and shows\npromising results on ADE20K. Code is available at PaddleSeg:\nhttps://github.com/PaddlePaddle/PaddleSeg.",
    "descriptor": "\nComments: NeurIPS2022\n",
    "authors": [
      "Jian Wang",
      "Chenhui Gou",
      "Qiman Wu",
      "Haocheng Feng",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07124"
  },
  {
    "id": "arXiv:2210.07126",
    "title": "How (Not) To Evaluate Explanation Quality",
    "abstract": "The importance of explainability is increasingly acknowledged in natural\nlanguage processing. However, it is still unclear how the quality of\nexplanations can be assessed effectively. The predominant approach is to\ncompare proxy scores (such as BLEU or explanation F1) evaluated against gold\nexplanations in the dataset. The assumption is that an increase of the proxy\nscore implies a higher utility of explanations to users. In this paper, we\nquestion this assumption. In particular, we (i) formulate desired\ncharacteristics of explanation quality that apply across tasks and domains,\n(ii) point out how current evaluation practices violate those characteristics,\nand (iii) propose actionable guidelines to overcome obstacles that limit\ntoday's evaluation of explanation quality and to enable the development of\nexplainable systems that provide tangible benefits for human users. We\nsubstantiate our theoretical claims (i.e., the lack of validity and temporal\ndecline of currently-used proxy scores) with empirical evidence from a\ncrowdsourcing case study in which we investigate the explanation quality of\nstate-of-the-art explainable question answering systems.",
    "descriptor": "",
    "authors": [
      "Hendrik Schuff",
      "Heike Adel",
      "Peng Qi",
      "Ngoc Thang Vu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.07126"
  },
  {
    "id": "arXiv:2210.07128",
    "title": "Language Models of Code are Few-Shot Commonsense Learners",
    "abstract": "We address the general task of structured commonsense reasoning: given a\nnatural language input, the goal is to generate a graph such as an event -- or\na reasoning-graph. To employ large language models (LMs) for this task,\nexisting approaches ``serialize'' the output graph as a flat list of nodes and\nedges. Although feasible, these serialized graphs strongly deviate from the\nnatural language corpora that LMs were pre-trained on, hindering LMs from\ngenerating them correctly. In this paper, we show that when we instead frame\nstructured commonsense reasoning tasks as code generation tasks, pre-trained\nLMs of code are better structured commonsense reasoners than LMs of natural\nlanguage, even when the downstream task does not involve source code at all. We\ndemonstrate our approach across three diverse structured commonsense reasoning\ntasks. In all these natural language tasks, we show that using our approach, a\ncode generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the\ntarget task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot\nsetting.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Aman Madaan",
      "Shuyan Zhou",
      "Uri Alon",
      "Yiming Yang",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07128"
  },
  {
    "id": "arXiv:2210.07134",
    "title": "The Hermite-Taylor Correction Function Method for Maxwell's Equations",
    "abstract": "The Hermite-Taylor method, introduced in 2005 by Goodrich, Hagstrom and\nLorenz, is highly efficient and accurate when applied to linear hyperbolic\nsystems on periodic domains. Unfortunately its widespread use has been\nprevented by the lack of a systematic approach to implementing boundary\nconditions. In this paper we present the Hermite-Taylor Correction Function\nmethod, which provides exactly such a systematic approach for handing boundary\nconditions. Here we focus on Maxwell's equations but note that the method is\neasily extended to other hyperbolic problems.",
    "descriptor": "",
    "authors": [
      "Yann-Meing Law",
      "Daniel Appel\u00f6"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.07134"
  },
  {
    "id": "arXiv:2210.07135",
    "title": "You Can Have Your Data and Balance It Too: Towards Balanced and  Efficient Multilingual Models",
    "abstract": "Multilingual models have been widely used for cross-lingual transfer to\nlow-resource languages. However, the performance on these languages is hindered\nby their underrepresentation in the pretraining data. To alleviate this\nproblem, we propose a novel multilingual training technique based on\nteacher-student knowledge distillation. In this setting, we utilize monolingual\nteacher models optimized for their language. We use those teachers along with\nbalanced (sub-sampled) data to distill the teachers' knowledge into a single\nmultilingual student. Our method outperforms standard training methods in\nlow-resource languages and retrains performance on high-resource languages\nwhile using the same amount of data. If applied widely, our approach can\nincrease the representation of low-resource languages in NLP systems.",
    "descriptor": "",
    "authors": [
      "Tomasz Limisiewicz",
      "Dan Malkin",
      "Gabriel Stanovsky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07135"
  },
  {
    "id": "arXiv:2210.07138",
    "title": "Counterfactual Multihop QA: A Cause-Effect Approach for Reducing  Disconnected Reasoning",
    "abstract": "Multi-hop QA requires reasoning over multiple supporting facts to answer the\nquestion. However, the existing QA models always rely on shortcuts, e.g.,\nproviding the true answer by only one fact, rather than multi-hop reasoning,\nwhich is referred as $\\textit{disconnected reasoning}$ problem. To alleviate\nthis issue, we propose a novel counterfactual multihop QA, a causal-effect\napproach that enables to reduce the disconnected reasoning. It builds upon\nexplicitly modeling of causality: 1) the direct causal effects of disconnected\nreasoning and 2) the causal effect of true multi-hop reasoning from the total\ncausal effect. With the causal graph, a counterfactual inference is proposed to\ndisentangle the disconnected reasoning from the total causal effect, which\nprovides us a new perspective and technology to learn a QA model that exploits\nthe true multi-hop reasoning instead of shortcuts. Extensive experiments have\nconducted on the benchmark HotpotQA dataset, which demonstrate that the\nproposed method can achieve notable improvement on reducing disconnected\nreasoning. For example, our method achieves 5.8% higher points of its Supp$_s$\nscore on HotpotQA through true multihop reasoning. The code is available at\nsupplementary material.",
    "descriptor": "\nComments: 10 pages, 2 figures\n",
    "authors": [
      "Wangzhen Guo",
      "Qinkang Gong",
      "Hanjiang Lai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07138"
  },
  {
    "id": "arXiv:2210.07140",
    "title": "U-HRNet: Delving into Improving Semantic Representation of High  Resolution Network for Dense Prediction",
    "abstract": "High resolution and advanced semantic representation are both vital for dense\nprediction. Empirically, low-resolution feature maps often achieve stronger\nsemantic representation, and high-resolution feature maps generally can better\nidentify local features such as edges, but contains weaker semantic\ninformation. Existing state-of-the-art frameworks such as HRNet has kept\nlow-resolution and high-resolution feature maps in parallel, and repeatedly\nexchange the information across different resolutions. However, we believe that\nthe lowest-resolution feature map often contains the strongest semantic\ninformation, and it is necessary to go through more layers to merge with\nhigh-resolution feature maps, while for high-resolution feature maps, the\ncomputational cost of each convolutional layer is very large, and there is no\nneed to go through so many layers. Therefore, we designed a U-shaped\nHigh-Resolution Network (U-HRNet), which adds more stages after the feature map\nwith strongest semantic representation and relaxes the constraint in HRNet that\nall resolutions need to be calculated parallel for a newly added stage. More\ncalculations are allocated to low-resolution feature maps, which significantly\nimproves the overall semantic representation. U-HRNet is a substitute for the\nHRNet backbone and can achieve significant improvement on multiple semantic\nsegmentation and depth prediction datasets, under the exactly same training and\ninference setting, with almost no increasing in the amount of calculation. Code\nis available at PaddleSeg: https://github.com/PaddlePaddle/PaddleSeg.",
    "descriptor": "\nComments: TechReport\n",
    "authors": [
      "Jian Wang",
      "Xiang Long",
      "Guowei Chen",
      "Zewu Wu",
      "Zeyu Chen",
      "Errui Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07140"
  },
  {
    "id": "arXiv:2210.07141",
    "title": "Scalable Multi-robot Motion Planning for Congested Environments Using  Topological Guidance",
    "abstract": "Multi-robot motion planning (MRMP) is the problem of finding collision-free\npaths for a set of robots in a continuous state space. The difficulty of MRMP\nincreases with the number of robots due to the increased potential for\ncollisions between robots. This problem is exacerbated in environments with\nnarrow passages that robots must pass through, like warehouses. In single-robot\nsettings, topology-guided motion planning methods have shown increased\nperformance in these constricted environments. We adapt an existing\ntopology-guided single-robot motion planning method to the multi-robot domain,\nintroducing topological guidance for the composite space. We demonstrate our\nmethod's ability to efficiently plan paths in complex environments with many\nnarrow passages, scaling to robot teams of size up to five times larger than\nexisting methods in this class of problems. By leveraging knowledge of the\ntopology of the environment, we also find higher quality solutions than other\nmethods.",
    "descriptor": "\nComments: This work has been submitted for review\n",
    "authors": [
      "Courtney McBeth",
      "James Motes",
      "Diane Uwacu",
      "Marco Morales",
      "Nancy M. Amato"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.07141"
  },
  {
    "id": "arXiv:2210.07142",
    "title": "Stability analysis of optimal control problems with time-dependent costs",
    "abstract": "We present stability conditions for deterministic time-varying nonlinear\ndiscrete-time systems whose inputs aim to minimize an infinite-horizon\ntime-dependent cost. Global asymptotic and exponential stability properties for\ngeneral attractors are established. This work covers and generalizes the\nrelated results on discounted optimal control problems to more general systems\nand cost functions.",
    "descriptor": "",
    "authors": [
      "Sifeddine Benahmed",
      "Romain Postoyan",
      "Mathieu Granzotto",
      "Lucian Bu\u015foniu",
      "Jamal Daafouz",
      "Dragan Ne\u0161i\u0107"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.07142"
  },
  {
    "id": "arXiv:2210.07143",
    "title": "Performance Evaluation of Query Plan Recommendation with Apache Hadoop  and Apache Spark",
    "abstract": "Access plan recommendation is a query optimization approach that executes new\nqueries using prior created query execution plans (QEPs). The query optimizer\ndivides the query space into clusters in the mentioned method. However,\ntraditional clustering algorithms take a significant amount of execution time\nfor clustering such large datasets. The MapReduce distributed computing model\nprovides efficient solutions for storing and processing vast quantities of\ndata. Apache Spark and Apache Hadoop frameworks are used in the present\ninvestigation to cluster different sizes of query datasets in the\nMapReduce-based access plan recommendation method. The performance evaluation\nis performed based on execution time. The results of the experiments\ndemonstrated the effectiveness of parallel query clustering in achieving high\nscalability. Furthermore, Apache Spark achieved better performance than Apache\nHadoop, reaching an average speedup of 2x.",
    "descriptor": "\nComments: 11pages, 4 figures\n",
    "authors": [
      "Elham Azhir",
      "Mehdi Hosseinzadeh",
      "Faheem Khan",
      "Amir Mosavi"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07143"
  },
  {
    "id": "arXiv:2210.07146",
    "title": "Efficient Algorithms for Obnoxious Facility Location on a Line Segment  or Circle",
    "abstract": "We study different restricted variations of the obnoxious facility location\nproblem on a plane. The first is the constrained obnoxious facility location on\na line segment (COFL-Line) problem. We provide an efficient algorithm for this\nproblem that executes in $O(n ^ 2 \\log k + n \\log k \\log (n^2 + k))$ time. Our\nresult improves on the best known result of $O((nk)^2 \\log(nk) + (n + k) \\log\n(nk))$ time obtained by Singireddy and Basappa\\cite{singireddy2022dispersing}.\nWe also study the same problem where the facilities must be placed on a given\ncircle (the constrained obnoxious facility location on a circle (COFL-Circ)\nproblem). We provide an efficient algorithm for this problem that executes in\n$O(n ^ 2 \\log k + n \\log k \\log (n^2 + k))$ time. Our result improves on the\nbest known result of $O((nk)^2 \\log(nk) + (n + k) \\log (nk))$ time obtained by\nSingireddy and Basappa\\cite{singireddy2022dispersing}. The third problem we\nstudy is the min-sum obnoxious facility location (MOFL) problem.We provide an\nefficient algorithm that executes in $O(nk\\cdot \\alpha(nk) \\log^3 {nk})$ time,\nwhere $\\alpha(.)$ is the inverse Ackermann function. The best known previous\nresult is an $O(n^3k)$ time obtained by Singireddy and\nBasappa\\cite{singireddy2022dispersing}.",
    "descriptor": "",
    "authors": [
      "Bowei Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.07146"
  },
  {
    "id": "arXiv:2210.07147",
    "title": "Global Explainability of GNNs via Logic Combination of Learned Concepts",
    "abstract": "While instance-level explanation of GNN is a well-studied problem with plenty\nof approaches being developed, providing a global explanation for the behaviour\nof a GNN is much less explored, despite its potential in interpretability and\ndebugging. Existing solutions either simply list local explanations for a given\nclass, or generate a synthetic prototypical graph with maximal score for a\ngiven class, completely missing any combinatorial aspect that the GNN could\nhave learned. In this work, we propose GLGExplainer (Global Logic-based GNN\nExplainer), the first Global Explainer capable of generating explanations as\narbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a\nfully differentiable architecture that takes local explanations as inputs and\ncombines them into a logic formula over graphical concepts, represented as\nclusters of local explanations. Contrary to existing solutions, GLGExplainer\nprovides accurate and human-interpretable global explanations that are\nperfectly aligned with ground-truth explanations (on synthetic data) or match\nexisting domain knowledge (on real-world data). Extracted formulas are faithful\nto the model predictions, to the point of providing insights into some\noccasionally incorrect rules learned by the model, making GLGExplainer a\npromising diagnostic tool for learned GNNs.",
    "descriptor": "",
    "authors": [
      "Steve Azzolin",
      "Antonio Longa",
      "Pietro Barbiero",
      "Pietro Li\u00f2",
      "Andrea Passerini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.07147"
  },
  {
    "id": "arXiv:2210.07158",
    "title": "HSurf-Net: Normal Estimation for 3D Point Clouds by Learning Hyper  Surfaces",
    "abstract": "We propose a novel normal estimation method called HSurf-Net, which can\naccurately predict normals from point clouds with noise and density variations.\nPrevious methods focus on learning point weights to fit neighborhoods into a\ngeometric surface approximated by a polynomial function with a predefined\norder, based on which normals are estimated. However, fitting surfaces\nexplicitly from raw point clouds suffers from overfitting or underfitting\nissues caused by inappropriate polynomial orders and outliers, which\nsignificantly limits the performance of existing methods. To address these\nissues, we introduce hyper surface fitting to implicitly learn hyper surfaces,\nwhich are represented by multi-layer perceptron (MLP) layers that take point\nfeatures as input and output surface patterns in a high dimensional feature\nspace. We introduce a novel space transformation module, which consists of a\nsequence of local aggregation layers and global shift layers, to learn an\noptimal feature space, and a relative position encoding module to effectively\nconvert point clouds into the learned feature space. Our model learns hyper\nsurfaces from the noise-less features and directly predicts normal vectors. We\njointly optimize the MLP weights and module parameters in a data-driven manner\nto make the model adaptively find the most suitable surface pattern for various\npoints. Experimental results show that our HSurf-Net achieves the\nstate-of-the-art performance on the synthetic shape dataset, the real-world\nindoor and outdoor scene datasets. The code, data and pretrained models are\npublicly available.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Qing Li",
      "Yu-Shen Liu",
      "Jin-San Cheng",
      "Cheng Wang",
      "Yi Fang",
      "Zhizhong Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07158"
  },
  {
    "id": "arXiv:2210.07161",
    "title": "A Logic of \"Black Box\" Classifier Systems",
    "abstract": "Binary classifiers are traditionally studied by propositional logic (PL). PL\ncan only represent them as white boxes, under the assumption that the\nunderlying Boolean function is fully known. Binary classifiers used in\npractical applications and trained by machine learning are however opaque. They\nare usually described as black boxes. In this paper, we provide a product modal\nlogic called PLC (Product modal Logic for binary input Classifier) in which the\nnotion of \"black box\" is interpreted as the uncertainty over a set of\nclassifiers. We give results about axiomatics and complexity of satisfiability\nchecking for our logic. Moreover, we present a dynamic extension in which the\nprocess of acquiring new information about the actual classifier can be\nrepresented.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Xinghan Liu",
      "Emiliano Lorini"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.07161"
  },
  {
    "id": "arXiv:2210.07171",
    "title": "SQuAT: Sharpness- and Quantization-Aware Training for BERT",
    "abstract": "Quantization is an effective technique to reduce memory footprint, inference\nlatency, and power consumption of deep learning models. However, existing\nquantization methods suffer from accuracy degradation compared to\nfull-precision (FP) models due to the errors introduced by coarse gradient\nestimation through non-differentiable quantization layers. The existence of\nsharp local minima in the loss landscapes of overparameterized models (e.g.,\nTransformers) tends to aggravate such performance penalty in low-bit (2, 4\nbits) settings. In this work, we propose sharpness- and quantization-aware\ntraining (SQuAT), which would encourage the model to converge to flatter minima\nwhile performing quantization-aware training. Our proposed method alternates\ntraining between sharpness objective and step-size objective, which could\npotentially let the model learn the most suitable parameter update magnitude to\nreach convergence near-flat minima. Extensive experiments show that our method\ncan consistently outperform state-of-the-art quantized BERT models under 2, 3,\nand 4-bit settings on GLUE benchmarks by 1%, and can sometimes even outperform\nfull precision (32-bit) models. Our experiments on empirical measurement of\nsharpness also suggest that our method would lead to flatter minima compared to\nother quantization methods.",
    "descriptor": "",
    "authors": [
      "Zheng Wang",
      "Juncheng B Li",
      "Shuhui Qu",
      "Florian Metze",
      "Emma Strubell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07171"
  },
  {
    "id": "arXiv:2210.07178",
    "title": "Challenges and Opportunities of Large Transnational Datasets: A Case  Study on European Administrative Crop Data",
    "abstract": "Expansive, informative datasets are vital in providing foundations and\npossibilities for scientific research and development across many fields of\nstudy. Assembly of grand datasets, however, frequently poses difficulty for the\nauthor and stakeholders alike, with a variety of considerations required\nthroughout the collaboration efforts and development lifecycle. In this work,\nwe discuss and analyse the challenges and opportunities we faced throughout the\ncreation of a transnational, European agricultural dataset containing reference\nlabels of cultivated crops. Together, this forms a succinct framework of\nimportant elements one should consider when forging a dataset of their own.",
    "descriptor": "\nComments: for associated GitHub repository, see this https URL\n",
    "authors": [
      "Maja Schneider",
      "Christian Marchington",
      "Marco K\u00f6rner"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "General Literature (cs.GL)"
    ],
    "url": "https://arxiv.org/abs/2210.07178"
  },
  {
    "id": "arXiv:2210.07179",
    "title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for  Vision-Language Few-Shot Prompting",
    "abstract": "Large pre-trained models have proved to be remarkable zero- and\n(prompt-based) few-shot learners in unimodal vision and language tasks. We\npropose MAPL, a simple and parameter-efficient method that reuses frozen\npre-trained unimodal models and leverages their strong generalization\ncapabilities in multimodal vision-language (VL) settings. MAPL learns a\nlightweight mapping between the representation spaces of unimodal models using\naligned image-text data, and can generalize to unseen VL tasks from just a few\nin-context examples. The small number of trainable parameters makes MAPL\neffective at low-data and in-domain learning. Moreover, MAPL's modularity\nenables easy extension to other pre-trained models. Extensive experiments on\nseveral visual question answering and image captioning benchmarks show that\nMAPL achieves superior or competitive performance compared to similar methods\nwhile training orders of magnitude fewer parameters. MAPL can be trained in\njust a few hours using modest computational resources and public datasets. We\nplan to release the code and pre-trained models.",
    "descriptor": "\nComments: 23 pages, 22 figures, 5 tables. Pau Rodriguez and Saba Ahmadi had equal contributions\n",
    "authors": [
      "Oscar Ma\u00f1as",
      "Pau Rodriguez",
      "Saba Ahmadi",
      "Aida Nematzadeh",
      "Yash Goyal",
      "Aishwarya Agrawal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07179"
  },
  {
    "id": "arXiv:2210.07181",
    "title": "Multiplane NeRF-Supervised Disentanglement of Depth and Camera Pose from  Videos",
    "abstract": "We propose to perform self-supervised disentanglement of depth and camera\npose from large-scale videos. We introduce an Autoencoder-based method to\nreconstruct the input video frames for training, without using any ground-truth\nannotations of depth and camera. The model encoders estimate the monocular\ndepth and the camera pose. The decoder then constructs a Multiplane NeRF\nrepresentation based on the depth encoder feature, and renders the input frames\nwith the estimated camera. The learning is supervised by the reconstruction\nerror, based on the assumption that the scene structure does not change in\nshort periods of time in videos. Once the model is learned, it can be applied\nto multiple applications including depth estimation, camera pose estimation,\nand single image novel view synthesis. We show substantial improvements over\nprevious self-supervised approaches on all tasks and even better results than\ncounterparts trained with camera ground-truths in some applications. Our code\nwill be made publicly available. Our project page is:\nhttps://oasisyang.github.io/self-mpinerf .",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Yang Fu",
      "Ishan Misra",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.07181"
  },
  {
    "id": "arXiv:2210.07182",
    "title": "PDEBENCH: An Extensive Benchmark for Scientific Machine Learning",
    "abstract": "Machine learning-based modeling of physical systems has experienced increased\ninterest in recent years. Despite some impressive progress, there is still a\nlack of benchmarks for Scientific ML that are easy to use but still challenging\nand representative of a wide range of problems. We introduce PDEBench, a\nbenchmark suite of time-dependent simulation tasks based on Partial\nDifferential Equations (PDEs). PDEBench comprises both code and data to\nbenchmark the performance of novel machine learning models against both\nclassical numerical simulations and machine learning baselines. Our proposed\nset of benchmark problems contribute the following unique features: (1) A much\nwider range of PDEs compared to existing benchmarks, ranging from relatively\ncommon examples to more realistic and difficult problems; (2) much larger\nready-to-use datasets compared to prior work, comprising multiple simulation\nruns across a larger number of initial and boundary conditions and PDE\nparameters; (3) more extensible source codes with user-friendly APIs for data\ngeneration and baseline results with popular machine learning models (FNO,\nU-Net, PINN, Gradient-Based Inverse Method). PDEBench allows researchers to\nextend the benchmark freely for their own purposes using a standardized API and\nto compare the performance of new models to existing baseline methods. We also\npropose new evaluation metrics with the aim to provide a more holistic\nunderstanding of learning methods in the context of Scientific ML. With those\nmetrics we identify tasks which are challenging for recent ML methods and\npropose these tasks as future challenges for the community. The code is\navailable at https://github.com/pdebench/PDEBench.",
    "descriptor": "\nComments: 10 pages, accepted for publication in NeurIPS 2022 Track Datasets and Benchmarks\n",
    "authors": [
      "Makoto Takamoto",
      "Timothy Praditia",
      "Raphael Leiteritz",
      "Dan MacKinlay",
      "Francesco Alesiani",
      "Dirk Pfl\u00fcger",
      "Mathias Niepert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Fluid Dynamics (physics.flu-dyn)",
      "Geophysics (physics.geo-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.07182"
  },
  {
    "id": "arXiv:2210.07183",
    "title": "Visual Classification via Description from Large Language Models",
    "abstract": "Vision-language models (VLMs) such as CLIP have shown promising performance\non a variety of recognition tasks using the standard zero-shot classification\nprocedure -- computing similarity between the query image and the embedded\nwords for each category. By only using the category name, they neglect to make\nuse of the rich context of additional information that language affords. The\nprocedure gives no intermediate understanding of why a category is chosen, and\nfurthermore provides no mechanism for adjusting the criteria used towards this\ndecision. We present an alternative framework for classification with VLMs,\nwhich we call classification by description. We ask VLMs to check for\ndescriptive features rather than broad categories: to find a tiger, look for\nits stripes; its claws; and more. By basing decisions on these descriptors, we\ncan provide additional cues that encourage using the features we want to be\nused. In the process, we can get a clear idea of what features the model uses\nto construct its decision; it gains some level of inherent explainability. We\nquery large language models (e.g., GPT-3) for these descriptors to obtain them\nin a scalable way. Extensive experiments show our framework has numerous\nadvantages past interpretability. We show improvements in accuracy on ImageNet\nacross distribution shifts; demonstrate the ability to adapt VLMs to recognize\nconcepts unseen during training; and illustrate how descriptors can be edited\nto effectively mitigate bias compared to the baseline.",
    "descriptor": "",
    "authors": [
      "Sachit Menon",
      "Carl Vondrick"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07183"
  },
  {
    "id": "arXiv:2210.07184",
    "title": "Towards Multi-Agent Reinforcement Learning driven Over-The-Counter  Market Simulations",
    "abstract": "We study a game between liquidity provider and liquidity taker agents\ninteracting in an over-the-counter market, for which the typical example is\nforeign exchange. We show how a suitable design of parameterized families of\nreward functions coupled with associated shared policy learning constitutes an\nefficient solution to this problem. Precisely, we show that our\ndeep-reinforcement-learning-driven agents learn emergent behaviors relative to\na wide spectrum of incentives encompassing profit-and-loss, optimal execution\nand market share, by playing against each other. In particular, we find that\nliquidity providers naturally learn to balance hedging and skewing as a\nfunction of their incentives, where the latter refers to setting their buy and\nsell prices asymmetrically as a function of their inventory. We further\nintroduce a novel RL-based calibration algorithm which we found performed well\nat imposing constraints on the game equilibrium, both on toy and real market\ndata.",
    "descriptor": "",
    "authors": [
      "Nelson Vadori",
      "Leo Ardon",
      "Sumitra Ganesh",
      "Thomas Spooner",
      "Selim Amrouni",
      "Jared Vann",
      "Mengda Xu",
      "Zeyu Zheng",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Computational Finance (q-fin.CP)"
    ],
    "url": "https://arxiv.org/abs/2210.07184"
  },
  {
    "id": "arXiv:2210.07185",
    "title": "On the Utility of Self-supervised Models for Prosody-related Tasks",
    "abstract": "Self-Supervised Learning (SSL) from speech data has produced models that have\nachieved remarkable performance in many tasks, and that are known to implicitly\nrepresent many aspects of information latently present in speech signals.\nHowever, relatively little is known about the suitability of such models for\nprosody-related tasks or the extent to which they encode prosodic information.\nWe present a new evaluation framework, SUPERB-prosody, consisting of three\nprosody-related downstream tasks and two pseudo tasks. We find that 13 of the\n15 SSL models outperformed the baseline on all the prosody-related tasks. We\nalso show good performance on two pseudo tasks: prosody reconstruction and\nfuture prosody prediction. We further analyze the layerwise contributions of\nthe SSL models. Overall we conclude that SSL speech models are highly effective\nfor prosody-related tasks.",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Guan-Ting Lin",
      "Chi-Luen Feng",
      "Wei-Ping Huang",
      "Yuan Tseng",
      "Tzu-Han Lin",
      "Chen-An Li",
      "Hung-yi Lee",
      "Nigel G. Ward"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.07185"
  },
  {
    "id": "arXiv:2210.07187",
    "title": "Exporting Geography Into A Virtual Landscape: A Global Pandemic Locally  Discussed",
    "abstract": "The COVID-19 pandemic has been a global health crisis playing out in the age\nof social media. Even though the virtual environment makes interaction possible\nregardless of physical location, many of the most pressing issues during the\npandemic -- case counts, lockdown policies, vaccine availability -- have played\nout in an intensely local fashion. Reflecting this locality, many of the online\nCOVID communities that formed have been closely tied to physical location, at\ndifferent spatial scales ranging from cities to countries to entire global\nplatforms. This provides an opportunity to study how the real-world geography\nof the pandemic translates into a virtual landscape. By analyzing almost 300\ngeographically-linked COVID discussion communities on Reddit, we show how these\ndiscussions were organized geographically and temporally in three aspects: what\nwere people talking about, who were they talking about it with, and how did\nthey self-organize these conversations?",
    "descriptor": "",
    "authors": [
      "Katherine Van Koevering",
      "Yiquan Hong",
      "Jon Kleinberg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.07187"
  },
  {
    "id": "arXiv:2210.07188",
    "title": "ezCoref: Towards Unifying Annotation Guidelines for Coreference  Resolution",
    "abstract": "Large-scale, high-quality corpora are critical for advancing research in\ncoreference resolution. However, existing datasets vary in their definition of\ncoreferences and have been collected via complex and lengthy guidelines that\nare curated for linguistic experts. These concerns have sparked a growing\ninterest among researchers to curate a unified set of guidelines suitable for\nannotators with various backgrounds. In this work, we develop a\ncrowdsourcing-friendly coreference annotation methodology, ezCoref, consisting\nof an annotation tool and an interactive tutorial. We use ezCoref to\nre-annotate 240 passages from seven existing English coreference datasets\n(spanning fiction, news, and multiple other domains) while teaching annotators\nonly cases that are treated similarly across these datasets. Surprisingly, we\nfind that reasonable quality annotations were already achievable (>90%\nagreement between the crowd and expert annotations) even without extensive\ntraining. On carefully analyzing the remaining disagreements, we identify the\npresence of linguistic cases that our annotators unanimously agree upon but\nlack unified treatments (e.g., generic pronouns, appositives) in existing\ndatasets. We propose the research community should revisit these phenomena when\ncurating future unified annotation guidelines.",
    "descriptor": "\nComments: preprint (19 pages), code in this https URL\n",
    "authors": [
      "Ankita Gupta",
      "Marzena Karpinska",
      "Wenlong Zhao",
      "Kalpesh Krishna",
      "Jack Merullo",
      "Luke Yeh",
      "Mohit Iyyer",
      "Brendan O'Connor"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07188"
  },
  {
    "id": "arXiv:2210.07189",
    "title": "On Compressing Sequences for Self-Supervised Speech Models",
    "abstract": "Compressing self-supervised models has become increasingly necessary, as\nself-supervised models become larger. While previous approaches have primarily\nfocused on compressing the model size, shortening sequences is also effective\nin reducing the computational cost. In this work, we study fixed-length and\nvariable-length subsampling along the time axis in self-supervised learning. We\nexplore how individual downstream tasks are sensitive to input frame rates.\nSubsampling while training self-supervised models not only improves the overall\nperformance on downstream tasks under certain frame rates, but also brings\nsignificant speed-up in inference. Variable-length subsampling performs\nparticularly well under low frame rates. In addition, if we have access to\nphonetic boundaries, we find no degradation in performance for an average frame\nrate as low as 10 Hz.",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Yen Meng",
      "Hsuan-Jui Chen",
      "Jiatong Shi",
      "Shinji Watanabe",
      "Paola Garcia",
      "Hung-yi Lee",
      "Hao Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.07189"
  },
  {
    "id": "arXiv:2210.07190",
    "title": "Chance-Constrained Motion Planning with Event-Triggered Estimation",
    "abstract": "We consider the problem of autonomous navigation using limited information\nfrom a remote sensor network. Because the remote sensors are power and\nbandwidth limited, we use event-triggered (ET) estimation to manage\ncommunication costs. We introduce a fast and efficient sampling-based planner\nwhich computes motion plans coupled with ET communication strategies that\nminimize communication costs, while satisfying constraints on the probability\nof reaching the goal region and the point-wise probability of collision. We\nderive a novel method for offline propagation of the expected state\ndistribution, and corresponding bounds on this distribution. These bounds are\nused to evaluate the chance constraints in the algorithm. Case studies\nestablish the validity of our approach, demonstrating fast computation of\noptimal plans.",
    "descriptor": "\nComments: 8 pages, submitted to IEEE International Conference on Robotics and Automation (ICRA), 2023\n",
    "authors": [
      "Anne Theurkauf",
      "Qi Heng Ho",
      "Roland Ilyes",
      "Nisar Ahmed",
      "Morteza Lahijanian"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.07190"
  },
  {
    "id": "arXiv:2210.07197",
    "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation",
    "abstract": "Multi-dimensional evaluation is the dominant paradigm for human evaluation in\nNatural Language Generation (NLG), i.e., evaluating the generated text from\nmultiple explainable dimensions, such as coherence and fluency. However,\nautomatic evaluation in NLG is still dominated by similarity-based metrics, and\nwe lack a reliable framework for a more comprehensive evaluation of advanced\nmodels. In this paper, we propose a unified multi-dimensional evaluator UniEval\nfor NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,\nand by guiding the model with different questions, we can use one evaluator to\nevaluate from multiple dimensions. Furthermore, thanks to the unified Boolean\nQA format, we are able to introduce an intermediate learning phase that enables\nUniEval to incorporate external knowledge from multiple related tasks and gain\nfurther improvement. Experiments on three typical NLG tasks show that UniEval\ncorrelates substantially better with human judgments than existing metrics.\nSpecifically, compared to the top-performing unified evaluators, UniEval\nachieves a 23% higher correlation on text summarization, and over 43% on\ndialogue response generation. Also, UniEval demonstrates a strong zero-shot\nlearning ability for unseen evaluation dimensions and tasks. Source code, data\nand all pre-trained evaluators are available on our GitHub repository\n(https://github.com/maszhongming/UniEval).",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ming Zhong",
      "Yang Liu",
      "Da Yin",
      "Yuning Mao",
      "Yizhu Jiao",
      "Pengfei Liu",
      "Chenguang Zhu",
      "Heng Ji",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07197"
  },
  {
    "id": "arXiv:2210.07198",
    "title": "Towards Trustworthy Automatic Diagnosis Systems by Emulating Doctors'  Reasoning with Deep Reinforcement Learning",
    "abstract": "The automation of the medical evidence acquisition and diagnosis process has\nrecently attracted increasing attention in order to reduce the workload of\ndoctors and democratize access to medical care. However, most works proposed in\nthe machine learning literature focus solely on improving the prediction\naccuracy of a patient's pathology. We argue that this objective is insufficient\nto ensure doctors' acceptability of such systems. In their initial interaction\nwith patients, doctors do not only focus on identifying the pathology a patient\nis suffering from; they instead generate a differential diagnosis (in the form\nof a short list of plausible diseases) because the medical evidence collected\nfrom patients is often insufficient to establish a final diagnosis. Moreover,\ndoctors explicitly explore severe pathologies before potentially ruling them\nout from the differential, especially in acute care settings. Finally, for\ndoctors to trust a system's recommendations, they need to understand how the\ngathered evidences led to the predicted diseases. In particular, interactions\nbetween a system and a patient need to emulate the reasoning of doctors. We\ntherefore propose to model the evidence acquisition and automatic diagnosis\ntasks using a deep reinforcement learning framework that considers three\nessential aspects of a doctor's reasoning, namely generating a differential\ndiagnosis using an exploration-confirmation approach while prioritizing severe\npathologies. We propose metrics for evaluating interaction quality based on\nthese three aspects. We show that our approach performs better than existing\nmodels while maintaining competitive pathology prediction accuracy.",
    "descriptor": "\nComments: Camera ready. NeurIPS 2022\n",
    "authors": [
      "Arsene Fansi Tchango",
      "Rishab Goel",
      "Julien Martel",
      "Zhi Wen",
      "Gaetan Marceau Caron",
      "Joumana Ghosn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07198"
  },
  {
    "id": "arXiv:2210.07199",
    "title": "Self-Supervised Geometric Correspondence for Category-Level 6D Object  Pose Estimation in the Wild",
    "abstract": "While 6D object pose estimation has wide applications across computer vision\nand robotics, it remains far from being solved due to the lack of annotations.\nThe problem becomes even more challenging when moving to category-level 6D\npose, which requires generalization to unseen instances. Current approaches are\nrestricted by leveraging annotations from simulation or collected from humans.\nIn this paper, we overcome this barrier by introducing a self-supervised\nlearning approach trained directly on large-scale real-world object videos for\ncategory-level 6D pose estimation in the wild. Our framework reconstructs the\ncanonical 3D shape of an object category and learns dense correspondences\nbetween input images and the canonical shape via surface embedding. For\ntraining, we propose novel geometrical cycle-consistency losses which construct\ncycles across 2D-3D spaces, across different instances and different time\nsteps. The learned correspondence can be applied for 6D pose estimation and\nother downstream tasks such as keypoint transfer. Surprisingly, our method,\nwithout any human annotations or simulators, can achieve on-par or even better\nperformance than previous supervised or semi-supervised methods on in-the-wild\nimages. Our project page is: https://kywind.github.io/self-pose .",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Kaifeng Zhang",
      "Yang Fu",
      "Shubhankar Borse",
      "Hong Cai",
      "Fatih Porikli",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.07199"
  },
  {
    "id": "arXiv:2210.07201",
    "title": "Searching for Better Database Queries in the Outputs of Semantic Parsers",
    "abstract": "The task of generating a database query from a question in natural language\nsuffers from ambiguity and insufficiently precise description of the goal. The\nproblem is amplified when the system needs to generalize to databases unseen at\ntraining. In this paper, we consider the case when, at the test time, the\nsystem has access to an external criterion that evaluates the generated\nqueries. The criterion can vary from checking that a query executes without\nerrors to verifying the query on a set of tests. In this setting, we augment\nneural autoregressive models with a search algorithm that looks for a query\nsatisfying the criterion. We apply our approach to the state-of-the-art\nsemantic parsers and report that it allows us to find many queries passing all\nthe tests on different datasets.",
    "descriptor": "",
    "authors": [
      "Anton Osokin",
      "Irina Saparina",
      "Ramil Yarullin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07201"
  },
  {
    "id": "arXiv:2210.07206",
    "title": "Best Practices in the Creation and Use of Emotion Lexicons",
    "abstract": "Words play a central role in how we express ourselves. Lexicons of\nword-emotion associations are widely used in research and real-world\napplications for sentiment analysis, tracking emotions associated with products\nand policies, studying health disorders, tracking emotional arcs of stories,\nand so on. However, inappropriate and incorrect use of these lexicons can lead\nto not just sub-optimal results, but also inferences that are directly harmful\nto people. This paper brings together ideas from Affective Computing and AI\nEthics to present, some of the practical and ethical considerations involved in\nthe creation and use of emotion lexicons -- best practices. The goal is to\nprovide a comprehensive set of relevant considerations, so that readers\n(especially those new to work with emotions) can find relevant information in\none place. We hope this work will facilitate more thoughtfulness when one is\ndeciding on what emotions to work on, how to create an emotion lexicon, how to\nuse an emotion lexicon, how to draw meaningful inferences, and how to judge\nsuccess.",
    "descriptor": "",
    "authors": [
      "Saif M. Mohammad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07206"
  },
  {
    "id": "arXiv:2210.07207",
    "title": "Attribution-aware Weight Transfer: A Warm-Start Initialization for  Class-Incremental Semantic Segmentation",
    "abstract": "In class-incremental semantic segmentation (CISS), deep learning\narchitectures suffer from the critical problems of catastrophic forgetting and\nsemantic background shift. Although recent works focused on these issues,\nexisting classifier initialization methods do not address the background shift\nproblem and assign the same initialization weights to both background and new\nforeground class classifiers. We propose to address the background shift with a\nnovel classifier initialization method which employs gradient-based attribution\nto identify the most relevant weights for new classes from the classifier's\nweights for the previous background and transfers these weights to the new\nclassifier. This warm-start weight initialization provides a general solution\napplicable to several CISS methods. Furthermore, it accelerates learning of new\nclasses while mitigating forgetting. Our experiments demonstrate significant\nimprovement in mIoU compared to the state-of-the-art CISS methods on the\nPascal-VOC 2012, ADE20K and Cityscapes datasets.",
    "descriptor": "\nComments: Accepted at WACV 2023\n",
    "authors": [
      "Dipam Goswami",
      "Ren\u00e9 Schuster",
      "Joost van de Weijer",
      "Didier Stricker"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07207"
  },
  {
    "id": "arXiv:2210.07208",
    "title": "A Local Macroscopic Conservative (LoMaC) low rank tensor method with the  discontinuous Galerkin method for the Vlasov dynamics",
    "abstract": "In this paper, we propose a novel Local Macroscopic Conservative (LoMaC) low\nrank tensor method with discontinuous Galerkin (DG) discretization for the\nphysical and phase spaces for simulating the Vlasov-Poisson (VP) system. The\nLoMaC property refers to the exact local conservation of macroscopic mass,\nmomentum and energy at the discrete level. The recently developed LoMaC low\nrank tensor algorithm (arXiv:2207.00518) simultaneously evolves the macroscopic\nconservation laws of mass, momentum and energy using the kinetic flux vector\nsplitting; then the LoMaC property is realized by projecting the low rank\nkinetic solution onto a subspace that shares the same macroscopic observables.\nThis paper is a generalization of our previous work, but with DG\ndiscretization to take advantage of its compactness and flexibility in handling\nboundary conditions and its superior accuracy in the long term. The algorithm\nis developed in a similar fashion as that for a finite difference scheme, by\nobserving that the DG method can be viewed equivalently in a nodal fashion.\nWith the nodal DG method, assuming a tensorized computational grid, one will be\nable to (1) derive differentiation matrices for different nodal points based on\na DG upwind discretization of transport terms, and (2) define a weighted inner\nproduct space based on the nodal DG grid points. The algorithm can be extended\nto the high dimensional problems by hierarchical Tucker decomposition of\nsolution tensors and a corresponding conservative projection algorithm. In a\nsimilar spirit, the algorithm can be extended to DG methods on nodal points of\nan unstructured mesh, or to other types of discretization, e.g. the spectral\nmethod in velocity direction. Extensive numerical results are performed to\nshowcase the efficacy of the method.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2207.00518\n",
    "authors": [
      "Wei Guo",
      "Jannatul Ferdous Ema",
      "Jing-Mei Qiu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.07208"
  },
  {
    "id": "arXiv:2210.07212",
    "title": "Haptic Teleoperation goes Wireless: Evaluation and Benchmarking of a  High-Performance Low-Power Wireless Control Technology",
    "abstract": "Communication delays and packet losses are commonly investigated issues in\nthe area of robotic teleoperation. This paper investigates application of a\nnovel low-power wireless control technology (GALLOP) in a haptic teleoperation\nscenario developed to aid in nuclear decommissioning. The new wireless control\nprotocol, which is based on an off-the-shelf Bluetooth chipset, is compared\nagainst standard implementations of wired and wireless TCP/IP data transport.\nResults, through objective and subjective data, show that GALLOP can be a\nreasonable substitute for a wired TCP/IP connection, and performs better than a\nstandard wireless TCP/IP method based on Wi-Fi connectivity.",
    "descriptor": "\nComments: Accepted for publication in IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR) 2022\n",
    "authors": [
      "Joseph Bolarinwa",
      "Alex Smith",
      "Adnan Aijaz",
      "Aleksandar Stanoev",
      "Mahesh Sooriyabandara",
      "Manuel Giuliani"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.07212"
  },
  {
    "id": "arXiv:2210.07213",
    "title": "FARE: Provably Fair Representation Learning",
    "abstract": "Fair representation learning (FRL) is a popular class of methods aiming to\nproduce fair classifiers via data preprocessing. However, recent work has shown\nthat prior methods achieve worse accuracy-fairness tradeoffs than originally\nsuggested by their results. This dictates the need for FRL methods that provide\nprovable upper bounds on unfairness of any downstream classifier, a challenge\nyet unsolved. In this work we address this challenge and propose Fairness with\nRestricted Encoders (FARE), the first FRL method with provable fairness\nguarantees. Our key insight is that restricting the representation space of the\nencoder enables us to derive suitable fairness guarantees, while allowing\nempirical accuracy-fairness tradeoffs comparable to prior work. FARE\ninstantiates this idea with a tree-based encoder, a choice motivated by\ninherent advantages of decision trees when applied in our setting. Crucially,\nwe develop and apply a practical statistical procedure that computes a\nhigh-confidence upper bound on the unfairness of any downstream classifier. In\nour experimental evaluation on several datasets and settings we demonstrate\nthat FARE produces tight upper bounds, often comparable with empirical results\nof prior methods, which establishes the practical value of our approach.",
    "descriptor": "",
    "authors": [
      "Nikola Jovanovi\u0107",
      "Mislav Balunovi\u0107",
      "Dimitar I. Dimitrov",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.07213"
  },
  {
    "id": "arXiv:2210.07219",
    "title": "Condition-number-independent Convergence Rate of Riemannian Hamiltonian  Monte Carlo with Numerical Integrators",
    "abstract": "We study the convergence rate of discretized Riemannian Hamiltonian Monte\nCarlo on sampling from distributions in the form of $e^{-f(x)}$ on a convex set\n$\\mathcal{M}\\subset\\mathbb{R}^{n}$. We show that for distributions in the form\nof $e^{-\\alpha^{\\top}x}$ on a polytope with $m$ constraints, the convergence\nrate of a family of commonly-used integrators is independent of $\\left\\Vert\n\\alpha\\right\\Vert_2$ and the geometry of the polytope. In particular, the\nImplicit Midpoint Method (IMM) and the generalized Leapfrog integrator (LM)\nhave a mixing time of $\\widetilde{O}\\left(mn^{3}\\right)$ to achieve $\\epsilon$\ntotal variation distance to the target distribution. These guarantees are based\non a general bound on the convergence rate for densities of the form\n$e^{-f(x)}$ in terms of parameters of the manifold and the integrator. Our\ntheoretical guarantee complements the empirical results of [KLSV22], which\nshows that RHMC with IMM can sample ill-conditioned, non-smooth and constrained\ndistributions in very high dimension efficiently in practice.",
    "descriptor": "\nComments: Theory for arXiv:2202.01908\n",
    "authors": [
      "Yunbum Kook",
      "Yin Tat Lee",
      "Ruoqi Shen",
      "Santosh S. Vempala"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07219"
  },
  {
    "id": "arXiv:2210.07222",
    "title": "Constructing Natural Language Explanations via Saliency Map  Verbalization",
    "abstract": "Saliency maps can explain a neural model's prediction by identifying\nimportant input features. While they excel in being faithful to the explained\nmodel, saliency maps in their entirety are difficult to interpret for humans,\nespecially for instances with many input features. In contrast, natural\nlanguage explanations (NLEs) are flexible and can be tuned to a recipient's\nexpectations, but are costly to generate: Rationalization models are usually\ntrained on specific tasks and require high-quality and diverse datasets of\nhuman annotations. We combine the advantages from both explainability methods\nby verbalizing saliency maps. We formalize this underexplored task and propose\na novel methodology that addresses two key challenges of this approach -- what\nand how to verbalize. Our approach utilizes efficient search methods that are\ntask- and model-agnostic and do not require another black-box model, and\nhand-crafted templates to preserve faithfulness. We conduct a human evaluation\nof explanation representations across two natural language processing (NLP)\ntasks: news topic classification and sentiment analysis. Our results suggest\nthat saliency map verbalization makes explanations more understandable and less\ncognitively challenging to humans than conventional heatmap visualization.",
    "descriptor": "",
    "authors": [
      "Nils Feldhus",
      "Leonhard Hennig",
      "Maximilian Dustin Nasert",
      "Christopher Ebert",
      "Robert Schwarzenberg",
      "Sebastian M\u00f6ller"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07222"
  },
  {
    "id": "arXiv:2210.07224",
    "title": "Exploring Long-Sequence Masked Autoencoders",
    "abstract": "Masked Autoencoding (MAE) has emerged as an effective approach for\npre-training representations across multiple domains. In contrast to discrete\ntokens in natural languages, the input for image MAE is continuous and subject\nto additional specifications. We systematically study each input specification\nduring the pre-training stage, and find sequence length is a key axis that\nfurther scales MAE. Our study leads to a long-sequence version of MAE with\nminimal changes to the original recipe, by just decoupling the mask size from\nthe patch size. For object detection and semantic segmentation, our\nlong-sequence MAE shows consistent gains across all the experimental setups\nwithout extra computation cost during the transfer. While long-sequence\npre-training is discerned most beneficial for detection and segmentation, we\nalso achieve strong results on ImageNet-1K classification by keeping a standard\nimage size and only increasing the sequence length. We hope our findings can\nprovide new insights and avenues for scaling in computer vision.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Ronghang Hu",
      "Shoubhik Debnath",
      "Saining Xie",
      "Xinlei Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07224"
  },
  {
    "id": "arXiv:2210.07225",
    "title": "Unified Vision and Language Prompt Learning",
    "abstract": "Prompt tuning, a parameter- and data-efficient transfer learning paradigm\nthat tunes only a small number of parameters in a model's input space, has\nbecome a trend in the vision community since the emergence of large\nvision-language models like CLIP. We present a systematic study on two\nrepresentative prompt tuning methods, namely text prompt tuning and visual\nprompt tuning. A major finding is that none of the unimodal prompt tuning\nmethods performs consistently well: text prompt tuning fails on data with high\nintra-class visual variances while visual prompt tuning cannot handle low\ninter-class variances. To combine the best from both worlds, we propose a\nsimple approach called Unified Prompt Tuning (UPT), which essentially learns a\ntiny neural network to jointly optimize prompts across different modalities.\nExtensive experiments on over 11 vision datasets show that UPT achieves a\nbetter trade-off than the unimodal counterparts on few-shot learning\nbenchmarks, as well as on domain generalization benchmarks. Code and models\nwill be released to facilitate future research.",
    "descriptor": "",
    "authors": [
      "Yuhang Zang",
      "Wei Li",
      "Kaiyang Zhou",
      "Chen Huang",
      "Chen Change Loy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.07225"
  },
  {
    "id": "arXiv:2210.07228",
    "title": "Language Model Decoding as Likelihood-Utility Alignment",
    "abstract": "A critical component of a successful language generation pipeline is the\ndecoding algorithm. However, the general principles that should guide the\nchoice of decoding algorithm remain unclear. Previous works only compare\ndecoding algorithms in narrow scenarios and their findings do not generalize\nacross tasks. To better structure the discussion, we introduce a taxonomy that\ngroups decoding strategies based on their implicit assumptions about how well\nthe model's likelihood is aligned with the task-specific notion of utility. We\nargue that this taxonomy allows a broader view of the decoding problem and can\nlead to generalizable statements because it is grounded on the interplay\nbetween the decoding algorithms and the likelihood-utility misalignment.\nSpecifically, by analyzing the correlation between the likelihood and the\nutility of predictions across a diverse set of tasks, we provide the first\nempirical evidence supporting the proposed taxonomy, and a set of principles to\nstructure reasoning when choosing a decoding algorithm. Crucially, our analysis\nis the first one to relate likelihood-based decoding strategies with strategies\nthat rely on external information such as value-guided methods and prompting,\nand covers the most diverse set of tasks up-to-date.",
    "descriptor": "",
    "authors": [
      "Martin Josifoski",
      "Maxime Peyrard",
      "Frano Rajic",
      "Jiheng Wei",
      "Debjit Paul",
      "Valentin Hartmann",
      "Barun Patra",
      "Vishrav Chaudhary",
      "Emre K\u0131c\u0131man",
      "Boi Faltings",
      "Robert West"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07228"
  },
  {
    "id": "arXiv:2210.07229",
    "title": "Mass-Editing Memory in a Transformer",
    "abstract": "Recent work has shown exciting promise in updating large language models with\nnew memories, so as to replace obsolete information or add specialized\nknowledge. However, this line of work is predominantly limited to updating\nsingle associations. We develop MEMIT, a method for directly updating a\nlanguage model with many memories, demonstrating experimentally that it can\nscale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),\nexceeding prior work by orders of magnitude. Our code and data are at\nhttps://memit.baulab.info.",
    "descriptor": "\nComments: 18 pages, 11 figures. Code and data at this https URL\n",
    "authors": [
      "Kevin Meng",
      "Arnab Sen Sharma",
      "Alex Andonian",
      "Yonatan Belinkov",
      "David Bau"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07229"
  },
  {
    "id": "arXiv:2210.07232",
    "title": "Decomposing User-APP Graph into Subgraphs for Effective APP and User  Embedding Learning",
    "abstract": "APP-installation information is helpful to describe the user's\ncharacteristics. The users with similar APPs installed might share several\ncommon interests and behave similarly in some scenarios. In this work, we learn\na user embedding vector based on each user's APP-installation information.\nSince the user APP-installation embedding is learnable without dependency on\nthe historical intra-APP behavioral data of the user, it complements the\nintra-APP embedding learned within each specific APP. Thus, they considerably\nhelp improve the effectiveness of the personalized advertising in each APP, and\nthey are particularly beneficial for the cold start of the new users in the\nAPP. In this paper, we formulate the APP-installation user embedding learning\ninto a bipartite graph embedding problem. The main challenge in learning an\neffective APP-installation user embedding is the imbalanced data distribution.\nIn this case, graph learning tends to be dominated by the popular APPs, which\nbillions of users have installed. In other words, some niche/specialized APPs\nmight have a marginal influence on graph learning. To effectively exploit the\nvaluable information from the niche APPs, we decompose the APP-installation\ngraph into a set of subgraphs. Each subgraph contains only one APP node and the\nusers who install the APP. For each mini-batch, we only sample the users from\nthe same subgraph in the training process. Thus, each APP can be involved in\nthe training process in a more balanced manner. After integrating the learned\nAPP-installation user embedding into our online personal advertising platform,\nwe obtained a considerable boost in CTR, CVR, and revenue.",
    "descriptor": "",
    "authors": [
      "Tan Yu",
      "Jun Zhi",
      "Yufei Zhang",
      "Jian Li",
      "Hongliang Fei",
      "Ping Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.07232"
  },
  {
    "id": "arXiv:2210.07233",
    "title": "Shape Preserving Facial Landmarks with Graph Attention Networks",
    "abstract": "Top-performing landmark estimation algorithms are based on exploiting the\nexcellent ability of large convolutional neural networks (CNNs) to represent\nlocal appearance. However, it is well known that they can only learn weak\nspatial relationships. To address this problem, we propose a model based on the\ncombination of a CNN with a cascade of Graph Attention Network regressors. To\nthis end, we introduce an encoding that jointly represents the appearance and\nlocation of facial landmarks and an attention mechanism to weigh the\ninformation according to its reliability. This is combined with a multi-task\napproach to initialize the location of graph nodes and a coarse-to-fine\nlandmark description scheme. Our experiments confirm that the proposed model\nlearns a global representation of the structure of the face, achieving top\nperformance in popular benchmarks on head pose and landmark estimation. The\nimprovement provided by our model is most significant in situations involving\nlarge changes in the local appearance of landmarks.",
    "descriptor": "\nComments: BMVC2022. Code available at this https URL\n",
    "authors": [
      "Andr\u00e9s Prados-Torreblanca",
      "Jos\u00e9 M. Buenaposada",
      "Luis Baumela"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07233"
  },
  {
    "id": "arXiv:2210.07236",
    "title": "Improved Bounds on Neural Complexity for Representing Piecewise Linear  Functions",
    "abstract": "A deep neural network using rectified linear units represents a continuous\npiecewise linear (CPWL) function and vice versa. Recent results in the\nliterature estimated that the number of neurons needed to exactly represent any\nCPWL function grows exponentially with the number of pieces or exponentially in\nterms of the factorial of the number of distinct linear components. Moreover,\nsuch growth is amplified linearly with the input dimension. These existing\nresults seem to indicate that the cost of representing a CPWL function is\nexpensive. In this paper, we propose much tighter bounds and establish a\npolynomial time algorithm to find a network satisfying these bounds for any\ngiven CPWL function. We prove that the number of hidden neurons required to\nexactly represent any CPWL function is at most a quadratic function of the\nnumber of pieces. In contrast to all previous results, this upper bound is\ninvariant to the input dimension. Besides the number of pieces, we also study\nthe number of distinct linear components in CPWL functions. When such a number\nis also given, we prove that the quadratic complexity turns into bilinear,\nwhich implies a lower neural complexity because the number of distinct linear\ncomponents is always not greater than the minimum number of pieces in a CPWL\nfunction. When the number of pieces is unknown, we prove that, in terms of the\nnumber of distinct linear components, the neural complexity of any CPWL\nfunction is at most polynomial growth for low-dimensional inputs and a\nfactorial growth for the worst-case scenario, which are significantly better\nthan existing results in the literature.",
    "descriptor": "\nComments: 30 pages. Accepted at NeurIPS 2022\n",
    "authors": [
      "Kuan-Lin Chen",
      "Harinath Garudadri",
      "Bhaskar D. Rao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Complexity (cs.CC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.07236"
  },
  {
    "id": "arXiv:2210.07239",
    "title": "Composite Learning for Robust and Effective Dense Predictions",
    "abstract": "Multi-task learning promises better model generalization on a target task by\njointly optimizing it with an auxiliary task. However, the current practice\nrequires additional labeling efforts for the auxiliary task, while not\nguaranteeing better model performance. In this paper, we find that jointly\ntraining a dense prediction (target) task with a self-supervised (auxiliary)\ntask can consistently improve the performance of the target task, while\neliminating the need for labeling auxiliary tasks. We refer to this joint\ntraining as Composite Learning (CompL). Experiments of CompL on monocular depth\nestimation, semantic segmentation, and boundary detection show consistent\nperformance improvements in fully and partially labeled datasets. Further\nanalysis on depth estimation reveals that joint training with self-supervision\noutperforms most labeled auxiliary tasks. We also find that CompL can improve\nmodel robustness when the models are evaluated in new domains. These results\ndemonstrate the benefits of self-supervision as an auxiliary task, and\nestablish the design of novel task-specific self-supervised methods as a new\naxis of investigation for future multi-task learning research.",
    "descriptor": "\nComments: Winter Conference on Applications of Computer Vision (WACV), 2023\n",
    "authors": [
      "Menelaos Kanakis",
      "Thomas E. Huang",
      "David Bruggemann",
      "Fisher Yu",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07239"
  },
  {
    "id": "arXiv:2210.07240",
    "title": "How to Train Vision Transformer on Small-scale Datasets?",
    "abstract": "Vision Transformer (ViT), a radically different architecture than\nconvolutional neural networks offers multiple advantages including design\nsimplicity, robustness and state-of-the-art performance on many vision tasks.\nHowever, in contrast to convolutional neural networks, Vision Transformer lacks\ninherent inductive biases. Therefore, successful training of such models is\nmainly attributed to pre-training on large-scale datasets such as ImageNet with\n1.2M or JFT with 300M images. This hinders the direct adaption of Vision\nTransformer for small-scale datasets. In this work, we show that\nself-supervised inductive biases can be learned directly from small-scale\ndatasets and serve as an effective weight initialization scheme for\nfine-tuning. This allows to train these models without large-scale\npre-training, changes to model architecture or loss functions. We present\nthorough experiments to successfully train monolithic and non-monolithic Vision\nTransformers on five small datasets including CIFAR10/100, CINIC10, SVHN,\nTiny-ImageNet and two fine-grained datasets: Aircraft and Cars. Our approach\nconsistently improves the performance of Vision Transformers while retaining\ntheir properties such as attention to salient regions and higher robustness.\nOur codes and pre-trained models are available at:\nhttps://github.com/hananshafi/vits-for-small-scale-datasets.",
    "descriptor": "\nComments: Accepted at BMVC 2022\n",
    "authors": [
      "Hanan Gani",
      "Muzammal Naseer",
      "Mohammad Yaqub"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.07240"
  },
  {
    "id": "arXiv:2210.07241",
    "title": "Visual Reinforcement Learning with Self-Supervised 3D Representations",
    "abstract": "A prominent approach to visual Reinforcement Learning (RL) is to learn an\ninternal state representation using self-supervised methods, which has the\npotential benefit of improved sample-efficiency and generalization through\nadditional learning signal and inductive biases. However, while the real world\nis inherently 3D, prior efforts have largely been focused on leveraging 2D\ncomputer vision techniques as auxiliary self-supervision. In this work, we\npresent a unified framework for self-supervised learning of 3D representations\nfor motor control. Our proposed framework consists of two phases: a pretraining\nphase where a deep voxel-based 3D autoencoder is pretrained on a large\nobject-centric dataset, and a finetuning phase where the representation is\njointly finetuned together with RL on in-domain data. We empirically show that\nour method enjoys improved sample efficiency in simulated manipulation tasks\ncompared to 2D representation learning methods. Additionally, our learned\npolicies transfer zero-shot to a real robot setup with only approximate\ngeometric correspondence, and successfully solve motor control tasks that\ninvolve grasping and lifting from a single, uncalibrated RGB camera. Code and\nvideos are available at https://yanjieze.com/3d4rl/ .",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Yanjie Ze",
      "Nicklas Hansen",
      "Yinbo Chen",
      "Mohit Jain",
      "Xiaolong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.07241"
  },
  {
    "id": "arXiv:2210.07242",
    "title": "OpenOOD: Benchmarking Generalized Out-of-Distribution Detection",
    "abstract": "Out-of-distribution (OOD) detection is vital to safety-critical machine\nlearning applications and has thus been extensively studied, with a plethora of\nmethods developed in the literature. However, the field currently lacks a\nunified, strictly formulated, and comprehensive benchmark, which often results\nin unfair comparisons and inconclusive results. From the problem setting\nperspective, OOD detection is closely related to neighboring fields including\nanomaly detection (AD), open set recognition (OSR), and model uncertainty,\nsince methods developed for one domain are often applicable to each other. To\nhelp the community to improve the evaluation and advance, we build a unified,\nwell-structured codebase called OpenOOD, which implements over 30 methods\ndeveloped in relevant fields and provides a comprehensive benchmark under the\nrecently proposed generalized OOD detection framework. With a comprehensive\ncomparison of these methods, we are gratified that the field has progressed\nsignificantly over the past few years, where both preprocessing methods and the\northogonal post-hoc methods show strong potential.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022 Datasets and Benchmarks Track. Codebase: this https URL\n",
    "authors": [
      "Jingkang Yang",
      "Pengyun Wang",
      "Dejian Zou",
      "Zitang Zhou",
      "Kunyuan Ding",
      "Wenxuan Peng",
      "Haoqi Wang",
      "Guangyao Chen",
      "Bo Li",
      "Yiyou Sun",
      "Xuefeng Du",
      "Kaiyang Zhou",
      "Wayne Zhang",
      "Dan Hendrycks",
      "Yixuan Li",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07242"
  },
  {
    "id": "arXiv:2206.13455",
    "title": "IBISCape: A Simulated Benchmark for multi-modal SLAM Systems Evaluation  in Large-scale Dynamic Environments",
    "abstract": "The development process of high-fidelity SLAM systems depends on their\nvalidation upon reliable datasets. Towards this goal, we propose IBISCape, a\nsimulated benchmark that includes data synchronization and acquisition APIs for\ntelemetry from heterogeneous sensors: stereo-RGB/DVS, Depth, IMU, and GPS,\nalong with the ground truth scene segmentation and vehicle ego-motion. Our\nbenchmark is built upon the CARLA simulator, whose back-end is the Unreal\nEngine rendering a high dynamic scenery simulating the real world. Moreover, we\noffer 34 multi-modal datasets suitable for autonomous vehicles navigation,\nincluding scenarios for scene understanding evaluation like accidents, along\nwith a wide range of frame quality based on a dynamic weather simulation class\nintegrated with our APIs. We also introduce the first calibration targets to\nCARLA maps to solve the unknown distortion parameters problem of CARLA\nsimulated DVS and RGB cameras. Finally, using IBISCape sequences, we evaluate\nfour ORB-SLAM3 systems (monocular RGB, stereo RGB, Stereo Visual Inertial\n(SVI), and RGB-D) performance and BASALT Visual-Inertial Odometry (VIO) system\non various sequences collected in simulated large-scale dynamic environments.\nKeywords: benchmark, multi-modal, datasets, Odometry, Calibration, DVS, SLAM",
    "descriptor": "\nComments: Submitted to the Journal of Intelligent & Robotic Systems (JINT - Special Issue)\n",
    "authors": [
      "Abanob Soliman",
      "Fabien Bonardi",
      "D\u00e9sir\u00e9 Sidib\u00e9",
      "Samia Bouchafa"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.13455"
  },
  {
    "id": "arXiv:2210.06471",
    "title": "Subject-specific quantitative susceptibility mapping using patch based  deep image priors",
    "abstract": "Quantitative Susceptibility Mapping is a parametric imaging technique to\nestimate the magnetic susceptibilities of biological tissues from MRI phase\nmeasurements. This problem of estimating the susceptibility map is ill posed.\nRegularized recovery approaches exploiting signal properties such as smoothness\nand sparsity improve reconstructions, but suffer from over-smoothing artifacts.\nDeep learning approaches have shown great potential and generate maps with\nreduced artifacts. However, for reasonable reconstructions and network\ngeneralization, they require numerous training datasets resulting in increased\ndata acquisition time. To overcome this issue, we proposed a subject-specific,\npatch-based, unsupervised learning algorithm to estimate the susceptibility\nmap. We make the problem well-posed by exploiting the redundancies across the\npatches of the map using a deep convolutional neural network. We formulated the\nrecovery of the susceptibility map as a regularized optimization problem and\nadopted an alternating minimization strategy to solve it. We tested the\nalgorithm on a 3D invivo dataset and, qualitatively and quantitatively,\ndemonstrated improved reconstructions over competing methods.",
    "descriptor": "",
    "authors": [
      "Arvind Balachandrasekaran",
      "Davood Karimi",
      "Camilo Jaimes",
      "Ali Gholipour"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06471"
  },
  {
    "id": "arXiv:2210.06478",
    "title": "Attention-Based Generative Neural Image Compression on Solar Dynamics  Observatory",
    "abstract": "NASA's Solar Dynamics Observatory (SDO) mission gathers 1.4 terabytes of data\neach day from its geosynchronous orbit in space. SDO data includes images of\nthe Sun captured at different wavelengths, with the primary scientific goal of\nunderstanding the dynamic processes governing the Sun. Recently, end-to-end\noptimized artificial neural networks (ANN) have shown great potential in\nperforming image compression. ANN-based compression schemes have outperformed\nconventional hand-engineered algorithms for lossy and lossless image\ncompression. We have designed an ad-hoc ANN-based image compression scheme to\nreduce the amount of data needed to be stored and retrieved on space missions\nstudying solar dynamics. In this work, we propose an attention module to make\nuse of both local and non-local attention mechanisms in an adversarially\ntrained neural image compression network. We have also demonstrated the\nsuperior perceptual quality of this neural image compressor. Our proposed\nalgorithm for compressing images downloaded from the SDO spacecraft performs\nbetter in rate-distortion trade-off than the popular currently-in-use image\ncompression codecs such as JPEG and JPEG2000. In addition we have shown that\nthe proposed method outperforms state-of-the art lossy transform coding\ncompression codec, i.e., BPG.",
    "descriptor": "\nComments: Accepted to ICMLA 2022 (Oral Presentation)\n",
    "authors": [
      "Ali Zafari",
      "Atefeh Khoshkhahtinat",
      "Piyush M. Mehta",
      "Nasser M. Nasrabadi",
      "Barbara J. Thompson",
      "Daniel da Silva",
      "Michael S. F. Kirk"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06478"
  },
  {
    "id": "arXiv:2210.06512",
    "title": "Quantifying U-Net Uncertainty in Multi-Parametric MRI-based Glioma  Segmentation by Spherical Image Projection",
    "abstract": "Purpose: To develop a U-Net segmentation uncertainty quantification method\nbased on spherical image projection of multi-parametric MRI (MP-MRI) in glioma\nsegmentation. Methods: The projection of planar MRI onto a spherical surface\nretains global anatomical information. By incorporating such image\ntransformation in our proposed spherical projection-based U-Net (SPU-Net)\nsegmentation model design, multiple segmentation predictions can be obtained\nfor a single MRI. The final segmentation is the average of all predictions, and\nthe variation can be shown as an uncertainty map. An uncertainty score was\nintroduced to compare the uncertainty measurements' performance. The SPU-Net\nmodel was implemented on 369 glioma patients with MP-MRI scans. Three SPU-Nets\nwere trained to segment enhancing tumor (ET), tumor core (TC), and whole tumor\n(WT), respectively. The SPU-Net was compared with (1) classic U-Net with\ntest-time augmentation (TTA) and (2) linear scaling-based U-Net (LSU-Net) in\nboth segmentation accuracy (Dice coefficient) and uncertainty (uncertainty map\nand uncertainty score). Results: The SPU-Net achieved low uncertainty for\ncorrect segmentation predictions (e.g., tumor interior or healthy tissue\ninterior) and high uncertainty for incorrect results (e.g., tumor boundaries).\nThis model could allow the identification of missed tumor targets or\nsegmentation errors in U-Net. The SPU-Net achieved the highest uncertainty\nscores for 3 targets (ET/TC/WT): 0.826/0.848/0.936, compared to\n0.784/0.643/0.872 for the U-Net with TTA and 0.743/0.702/0.876 for the LSU-Net.\nThe SPU-Net also achieved statistically significantly higher Dice coefficients.\nConclusion: The SPU-Net offers a powerful tool to quantify glioma segmentation\nuncertainty while improving segmentation accuracy. The proposed method can be\ngeneralized to other medical image-related deep-learning applications for\nuncertainty evaluation.",
    "descriptor": "\nComments: 28 pages, 8 figures, 1 table\n",
    "authors": [
      "Zhenyu Yang",
      "Kyle Lafata",
      "Eugene Vaios",
      "Zongsheng Hu",
      "Trey Mullikin",
      "Fang-Fang Yin",
      "Chunhao Wang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.06512"
  },
  {
    "id": "arXiv:2210.06526",
    "title": "Microscopy is All You Need",
    "abstract": "We pose that microscopy offers an ideal real-world experimental environment\nfor the development and deployment of active Bayesian and reinforcement\nlearning methods. Indeed, the tremendous progress achieved by machine learning\n(ML) and artificial intelligence over the last decade has been largely achieved\nvia the utilization of static data sets, from the paradigmatic MNIST to the\nbespoke corpora of text and image data used to train large models such as GPT3,\nDALLE and others. However, it is now recognized that continuous, minute\nimprovements to state-of-the-art do not necessarily translate to advances in\nreal-world applications. We argue that a promising pathway for the development\nof ML methods is via the route of domain-specific deployable algorithms in\nareas such as electron and scanning probe microscopy and chemical imaging. This\nwill benefit both fundamental physical studies and serve as a test bed for more\ncomplex autonomous systems such as robotics and manufacturing. Favorable\nenvironment characteristics of scanning and electron microscopy include low\nrisk, extensive availability of domain-specific priors and rewards, relatively\nsmall effects of exogeneous variables, and often the presence of both upstream\nfirst principles as well as downstream learnable physical models for both\nstatics and dynamics. Recent developments in programmable interfaces, edge\ncomputing, and access to APIs facilitating microscope control, all render the\ndeployment of ML codes on operational microscopes straightforward. We discuss\nthese considerations and hope that these arguments will lead to creating a\nnovel set of development targets for the ML community by accelerating both\nreal-world ML applications and scientific progress.",
    "descriptor": "",
    "authors": [
      "Sergei V. Kalinin",
      "Rama Vasudevan",
      "Yongtao Liu",
      "Ayana Ghosh",
      "Kevin Roccapriore",
      "Maxim Ziatdinov"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06526"
  },
  {
    "id": "arXiv:2210.06539",
    "title": "Quantum Algorithms for Sampling Log-Concave Distributions and Estimating  Normalizing Constants",
    "abstract": "Given a convex function $f\\colon\\mathbb{R}^{d}\\to\\mathbb{R}$, the problem of\nsampling from a distribution $\\propto e^{-f(x)}$ is called log-concave\nsampling. This task has wide applications in machine learning, physics,\nstatistics, etc. In this work, we develop quantum algorithms for sampling\nlog-concave distributions and for estimating their normalizing constants\n$\\int_{\\mathbb{R}^d}e^{-f(x)}\\mathrm{d} x$. First, we use underdamped Langevin\ndiffusion to develop quantum algorithms that match the query complexity (in\nterms of the condition number $\\kappa$ and dimension $d$) of analogous\nclassical algorithms that use gradient (first-order) queries, even though the\nquantum algorithms use only evaluation (zeroth-order) queries. For estimating\nnormalizing constants, these algorithms also achieve quadratic speedup in the\nmultiplicative error $\\epsilon$. Second, we develop quantum Metropolis-adjusted\nLangevin algorithms with query complexity $\\widetilde{O}(\\kappa^{1/2}d)$ and\n$\\widetilde{O}(\\kappa^{1/2}d^{3/2}/\\epsilon)$ for log-concave sampling and\nnormalizing constant estimation, respectively, achieving polynomial speedups in\n$\\kappa,d,\\epsilon$ over the best known classical algorithms by exploiting\nquantum analogs of the Monte Carlo method and quantum walks. We also prove a\n$1/\\epsilon^{1-o(1)}$ quantum lower bound for estimating normalizing constants,\nimplying near-optimality of our quantum algorithms in $\\epsilon$.",
    "descriptor": "\nComments: To appear in the proceedings of NeurIPS 2022\n",
    "authors": [
      "Andrew M. Childs",
      "Tongyang Li",
      "Jin-Peng Liu",
      "Chunhao Wang",
      "Ruizhe Zhang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.06539"
  },
  {
    "id": "arXiv:2210.06543",
    "title": "A General Stochastic Optimization Framework for Convergence Bidding",
    "abstract": "We introduce a general stochastic optimization framework to obtain optimal\nconvergence (virtual) bid curves. Within this framework, we develop a\ncomputationally tractable linear programming-based optimization model, which\nproduces bid prices and volumes simultaneously. We also show that different\napproximations and simplifications in the general model lead naturally to\nwell-known convergence bidding approaches, such as self-scheduling and\nopportunistic approaches.",
    "descriptor": "",
    "authors": [
      "Letif Mones",
      "Sean Lovett"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06543"
  },
  {
    "id": "arXiv:2210.06564",
    "title": "Robust Neural Posterior Estimation and Statistical Model Criticism",
    "abstract": "Computer simulations have proven a valuable tool for understanding complex\nphenomena across the sciences. However, the utility of simulators for modelling\nand forecasting purposes is often restricted by low data quality, as well as\npractical limits to model fidelity. In order to circumvent these difficulties,\nwe argue that modellers must treat simulators as idealistic representations of\nthe true data generating process, and consequently should thoughtfully consider\nthe risk of model misspecification. In this work we revisit neural posterior\nestimation (NPE), a class of algorithms that enable black-box parameter\ninference in simulation models, and consider the implication of a\nsimulation-to-reality gap. While recent works have demonstrated reliable\nperformance of these methods, the analyses have been performed using synthetic\ndata generated by the simulator model itself, and have therefore only addressed\nthe well-specified case. In this paper, we find that the presence of\nmisspecification, in contrast, leads to unreliable inference when NPE is used\nnaively. As a remedy we argue that principled scientific inquiry with\nsimulators should incorporate a model criticism component, to facilitate\ninterpretable identification of misspecification and a robust inference\ncomponent, to fit 'wrong but useful' models. We propose robust neural posterior\nestimation (RNPE), an extension of NPE to simultaneously achieve both these\naims, through explicitly modelling the discrepancies between simulations and\nthe observed data. We assess the approach on a range of artificially\nmisspecified examples, and find RNPE performs well across the tasks, whereas\nnaively using NPE leads to misleading and erratic posteriors.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Daniel Ward",
      "Patrick Cannon",
      "Mark Beaumont",
      "Matteo Fasiolo",
      "Sebastian M Schmon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.06564"
  },
  {
    "id": "arXiv:2210.06574",
    "title": "Gaussian Processes on Distributions based on Regularized Optimal  Transport",
    "abstract": "We present a novel kernel over the space of probability measures based on the\ndual formulation of optimal regularized transport. We propose an Hilbertian\nembedding of the space of probabilities using their Sinkhorn potentials, which\nare solutions of the dual entropic relaxed optimal transport between the\nprobabilities and a reference measure $\\mathcal{U}$. We prove that this\nconstruction enables to obtain a valid kernel, by using the Hilbert norms. We\nprove that the kernel enjoys theoretical properties such as universality and\nsome invariances, while still being computationally feasible. Moreover we\nprovide theoretical guarantees on the behaviour of a Gaussian process based on\nthis kernel. The empirical performances are compared with other traditional\nchoices of kernels for processes indexed on distributions.",
    "descriptor": "",
    "authors": [
      "Fran\u00e7ois Bachoc",
      "Louis B\u00e9thune",
      "Alberto Gonzalez-Sanz",
      "Jean-Michel Loubes"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06574"
  },
  {
    "id": "arXiv:2210.06591",
    "title": "Rigorous dynamical mean field theory for stochastic gradient descent  methods",
    "abstract": "We prove closed-form equations for the exact high-dimensional asymptotics of\na family of first order gradient-based methods, learning an estimator (e.g.\nM-estimator, shallow neural network, ...) from observations on Gaussian data\nwith empirical risk minimization. This includes widely used algorithms such as\nstochastic gradient descent (SGD) or Nesterov acceleration. The obtained\nequations match those resulting from the discretization of dynamical mean-field\ntheory (DMFT) equations from statistical physics when applied to gradient flow.\nOur proof method allows us to give an explicit description of how memory\nkernels build up in the effective dynamics, and to include non-separable update\nfunctions, allowing datasets with non-identity covariance matrices. Finally, we\nprovide numerical implementations of the equations for SGD with generic\nextensive batch-size and with constant learning rates.",
    "descriptor": "\nComments: 34 pages, 4 figures\n",
    "authors": [
      "Cedric Gerbelot",
      "Emanuele Troiani",
      "Francesca Mignacco",
      "Florent Krzakala",
      "Lenka Zdeborova"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06591"
  },
  {
    "id": "arXiv:2210.06622",
    "title": "When does deep learning fail and how to tackle it? A critical analysis  on polymer sequence-property surrogate models",
    "abstract": "Deep learning models are gaining popularity and potency in predicting polymer\nproperties. These models can be built using pre-existing data and are useful\nfor the rapid prediction of polymer properties. However, the performance of a\ndeep learning model is intricately connected to its topology and the volume of\ntraining data. There is no facile protocol available to select a deep learning\narchitecture, and there is a lack of a large volume of homogeneous\nsequence-property data of polymers. These two factors are the primary\nbottleneck for the efficient development of deep learning models. Here we\nassess the severity of these factors and propose new algorithms to address\nthem. We show that a linear layer-by-layer expansion of a neural network can\nhelp in identifying the best neural network topology for a given problem.\nMoreover, we map the discrete sequence space of a polymer to a continuous\none-dimensional latent space using a machine learning pipeline to identify\nminimal data points for building a universal deep learning model. We implement\nthese approaches for three representative cases of building sequence-property\nsurrogate models, viz., the single-molecule radius of gyration of a copolymer,\nadhesive free energy of a copolymer, and copolymer compatibilizer,\ndemonstrating the generality of the proposed strategies. This work establishes\nefficient methods for building universal deep learning models with minimal data\nand hyperparameters for predicting sequence-defined properties of polymers.",
    "descriptor": "",
    "authors": [
      "Himanshu",
      "Tarak K Patra"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06622"
  },
  {
    "id": "arXiv:2210.06624",
    "title": "Approximate Discrete Entropy Monotonicity for Log-Concave Sums",
    "abstract": "It is proved that for any $n \\geq 1,$ if $X_1,\\ldots,X_n$ are i.i.d.\ninteger-valued, log-concave random variables then $$ h(X_1+\\ldots+X_n +\nU_1+\\ldots+U_n) = H(X_1+\\ldots+X_n) + o(1), $$ as $H(X_1) \\to \\infty$, where\n$h$ stands for the differential entropy, $H$ dentoes the (discrete) Shannon\nentropy and $U_1,\\ldots,U_n$ are independent continuous uniforms on $(0,1)$. As\na corollary, it is shown that a conjecture of Tao (2010) holds true for\nlog-concave random variables on the integers: $$ H(X_1+\\ldots+X_{n+1}) \\geq\nH(X_1+\\cdots+X_{n}) + \\frac{1}{2}\\log{\\Bigl(\\frac{n+1}{n}\\Bigr)} - o(1) $$ as\n$H(X_1) \\to \\infty$. Explicit bounds for the $o(1)$-terms are provided.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Lampros Gavalakis"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.06624"
  },
  {
    "id": "arXiv:2210.06635",
    "title": "A Bayesian Optimization Framework for Finding Local Optima in Expensive  Multi-Modal Functions",
    "abstract": "Bayesian optimization (BO) is a popular global optimization scheme for\nsample-efficient optimization in domains with expensive function evaluations.\nThe existing BO techniques are capable of finding a single global optimum\nsolution. However, finding a set of global and local optimum solutions is\ncrucial in a wide range of real-world problems, as implementing some of the\noptimal solutions might not be feasible due to various practical restrictions\n(e.g., resource limitation, physical constraints, etc.). In such domains, if\nmultiple solutions are known, the implementation can be quickly switched to\nanother solution, and the best possible system performance can still be\nobtained. This paper develops a multi-modal BO framework to effectively find a\nset of local/global solutions for expensive-to-evaluate multi-modal objective\nfunctions. We consider the standard BO setting with Gaussian process regression\nrepresenting the objective function. We analytically derive the joint\ndistribution of the objective function and its first-order gradients. This\njoint distribution is used in the body of the BO acquisition functions to\nsearch for local optima during the optimization process. We introduce variants\nof the well-known BO acquisition functions to the multi-modal setting and\ndemonstrate the performance of the proposed framework in locating a set of\nlocal optimum solutions using multiple optimization problems.",
    "descriptor": "",
    "authors": [
      "Yongsheng Mei",
      "Tian Lan",
      "Mahdi Imani",
      "Suresh Subramaniam"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06635"
  },
  {
    "id": "arXiv:2210.06664",
    "title": "Are Macula or Optic Nerve Head Structures better at Diagnosing Glaucoma?  An Answer using AI and Wide-Field Optical Coherence Tomography",
    "abstract": "Purpose: (1) To develop a deep learning algorithm to automatically segment\nstructures of the optic nerve head (ONH) and macula in 3D wide-field optical\ncoherence tomography (OCT) scans; (2) To assess whether 3D macula or ONH\nstructures (or the combination of both) provide the best diagnostic power for\nglaucoma. Methods: A cross-sectional comparative study was performed which\nincluded wide-field swept-source OCT scans from 319 glaucoma subjects and 298\nnon-glaucoma subjects. All scans were compensated to improve deep-tissue\nvisibility. We developed a deep learning algorithm to automatically label all\nmajor ONH tissue structures by using 270 manually annotated B-scans for\ntraining. The performance of our algorithm was assessed using the Dice\ncoefficient (DC). A glaucoma classification algorithm (3D CNN) was then\ndesigned using a combination of 500 OCT volumes and their corresponding\nautomatically segmented masks. This algorithm was trained and tested on 3\ndatasets: OCT scans cropped to contain the macular tissues only, those to\ncontain the ONH tissues only, and the full wide-field OCT scans. The\nclassification performance for each dataset was reported using the AUC.\nResults: Our segmentation algorithm was able to segment ONH and macular tissues\nwith a DC of 0.94 $\\pm$ 0.003. The classification algorithm was best able to\ndiagnose glaucoma using wide-field 3D-OCT volumes with an AUC of 0.99 $\\pm$\n0.01, followed by ONH volumes with an AUC of 0.93 $\\pm$ 0.06, and finally\nmacular volumes with an AUC of 0.91 $\\pm$ 0.11. Conclusions: this study showed\nthat using wide-field OCT as compared to the typical OCT images containing just\nthe ONH or macular may allow for a significantly improved glaucoma diagnosis.\nThis may encourage the mainstream adoption of 3D wide-field OCT scans. For\nclinical AI studies that use traditional machines, we would recommend the use\nof ONH scans as opposed to macula scans.",
    "descriptor": "\nComments: 23 pages, 5 figures\n",
    "authors": [
      "Charis Y.N. Chiang",
      "Fabian Braeu",
      "Thanadet Chuangsuwanich",
      "Royston K.Y. Tan",
      "Jacqueline Chua",
      "Leopold Schmetterer",
      "Alexandre Thiery",
      "Martin Buist",
      "Micha\u00ebl J.A. Girard"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06664"
  },
  {
    "id": "arXiv:2210.06672",
    "title": "Variance-Aware Estimation of Kernel Mean Embedding",
    "abstract": "An important feature of kernel mean embeddings (KME) is that the rate of\nconvergence of the empirical KME to the true distribution KME can be bounded\nindependently of the dimension of the space, properties of the distribution and\nsmoothness features of the kernel. We show how to speed-up convergence by\nleveraging variance information in the RKHS. Furthermore, we show that even\nwhen such information is a priori unknown, we can efficiently estimate it from\nthe data, recovering the desiderata of a distribution agnostic bound that\nenjoys acceleration in fortuitous settings. We illustrate our methods in the\ncontext of hypothesis testing and robust parametric estimation.",
    "descriptor": "",
    "authors": [
      "Geoffrey Wolfer",
      "Pierre Alquier"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06672"
  },
  {
    "id": "arXiv:2210.06687",
    "title": "RWN: A Novel Neighborhood-Based Method for Statistical Disclosure  Control",
    "abstract": "A novel variation of the data swapping approach to statistical disclosure\ncontrol is presented, aimed particularly at preservation of multivariate\nrelations in the original dataset. A theorem is proved in support of the\nmethod, and extensive empirical investigation is reported.",
    "descriptor": "",
    "authors": [
      "Noah Perry",
      "Norman Matloff",
      "Patrick Tendick"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06687"
  },
  {
    "id": "arXiv:2210.06693",
    "title": "Non-uniformity and Quantum Advice in the Quantum Random Oracle Model",
    "abstract": "QROM (quantum random oracle model), introduced by Boneh et al. (Asiacrypt\n2011), captures all generic algorithms. However, it fails to describe\nnon-uniform quantum algorithms with preprocessing power, which receives a piece\nof bounded classical or quantum advice. As non-uniform algorithms are largely\nbelieved to be the right model for attackers, starting from the work by Nayebi,\nAaronson, Belovs, and Trevisan (QIC 2015), a line of works investigates\nnon-uniform security in the random oracle model. Chung, Guo, Liu, and Qian\n(FOCS 2020) provide a framework and establish non-uniform security for many\ncryptographic applications.\nIn this work, we continue the study on quantum advice in the QROM. We provide\na new idea that generalizes the previous multi-instance framework, which we\nbelieve is more quantum-friendly and should be the quantum analogue of\nmulti-instance games. To this end, we match the bounds with quantum advice to\nthose with classical advice by Chung et al., showing quantum advice is almost\nas good/bad as classical advice for many natural security games in the QROM.\nFinally, we show that for some contrived games in the QROM, quantum advice\ncan be exponentially better than classical advice for some parameter regimes.\nTo our best knowledge, it provides some evidence of a general separation\nbetween quantum and classical advice relative to an unstructured oracle.",
    "descriptor": "",
    "authors": [
      "Qipeng Liu"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06693"
  },
  {
    "id": "arXiv:2210.06723",
    "title": "Noise can be helpful for variational quantum algorithms",
    "abstract": "Saddle points constitute a crucial challenge for first-order gradient descent\nalgorithms. In notions of classical machine learning, they are avoided for\nexample by means of stochastic gradient descent methods. In this work, we\nprovide evidence that the saddle points problem can be naturally avoided in\nvariational quantum algorithms by exploiting the presence of stochasticity. We\nprove convergence guarantees of the approach and its practical functioning at\nhand of examples. We argue that the natural stochasticity of variational\nalgorithms can be beneficial for avoiding strict saddle points, i.e., those\nsaddle points with at least one negative Hessian eigenvalue. This insight that\nsome noise levels could help in this perspective is expected to add a new\nperspective to notions of near-term variational quantum algorithms.",
    "descriptor": "\nComments: 13 pages, 11 figures\n",
    "authors": [
      "Junyu Liu",
      "Frederik Wilde",
      "Antonio Anna Mele",
      "Liang Jiang",
      "Jens Eisert"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06723"
  },
  {
    "id": "arXiv:2210.06728",
    "title": "On the Efficient Implementation of High Accuracy Optimality of Profile  Maximum Likelihood",
    "abstract": "We provide an efficient unified plug-in approach for estimating symmetric\nproperties of distributions given $n$ independent samples. Our estimator is\nbased on profile-maximum-likelihood (PML) and is sample optimal for estimating\nvarious symmetric properties when the estimation error $\\epsilon \\gg n^{-1/3}$.\nThis result improves upon the previous best accuracy threshold of $\\epsilon \\gg\nn^{-1/4}$ achievable by polynomial time computable PML-based universal\nestimators [ACSS21, ACSS20]. Our estimator reaches a theoretical limit for\nuniversal symmetric property estimation as [Han21] shows that a broad class of\nuniversal estimators (containing many well known approaches including ours)\ncannot be sample optimal for every $1$-Lipschitz property when $\\epsilon \\ll\nn^{-1/3}$.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Moses Charikar",
      "Zhihao Jiang",
      "Kirankumar Shiragur",
      "Aaron Sidford"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.06728"
  },
  {
    "id": "arXiv:2210.06753",
    "title": "Using Physics Simulations to Find Targeting Strategies in Competitive  Bowling",
    "abstract": "This article demonstrates a new approach to finding ideal bowling targeting\nstrategies through computer simulation. To model bowling ball behaviour, a\nsystem of five coupled differential equations is derived using Euler equations\nfor rigid body rotations. We used a computer program to demonstrate the phases\nof ball motion and output a plot that displays the optimum initial conditions\nthat can lead to a strike. When the bowler is modeled to be imperfect, it is\nshown that some targeting strategies can lead to higher strike rates due to the\nmiss room created by the inhomogeneity of the oil pattern.",
    "descriptor": "",
    "authors": [
      "Simon Ji",
      "Shouzhuo Yang",
      "Wilber Dominguez",
      "Cacey Bester"
    ],
    "subjectives": [
      "Popular Physics (physics.pop-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06753"
  },
  {
    "id": "arXiv:2210.06757",
    "title": "Measuring decoherence by commutation relations decay for quasilinear  quantum stochastic systems",
    "abstract": "This paper considers a class of open quantum systems with an algebraic\nstructure of dynamic variables, including the Pauli matrices for finite-level\nsystems as a particular case. The Hamiltonian and the operators of coupling of\nthe system to the external bosonic fields depend linearly on the system\nvariables. The fields are represented by quantum Wiener processes which drive\nthe system dynamics in the form of a quasilinear Hudson-Parthasarathy quantum\nstochastic differential equation whose drift vector and dispersion matrix are\naffine and linear functions of the system variables. This quasilinearity leads\nto a tractable evolution of the two-point commutator matrix of the system\nvariables (and their multi-point mixed moments in the case of vacuum input\nfields) involving time-ordered operator exponentials. The resulting exponential\ndecay in the two-point commutation relations is a manifestation of quantum\ndecoherence, caused by the dissipative system-field interaction and making the\nsystem lose specific unitary dynamics features which it would have in isolation\nfrom the environment. We quantify the decoherence in terms of the rate of the\ncommutation relations decay and apply system theoretic and matrix analytic\ntechniques, such as algebraic Lyapunov inequalities and spectrum perturbation\nresults, to the study of the asymptotic behaviour of the related Lyapunov\nexponents in the presence of a small scaling parameter in the system-field\ncoupling. These findings are illustrated for finite-level quantum systems (and\ntheir interconnections through a direct energy coupling) with multichannel\nexternal fields and the Pauli matrices as internal variables.",
    "descriptor": "\nComments: 41 pages, 1 figure\n",
    "authors": [
      "Igor G. Vladimirov",
      "Ian R. Petersen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.06757"
  },
  {
    "id": "arXiv:2210.06785",
    "title": "An efficient combination strategy for hybird quantum ensemble classifier",
    "abstract": "Quantum machine learning has shown advantages in many ways compared to\nclassical machine learning. In machine learning, a difficult problem is how to\nlearn a model with high robustness and strong generalization ability from a\nlimited feature space. Combining multiple models as base learners, ensemble\nlearning (EL) can effectively improve the accuracy, generalization ability, and\nrobustness of the final model. The key to EL lies in two aspects, the\nperformance of base learners and the choice of the combination strategy.\nRecently, quantum EL (QEL) has been studied. However, existing combination\nstrategies in QEL are inadequate in considering the accuracy and variance among\nbase learners. This paper presents a hybrid EL framework that combines quantum\nand classical advantages. More importantly, we propose an efficient combination\nstrategy for improving the accuracy of classification in the framework. We\nverify the feasibility and efficiency of our framework and strategy by using\nthe MNIST dataset. Simulation results show that the hybrid EL framework with\nour combination strategy not only has a higher accuracy and lower variance than\nthe single model without the ensemble, but also has a better accuracy than the\nmajority voting and the weighted voting strategies in most cases.",
    "descriptor": "",
    "authors": [
      "Xiao-Ying Zhang",
      "Ming-Ming Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06785"
  },
  {
    "id": "arXiv:2210.06786",
    "title": "Evaluating the Label Efficiency of Contrastive Self-Supervised Learning  for Multi-Resolution Satellite Imagery",
    "abstract": "The application of deep neural networks to remote sensing imagery is often\nconstrained by the lack of ground-truth annotations. Adressing this issue\nrequires models that generalize efficiently from limited amounts of labeled\ndata, allowing us to tackle a wider range of Earth observation tasks. Another\nchallenge in this domain is developing algorithms that operate at variable\nspatial resolutions, e.g., for the problem of classifying land use at different\nscales. Recently, self-supervised learning has been applied in the remote\nsensing domain to exploit readily-available unlabeled data, and was shown to\nreduce or even close the gap with supervised learning. In this paper, we study\nself-supervised visual representation learning through the lens of label\nefficiency, for the task of land use classification on\nmulti-resolution/multi-scale satellite images. We benchmark two contrastive\nself-supervised methods adapted from Momentum Contrast (MoCo) and provide\nevidence that these methods can be perform effectively given little downstream\nsupervision, where randomly initialized networks fail to generalize. Moreover,\nthey outperform out-of-domain pretraining alternatives. We use the large-scale\nfMoW dataset to pretrain and evaluate the networks, and validate our\nobservations with transfer to the RESISC45 dataset.",
    "descriptor": "",
    "authors": [
      "Jules BOURCIER",
      "Gohar Dashyan",
      "Jocelyn Chanussot",
      "Karteek Alahari"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06786"
  },
  {
    "id": "arXiv:2210.06817",
    "title": "An Analysis Method for Metric-Level Switching in Beat Tracking",
    "abstract": "For expressive music, the tempo may change over time, posing challenges to\ntracking the beats by an automatic model. The model may first tap to the\ncorrect tempo, but then may fail to adapt to a tempo change, or switch between\nseveral incorrect but perceptually plausible ones (e.g., half- or\ndouble-tempo). Existing evaluation metrics for beat tracking do not reflect\nsuch behaviors, as they typically assume a fixed relationship between the\nreference beats and estimated beats. In this paper, we propose a new\nperformance analysis method, called annotation coverage ratio (ACR), that\naccounts for a variety of possible metric-level switching behaviors of beat\ntrackers. The idea is to derive sequences of modified reference beats of all\nmetrical levels for every two consecutive reference beats, and compare every\nsequence of modified reference beats to the subsequences of estimated beats. We\nshow via experiments on three datasets of different genres the usefulness of\nACR when utilized alongside existing metrics, and discuss the new insights to\nbe gained.",
    "descriptor": "\nComments: Accepted to IEEE Signal Processing Letters (Oct. 2022)\n",
    "authors": [
      "Ching-Yu Chiu",
      "Meinard M\u00fcller",
      "Matthew E. P. Davies",
      "Alvin Wen-Yu Su",
      "Yi-Hsuan Yang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.06817"
  },
  {
    "id": "arXiv:2210.06818",
    "title": "Deepfake Detection System for the ADD Challenge Track 3.2 Based on Score  Fusion",
    "abstract": "This paper describes the deepfake audio detection system submitted to the\nAudio Deep Synthesis Detection (ADD) Challenge Track 3.2 and gives an analysis\nof score fusion. The proposed system is a score-level fusion of several light\nconvolutional neural network (LCNN) based models. Various front-ends are used\nas input features, including low-frequency short-time Fourier transform and\nConstant Q transform. Due to the complex noise and rich synthesis algorithms,\nit is difficult to obtain the desired performance using the training set\ndirectly. Online data augmentation methods effectively improve the robustness\nof fake audio detection systems. In particular, the reasons for the poor\nimprovement of score fusion are explored through visualization of the score\ndistributions and comparison with score distribution on another dataset. The\noverfitting of the model to the training set leads to extreme values of the\nscores and low correlation of the score distributions, which makes score fusion\ndifficult. Fusion with partially fake audio detection system improves system\nperformance further. The submission on track 3.2 obtained the weighted equal\nerror rate (WEER) of 11.04\\%, which is one of the best performing systems in\nthe challenge.",
    "descriptor": "\nComments: Accepted by ACM Multimedia 2022 Workshop: First International Workshop on Deepfake Detection for Audio Multimedia\n",
    "authors": [
      "Yuxiang Zhang",
      "Jingze Lu",
      "Xingming Wang",
      "Zhuo Li",
      "Runqiu Xiao",
      "Wenchao Wang",
      "Ming Li",
      "Pengyuan Zhang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.06818"
  },
  {
    "id": "arXiv:2210.06872",
    "title": "Dirichlet process mixture models for non-stationary data streams",
    "abstract": "In recent years, we have seen a handful of work on inference algorithms over\nnon-stationary data streams. Given their flexibility, Bayesian non-parametric\nmodels are a good candidate for these scenarios. However, reliable streaming\ninference under the concept drift phenomenon is still an open problem for these\nmodels. In this work, we propose a variational inference algorithm for\nDirichlet process mixture models. Our proposal deals with the concept drift by\nincluding an exponential forgetting over the prior global parameters. Our\nalgorithm allows to adapt the learned model to the concept drifts\nautomatically. We perform experiments in both synthetic and real data, showing\nthat the proposed model is competitive with the state-of-the-art algorithms in\nthe density estimation problem, and it outperforms them in the clustering\nproblem.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Ioar Casado",
      "Aritz P\u00e9rez"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06872"
  },
  {
    "id": "arXiv:2210.06971",
    "title": "Reliable quantum kernel classification using fewer circuit evaluations",
    "abstract": "Quantum kernel methods are a candidate for quantum speed-ups in supervised\nmachine learning. The number of quantum measurements $N$ required for a\nreasonable kernel estimate is a critical resource, both from complexity\nconsiderations and because of the constraints of near-term quantum hardware. We\nemphasize that for classification tasks, the aim is accurate classification and\nnot accurate kernel evaluation, and demonstrate that the former is more\nresource efficient. In general, the uncertainty in the quantum kernel, arising\nfrom finite sampling, leads to misclassifications over some kernel\ninstantiations. We introduce a suitable performance metric that characterizes\nthe robustness or reliability of classification over a dataset, and obtain a\nbound for $N$ which ensures, with high probability, that classification errors\nover a dataset are bounded by the margin errors of an idealized quantum kernel\nclassifier. Using techniques of robust optimization, we then show that the\nnumber of quantum measurements can be significantly reduced by a robust\nformulation of the original support vector machine. We consider the SWAP test\nand the GATES test quantum circuits for kernel evaluations, and show that the\nSWAP test is always less reliable than the GATES test for any $N$. Our strategy\nis applicable to uncertainty in quantum kernels arising from {\\em any} source\nof noise, although we only consider the statistical sampling noise in our\nanalysis.",
    "descriptor": "",
    "authors": [
      "Abhay Shastry",
      "Abhijith J",
      "Apoorva Patel",
      "Chiranjib Bhattacharyya"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06971"
  },
  {
    "id": "arXiv:2210.06982",
    "title": "Ultra-tuning of nonlinear drumhead MEMS resonators by  electro-thermoelastic buckling",
    "abstract": "Nonlinear micro-electro-mechanical systems (MEMS) resonators open new\nopportunities in sensing and signal manipulation compared to their linear\ncounterparts by enabling frequency tuning and increased bandwidth. Here, we\ndesign, fabricate and study drumhead resonators exhibiting strongly nonlinear\ndynamics and develop a reduced order model (ROM) to capture their response\naccurately. The resonators undergo electrostatically-mediated thermoelastic\nbuckling which tunes their natural frequency from 4.7 to 11.3 MHz, a factor of\n2.4x tunability. Moreover, the imposed buckling switches the nonlinearity of\nthe resonators between purely stiffening, purely softening, and even\nsoftening-to-stiffening. Accessing these exotic dynamics requires precise\ncontrol of the temperature and the DC electrostatic forces near the resonator's\ncritical-buckling point. To explain the observed tunability, we develop a\none-dimensional physics-based ROM that predicts the linear and nonlinear\nresponse of the fundamental bending mode of these drumhead resonators. The ROM\ncaptures the dynamic effects of the internal stresses resulting from three\nsources: The residual stresses from the fabrication process, the mismatch in\nthermal expansion between the constituent layers, and lastly, the applied\nelectrostatic forces. The ROM replicates the observed tunability of linear\n(within 5.5% error) and nonlinear responses even near the states of critical\nbuckling. These remarkable nonlinear and large tunability of the natural\nfrequency are valuable features for on-chip acoustic devices in broad\napplications such as signal manipulation, filtering, and MEMS waveguides.",
    "descriptor": "",
    "authors": [
      "Ali Kanj",
      "Paolo F. Ferrari",
      "Arend M. van der Zande",
      "Alexander F. Vakakis",
      "Sameh Tawfick"
    ],
    "subjectives": [
      "Applied Physics (physics.app-ph)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06982"
  },
  {
    "id": "arXiv:2210.07087",
    "title": "svMorph: Interactive geometry-editing tools for virtual patient-specific  vascular anatomies",
    "abstract": "We propose svMorph, a framework for interactive virtual sculpting of\npatient-specific vascular anatomic models. Our framework includes three tools\nfor the creation of tortuosity, aneurysms, and stenoses in tubular vascular\ngeometries. These shape edits are performed via geometric operations on the\nsurface mesh and vessel centerline curves of the input model. The tortuosity\ntool also uses the physics-based Oriented Particles method, coupled with linear\nblend skinning, to achieve smooth, elastic-like deformations. Our tools can be\napplied separately or in combination to produce simulation-suitable morphed\nmodels. They are also compatible with popular vascular modeling software, such\nas SimVascular. To illustrate our tools, we morph several image-based,\npatient-specific models to create a range of shape changes and simulate the\nresulting hemodynamics via three-dimensional, computational fluid dynamics. We\nalso demonstrate the ability to quickly estimate the hemodynamic effects of the\nshape changes via automated generation of associated zero-dimensional\nlumped-parameter models.",
    "descriptor": "",
    "authors": [
      "Jonathan Pham",
      "Sofia Wyetzner",
      "Martin R. Pfaller",
      "David W. Parker",
      "Doug L. James",
      "Alison L. Marsden"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.07087"
  },
  {
    "id": "arXiv:2210.07102",
    "title": "Corneal endothelium assessment in specular microscopy images with Fuchs'  dystrophy via deep regression of signed distance maps",
    "abstract": "Specular microscopy assessment of the human corneal endothelium (CE) in\nFuchs' dystrophy is challenging due to the presence of dark image regions\ncalled guttae. This paper proposes a UNet-based segmentation approach that\nrequires minimal post-processing and achieves reliable CE morphometric\nassessment and guttae identification across all degrees of Fuchs' dystrophy. We\ncast the segmentation problem as a regression task of the cell and gutta signed\ndistance maps instead of a pixel-level classification task as typically done\nwith UNets. Compared to the conventional UNet classification approach, the\ndistance-map regression approach converges faster in clinically relevant\nparameters. It also produces morphometric parameters that agree with the\nmanually-segmented ground-truth data, namely the average cell density\ndifference of -41.9 cells/mm2 (95% confidence interval (CI) [-306.2, 222.5])\nand the average difference of mean cell area of 14.8 um2 (95% CI [-41.9,\n71.5]). These results suggest a promising alternative for CE assessment.",
    "descriptor": "",
    "authors": [
      "Juan S. Sierra",
      "Jesus Pineda",
      "Daniela Rueda",
      "Alejandro Tello",
      "Angelica M. Prada",
      "Virgilio Galvis",
      "Giovanni Volpe",
      "Maria S. Millan",
      "Lenny A. Romero",
      "Andres G. Marrugo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07102"
  },
  {
    "id": "arXiv:2210.07115",
    "title": "Precision QCD corrections to gluon-initiated diphoton-plus-jet  production at the LHC",
    "abstract": "In this thesis, we present recent advances at the precision frontier of\nhigher-order quantum chromodynamics (QCD) calculations. We consider massless\ntwo-loop five-point amplitudes, with a particular focus on diphoton-plus-jet\nproduction through gluon fusion. We build a library of infrared functions up to\nat most next-to-next-to-leading order (NNLO) in QCD, which can be used to\nvalidate amplitudes and construct counterterms in subtraction schemes at NNLO.\nWe review progress in the novel use of machine learning technology to optimise\nthe evaluation of amplitudes in hadron collider simulations. We present the\nfull-colour virtual QCD corrections to diphoton-plus-jet production through\ngluon fusion, discussing the new techniques developed to calculate these\nnon-planar two-loop amplitudes. We use these amplitudes to compute the\nnext-to-leading QCD corrections to the differential cross sections of\ndiphoton-plus-jet production through gluon fusion at the Large Hadron Collider.\nWe also present the leading-colour double-virtual corrections to hadronic\ntrijet production. All derived amplitudes are made available in a public\nimplementation that is ready for further phenomenological application.",
    "descriptor": "\nComments: PhD thesis, 178 pages, many figures, partially based on arXiv:2106.08664, arXiv:2109.12003, and arXiv:2202.04506\n",
    "authors": [
      "Ryan Moodie"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07115"
  },
  {
    "id": "arXiv:2210.07132",
    "title": "Learning Multivariate CDFs and Copulas using Tensor Factorization",
    "abstract": "Learning the multivariate distribution of data is a core challenge in\nstatistics and machine learning. Traditional methods aim for the probability\ndensity function (PDF) and are limited by the curse of dimensionality. Modern\nneural methods are mostly based on black-box models, lacking identifiability\nguarantees. In this work, we aim to learn multivariate cumulative distribution\nfunctions (CDFs), as they can handle mixed random variables, allow efficient\nbox probability evaluation, and have the potential to overcome local sample\nscarcity owing to their cumulative nature. We show that any grid sampled\nversion of a joint CDF of mixed random variables admits a universal\nrepresentation as a naive Bayes model via the Canonical Polyadic (tensor-rank)\ndecomposition. By introducing a low-rank model, either directly in the raw data\ndomain, or indirectly in a transformed (Copula) domain, the resulting model\naffords efficient sampling, closed form inference and uncertainty\nquantification, and comes with uniqueness guarantees under relatively mild\nconditions. We demonstrate the superior performance of the proposed model in\nseveral synthetic and real datasets and applications including regression,\nsampling and data imputation. Interestingly, our experiments with real data\nshow that it is possible to obtain better density/mass estimates indirectly via\na low-rank CDF model, than a low-rank PDF/PMF model.",
    "descriptor": "",
    "authors": [
      "Magda Amiridi",
      "Nicholas D. Sidiropoulos"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.07132"
  },
  {
    "id": "arXiv:2210.07144",
    "title": "Reprogramming Large Pretrained Language Models for Antibody Sequence  Infilling",
    "abstract": "Antibodies comprise the most versatile class of binding molecules, with\nnumerous applications in biomedicine. Therapeutic antibody development requires\ndesigning novel and diverse sequences with improved properties, while\nmaintaining the structural consistency. Recently, deep language models and\ngraph neural networks have shown impressive success in antibody sequence\ngeneration. Since only a limited number of antibody structures are known,\ntraining a model using this limited data can lead to degraded performance,\nparticularly lacking diversity in the generated samples. To address such\nissues, we leverage the method of Model Reprogramming (MR), which focuses on\nrepurposing pretrained machine learning models for target domain tasks with\nscarce data, where it may be difficult to train a high-performing model from\nscratch. We introduce Reprogramming for Protein Sequence Infilling, a framework\nin which pretrained natural language models are repurposed for protein sequence\ninfilling via reprogramming, to infill protein sequence templates as a method\nof novel protein generation. For variable CDR sequence design, we formulate the\ntask as text infilling that uses the constant region of an antibody as the\nsequence template. Results on antibody design benchmarks show that our\nreprogrammed model on low resourced antibody sequence dataset provides highly\ndiverse CDR sequences, up to more than a two-fold increase of diversity over\nthe baselines, without losing structural integrity and naturalness. The\nperformance benefit of the reprogrammed model learning only from antibody\nsequences is more evident for longer CDR design or for multiple loop infilling\nat once, compared to existing graph-based models that require additional\nstructural information. The generated sequences also demonstrate enhanced\nantigen binding specificity or virus neutralization ability.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Igor Melnyk",
      "Vijil Chenthamarakshan",
      "Pin-Yu Chen",
      "Payel Das",
      "Amit Dhurandhar",
      "Inkit Padhi",
      "Devleena Das"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07144"
  },
  {
    "id": "arXiv:2210.07145",
    "title": "Accurate, reliable and interpretable solubility prediction of druglike  molecules with attention pooling and Bayesian learning",
    "abstract": "In drug discovery, aqueous solubility is an important pharmacokinetic\nproperty which affects absorption and assay availability of drug. Thus, in\nsilico prediction of solubility has been studied for its utility in virtual\nscreening and lead optimization. Recently, machine learning (ML) methods using\nexperimental data has been popular because physics-based methods like quantum\nmechanics and molecular dynamics are not suitable for high-throughput tasks due\nto its computational costs. However, ML method can exhibit over-fitting problem\nin a data-deficient condition, and this is the case for most chemical property\ndatasets. In addition, ML methods are regarded as a black box function in that\nit is difficult to interpret contribution of hidden features to outputs,\nhindering analysis and modification of structure-activity relationship. To deal\nwith mentioned issues, we developed Bayesian graph neural networks (GNNs) with\nthe self-attention readout layer. Unlike most GNNs using self-attention in node\nupdates, self-attention applied at readout layer enabled a model to improve\nprediction performance as well as to identify atom-wise importance, which can\nhelp lead optimization as exemplified for three FDA-approved drugs. Also,\nBayesian inference enables us to separate more or less accurate results\naccording to uncertainty in solubility prediction task We expect that our\naccurate, reliable and interpretable model can be used for more careful\ndecision-making and various applications in the development of drugs.",
    "descriptor": "",
    "authors": [
      "Seongok Ryu",
      "Sumin Lee"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07145"
  },
  {
    "id": "arXiv:2210.07152",
    "title": "Smooth Calibration, Leaky Forecasts, Finite Recall, and Nash Dynamics",
    "abstract": "We propose to smooth out the calibration score, which measures how good a\nforecaster is, by combining nearby forecasts. While regular calibration can be\nguaranteed only by randomized forecasting procedures, we show that smooth\ncalibration can be guaranteed by deterministic procedures. As a consequence, it\ndoes not matter if the forecasts are leaked, i.e., made known in advance:\nsmooth calibration can nevertheless be guaranteed (while regular calibration\ncannot). Moreover, our procedure has finite recall, is stationary, and all\nforecasts lie on a finite grid. To construct the procedure, we deal also with\nthe related setups of online linear regression and weak calibration. Finally,\nwe show that smooth calibration yields uncoupled finite-memory dynamics in\nn-person games \"smooth calibrated learning\" in which the players play\napproximate Nash equilibria in almost all periods (by contrast, calibrated\nlearning, which uses regular calibration, yields only that the time-averages of\nplay are approximate correlated equilibria).",
    "descriptor": "\nComments: this http URL\n",
    "authors": [
      "Dean P. Foster",
      "Sergiu Hart"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.07152"
  },
  {
    "id": "arXiv:2210.07169",
    "title": "Forecast Hedging and Calibration",
    "abstract": "Calibration means that forecasts and average realized frequencies are close.\nWe develop the concept of forecast hedging, which consists of choosing the\nforecasts so as to guarantee that the expected track record can only improve.\nThis yields all the calibration results by the same simple basic argument while\ndifferentiating between them by the forecast-hedging tools used: deterministic\nand fixed point based versus stochastic and minimax based. Additional\ncontributions are an improved definition of continuous calibration, ensuing\ngame dynamics that yield Nash equilibria in the long run, and a new calibrated\nforecasting procedure for binary events that is simpler than all known such\nprocedures.",
    "descriptor": "\nComments: this http URL\n",
    "authors": [
      "Dean P. Foster",
      "Sergiu Hart"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.07169"
  },
  {
    "id": "arXiv:2210.07194",
    "title": "Testing platform-independent quantum error mitigation on noisy quantum  computers",
    "abstract": "We apply quantum error mitigation techniques to a variety of benchmark\nproblems and quantum computers to evaluate the performance of quantum error\nmitigation in practice. To do so, we define an empirically motivated,\nresource-normalized metric of the improvement of error mitigation which we call\nthe improvement factor, and calculate this metric for each experiment we\nperform. The experiments we perform consist of zero-noise extrapolation and\nprobabilistic error cancellation applied to two benchmark problems run on IBM,\nIonQ, and Rigetti quantum computers, as well as noisy quantum computer\nsimulators. Our results show that error mitigation is on average more\nbeneficial than no error mitigation - even when normalized by the additional\nresources used - but also emphasize that the performance of quantum error\nmitigation depends on the underlying computer.",
    "descriptor": "",
    "authors": [
      "Vincent Russo",
      "Andrea Mari",
      "Nathan Shammah",
      "Ryan LaRose",
      "William J. Zeng"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.07194"
  },
  {
    "id": "arXiv:2210.07209",
    "title": "Computer-Aided Multi-Objective Optimization in Small Molecule Discovery",
    "abstract": "Molecular discovery is a multi-objective optimization problem that requires\nidentifying a molecule or set of molecules that balance multiple, often\ncompeting, properties. Multi-objective molecular design is commonly addressed\nby combining properties of interest into a single objective function using\nscalarization, which imposes assumptions about relative importance and uncovers\nlittle about the trade-offs between objectives. In contrast to scalarization,\nPareto optimization does not require knowledge of relative importance and\nreveals the trade-offs between objectives. However, it introduces additional\nconsiderations in algorithm design. In this review, we describe pool-based and\nde novo generative approaches to multi-objective molecular discovery with a\nfocus on Pareto optimization algorithms. We show how pool-based molecular\ndiscovery is a relatively direct extension of multi-objective Bayesian\noptimization and how the plethora of different generative models extend from\nsingle-objective to multi-objective optimization in similar ways using\nnon-dominated sorting in the reward function (reinforcement learning) or to\nselect molecules for retraining (distribution learning) or propagation (genetic\nalgorithms). Finally, we discuss some remaining challenges and opportunities in\nthe field, emphasizing the opportunity to adopt Bayesian optimization\ntechniques into multi-objective de novo design.",
    "descriptor": "",
    "authors": [
      "Jenna C. Fromer",
      "Connor W. Coley"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07209"
  },
  {
    "id": "arXiv:2210.07234",
    "title": "The Complexity of NISQ",
    "abstract": "The recent proliferation of NISQ devices has made it imperative to understand\ntheir computational power. In this work, we define and study the complexity\nclass $\\textsf{NISQ} $, which is intended to encapsulate problems that can be\nefficiently solved by a classical computer with access to a NISQ device. To\nmodel existing devices, we assume the device can (1) noisily initialize all\nqubits, (2) apply many noisy quantum gates, and (3) perform a noisy measurement\non all qubits. We first give evidence that $\\textsf{BPP}\\subsetneq\n\\textsf{NISQ}\\subsetneq \\textsf{BQP}$, by demonstrating super-polynomial oracle\nseparations among the three classes, based on modifications of Simon's problem.\nWe then consider the power of $\\textsf{NISQ}$ for three well-studied problems.\nFor unstructured search, we prove that $\\textsf{NISQ}$ cannot achieve a\nGrover-like quadratic speedup over $\\textsf{BPP}$. For the Bernstein-Vazirani\nproblem, we show that $\\textsf{NISQ}$ only needs a number of queries\nlogarithmic in what is required for $\\textsf{BPP}$. Finally, for a quantum\nstate learning problem, we prove that $\\textsf{NISQ}$ is exponentially weaker\nthan classical computation with access to noiseless constant-depth quantum\ncircuits.",
    "descriptor": "\nComments: 15+37 pages, 3 figures\n",
    "authors": [
      "Sitan Chen",
      "Jordan Cotler",
      "Hsin-Yuan Huang",
      "Jerry Li"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07234"
  },
  {
    "id": "arXiv:2210.07237",
    "title": "Forces are not Enough: Benchmark and Critical Evaluation for Machine  Learning Force Fields with Molecular Simulations",
    "abstract": "Molecular dynamics (MD) simulation techniques are widely used for various\nnatural science applications. Increasingly, machine learning (ML) force field\n(FF) models begin to replace ab-initio simulations by predicting forces\ndirectly from atomic structures. Despite significant progress in this area,\nsuch techniques are primarily benchmarked by their force/energy prediction\nerrors, even though the practical use case would be to produce realistic MD\ntrajectories. We aim to fill this gap by introducing a novel benchmark suite\nfor ML MD simulation. We curate representative MD systems, including water,\norganic molecules, peptide, and materials, and design evaluation metrics\ncorresponding to the scientific objectives of respective systems. We benchmark\na collection of state-of-the-art (SOTA) ML FF models and illustrate, in\nparticular, how the commonly benchmarked force accuracy is not well aligned\nwith relevant simulation metrics. We demonstrate when and how selected SOTA\nmethods fail, along with offering directions for further improvement.\nSpecifically, we identify stability as a key metric for ML models to improve.\nOur benchmark suite comes with a comprehensive open-source codebase for\ntraining and simulation with ML FFs to facilitate further work.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Xiang Fu",
      "Zhenghao Wu",
      "Wujie Wang",
      "Tian Xie",
      "Sinan Keten",
      "Rafael Gomez-Bombarelli",
      "Tommi Jaakkola"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.07237"
  },
  {
    "id": "arXiv:1712.10163",
    "title": "Estimation under group actions: recovering orbits from invariants",
    "abstract": "Comments: 80 pages. Significant revisions since the last version",
    "descriptor": "\nComments: 80 pages. Significant revisions since the last version\n",
    "authors": [
      "Afonso S. Bandeira",
      "Ben Blum-Smith",
      "Joe Kileel",
      "Amelia Perry",
      "Jonathan Weed",
      "Alexander S. Wein"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Data Structures and Algorithms (cs.DS)",
      "Information Theory (cs.IT)",
      "Commutative Algebra (math.AC)"
    ],
    "url": "https://arxiv.org/abs/1712.10163"
  },
  {
    "id": "arXiv:1803.01557",
    "title": "Automatic Translating between Ancient Chinese and Contemporary Chinese  with Limited Aligned Corpora",
    "abstract": "Comments: Accepted by NLPCC 2019",
    "descriptor": "\nComments: Accepted by NLPCC 2019\n",
    "authors": [
      "Zhiyuan Zhang",
      "Wei Li",
      "Qi Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/1803.01557"
  },
  {
    "id": "arXiv:1811.01394",
    "title": "A method to construct exponential families by representation theory",
    "abstract": "A method to construct exponential families by representation theory",
    "descriptor": "",
    "authors": [
      "Koichi Tojo",
      "Taro Yoshino"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/1811.01394"
  },
  {
    "id": "arXiv:1910.08322",
    "title": "A Multilabel Classification Framework for Approximate Nearest Neighbor  Search",
    "abstract": "Comments: To appear in the proceedings of Conference on Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: To appear in the proceedings of Conference on Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Ville Hyv\u00f6nen",
      "Elias J\u00e4\u00e4saari",
      "Teemu Roos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.08322"
  },
  {
    "id": "arXiv:2003.04315",
    "title": "LIMEADE: From AI Explanations to Advice Taking",
    "abstract": "Comments: 17 pages, 7 figures",
    "descriptor": "\nComments: 17 pages, 7 figures\n",
    "authors": [
      "Benjamin Charles Germain Lee",
      "Doug Downey",
      "Kyle Lo",
      "Daniel S. Weld"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2003.04315"
  },
  {
    "id": "arXiv:2006.11809",
    "title": "On the Theoretical Equivalence of Several Trade-Off Curves Assessing  Statistical Proximity",
    "abstract": "Comments: 32 pages, 3 figures",
    "descriptor": "\nComments: 32 pages, 3 figures\n",
    "authors": [
      "Rodrigue Siry",
      "Ryan Webster",
      "Loic Simon",
      "Julien Rabin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2006.11809"
  },
  {
    "id": "arXiv:2007.16162",
    "title": "Imitative Planning using Conditional Normalizing Flow",
    "abstract": "Imitative Planning using Conditional Normalizing Flow",
    "descriptor": "",
    "authors": [
      "Shubhankar Agarwal",
      "Harshit Sikchi",
      "Cole Gulino",
      "Eric Wilkinson",
      "Shivam Gautam"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2007.16162"
  },
  {
    "id": "arXiv:2008.11921",
    "title": "Unsupervised MRI Super-Resolution Using Deep External Learning and  Guided Residual Dense Network with Multimodal Image Priors",
    "abstract": "Comments: 10 pages, 3 figures, Accepted by IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI)",
    "descriptor": "\nComments: 10 pages, 3 figures, Accepted by IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI)\n",
    "authors": [
      "Yutaro Iwamoto",
      "Kyohei Takeda",
      "Yinhao Li",
      "Akihiko Shiino",
      "Yen-Wei Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2008.11921"
  },
  {
    "id": "arXiv:2010.01979",
    "title": "BayesAdapter: Being Bayesian, Inexpensively and Reliably, via Bayesian  Fine-tuning",
    "abstract": "Comments: 14th Asian Conference on Machine Learning (ACML 2022)",
    "descriptor": "\nComments: 14th Asian Conference on Machine Learning (ACML 2022)\n",
    "authors": [
      "Zhijie Deng",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.01979"
  },
  {
    "id": "arXiv:2012.08626",
    "title": "Computation Against a Neighbour: Addressing Large-Scale Distribution and  Adaptivity with Functional Programming and Scala",
    "abstract": "Comments: 59 pages, 17 figures, 1 table",
    "descriptor": "\nComments: 59 pages, 17 figures, 1 table\n",
    "authors": [
      "Giorgio Audrito",
      "Roberto Casadei",
      "Ferruccio Damiani",
      "Mirko Viroli"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2012.08626"
  },
  {
    "id": "arXiv:2012.12466",
    "title": "A Framework for Conditional Statement Technical Debt Identification and  Description",
    "abstract": "A Framework for Conditional Statement Technical Debt Identification and  Description",
    "descriptor": "",
    "authors": [
      "Abdulaziz Alhefdhi",
      "Hoa Khanh Dam",
      "Yusuf Sulistyo Nugroho",
      "Hideaki Hata",
      "Takashi Ishio",
      "Aditya Ghose"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2012.12466"
  },
  {
    "id": "arXiv:2101.00694",
    "title": "Solving Cut-Problems in Quadratic Time for Graphs With Bounded Treewidth",
    "abstract": "Solving Cut-Problems in Quadratic Time for Graphs With Bounded Treewidth",
    "descriptor": "",
    "authors": [
      "Hauke Brinkop",
      "Klaus Jansen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2101.00694"
  },
  {
    "id": "arXiv:2101.07220",
    "title": "A Tensor-Based Formulation of Hetero-functional Graph Theory",
    "abstract": "A Tensor-Based Formulation of Hetero-functional Graph Theory",
    "descriptor": "",
    "authors": [
      "Amro M. Farid",
      "Dakota Thompson",
      "Wester Schoonenberg"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2101.07220"
  },
  {
    "id": "arXiv:2102.06700",
    "title": "On the Paradox of Certified Training",
    "abstract": "Comments: Published in Transactions on Machine Learning Research (TMLR) 10/2022",
    "descriptor": "\nComments: Published in Transactions on Machine Learning Research (TMLR) 10/2022\n",
    "authors": [
      "Nikola Jovanovi\u0107",
      "Mislav Balunovi\u0107",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2102.06700"
  },
  {
    "id": "arXiv:2102.09413",
    "title": "Temporal Locality in Online Algorithms",
    "abstract": "Comments: 46 pages, 4 figures",
    "descriptor": "\nComments: 46 pages, 4 figures\n",
    "authors": [
      "Maciej Pacut",
      "Mahmoud Parham",
      "Joel Rybicki",
      "Stefan Schmid",
      "Jukka Suomela",
      "Aleksandr Tereshchenko"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2102.09413"
  },
  {
    "id": "arXiv:2102.09703",
    "title": "Near-Optimal Randomized Exploration for Tabular Markov Decision  Processes",
    "abstract": "Comments: 41 pages, 3 figures, Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: 41 pages, 3 figures, Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Zhihan Xiong",
      "Ruoqi Shen",
      "Qiwen Cui",
      "Maryam Fazel",
      "Simon S. Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.09703"
  },
  {
    "id": "arXiv:2102.10185",
    "title": "Cornus: Atomic Commit for a Cloud DBMS with Storage Disaggregation  (Extended Version)",
    "abstract": "Cornus: Atomic Commit for a Cloud DBMS with Storage Disaggregation  (Extended Version)",
    "descriptor": "",
    "authors": [
      "Zhihan Guo",
      "Xinyu Zeng",
      "Kan Wu",
      "Wuh-Chwen Hwang",
      "Ziwei Ren",
      "Xiangyao Yu",
      "Mahesh Balakrishnan",
      "Philip A. Bernstein"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2102.10185"
  },
  {
    "id": "arXiv:2104.13888",
    "title": "One-to-Two-Player Lifting for Mildly Growing Memory",
    "abstract": "Comments: Preprint submitted to Logical Methods in Computer Science",
    "descriptor": "\nComments: Preprint submitted to Logical Methods in Computer Science\n",
    "authors": [
      "Alexander Kozachinskiy"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2104.13888"
  },
  {
    "id": "arXiv:2104.14671",
    "title": "SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics",
    "abstract": "SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics",
    "descriptor": "",
    "authors": [
      "Toufique Ahmed",
      "Noah Rose Ledesma",
      "Premkumar Devanbu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.14671"
  },
  {
    "id": "arXiv:2105.05502",
    "title": "Probabilistic modeling of rational communication with conditionals",
    "abstract": "Probabilistic modeling of rational communication with conditionals",
    "descriptor": "",
    "authors": [
      "Britta Grusdt",
      "Daniel Lassiter",
      "Michael Franke"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.05502"
  },
  {
    "id": "arXiv:2105.14452",
    "title": "A unified logical framework for explanations in classifier systems",
    "abstract": "Comments: 37 pages",
    "descriptor": "\nComments: 37 pages\n",
    "authors": [
      "Xinghan Liu",
      "Emiliano Lorini"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2105.14452"
  },
  {
    "id": "arXiv:2105.15081",
    "title": "Optimal Spectral Recovery of a Planted Vector in a Subspace",
    "abstract": "Comments: 54 pages",
    "descriptor": "\nComments: 54 pages\n",
    "authors": [
      "Cheng Mao",
      "Alexander S. Wein"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.15081"
  },
  {
    "id": "arXiv:2106.03524",
    "title": "Theoretically Better and Numerically Faster Distributed Optimization  with Smoothness-Aware Quantization Techniques",
    "abstract": "Comments: To appear in NeurIPS 2022",
    "descriptor": "\nComments: To appear in NeurIPS 2022\n",
    "authors": [
      "Bokun Wang",
      "Mher Safaryan",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.03524"
  },
  {
    "id": "arXiv:2106.08928",
    "title": "RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent  Neural Networks",
    "abstract": "Comments: Published as a conference paper at NeurIPS 2022",
    "descriptor": "\nComments: Published as a conference paper at NeurIPS 2022\n",
    "authors": [
      "Leo Kozachkov",
      "Michaela Ennis",
      "Jean-Jacques Slotine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2106.08928"
  },
  {
    "id": "arXiv:2106.08970",
    "title": "Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks  Trained from Scratch",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Hossein Souri",
      "Liam Fowl",
      "Rama Chellappa",
      "Micah Goldblum",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.08970"
  },
  {
    "id": "arXiv:2106.09305",
    "title": "SCINet: Time Series Modeling and Forecasting with Sample Convolution and  Interaction",
    "abstract": "Comments: This paper presents a novel convolutional neural network for time series forecasting, achieving significant accuracy improvements",
    "descriptor": "\nComments: This paper presents a novel convolutional neural network for time series forecasting, achieving significant accuracy improvements\n",
    "authors": [
      "Minhao Liu",
      "Ailing Zeng",
      "Muxi Chen",
      "Zhijian Xu",
      "Qiuxia Lai",
      "Lingna Ma",
      "Qiang Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2106.09305"
  },
  {
    "id": "arXiv:2106.11180",
    "title": "Generalization Bounds with Minimal Dependency on Hypothesis Class via  Distributionally Robust Optimization",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Yibo Zeng",
      "Henry Lam"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2106.11180"
  },
  {
    "id": "arXiv:2106.15691",
    "title": "Deep Multiagent Reinforcement Learning: Challenges and Directions",
    "abstract": "Comments: 41 pages, 6 figures",
    "descriptor": "\nComments: 41 pages, 6 figures\n",
    "authors": [
      "Annie Wong",
      "Thomas B\u00e4ck",
      "Anna V. Kononova",
      "Aske Plaat"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2106.15691"
  },
  {
    "id": "arXiv:2107.00938",
    "title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ  Labs Instagram Page",
    "abstract": "Comments: 22 pages, 5 figures, 6 tables",
    "descriptor": "\nComments: 22 pages, 5 figures, 6 tables\n",
    "authors": [
      "Mathias-Felipe de-Lima-Santos",
      "Arwa Kooli"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Databases (cs.DB)",
      "Graphics (cs.GR)",
      "Multimedia (cs.MM)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2107.00938"
  },
  {
    "id": "arXiv:2108.03837",
    "title": "Online Minimax Multiobjective Optimization: Multicalibeating and Other  Applications",
    "abstract": "Comments: Appears in NeurIPS 2022",
    "descriptor": "\nComments: Appears in NeurIPS 2022\n",
    "authors": [
      "Daniel Lee",
      "Georgy Noarov",
      "Mallesh Pai",
      "Aaron Roth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2108.03837"
  },
  {
    "id": "arXiv:2108.07470",
    "title": "Three linear, unconditionally stable, second order decoupling methods  for the Allen--Cahn--Navier--Stokes phase field model",
    "abstract": "Comments: 24 pages, 7 figures, 4 tables",
    "descriptor": "\nComments: 24 pages, 7 figures, 4 tables\n",
    "authors": [
      "Ruonan Cao",
      "Nan Jiang",
      "Huanhuan Yang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2108.07470"
  },
  {
    "id": "arXiv:2108.07682",
    "title": "Fully Convolutional Networks for Panoptic Segmentation with Point-based  Supervision",
    "abstract": "Comments: Accepted to TPAMI. arXiv admin note: substantial text overlap with arXiv:2012.00720",
    "descriptor": "\nComments: Accepted to TPAMI. arXiv admin note: substantial text overlap with arXiv:2012.00720\n",
    "authors": [
      "Yanwei Li",
      "Hengshuang Zhao",
      "Xiaojuan Qi",
      "Yukang Chen",
      "Lu Qi",
      "Liwei Wang",
      "Zeming Li",
      "Jian Sun",
      "Jiaya Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.07682"
  },
  {
    "id": "arXiv:2108.08245",
    "title": "Observable Error Bounds of the Time-splitting Scheme for  Quantum-Classical Molecular Dynamics",
    "abstract": "Observable Error Bounds of the Time-splitting Scheme for  Quantum-Classical Molecular Dynamics",
    "descriptor": "",
    "authors": [
      "Di Fang",
      "Albert Tres"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2108.08245"
  },
  {
    "id": "arXiv:2109.08213",
    "title": "Reliable Neural Networks for Regression Uncertainty Estimation",
    "abstract": "Reliable Neural Networks for Regression Uncertainty Estimation",
    "descriptor": "",
    "authors": [
      "Tony Tohme",
      "Kevin Vanslette",
      "Kamal Youcef-Toumi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2109.08213"
  },
  {
    "id": "arXiv:2109.13335",
    "title": "Algorithms for matrix multiplication via sampling and opportunistic  matrix multiplication",
    "abstract": "Algorithms for matrix multiplication via sampling and opportunistic  matrix multiplication",
    "descriptor": "",
    "authors": [
      "David G. Harris"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2109.13335"
  },
  {
    "id": "arXiv:2109.15048",
    "title": "Scale-invariant Learning by Physics Inversion",
    "abstract": "Comments: NeurIPS 2022 version, appendix included",
    "descriptor": "\nComments: NeurIPS 2022 version, appendix included\n",
    "authors": [
      "Philipp Holl",
      "Vladlen Koltun",
      "Nils Thuerey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2109.15048"
  },
  {
    "id": "arXiv:2110.00809",
    "title": "Characterizing SARS-CoV-2 Spike Sequences Based on Geographical Location",
    "abstract": "Comments: Accepted at Journal of Computational Biology (JCB)",
    "descriptor": "\nComments: Accepted at Journal of Computational Biology (JCB)\n",
    "authors": [
      "Sarwan Ali",
      "Babatunde Bello",
      "Zahra Tayebi",
      "Murray Patterson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2110.00809"
  },
  {
    "id": "arXiv:2110.01212",
    "title": "Inducing Equilibria via Incentives: Simultaneous Design-and-Play Ensures  Global Convergence",
    "abstract": "Comments: Accepted by the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Accepted by the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Boyi Liu",
      "Jiayang Li",
      "Zhuoran Yang",
      "Hoi-To Wai",
      "Mingyi Hong",
      "Yu Marco Nie",
      "Zhaoran Wang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01212"
  },
  {
    "id": "arXiv:2110.02248",
    "title": "Contextual Combinatorial Bandits with Changing Action Sets via Gaussian  Processes",
    "abstract": "Comments: 34 pages, 7 figures",
    "descriptor": "\nComments: 34 pages, 7 figures\n",
    "authors": [
      "Andi Nika",
      "Sepehr Elahi",
      "Cem Tekin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.02248"
  },
  {
    "id": "arXiv:2110.03922",
    "title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel  Regression and Wide Neural Networks",
    "abstract": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel  Regression and Wide Neural Networks",
    "descriptor": "",
    "authors": [
      "James B. Simon",
      "Madeline Dickens",
      "Dhruva Karkada",
      "Michael R. DeWeese"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.03922"
  },
  {
    "id": "arXiv:2110.06506",
    "title": "New allocation rule of directed hypergraphs",
    "abstract": "Comments: 11 pages, 3 figures",
    "descriptor": "\nComments: 11 pages, 3 figures\n",
    "authors": [
      "Taiki Yamada",
      "Taisuke Matsubae"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Theoretical Economics (econ.TH)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2110.06506"
  },
  {
    "id": "arXiv:2111.08452",
    "title": "On minimizers and convolutional filters: a partial justification for the  effectiveness of CNNs in categorical sequence analysis",
    "abstract": "Comments: 13 pages, 2 figures, submitted to a conference",
    "descriptor": "\nComments: 13 pages, 2 figures, submitted to a conference\n",
    "authors": [
      "Yun William Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Genomics (q-bio.GN)"
    ],
    "url": "https://arxiv.org/abs/2111.08452"
  },
  {
    "id": "arXiv:2111.14210",
    "title": "Emergent Graphical Conventions in a Visual Communication Game",
    "abstract": "Emergent Graphical Conventions in a Visual Communication Game",
    "descriptor": "",
    "authors": [
      "Shuwen Qiu",
      "Sirui Xie",
      "Lifeng Fan",
      "Tao Gao",
      "Jungseock Joo",
      "Song-Chun Zhu",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.14210"
  },
  {
    "id": "arXiv:2112.00995",
    "title": "SwinTrack: A Simple and Strong Baseline for Transformer Tracking",
    "abstract": "Comments: 22 pages, 10 figures",
    "descriptor": "\nComments: 22 pages, 10 figures\n",
    "authors": [
      "Liting Lin",
      "Heng Fan",
      "Zhipeng Zhang",
      "Yong Xu",
      "Haibin Ling"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.00995"
  },
  {
    "id": "arXiv:2112.01641",
    "title": "Hamiltonian latent operators for content and motion disentanglement in  image sequences",
    "abstract": "Comments: Conference paper at NeurIPS 2022",
    "descriptor": "\nComments: Conference paper at NeurIPS 2022\n",
    "authors": [
      "Asif Khan",
      "Amos Storkey"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.01641"
  },
  {
    "id": "arXiv:2112.02271",
    "title": "Cooperation, Retaliation and Forgiveness in Revision Games",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Dong Hao",
      "Qi Shi",
      "Jinyan Su",
      "Bo An"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)",
      "Theoretical Economics (econ.TH)"
    ],
    "url": "https://arxiv.org/abs/2112.02271"
  },
  {
    "id": "arXiv:2112.06172",
    "title": "Temporal Interval Cliques and Independent Sets",
    "abstract": "Temporal Interval Cliques and Independent Sets",
    "descriptor": "",
    "authors": [
      "Danny Hermelin",
      "Yuval Itzhaki",
      "Hendrik Molter",
      "Rolf Niedermeier"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2112.06172"
  },
  {
    "id": "arXiv:2112.06324",
    "title": "Pool-Party: Exploiting Browser Resource Pools as Side-Channels for Web  Tracking",
    "abstract": "Pool-Party: Exploiting Browser Resource Pools as Side-Channels for Web  Tracking",
    "descriptor": "",
    "authors": [
      "Peter Snyder",
      "Soroush Karami",
      "Arthur Edelstein",
      "Benjamin Livshits",
      "Hamed Haddadi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2112.06324"
  },
  {
    "id": "arXiv:2112.07066",
    "title": "Continual Learning In Environments With Polynomial Mixing Times",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Matthew Riemer",
      "Sharath Chandra Raparthy",
      "Ignacio Cases",
      "Gopeshh Subbaraj",
      "Maximilian Puelma Touzel",
      "Irina Rish"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.07066"
  },
  {
    "id": "arXiv:2112.07812",
    "title": "Structure-Aware Image Segmentation with Homotopy Warping",
    "abstract": "Comments: 21 pages, 12 figures",
    "descriptor": "\nComments: 21 pages, 12 figures\n",
    "authors": [
      "Xiaoling Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2112.07812"
  },
  {
    "id": "arXiv:2112.07888",
    "title": "Event Linking: Grounding Event Mentions to Wikipedia",
    "abstract": "Comments: 9 pages, 9 tables, 1 figure",
    "descriptor": "\nComments: 9 pages, 9 tables, 1 figure\n",
    "authors": [
      "Xiaodong Yu",
      "Wenpeng Yin",
      "Nitish Gupta",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.07888"
  },
  {
    "id": "arXiv:2112.11628",
    "title": "SkipNode: On Alleviating Performance Degradation for Deep Graph  Convolutional Networks",
    "abstract": "Comments: Under Review",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Weigang Lu",
      "Yibing Zhan",
      "Binbin Lin",
      "Ziyu Guan",
      "Liu Liu",
      "Baosheng Yu",
      "Wei Zhao",
      "Yaming Yang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.11628"
  },
  {
    "id": "arXiv:2112.14337",
    "title": "Closer Look at the Transferability of Adversarial Examples: How They  Fool Different Models Differently",
    "abstract": "Comments: 25 pages, 13 figures, Accepted at the IEEE Winter Conference on Applications of Computer Vision, WACV 2023",
    "descriptor": "\nComments: 25 pages, 13 figures, Accepted at the IEEE Winter Conference on Applications of Computer Vision, WACV 2023\n",
    "authors": [
      "Futa Waseda",
      "Sosuke Nishikawa",
      "Trung-Nghia Le",
      "Huy H. Nguyen",
      "Isao Echizen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.14337"
  },
  {
    "id": "arXiv:2201.00649",
    "title": "SAE: Sequential Anchored Ensembles",
    "abstract": "Comments: 4 pages, NeurIPS 2021 Bayesian Deep Learning workshop",
    "descriptor": "\nComments: 4 pages, NeurIPS 2021 Bayesian Deep Learning workshop\n",
    "authors": [
      "Arnaud Delaunoy",
      "Gilles Louppe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.00649"
  },
  {
    "id": "arXiv:2201.03169",
    "title": "FedDTG:Federated Data-Free Knowledge Distillation via Three-Player  Generative Adversarial Networks",
    "abstract": "FedDTG:Federated Data-Free Knowledge Distillation via Three-Player  Generative Adversarial Networks",
    "descriptor": "",
    "authors": [
      "Zhenyuan Zhang",
      "Tao Shen",
      "Jie Zhang",
      "Tao Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.03169"
  },
  {
    "id": "arXiv:2201.04337",
    "title": "PromptBERT: Improving BERT Sentence Embeddings with Prompts",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ting Jiang",
      "Jian Jiao",
      "Shaohan Huang",
      "Zihan Zhang",
      "Deqing Wang",
      "Fuzhen Zhuang",
      "Furu Wei",
      "Haizhen Huang",
      "Denvy Deng",
      "Qi Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.04337"
  },
  {
    "id": "arXiv:2201.10983",
    "title": "Online POI Recommendation: Learning Dynamic Geo-Human Interactions in  Streams",
    "abstract": "Online POI Recommendation: Learning Dynamic Geo-Human Interactions in  Streams",
    "descriptor": "",
    "authors": [
      "Dongjie Wang",
      "Kunpeng Liu",
      "Hui Xiong",
      "Yanjie Fu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.10983"
  },
  {
    "id": "arXiv:2201.11793",
    "title": "Denoising Diffusion Restoration Models",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Bahjat Kawar",
      "Michael Elad",
      "Stefano Ermon",
      "Jiaming Song"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11793"
  },
  {
    "id": "arXiv:2201.12380",
    "title": "Explaining Graph Neural Networks with Structure-Aware Cooperative Games",
    "abstract": "Explaining Graph Neural Networks with Structure-Aware Cooperative Games",
    "descriptor": "",
    "authors": [
      "Shichang Zhang",
      "Yozen Liu",
      "Neil Shah",
      "Yizhou Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12380"
  },
  {
    "id": "arXiv:2201.13065",
    "title": "Rigidity Preserving Image Transformations and Equivariance in  Perspective",
    "abstract": "Comments: v2: Substantially revised version. Among other things, experiments with the PixLoc model added",
    "descriptor": "\nComments: v2: Substantially revised version. Among other things, experiments with the PixLoc model added\n",
    "authors": [
      "Lucas Brynte",
      "Georg B\u00f6kman",
      "Axel Flinth",
      "Fredrik Kahl"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13065"
  },
  {
    "id": "arXiv:2202.00661",
    "title": "When Do Flat Minima Optimizers Work?",
    "abstract": "When Do Flat Minima Optimizers Work?",
    "descriptor": "",
    "authors": [
      "Jean Kaddour",
      "Linqing Liu",
      "Ricardo Silva",
      "Matt J. Kusner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.00661"
  },
  {
    "id": "arXiv:2202.00932",
    "title": "Automatic Creation of Acceptance Tests by Extracting Conditionals from  Requirements: NLP Approach and Case Study",
    "abstract": "Automatic Creation of Acceptance Tests by Extracting Conditionals from  Requirements: NLP Approach and Case Study",
    "descriptor": "",
    "authors": [
      "Jannik Fischbach",
      "Julian Frattini",
      "Andreas Vogelsang",
      "Daniel Mendez",
      "Michael Unterkalmsteiner",
      "Andreas Wehrle",
      "Pablo Restrepo Henao",
      "Parisa Yousefi",
      "Tedi Juricic",
      "Jeannette Radduenz",
      "Carsten Wiecher"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2202.00932"
  },
  {
    "id": "arXiv:2202.01085",
    "title": "Giga-scale Kernel Matrix Vector Multiplication on GPU",
    "abstract": "Giga-scale Kernel Matrix Vector Multiplication on GPU",
    "descriptor": "",
    "authors": [
      "Robert Hu",
      "Siu Lun Chau",
      "Dino Sejdinovic",
      "Joan Alexis Glaun\u00e8s"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Mathematical Software (cs.MS)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2202.01085"
  },
  {
    "id": "arXiv:2202.01087",
    "title": "Communication Efficient Federated Learning for Generalized Linear  Bandits",
    "abstract": "Comments: 38 pages, 3 figures",
    "descriptor": "\nComments: 38 pages, 3 figures\n",
    "authors": [
      "Chuanhao Li",
      "Hongning Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.01087"
  },
  {
    "id": "arXiv:2202.01651",
    "title": "A Survey of Methods for Automated Algorithm Configuration",
    "abstract": "A Survey of Methods for Automated Algorithm Configuration",
    "descriptor": "",
    "authors": [
      "Elias Schede",
      "Jasmin Brandt",
      "Alexander Tornede",
      "Marcel Wever",
      "Viktor Bengs",
      "Eyke H\u00fcllermeier",
      "Kevin Tierney"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2202.01651"
  },
  {
    "id": "arXiv:2202.04589",
    "title": "Adjoint-aided inference of Gaussian process driven differential  equations",
    "abstract": "Comments: 18 pages, 8 figures",
    "descriptor": "\nComments: 18 pages, 8 figures\n",
    "authors": [
      "Paterne Gahungu",
      "Christopher W Lanyon",
      "Mauricio A Alvarez",
      "Engineer Bainomugisha",
      "Michael Smith",
      "Richard D. Wilkinson"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04589"
  },
  {
    "id": "arXiv:2202.04593",
    "title": "Stochastic Contextual Dueling Bandits under Linear Stochastic  Transitivity Models",
    "abstract": "Stochastic Contextual Dueling Bandits under Linear Stochastic  Transitivity Models",
    "descriptor": "",
    "authors": [
      "Viktor Bengs",
      "Aadirupa Saha",
      "Eyke H\u00fcllermeier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.04593"
  },
  {
    "id": "arXiv:2202.04598",
    "title": "Reproducibility in Optimization: Theoretical Framework and Limits",
    "abstract": "Comments: 45 Pages; Accepted to NeurIPS 2022",
    "descriptor": "\nComments: 45 Pages; Accepted to NeurIPS 2022\n",
    "authors": [
      "Kwangjun Ahn",
      "Prateek Jain",
      "Ziwei Ji",
      "Satyen Kale",
      "Praneeth Netrapalli",
      "Gil I. Shamir"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.04598"
  },
  {
    "id": "arXiv:2202.06690",
    "title": "ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers",
    "abstract": "ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers",
    "descriptor": "",
    "authors": [
      "Federico Ruggeri",
      "Mohsen Mesgar",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.06690"
  },
  {
    "id": "arXiv:2202.06727",
    "title": "STG-GAN: A spatiotemporal graph generative adversarial networks for  short-term passenger flow prediction in urban rail transit systems",
    "abstract": "Comments: 13 pages, 10 figures, 5 tables",
    "descriptor": "\nComments: 13 pages, 10 figures, 5 tables\n",
    "authors": [
      "Jinlei Zhang",
      "Hua Li",
      "Lixing Yang",
      "Guangyin Jin",
      "Jianguo Qi",
      "Ziyou Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2202.06727"
  },
  {
    "id": "arXiv:2202.06985",
    "title": "Deep Ensembles Work, But Are They Necessary?",
    "abstract": "Deep Ensembles Work, But Are They Necessary?",
    "descriptor": "",
    "authors": [
      "Taiga Abe",
      "E. Kelly Buchanan",
      "Geoff Pleiss",
      "Richard Zemel",
      "John P. Cunningham"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.06985"
  },
  {
    "id": "arXiv:2202.07932",
    "title": "On the Complexity of Scheduling Problems With a Fixed Number of Parallel  Identical Machines",
    "abstract": "Comments: A shorter version is to be published in SOFSEM 2023 Proceedings. Changes to previous versions are mainly improvements to readability. 33 pages, 1 figure",
    "descriptor": "\nComments: A shorter version is to be published in SOFSEM 2023 Proceedings. Changes to previous versions are mainly improvements to readability. 33 pages, 1 figure\n",
    "authors": [
      "Klaus Jansen",
      "Kai Kahler"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2202.07932"
  },
  {
    "id": "arXiv:2202.10638",
    "title": "Invariance Learning in Deep Neural Networks with Differentiable Laplace  Approximations",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Alexander Immer",
      "Tycho F.A. van der Ouderaa",
      "Gunnar R\u00e4tsch",
      "Vincent Fortuin",
      "Mark van der Wilk"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10638"
  },
  {
    "id": "arXiv:2202.12662",
    "title": "Validating Labelled State Transition and Message Production Systems: A  Theory for Modelling Faulty Distributed Systems",
    "abstract": "Comments: 28 pages",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Vlad Zamfir",
      "Mihai Calancea",
      "Denisa Diaconescu",
      "Wojciech Ko\u0142owski",
      "Brandon Moore",
      "Karl Palmskog",
      "Traian Florin \u015eerb\u0103nu\u0163\u0103",
      "Michael Stay",
      "Dafina Trufas",
      "Jan Tu\u0161il"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2202.12662"
  },
  {
    "id": "arXiv:2202.12921",
    "title": "Refining Self-Supervised Learning in Imaging: Beyond Linear Metric",
    "abstract": "Refining Self-Supervised Learning in Imaging: Beyond Linear Metric",
    "descriptor": "",
    "authors": [
      "Bo Jiang",
      "Hamid Krim",
      "Tianfu Wu",
      "Derya Cansever"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2202.12921"
  },
  {
    "id": "arXiv:2203.00314",
    "title": "VScript: Controllable Script Generation with Visual Presentation",
    "abstract": "VScript: Controllable Script Generation with Visual Presentation",
    "descriptor": "",
    "authors": [
      "Ziwei Ji",
      "Yan Xu",
      "I-Tsun Cheng",
      "Samuel Cahyawijaya",
      "Rita Frieske",
      "Etsuko Ishii",
      "Min Zeng",
      "Andrea Madotto",
      "Pascale Fung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.00314"
  },
  {
    "id": "arXiv:2203.01707",
    "title": "Testing Stationarity and Change Point Detection in Reinforcement  Learning",
    "abstract": "Testing Stationarity and Change Point Detection in Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Mengbing Li",
      "Chengchun Shi",
      "Zhenke Wu",
      "Piotr Fryzlewicz"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.01707"
  },
  {
    "id": "arXiv:2203.02018",
    "title": "Zero-shot Transfer Learning within a Heterogeneous Graph via Knowledge  Transfer Networks",
    "abstract": "Zero-shot Transfer Learning within a Heterogeneous Graph via Knowledge  Transfer Networks",
    "descriptor": "",
    "authors": [
      "Minji Yoon",
      "John Palowitch",
      "Dustin Zelle",
      "Ziniu Hu",
      "Ruslan Salakhutdinov",
      "Bryan Perozzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.02018"
  },
  {
    "id": "arXiv:2203.03959",
    "title": "Enhancing Door-Status Detection for Autonomous Mobile Robots during  Environment-Specific Operational Use",
    "abstract": "Comments: Preprint submitted for revision at ICRA 2023",
    "descriptor": "\nComments: Preprint submitted for revision at ICRA 2023\n",
    "authors": [
      "Michele Antonazzi",
      "Matteo Luperto",
      "Nicola Basilico",
      "N. Alberto Borghese"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.03959"
  },
  {
    "id": "arXiv:2203.06102",
    "title": "Pitfalls of Epistemic Uncertainty Quantification through Loss  Minimisation",
    "abstract": "Pitfalls of Epistemic Uncertainty Quantification through Loss  Minimisation",
    "descriptor": "",
    "authors": [
      "Viktor Bengs",
      "Eyke H\u00fcllermeier",
      "Willem Waegeman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.06102"
  },
  {
    "id": "arXiv:2203.07116",
    "title": "Deep Transformers Thirst for Comprehensive-Frequency Data",
    "abstract": "Comments: 12 pages, 13 figures",
    "descriptor": "\nComments: 12 pages, 13 figures\n",
    "authors": [
      "Rui Xia",
      "Chao Xue",
      "Boyu Deng",
      "Fang Wang",
      "Jingchao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.07116"
  },
  {
    "id": "arXiv:2203.09065",
    "title": "STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point  Cloud Dataset",
    "abstract": "STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point  Cloud Dataset",
    "descriptor": "",
    "authors": [
      "Meida Chen",
      "Zifan Yu",
      "Qingyong Hu",
      "Hugues Thomas",
      "Andrew Feng",
      "Yu Hou",
      "Kyle McCullough",
      "Fengbo Ren",
      "Lucio Soibelman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.09065"
  },
  {
    "id": "arXiv:2203.10190",
    "title": "Fair Federated Learning via Bounded Group Loss",
    "abstract": "Comments: 19 pages",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Shengyuan Hu",
      "Zhiwei Steven Wu",
      "Virginia Smith"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2203.10190"
  },
  {
    "id": "arXiv:2203.10379",
    "title": "Lazy Rearrangement Planning in Confined Spaces",
    "abstract": "Comments: Accepted to the 32nd International Conference on Automated Planning and Scheduling (ICAPS 2022)",
    "descriptor": "\nComments: Accepted to the 32nd International Conference on Automated Planning and Scheduling (ICAPS 2022)\n",
    "authors": [
      "Rui Wang",
      "Kai Gao",
      "Jingjin Yu",
      "Kostas Bekris"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.10379"
  },
  {
    "id": "arXiv:2203.12644",
    "title": "Linearizing Transformer with Key-Value Memory",
    "abstract": "Comments: EMNLP2022. The two authors contributed equally",
    "descriptor": "\nComments: EMNLP2022. The two authors contributed equally\n",
    "authors": [
      "Yizhe Zhang",
      "Deng Cai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.12644"
  },
  {
    "id": "arXiv:2203.14680",
    "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts  in the Vocabulary Space",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Mor Geva",
      "Avi Caciularu",
      "Kevin Ro Wang",
      "Yoav Goldberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.14680"
  },
  {
    "id": "arXiv:2204.00618",
    "title": "ASR data augmentation using cross-lingual multi-speaker TTS and  cross-lingual voice conversion",
    "abstract": "Comments: The paper is under consideration at the 48th IEEE International Conference on Acoustics, Speech, & Signal Processing (ICASSP)",
    "descriptor": "\nComments: The paper is under consideration at the 48th IEEE International Conference on Acoustics, Speech, & Signal Processing (ICASSP)\n",
    "authors": [
      "Edresson Casanova",
      "Christopher Shulby",
      "Alexander Korolev",
      "Arnaldo Candido Junior",
      "Anderson da Silva Soares",
      "Sandra Alu\u00edsio",
      "Moacir Antonelli Ponti"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2204.00618"
  },
  {
    "id": "arXiv:2204.00943",
    "title": "Efficient Convolutional Neural Networks on Raspberry Pi for Image  Classification",
    "abstract": "Efficient Convolutional Neural Networks on Raspberry Pi for Image  Classification",
    "descriptor": "",
    "authors": [
      "Rui-Yang Ju",
      "Jen-Shiun Chiang",
      "Jia-Hao Jian",
      "Ting-Yu Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.00943"
  },
  {
    "id": "arXiv:2204.01105",
    "title": "Better Lattice Quantizers Constructed from Complex Integers",
    "abstract": "Comments: To appear in IEEE Transactions on Communications. 10 pages",
    "descriptor": "\nComments: To appear in IEEE Transactions on Communications. 10 pages\n",
    "authors": [
      "Shanxiang Lyu",
      "Zheng Wang",
      "Cong Ling",
      "Hao Chen"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2204.01105"
  },
  {
    "id": "arXiv:2204.01499",
    "title": "FedRecAttack: Model Poisoning Attack to Federated Recommendation",
    "abstract": "Comments: This paper has been accepted by IEEE International Conference on Data Engineering 2022 (Second Research Round)",
    "descriptor": "\nComments: This paper has been accepted by IEEE International Conference on Data Engineering 2022 (Second Research Round)\n",
    "authors": [
      "Dazhong Rong",
      "Shuai Ye",
      "Ruoyan Zhao",
      "Hon Ning Yuen",
      "Jianhai Chen",
      "Qinming He"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.01499"
  },
  {
    "id": "arXiv:2204.04078",
    "title": "General Incremental Learning with Domain-aware Categorical  Representations",
    "abstract": "Comments: Accepted by CVPR2022",
    "descriptor": "\nComments: Accepted by CVPR2022\n",
    "authors": [
      "Jiangwei Xie",
      "Shipeng Yan",
      "Xuming He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.04078"
  },
  {
    "id": "arXiv:2204.05781",
    "title": "Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis  of BERT Classifiers and Weak Supervision",
    "abstract": "Comments: 29 pages",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Duygu Ider",
      "Stefan Lessmann"
    ],
    "subjectives": [
      "Statistical Finance (q-fin.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.05781"
  },
  {
    "id": "arXiv:2204.06386",
    "title": "Efficient Deep Neural Network Accelerator Using Controlled Ferroelectric  Domain Dynamics",
    "abstract": "Efficient Deep Neural Network Accelerator Using Controlled Ferroelectric  Domain Dynamics",
    "descriptor": "",
    "authors": [
      "Sayani Majumdar"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2204.06386"
  },
  {
    "id": "arXiv:2204.06674",
    "title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text  Generation",
    "abstract": "Comments: Accepted as a Main Conference Long paper at COLING 2022",
    "descriptor": "\nComments: Accepted as a Main Conference Long paper at COLING 2022\n",
    "authors": [
      "Anthony Colas",
      "Mehrdad Alvandipour",
      "Daisy Zhe Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.06674"
  },
  {
    "id": "arXiv:2204.11071",
    "title": "Intelligent Reflecting Surface Enabled Sensing: Cram\u00e9r-Rao Lower Bound  Optimization",
    "abstract": "Comments: To be appear in 2022 IEEE Globecom Workshop; 6 pages, 3 figures",
    "descriptor": "\nComments: To be appear in 2022 IEEE Globecom Workshop; 6 pages, 3 figures\n",
    "authors": [
      "Xianxin Song",
      "Jie Xu",
      "Fan Liu",
      "Tony Xiao Han",
      "Yonina C. Eldar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2204.11071"
  },
  {
    "id": "arXiv:2204.12130",
    "title": "LM-Debugger: An Interactive Tool for Inspection and Intervention in  Transformer-Based Language Models",
    "abstract": "Comments: EMNLP 2022 System Demonstrations",
    "descriptor": "\nComments: EMNLP 2022 System Demonstrations\n",
    "authors": [
      "Mor Geva",
      "Avi Caciularu",
      "Guy Dar",
      "Paul Roit",
      "Shoval Sadde",
      "Micah Shlain",
      "Bar Tamir",
      "Yoav Goldberg"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.12130"
  },
  {
    "id": "arXiv:2204.12484",
    "title": "ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation",
    "abstract": "Comments: Neurips 2022. 81.1 mAP on MS COCO Keypoint Detection test-dev set. V2: Update Multi-task training results: 92.8 AP on OCHuman, 78.3 AP on CrowdPose, 94.3 PCKh on MPII, and 43.2 AP on AI Challenger",
    "descriptor": "\nComments: Neurips 2022. 81.1 mAP on MS COCO Keypoint Detection test-dev set. V2: Update Multi-task training results: 92.8 AP on OCHuman, 78.3 AP on CrowdPose, 94.3 PCKh on MPII, and 43.2 AP on AI Challenger\n",
    "authors": [
      "Yufei Xu",
      "Jing Zhang",
      "Qiming Zhang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.12484"
  },
  {
    "id": "arXiv:2204.13806",
    "title": "Practical Considerations in Direct Detection Under Tukey Signalling",
    "abstract": "Comments: Submitted to J. Lightwave Techn. on March 3rd, 2022, revised on October 5th, 2022",
    "descriptor": "\nComments: Submitted to J. Lightwave Techn. on March 3rd, 2022, revised on October 5th, 2022\n",
    "authors": [
      "Amir Tasbihi",
      "Frank R. Kschischang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2204.13806"
  },
  {
    "id": "arXiv:2205.00267",
    "title": "Probing Cross-Lingual Lexical Knowledge from Multilingual Sentence  Encoders",
    "abstract": "Probing Cross-Lingual Lexical Knowledge from Multilingual Sentence  Encoders",
    "descriptor": "",
    "authors": [
      "Ivan Vuli\u0107",
      "Goran Glava\u0161",
      "Fangyu Liu",
      "Nigel Collier",
      "Edoardo Maria Ponti",
      "Anna Korhonen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.00267"
  },
  {
    "id": "arXiv:2205.02613",
    "title": "Exploiting Global and Local Hierarchies for Hierarchical Text  Classification",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ting Jiang",
      "Deqing Wang",
      "Leilei Sun",
      "Zhongzhi Chen",
      "Fuzhen Zhuang",
      "Qinghong Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.02613"
  },
  {
    "id": "arXiv:2205.03076",
    "title": "Beyond backpropagation: implicit gradients for bilevel optimization",
    "abstract": "Beyond backpropagation: implicit gradients for bilevel optimization",
    "descriptor": "",
    "authors": [
      "Nicolas Zucchet",
      "Jo\u00e3o Sacramento"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.03076"
  },
  {
    "id": "arXiv:2205.03401",
    "title": "The Unreliability of Explanations in Few-shot Prompting for Textual  Reasoning",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Xi Ye",
      "Greg Durrett"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.03401"
  },
  {
    "id": "arXiv:2205.03627",
    "title": "Sparse Regularized Correlation Filter for UAV Object Tracking with  adaptive Contextual Learning and Keyfilter Selection",
    "abstract": "Sparse Regularized Correlation Filter for UAV Object Tracking with  adaptive Contextual Learning and Keyfilter Selection",
    "descriptor": "",
    "authors": [
      "Zhangjian Ji",
      "Kai Feng",
      "Yuhua Qian",
      "Jiye Liang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.03627"
  },
  {
    "id": "arXiv:2205.04892",
    "title": "GRU-TV: Time- and velocity-aware GRU for patient representation on  multivariate clinical time-series data",
    "abstract": "GRU-TV: Time- and velocity-aware GRU for patient representation on  multivariate clinical time-series data",
    "descriptor": "",
    "authors": [
      "Ningtao Liu",
      "Ruoxi Gao",
      "Jing Yuan",
      "Calire Park",
      "Shuwei Xing",
      "Shuiping Gou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.04892"
  },
  {
    "id": "arXiv:2205.05040",
    "title": "A Communication-Efficient Distributed Gradient Clipping Algorithm for  Training Deep Neural Networks",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Mingrui Liu",
      "Zhenxun Zhuang",
      "Yunwei Lei",
      "Chunyang Liao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.05040"
  },
  {
    "id": "arXiv:2205.06655",
    "title": "Unified Modeling of Multi-Domain Multi-Device ASR Systems",
    "abstract": "Comments: We will update the paper completely with our latest experiments and analysis",
    "descriptor": "\nComments: We will update the paper completely with our latest experiments and analysis\n",
    "authors": [
      "Soumyajit Mitra",
      "Swayambhu Nath Ray",
      "Bharat Padi",
      "Arunasish Sen",
      "Raghavendra Bilgi",
      "Harish Arsikere",
      "Shalini Ghosh",
      "Ajay Srinivasamurthy",
      "Sri Garimella"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2205.06655"
  },
  {
    "id": "arXiv:2205.08387",
    "title": "rom.js/cfd.xyz: An open-source framework for generating and visualizing  parametric CFD results",
    "abstract": "Comments: This article has been published in the OpenFOAM Journal",
    "descriptor": "\nComments: This article has been published in the OpenFOAM Journal\n",
    "authors": [
      "Carlos Pe\u00f1a-Monferrer",
      "Carmen D\u00edaz-Mar\u00edn"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.08387"
  },
  {
    "id": "arXiv:2205.09148",
    "title": "DDXPlus: A New Dataset For Automatic Medical Diagnosis",
    "abstract": "Comments: Camera ready. NeurIPS 2022 Datasets and Benchmarks Track",
    "descriptor": "\nComments: Camera ready. NeurIPS 2022 Datasets and Benchmarks Track\n",
    "authors": [
      "Arsene Fansi Tchango",
      "Rishab Goel",
      "Zhi Wen",
      "Julien Martel",
      "Joumana Ghosn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09148"
  },
  {
    "id": "arXiv:2205.09459",
    "title": "Neural Network Architecture Beyond Width and Depth",
    "abstract": "Neural Network Architecture Beyond Width and Depth",
    "descriptor": "",
    "authors": [
      "Zuowei Shen",
      "Haizhao Yang",
      "Shijun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09459"
  },
  {
    "id": "arXiv:2205.09727",
    "title": "The Franz-Parisi Criterion and Computational Trade-offs in High  Dimensional Statistics",
    "abstract": "Comments: 52 pages, 1 figure",
    "descriptor": "\nComments: 52 pages, 1 figure\n",
    "authors": [
      "Afonso S. Bandeira",
      "Ahmed El Alaoui",
      "Samuel B. Hopkins",
      "Tselil Schramm",
      "Alexander S. Wein",
      "Ilias Zadik"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09727"
  },
  {
    "id": "arXiv:2205.09853",
    "title": "MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and  Interpolation",
    "abstract": "Comments: NeurIPS 2022 ; 10 pages, 4 figures, 7 tables",
    "descriptor": "\nComments: NeurIPS 2022 ; 10 pages, 4 figures, 7 tables\n",
    "authors": [
      "Vikram Voleti",
      "Alexia Jolicoeur-Martineau",
      "Christopher Pal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09853"
  },
  {
    "id": "arXiv:2205.09921",
    "title": "KERPLE: Kernelized Relative Positional Embedding for Length  Extrapolation",
    "abstract": "Comments: Accepted at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). The first two authors contributed equally to this work",
    "descriptor": "\nComments: Accepted at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). The first two authors contributed equally to this work\n",
    "authors": [
      "Ta-Chung Chi",
      "Ting-Han Fan",
      "Peter J. Ramadge",
      "Alexander I. Rudnicky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09921"
  },
  {
    "id": "arXiv:2205.10093",
    "title": "Visual Concepts Tokenization",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Tao Yang",
      "Yuwang Wang",
      "Yan Lu",
      "Nanning Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10093"
  },
  {
    "id": "arXiv:2205.10747",
    "title": "Language Models with Image Descriptors are Strong Few-Shot  Video-Language Learners",
    "abstract": "Language Models with Image Descriptors are Strong Few-Shot  Video-Language Learners",
    "descriptor": "",
    "authors": [
      "Zhenhailong Wang",
      "Manling Li",
      "Ruochen Xu",
      "Luowei Zhou",
      "Jie Lei",
      "Xudong Lin",
      "Shuohang Wang",
      "Ziyi Yang",
      "Chenguang Zhu",
      "Derek Hoiem",
      "Shih-Fu Chang",
      "Mohit Bansal",
      "Heng Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10747"
  },
  {
    "id": "arXiv:2205.10868",
    "title": "Memory-efficient Reinforcement Learning with Knowledge Consolidation",
    "abstract": "Memory-efficient Reinforcement Learning with Knowledge Consolidation",
    "descriptor": "",
    "authors": [
      "Qingfeng Lan",
      "Yangchen Pan",
      "Jun Luo",
      "A. Rupam Mahmood"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10868"
  },
  {
    "id": "arXiv:2205.11107",
    "title": "Learning to branch with Tree MDPs",
    "abstract": "Comments: 10 pages, 2 figures, plus supplementary material",
    "descriptor": "\nComments: 10 pages, 2 figures, plus supplementary material\n",
    "authors": [
      "Lara Scavuzzo",
      "Feng Yang Chen",
      "Didier Ch\u00e9telat",
      "Maxime Gasse",
      "Andrea Lodi",
      "Neil Yorke-Smith",
      "Karen Aardal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.11107"
  },
  {
    "id": "arXiv:2205.11490",
    "title": "Local Byte Fusion for Neural Machine Translation",
    "abstract": "Local Byte Fusion for Neural Machine Translation",
    "descriptor": "",
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Xiangpeng Wan",
      "Yu Cheng",
      "Junjie Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11490"
  },
  {
    "id": "arXiv:2205.11558",
    "title": "Using Natural Language and Program Abstractions to Instill Human  Inductive Biases in Machines",
    "abstract": "Comments: In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Sreejan Kumar",
      "Carlos G. Correa",
      "Ishita Dasgupta",
      "Raja Marjieh",
      "Michael Y. Hu",
      "Robert D. Hawkins",
      "Nathaniel D. Daw",
      "Jonathan D. Cohen",
      "Karthik Narasimhan",
      "Thomas L. Griffiths"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11558"
  },
  {
    "id": "arXiv:2205.12006",
    "title": "Neur2SP: Neural Two-Stage Stochastic Programming",
    "abstract": "Comments: To appear in the proceedings of NeurIPS 2022",
    "descriptor": "\nComments: To appear in the proceedings of NeurIPS 2022\n",
    "authors": [
      "Justin Dumouchelle",
      "Rahul Patel",
      "Elias B. Khalil",
      "Merve Bodur"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12006"
  },
  {
    "id": "arXiv:2205.12394",
    "title": "MaskEval: Weighted MLM-Based Evaluation for Text Summarization and  Simplification",
    "abstract": "MaskEval: Weighted MLM-Based Evaluation for Text Summarization and  Simplification",
    "descriptor": "",
    "authors": [
      "Yu Lu Liu",
      "Rachel Bawden",
      "Thomas Scialom",
      "Beno\u00eet Sagot",
      "Jackie Chi Kit Cheung"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12394"
  },
  {
    "id": "arXiv:2205.12399",
    "title": "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT",
    "abstract": "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT",
    "descriptor": "",
    "authors": [
      "James Lee-Thorp",
      "Joshua Ainslie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12399"
  },
  {
    "id": "arXiv:2205.12554",
    "title": "Helpfulness and Fairness of Task-Oriented Dialogue Systems",
    "abstract": "Comments: 16 pages, 5 figures and 8 tables",
    "descriptor": "\nComments: 16 pages, 5 figures and 8 tables\n",
    "authors": [
      "Jiao Sun",
      "Yu Hou",
      "Jiin Kim",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12554"
  },
  {
    "id": "arXiv:2205.12586",
    "title": "Perturbation Augmentation for Fairer NLP",
    "abstract": "Perturbation Augmentation for Fairer NLP",
    "descriptor": "",
    "authors": [
      "Rebecca Qian",
      "Candace Ross",
      "Jude Fernandes",
      "Eric Smith",
      "Douwe Kiela",
      "Adina Williams"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12586"
  },
  {
    "id": "arXiv:2205.13371",
    "title": "A Rotated Hyperbolic Wrapped Normal Distribution for Hierarchical  Representation Learning",
    "abstract": "Comments: 23 pages, Thirty-sixth Conference on Neural Information Processing Systems, 2022",
    "descriptor": "\nComments: 23 pages, Thirty-sixth Conference on Neural Information Processing Systems, 2022\n",
    "authors": [
      "Seunghyuk Cho",
      "Juyong Lee",
      "Jaesik Park",
      "Dongwoo Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13371"
  },
  {
    "id": "arXiv:2205.13574",
    "title": "Pruning has a disparate impact on model accuracy",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Cuong Tran",
      "Ferdinando Fioretto",
      "Jung-Eun Kim",
      "Rakshit Naidu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13574"
  },
  {
    "id": "arXiv:2205.13699",
    "title": "Maximum Likelihood Training of Implicit Nonlinear Diffusion Models",
    "abstract": "Maximum Likelihood Training of Implicit Nonlinear Diffusion Models",
    "descriptor": "",
    "authors": [
      "Dongjun Kim",
      "Byeonghu Na",
      "Se Jung Kwon",
      "Dongsoo Lee",
      "Wanmo Kang",
      "Il-Chul Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13699"
  },
  {
    "id": "arXiv:2205.14240",
    "title": "Deterministic Langevin Monte Carlo with Normalizing Flows for Bayesian  Inference",
    "abstract": "Comments: 17 pages, 9 figures, Accepted at NeurIPS 2022",
    "descriptor": "\nComments: 17 pages, 9 figures, Accepted at NeurIPS 2022\n",
    "authors": [
      "Richard D.P. Grumitt",
      "Biwei Dai",
      "Uros Seljak"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.14240"
  },
  {
    "id": "arXiv:2205.14764",
    "title": "6N-DoF Pose Tracking for Tensegrity Robots",
    "abstract": "6N-DoF Pose Tracking for Tensegrity Robots",
    "descriptor": "",
    "authors": [
      "Shiyang Lu",
      "William R. Johnson III",
      "Kun Wang",
      "Xiaonan Huang",
      "Joran Booth",
      "Rebecca Kramer-Bottiglio",
      "Kostas Bekris"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14764"
  },
  {
    "id": "arXiv:2205.14819",
    "title": "Universality of Group Convolutional Neural Networks Based on Ridgelet  Analysis on Groups",
    "abstract": "Comments: replaced with the published version (NeurIPS2022)",
    "descriptor": "\nComments: replaced with the published version (NeurIPS2022)\n",
    "authors": [
      "Sho Sonoda",
      "Isao Ishikawa",
      "Masahiro Ikeda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Representation Theory (math.RT)"
    ],
    "url": "https://arxiv.org/abs/2205.14819"
  },
  {
    "id": "arXiv:2205.15156",
    "title": "Towards Efficient 3D Object Detection with Knowledge Distillation",
    "abstract": "Towards Efficient 3D Object Detection with Knowledge Distillation",
    "descriptor": "",
    "authors": [
      "Jihan Yang",
      "Shaoshuai Shi",
      "Runyu Ding",
      "Zhe Wang",
      "Xiaojuan Qi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15156"
  },
  {
    "id": "arXiv:2205.15235",
    "title": "Non-convex online learning via algorithmic equivalence",
    "abstract": "Non-convex online learning via algorithmic equivalence",
    "descriptor": "",
    "authors": [
      "Udaya Ghai",
      "Zhou Lu",
      "Elad Hazan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.15235"
  },
  {
    "id": "arXiv:2205.15580",
    "title": "A Computation and Communication Efficient Method for Distributed  Nonconvex Problems in the Partial Participation Setting",
    "abstract": "A Computation and Communication Efficient Method for Distributed  Nonconvex Problems in the Partial Participation Setting",
    "descriptor": "",
    "authors": [
      "Alexander Tyurin",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.15580"
  },
  {
    "id": "arXiv:2205.15891",
    "title": "One Policy is Enough: Parallel Exploration with a Single Policy is  Near-Optimal for Reward-Free Reinforcement Learning",
    "abstract": "One Policy is Enough: Parallel Exploration with a Single Policy is  Near-Optimal for Reward-Free Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Pedro Cisneros-Velarde",
      "Boxiang Lyu",
      "Sanmi Koyejo",
      "Mladen Kolar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15891"
  },
  {
    "id": "arXiv:2206.00024",
    "title": "Online PAC-Bayes Learning",
    "abstract": "Comments: 21 pages",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Maxime Haddouche",
      "Benjamin Guedj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00024"
  },
  {
    "id": "arXiv:2206.00241",
    "title": "Asymptotic Properties for Bayesian Neural Network in Besov Space",
    "abstract": "Asymptotic Properties for Bayesian Neural Network in Besov Space",
    "descriptor": "",
    "authors": [
      "Kyeongwon Lee",
      "Jaeyong Lee"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.00241"
  },
  {
    "id": "arXiv:2206.00481",
    "title": "Where are my Neighbors? Exploiting Patches Relations in Self-Supervised  Vision Transformer",
    "abstract": "Comments: Accepted to BMVC 2022",
    "descriptor": "\nComments: Accepted to BMVC 2022\n",
    "authors": [
      "Guglielmo Camporese",
      "Elena Izzo",
      "Lamberto Ballan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00481"
  },
  {
    "id": "arXiv:2206.00630",
    "title": "Unifying Voxel-based Representation with Transformer for 3D Object  Detection",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Yanwei Li",
      "Yilun Chen",
      "Xiaojuan Qi",
      "Zeming Li",
      "Jian Sun",
      "Jiaya Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00630"
  },
  {
    "id": "arXiv:2206.01323",
    "title": "SPD domain-specific batch normalization to crack interpretable  unsupervised domain adaptation in EEG",
    "abstract": "Comments: 10 pages, accepted at NeurIPS 2022",
    "descriptor": "\nComments: 10 pages, accepted at NeurIPS 2022\n",
    "authors": [
      "Reinmar J Kobler",
      "Jun-ichiro Hirayama",
      "Qibin Zhao",
      "Motoaki Kawanabe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.01323"
  },
  {
    "id": "arXiv:2206.01670",
    "title": "Egocentric Video-Language Pretraining",
    "abstract": "Comments: Accepted by NeurIPS 2022. Double champions at Ego4D and EPIC-Kitchens, CVPR 2022 challenges. 23 pages, 13 figures, 12 tables. Code: this https URL",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. Double champions at Ego4D and EPIC-Kitchens, CVPR 2022 challenges. 23 pages, 13 figures, 12 tables. Code: this https URL\n",
    "authors": [
      "Kevin Qinghong Lin",
      "Alex Jinpeng Wang",
      "Mattia Soldan",
      "Michael Wray",
      "Rui Yan",
      "Eric Zhongcong Xu",
      "Difei Gao",
      "Rongcheng Tu",
      "Wenzhe Zhao",
      "Weijie Kong",
      "Chengfei Cai",
      "Hongfa Wang",
      "Dima Damen",
      "Bernard Ghanem",
      "Wei Liu",
      "Mike Zheng Shou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.01670"
  },
  {
    "id": "arXiv:2206.02136",
    "title": "LDRNet: Enabling Real-time Document Localization on Mobile Devices",
    "abstract": "Comments: In the proceedings of ECML-PKDD 2022 Workshop on IoT, Edge, and Mobile for Embedded Machine Learning (ITEM)",
    "descriptor": "\nComments: In the proceedings of ECML-PKDD 2022 Workshop on IoT, Edge, and Mobile for Embedded Machine Learning (ITEM)\n",
    "authors": [
      "Han Wu",
      "Holland Qian",
      "Huaming Wu",
      "Aad van Moorsel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2206.02136"
  },
  {
    "id": "arXiv:2206.02928",
    "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
    "abstract": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
    "descriptor": "",
    "authors": [
      "Yujie Lu",
      "Weixi Feng",
      "Wanrong Zhu",
      "Wenda Xu",
      "Xin Eric Wang",
      "Miguel Eckstein",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02928"
  },
  {
    "id": "arXiv:2206.03262",
    "title": "Using sensitive data to prevent discrimination by artificial  intelligence: Does the GDPR need a new exception?",
    "abstract": "Using sensitive data to prevent discrimination by artificial  intelligence: Does the GDPR need a new exception?",
    "descriptor": "",
    "authors": [
      "Marvin van Bekkum",
      "Frederik Zuiderveen Borgesius"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.03262"
  },
  {
    "id": "arXiv:2206.03655",
    "title": "pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning",
    "abstract": "Comments: Accepted by NeurIPS 2022, Datasets and Benchmarks track",
    "descriptor": "\nComments: Accepted by NeurIPS 2022, Datasets and Benchmarks track\n",
    "authors": [
      "Daoyuan Chen",
      "Dawei Gao",
      "Weirui Kuang",
      "Yaliang Li",
      "Bolin Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03655"
  },
  {
    "id": "arXiv:2206.03931",
    "title": "Learning to Generate Prompts for Dialogue Generation through  Reinforcement Learning",
    "abstract": "Learning to Generate Prompts for Dialogue Generation through  Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Hsuan Su",
      "Pohan Chi",
      "Shih-Cheng Huang",
      "Chung Ho Lam",
      "Saurav Sahay",
      "Shang-Tse Chen",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03931"
  },
  {
    "id": "arXiv:2206.04670",
    "title": "PointNeXt: Revisiting PointNet++ with Improved Training and Scaling  Strategies",
    "abstract": "Comments: Accepted by NeurIPS'22. Code and models are available at this https URL",
    "descriptor": "\nComments: Accepted by NeurIPS'22. Code and models are available at this https URL\n",
    "authors": [
      "Guocheng Qian",
      "Yuchen Li",
      "Houwen Peng",
      "Jinjie Mai",
      "Hasan Abed Al Kader Hammoud",
      "Mohamed Elhoseiny",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04670"
  },
  {
    "id": "arXiv:2206.04754",
    "title": "AFIA: ATPG-Guided Fault Injection Attack on Secure Logic Locking",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2007.10512",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2007.10512\n",
    "authors": [
      "Yadi Zhong",
      "Ayush Jain",
      "M. Tanjidur Rahman",
      "Navid Asadizanjani",
      "Jiafeng Xie",
      "Ujjwal Guin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.04754"
  },
  {
    "id": "arXiv:2206.04835",
    "title": "Communication Efficient Distributed Learning for Kernelized Contextual  Bandits",
    "abstract": "Comments: 30 pages, 3 figures",
    "descriptor": "\nComments: 30 pages, 3 figures\n",
    "authors": [
      "Chuanhao Li",
      "Huazheng Wang",
      "Mengdi Wang",
      "Hongning Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04835"
  },
  {
    "id": "arXiv:2206.05266",
    "title": "Does Self-supervised Learning Really Improve Reinforcement Learning from  Pixels?",
    "abstract": "Comments: To appear at NeurIPS 2022. Code for ELo-SACv3 is at this https URL and code for ELo-Rainbow is at this https URL",
    "descriptor": "\nComments: To appear at NeurIPS 2022. Code for ELo-SACv3 is at this https URL and code for ELo-Rainbow is at this https URL\n",
    "authors": [
      "Xiang Li",
      "Jinghuan Shang",
      "Srijan Das",
      "Michael S. Ryoo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.05266"
  },
  {
    "id": "arXiv:2206.05608",
    "title": "Gradient Boosting Performs Gaussian Process Inference",
    "abstract": "Gradient Boosting Performs Gaussian Process Inference",
    "descriptor": "",
    "authors": [
      "Aleksei Ustimenko",
      "Artem Beliakov",
      "Liudmila Prokhorenkova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05608"
  },
  {
    "id": "arXiv:2206.05683",
    "title": "APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking",
    "abstract": "Comments: Neurips 2022 dataset and benchmark track",
    "descriptor": "\nComments: Neurips 2022 dataset and benchmark track\n",
    "authors": [
      "Yuxiang Yang",
      "Junjie Yang",
      "Yufei Xu",
      "Jing Zhang",
      "Long Lan",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.05683"
  },
  {
    "id": "arXiv:2206.06126",
    "title": "Robust Time Series Denoising with Learnable Wavelet Packet Transform",
    "abstract": "Comments: 15 pages, 13 figures, 8 tables",
    "descriptor": "\nComments: 15 pages, 13 figures, 8 tables\n",
    "authors": [
      "Gaetan Frusque",
      "Olga Fink"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.06126"
  },
  {
    "id": "arXiv:2206.06801",
    "title": "Peripheral Vision Transformer",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Juhong Min",
      "Yucheng Zhao",
      "Chong Luo",
      "Minsu Cho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.06801"
  },
  {
    "id": "arXiv:2206.08022",
    "title": "Partial Identifiability for Nonnegative Matrix Factorization",
    "abstract": "Comments: 27 pages, 8 figures, 7 examples. This third version makes minor modifications. Paper accepted in SIAM J. on Matrix Analysis and Applications",
    "descriptor": "\nComments: 27 pages, 8 figures, 7 examples. This third version makes minor modifications. Paper accepted in SIAM J. on Matrix Analysis and Applications\n",
    "authors": [
      "Nicolas Gillis",
      "R\u00f3bert Rajk\u00f3"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08022"
  },
  {
    "id": "arXiv:2206.09012",
    "title": "Diffusion models as plug-and-play priors",
    "abstract": "Comments: NeurIPS 2022; code: this https URL",
    "descriptor": "\nComments: NeurIPS 2022; code: this https URL\n",
    "authors": [
      "Alexandros Graikos",
      "Nikolay Malkin",
      "Nebojsa Jojic",
      "Dimitris Samaras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.09012"
  },
  {
    "id": "arXiv:2206.09311",
    "title": "Primal Estimated Subgradient Solver for SVM for Imbalanced  Classification",
    "abstract": "Comments: 10 pages, 4 tables, 3 figures",
    "descriptor": "\nComments: 10 pages, 4 tables, 3 figures\n",
    "authors": [
      "John Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2206.09311"
  },
  {
    "id": "arXiv:2206.09674",
    "title": "EAGER: Asking and Answering Questions for Automatic Reward Shaping in  Language-guided RL",
    "abstract": "Comments: 24 pages, 16 figures, 5 tables",
    "descriptor": "\nComments: 24 pages, 16 figures, 5 tables\n",
    "authors": [
      "Thomas Carta",
      "Pierre-Yves Oudeyer",
      "Olivier Sigaud",
      "Sylvain Lamprier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.09674"
  },
  {
    "id": "arXiv:2206.09888",
    "title": "SoteriaFL: A Unified Framework for Private Federated Learning with  Communication Compression",
    "abstract": "Comments: 39 pages",
    "descriptor": "\nComments: 39 pages\n",
    "authors": [
      "Zhize Li",
      "Haoyu Zhao",
      "Boyue Li",
      "Yuejie Chi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.09888"
  },
  {
    "id": "arXiv:2206.10586",
    "title": "D-CIPHER: Discovery of Closed-form Partial Differential Equations",
    "abstract": "D-CIPHER: Discovery of Closed-form Partial Differential Equations",
    "descriptor": "",
    "authors": [
      "Krzysztof Kacprzyk",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.10586"
  },
  {
    "id": "arXiv:2206.10680",
    "title": "Learning Neuro-Symbolic Skills for Bilevel Planning",
    "abstract": "Comments: CoRL 2022",
    "descriptor": "\nComments: CoRL 2022\n",
    "authors": [
      "Tom Silver",
      "Ashay Athalye",
      "Joshua B. Tenenbaum",
      "Tomas Lozano-Perez",
      "Leslie Pack Kaelbling"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.10680"
  },
  {
    "id": "arXiv:2206.10693",
    "title": "A consistent and flexible framework for deep matrix factorizations",
    "abstract": "Comments: 28 pages, few minor modifications. Accepted in Pattern Recognition",
    "descriptor": "\nComments: 28 pages, few minor modifications. Accepted in Pattern Recognition\n",
    "authors": [
      "Pierre De Handschutter",
      "Nicolas Gillis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.10693"
  },
  {
    "id": "arXiv:2206.11895",
    "title": "Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens  in 3D Space",
    "abstract": "Comments: NeurIPS 2022. Our code is at this https URL Our project page is at this https URL",
    "descriptor": "\nComments: NeurIPS 2022. Our code is at this https URL Our project page is at this https URL\n",
    "authors": [
      "Jinghuan Shang",
      "Srijan Das",
      "Michael S. Ryoo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.11895"
  },
  {
    "id": "arXiv:2206.12139",
    "title": "HARU: Haptic Augmented Reality-Assisted User-Centric Industrial Network  Planning",
    "abstract": "HARU: Haptic Augmented Reality-Assisted User-Centric Industrial Network  Planning",
    "descriptor": "",
    "authors": [
      "Qi Liao",
      "Tianlun Hu",
      "Nikolaj Marchenko",
      "Peter Kulics",
      "Lutz Ewe"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.12139"
  },
  {
    "id": "arXiv:2206.13559",
    "title": "ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning",
    "abstract": "Comments: Accepted in NeurIPS 2022",
    "descriptor": "\nComments: Accepted in NeurIPS 2022\n",
    "authors": [
      "Junting Pan",
      "Ziyi Lin",
      "Xiatian Zhu",
      "Jing Shao",
      "Hongsheng Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.13559"
  },
  {
    "id": "arXiv:2207.00812",
    "title": "A systematic review of biologically-informed deep learning models for  cancer: fundamental trends for encoding and interpreting oncology data",
    "abstract": "Comments: 25 pages, 5 figures",
    "descriptor": "\nComments: 25 pages, 5 figures\n",
    "authors": [
      "Magdalena Wysocka",
      "Oskar Wysocki",
      "Marie Zufferey",
      "D\u00f3nal Landers",
      "Andr\u00e9 Freitas"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.00812"
  },
  {
    "id": "arXiv:2207.02589",
    "title": "Cascaded Deep Hybrid Models for Multistep Household Energy Consumption  Forecasting",
    "abstract": "Comments: Under consideration at Pattern Recognition Letters",
    "descriptor": "\nComments: Under consideration at Pattern Recognition Letters\n",
    "authors": [
      "Lyes Saad Saoud",
      "Hasan AlMarzouqi",
      "Ramy Hussein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2207.02589"
  },
  {
    "id": "arXiv:2207.03482",
    "title": "Bridging the Gap between Object and Image-level Representations for  Open-Vocabulary Detection",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Hanoona Rasheed",
      "Muhammad Maaz",
      "Muhammad Uzair Khattak",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.03482"
  },
  {
    "id": "arXiv:2207.04892",
    "title": "Adversarial Style Augmentation for Domain Generalized Urban-Scene  Segmentation",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Zhun Zhong",
      "Yuyang Zhao",
      "Gim Hee Lee",
      "Nicu Sebe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.04892"
  },
  {
    "id": "arXiv:2207.06046",
    "title": "DeepTime: Deep Time-Index Meta-Learning for Non-Stationary Time-Series  Forecasting",
    "abstract": "DeepTime: Deep Time-Index Meta-Learning for Non-Stationary Time-Series  Forecasting",
    "descriptor": "",
    "authors": [
      "Gerald Woo",
      "Chenghao Liu",
      "Doyen Sahoo",
      "Akshat Kumar",
      "Steven Hoi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.06046"
  },
  {
    "id": "arXiv:2207.06214",
    "title": "Is one annotation enough? A data-centric image classification benchmark  for noisy and ambiguous label estimation",
    "abstract": "Comments: Accepted at NeurIPS 2022, Benchmark and Dataset Track, Code and Link to data available at this https URL",
    "descriptor": "\nComments: Accepted at NeurIPS 2022, Benchmark and Dataset Track, Code and Link to data available at this https URL\n",
    "authors": [
      "Lars Schmarje",
      "Vasco Grossmann",
      "Claudius Zelenka",
      "Sabine Dippel",
      "Rainer Kiko",
      "Mariusz Oszust",
      "Matti Pastell",
      "Jenny Stracke",
      "Anna Valros",
      "Nina Volkmann",
      "Reinahrd Koch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.06214"
  },
  {
    "id": "arXiv:2207.07110",
    "title": "Fine-grained Few-shot Recognition by Deep Object Parsing",
    "abstract": "Fine-grained Few-shot Recognition by Deep Object Parsing",
    "descriptor": "",
    "authors": [
      "Ruizhao Zhu",
      "Pengkai Zhu",
      "Samarth Mishra",
      "Venkatesh Saligrama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.07110"
  },
  {
    "id": "arXiv:2207.07499",
    "title": "Formalising Szemer\u00e9di's Regularity Lemma and Roth's Theorem on  Arithmetic Progressions in Isabelle/HOL",
    "abstract": "Formalising Szemer\u00e9di's Regularity Lemma and Roth's Theorem on  Arithmetic Progressions in Isabelle/HOL",
    "descriptor": "",
    "authors": [
      "Chelsea Edmonds",
      "Angeliki Koutsoukou-Argyraki",
      "Lawrence C. Paulson"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2207.07499"
  },
  {
    "id": "arXiv:2207.08645",
    "title": "Active Exploration for Inverse Reinforcement Learning",
    "abstract": "Comments: Presented at Conference on Neural Information Processing Systems (NeurIPS), 2022",
    "descriptor": "\nComments: Presented at Conference on Neural Information Processing Systems (NeurIPS), 2022\n",
    "authors": [
      "David Lindner",
      "Andreas Krause",
      "Giorgia Ramponi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.08645"
  },
  {
    "id": "arXiv:2207.09446",
    "title": "ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model",
    "abstract": "ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model",
    "descriptor": "",
    "authors": [
      "Rao Fu",
      "Xiao Zhan",
      "Yiwen Chen",
      "Daniel Ritchie",
      "Srinath Sridhar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.09446"
  },
  {
    "id": "arXiv:2207.12077",
    "title": "Decision-oriented two-parameter Fisher information sensitivity using  symplectic decomposition",
    "abstract": "Comments: 15 pages, 10 figures, and a Supplement Material as ancillary files. New version adds a benchmark section. The datasets generated during and/or analysed during the current study are available in the GitHub repository: this https URL",
    "descriptor": "\nComments: 15 pages, 10 figures, and a Supplement Material as ancillary files. New version adds a benchmark section. The datasets generated during and/or analysed during the current study are available in the GitHub repository: this https URL\n",
    "authors": [
      "Jiannan Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Numerical Analysis (math.NA)",
      "Symplectic Geometry (math.SG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2207.12077"
  },
  {
    "id": "arXiv:2207.14676",
    "title": "Global-Local Self-Distillation for Visual Representation Learning",
    "abstract": "Comments: WACV 2023",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Tim Lebailly",
      "Tinne Tuytelaars"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.14676"
  },
  {
    "id": "arXiv:2208.01582",
    "title": "ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries",
    "abstract": "Comments: Project page is at this https URL",
    "descriptor": "\nComments: Project page is at this https URL\n",
    "authors": [
      "Junru Gu",
      "Chenxu Hu",
      "Tianyuan Zhang",
      "Xuanyao Chen",
      "Yilun Wang",
      "Yue Wang",
      "Hang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2208.01582"
  },
  {
    "id": "arXiv:2208.03249",
    "title": "Parameter Averaging for Feature Ranking",
    "abstract": "Comments: 38 pages",
    "descriptor": "\nComments: 38 pages\n",
    "authors": [
      "Talip Ucar",
      "Ehsan Hajiramezanali"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.03249"
  },
  {
    "id": "arXiv:2208.04107",
    "title": "A Local Discontinuous Galerkin approximation for the $p$-Navier-Stokes  system, Part II: Convergence rates for the velocity",
    "abstract": "Comments: 21 pages, 3 tables. arXiv admin note: text overlap with arXiv:2208.04106",
    "descriptor": "\nComments: 21 pages, 3 tables. arXiv admin note: text overlap with arXiv:2208.04106\n",
    "authors": [
      "Alex Kaltenbach",
      "Michael R\u016f\u017ei\u010dka"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2208.04107"
  },
  {
    "id": "arXiv:2208.05408",
    "title": "Pikachu: Securing PoS Blockchains from Long-Range Attacks by  Checkpointing into Bitcoin PoW using Taproot",
    "abstract": "Comments: To appear at ConsensusDay 22 (ACM CCS 2022 Workshop)",
    "descriptor": "\nComments: To appear at ConsensusDay 22 (ACM CCS 2022 Workshop)\n",
    "authors": [
      "Sarah Azouvi",
      "Marko Vukoli\u0107"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2208.05408"
  },
  {
    "id": "arXiv:2208.07282",
    "title": "Differentiable WORLD Synthesizer-based Neural Vocoder With Application  To End-To-End Audio Style Transfer",
    "abstract": "Comments: 12 pages, 4 figures",
    "descriptor": "\nComments: 12 pages, 4 figures\n",
    "authors": [
      "Shahan Nercessian"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2208.07282"
  },
  {
    "id": "arXiv:2208.07363",
    "title": "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control",
    "abstract": "Comments: Appearing in NeurIPS 2022 Datasets and Benchmarks Track",
    "descriptor": "\nComments: Appearing in NeurIPS 2022 Datasets and Benchmarks Track\n",
    "authors": [
      "Nolan Wagener",
      "Andrey Kolobov",
      "Felipe Vieira Frujeri",
      "Ricky Loynd",
      "Ching-An Cheng",
      "Matthew Hausknecht"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2208.07363"
  },
  {
    "id": "arXiv:2208.08155",
    "title": "A Monotonicity Constrained Attention Module for Emotion Classification  with Limited EEG Data",
    "abstract": "Comments: A Preprint for the accepted work by MICCAI 2022 workshop: Medical Image Learning with Noisy and Limited Data",
    "descriptor": "\nComments: A Preprint for the accepted work by MICCAI 2022 workshop: Medical Image Learning with Noisy and Limited Data\n",
    "authors": [
      "Dongyang Kuang",
      "Craig Michoski",
      "Wenting Li",
      "Rui Guo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.08155"
  },
  {
    "id": "arXiv:2208.13405",
    "title": "Interpreting Black-box Machine Learning Models for High Dimensional  Datasets",
    "abstract": "Comments: This paper is currently under review in a journal",
    "descriptor": "\nComments: This paper is currently under review in a journal\n",
    "authors": [
      "Md. Rezaul Karim",
      "Md. Shajalal",
      "Alex Gra\u00df",
      "Till D\u00f6hmen",
      "Sisay Adugna Chala",
      "Christian Beecks",
      "Stefan Decker"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.13405"
  },
  {
    "id": "arXiv:2208.14837",
    "title": "Batch-Size Independent Regret Bounds for Combinatorial Semi-Bandits with  Probabilistically Triggered Arms or Independent Arms",
    "abstract": "Batch-Size Independent Regret Bounds for Combinatorial Semi-Bandits with  Probabilistically Triggered Arms or Independent Arms",
    "descriptor": "",
    "authors": [
      "Xutong Liu",
      "Jinhang Zuo",
      "Siwei Wang",
      "Carlee Joe-Wong",
      "John C.S. Lui",
      "Wei Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.14837"
  },
  {
    "id": "arXiv:2209.00484",
    "title": "Focus-Driven Contrastive Learniang for Medical Question Summarization",
    "abstract": "Comments: Accepted by COLING 2022, long paper",
    "descriptor": "\nComments: Accepted by COLING 2022, long paper\n",
    "authors": [
      "Ming Zhang",
      "Shuai Dou",
      "Ziyang Wang",
      "Yunfang Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.00484"
  },
  {
    "id": "arXiv:2209.00653",
    "title": "Effective Class-Imbalance learning based on SMOTE and Convolutional  Neural Networks",
    "abstract": "Effective Class-Imbalance learning based on SMOTE and Convolutional  Neural Networks",
    "descriptor": "",
    "authors": [
      "Javad Hassannataj Joloudari",
      "Abdolreza Marefat",
      "Mohammad Ali Nematollahi",
      "Solomon Sunday Oyelere",
      "Sadiq Hussain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.00653"
  },
  {
    "id": "arXiv:2209.01814",
    "title": "RLIP: Relational Language-Image Pre-training for Human-Object  Interaction Detection",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Hangjie Yuan",
      "Jianwen Jiang",
      "Samuel Albanie",
      "Tao Feng",
      "Ziyuan Huang",
      "Dong Ni",
      "Mingqian Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.01814"
  },
  {
    "id": "arXiv:2209.03299",
    "title": "Geometric multimodal representation learning",
    "abstract": "Comments: 28 pages, 5 figures, 2 boxes",
    "descriptor": "\nComments: 28 pages, 5 figures, 2 boxes\n",
    "authors": [
      "Yasha Ektefaie",
      "George Dasoulas",
      "Ayush Noori",
      "Maha Farhat",
      "Marinka Zitnik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.03299"
  },
  {
    "id": "arXiv:2209.03704",
    "title": "Kernel-Segregated Transpose Convolution Operation",
    "abstract": "Kernel-Segregated Transpose Convolution Operation",
    "descriptor": "",
    "authors": [
      "Vijay Srinivas Tida",
      "Sai Venkatesh Chilukoti",
      "Xiali Hei",
      "Sonya Hsu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.03704"
  },
  {
    "id": "arXiv:2209.04692",
    "title": "Distributed Learning over a Wireless Network with Non-coherent Majority  Vote Computation",
    "abstract": "Comments: 15 pages, 9 figures. arXiv admin note: text overlap with arXiv:2111.01850",
    "descriptor": "\nComments: 15 pages, 9 figures. arXiv admin note: text overlap with arXiv:2111.01850\n",
    "authors": [
      "Alphan Sahin"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.04692"
  },
  {
    "id": "arXiv:2209.06054",
    "title": "SongDriver: Real-time Music Accompaniment Generation without Logical  Latency nor Exposure Bias",
    "abstract": "Comments: *Both Zihao Wang and Qihao Liang contribute equally to the paper and share the co-first authorship. This paper has been accepted by ACM Multimedia 2022, oral session, full paper (main track)",
    "descriptor": "\nComments: *Both Zihao Wang and Qihao Liang contribute equally to the paper and share the co-first authorship. This paper has been accepted by ACM Multimedia 2022, oral session, full paper (main track)\n",
    "authors": [
      "Zihao Wang",
      "Qihao Liang",
      "Kejun Zhang",
      "Yuxing Wang",
      "Chen Zhang",
      "Pengfei Yu",
      "Yongsheng Feng",
      "Wenbo Liu",
      "Yikai Wang",
      "Yuntai Bao",
      "Yiheng Yang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.06054"
  },
  {
    "id": "arXiv:2209.06975",
    "title": "Wasserstein $K$-means for clustering probability distributions",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Yubo Zhuang",
      "Xiaohui Chen",
      "Yun Yang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.06975"
  },
  {
    "id": "arXiv:2209.07285",
    "title": "Identifying research supporting the United Nations Sustainable  Development Goals",
    "abstract": "Comments: 12 pages, 3 figures, 7 tables, 19 references",
    "descriptor": "\nComments: 12 pages, 3 figures, 7 tables, 19 references\n",
    "authors": [
      "Yury Kashnitsky",
      "Guillaume Roberge",
      "Jingwen Mu",
      "Kevin Kang",
      "Weiwei Wang",
      "Maurice Vanderfeesten",
      "Maxim Rivest",
      "Lennart Ke\u00dfler",
      "Robert Jaworek",
      "Ma\u00e9va Vignes",
      "Bamini Jayabalasingham",
      "Finne Boonen",
      "Chris James",
      "Marius Doornenbal",
      "Isabelle Labrosse"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2209.07285"
  },
  {
    "id": "arXiv:2209.07417",
    "title": "Examining Large Pre-Trained Language Models for Machine Translation:  What You Don't Know About It",
    "abstract": "Comments: System paper Accepted to WMT2022: BiomedicalMT Track (ClinSpEn2022)",
    "descriptor": "\nComments: System paper Accepted to WMT2022: BiomedicalMT Track (ClinSpEn2022)\n",
    "authors": [
      "Lifeng Han",
      "Gleb Erofeev",
      "Irina Sorokina",
      "Serge Gladkoff",
      "Goran Nenadic"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.07417"
  },
  {
    "id": "arXiv:2209.07507",
    "title": "Bidirectional Learning for Offline Infinite-width Model-based  Optimization",
    "abstract": "Comments: NeurIPS2022 camera-ready version; AI4Science; Drug discovery; Offline model-based optimization; Neural tangent kernel; Bi-level optimization",
    "descriptor": "\nComments: NeurIPS2022 camera-ready version; AI4Science; Drug discovery; Offline model-based optimization; Neural tangent kernel; Bi-level optimization\n",
    "authors": [
      "Can Chen",
      "Yingxue Zhang",
      "Jie Fu",
      "Xue Liu",
      "Mark Coates"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2209.07507"
  },
  {
    "id": "arXiv:2209.07519",
    "title": "Multi-Modal Beam Prediction Challenge 2022: Towards Generalization",
    "abstract": "Comments: The dataset is available on the ML competition page: this https URL",
    "descriptor": "\nComments: The dataset is available on the ML competition page: this https URL\n",
    "authors": [
      "Gouranga Charan",
      "Umut Demirhan",
      "Jo\u00e3o Morais",
      "Arash Behboodi",
      "Hamed Pezeshki",
      "Ahmed Alkhateeb"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.07519"
  },
  {
    "id": "arXiv:2209.07686",
    "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango",
    "abstract": "Comments: Shortened version with additional results from CODEX and GPT-3. The authors contributed equally. Work done when Aman Madaan was a student researcher at Google Research, Brain Team. arXiv admin note: text overlap with arXiv:2201.11903",
    "descriptor": "\nComments: Shortened version with additional results from CODEX and GPT-3. The authors contributed equally. Work done when Aman Madaan was a student researcher at Google Research, Brain Team. arXiv admin note: text overlap with arXiv:2201.11903\n",
    "authors": [
      "Aman Madaan",
      "Amir Yazdanbakhsh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.07686"
  },
  {
    "id": "arXiv:2209.08281",
    "title": "Improved Generalization Bound and Learning of Sparsity Patterns for  Data-Driven Low-Rank Approximation",
    "abstract": "Improved Generalization Bound and Learning of Sparsity Patterns for  Data-Driven Low-Rank Approximation",
    "descriptor": "",
    "authors": [
      "Shinsaku Sakaue",
      "Taihei Oki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.08281"
  },
  {
    "id": "arXiv:2209.10299",
    "title": "DPCN: Towards Deadline-aware Payment Channel Networks",
    "abstract": "DPCN: Towards Deadline-aware Payment Channel Networks",
    "descriptor": "",
    "authors": [
      "Wenhui Wang",
      "Ke Mu",
      "Xuetao Wei"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.10299"
  },
  {
    "id": "arXiv:2209.10968",
    "title": "Proximal Point Imitation Learning",
    "abstract": "Proximal Point Imitation Learning",
    "descriptor": "",
    "authors": [
      "Luca Viano",
      "Angeliki Kamoutsi",
      "Gergely Neu",
      "Igor Krawczuk",
      "Volkan Cevher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.10968"
  },
  {
    "id": "arXiv:2209.10974",
    "title": "Identifiability and generalizability from multiple experts in Inverse  Reinforcement Learning",
    "abstract": "Identifiability and generalizability from multiple experts in Inverse  Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Paul Rolland",
      "Luca Viano",
      "Norman Schuerhoff",
      "Boris Nikolov",
      "Volkan Cevher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.10974"
  },
  {
    "id": "arXiv:2209.11128",
    "title": "Learning Interpretable Latent Dialogue Actions With Less Supervision",
    "abstract": "Comments: 9 pages, accepted to AACL-IJCNLP 2022. Available online at this https URL",
    "descriptor": "\nComments: 9 pages, accepted to AACL-IJCNLP 2022. Available online at this https URL\n",
    "authors": [
      "Vojt\u011bch Hude\u010dek",
      "Ond\u0159ej Du\u0161ek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.11128"
  },
  {
    "id": "arXiv:2209.11178",
    "title": "Poisson Flow Generative Models",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Yilun Xu",
      "Ziming Liu",
      "Max Tegmark",
      "Tommi Jaakkola"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.11178"
  },
  {
    "id": "arXiv:2209.11486",
    "title": "MetaPrompting: Learning to Learn Better Prompts",
    "abstract": "Comments: Accepted as COLING 2022 long paper",
    "descriptor": "\nComments: Accepted as COLING 2022 long paper\n",
    "authors": [
      "Yutai Hou",
      "Hongyuan Dong",
      "Xinghao Wang",
      "Bohan Li",
      "Wanxiang Che"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.11486"
  },
  {
    "id": "arXiv:2209.12202",
    "title": "Multimodal Exponentially Modified Gaussian Oscillators",
    "abstract": "Comments: IEEE International Ultrasonic Symposium 2022",
    "descriptor": "\nComments: IEEE International Ultrasonic Symposium 2022\n",
    "authors": [
      "Christopher Hahne"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Audio and Speech Processing (eess.AS)",
      "Applied Physics (physics.app-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.12202"
  },
  {
    "id": "arXiv:2209.13322",
    "title": "The *-product approach for linear ODEs: a numerical study of the scalar  case",
    "abstract": "The *-product approach for linear ODEs: a numerical study of the scalar  case",
    "descriptor": "",
    "authors": [
      "Stefano Pozza",
      "Niel Van Buggenhout"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.13322"
  },
  {
    "id": "arXiv:2209.13520",
    "title": "RADio -- Rank-Aware Divergence Metrics to Measure Normative Diversity in  News Recommendations",
    "abstract": "Comments: Accepted to Recsys 2022 as a full paper - Best Paper Runner Up award",
    "descriptor": "\nComments: Accepted to Recsys 2022 as a full paper - Best Paper Runner Up award\n",
    "authors": [
      "Sanne Vrijenhoek",
      "Gabriel B\u00e9n\u00e9dict",
      "Mateo Gutierrez Granada",
      "Daan Odijk",
      "Maarten de Rijke"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13520"
  },
  {
    "id": "arXiv:2209.14842",
    "title": "Classification of Vocal Bursts for ACII 2022 A-VB-Type Competition using  Convolutional Neural Networks and Deep Acoustic Embeddings",
    "abstract": "Comments: Report for our submission to the ACII 2022 Affective Vocal Bursts (A-VB) Competition",
    "descriptor": "\nComments: Report for our submission to the ACII 2022 Affective Vocal Bursts (A-VB) Competition\n",
    "authors": [
      "Muhammad Shehram Shah Syed",
      "Zafi Sherhan Syed",
      "Abbas Syed"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.14842"
  },
  {
    "id": "arXiv:2209.15236",
    "title": "Language-Family Adapters for Multilingual Neural Machine Translation",
    "abstract": "Comments: Minor updates",
    "descriptor": "\nComments: Minor updates\n",
    "authors": [
      "Alexandra Chronopoulou",
      "Dario Stojanovski",
      "Alexander Fraser"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15236"
  },
  {
    "id": "arXiv:2210.01010",
    "title": "A general framework for probabilistic sensitivity analysis with respect  to distribution parameters",
    "abstract": "Comments: The datasets generated during and/or analysed during the current study are available in the GitHub repository: this https URL",
    "descriptor": "\nComments: The datasets generated during and/or analysed during the current study are available in the GitHub repository: this https URL\n",
    "authors": [
      "Jiannan Yang"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Numerical Analysis (math.NA)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.01010"
  },
  {
    "id": "arXiv:2210.01437",
    "title": "Shielding Federated Learning: Mitigating Byzantine Attacks with Less  Constraints",
    "abstract": "Comments: This paper has been accepted by the 18th International Conference on Mobility, Sensing and Networking (MSN 2022)",
    "descriptor": "\nComments: This paper has been accepted by the 18th International Conference on Mobility, Sensing and Networking (MSN 2022)\n",
    "authors": [
      "Minghui Li",
      "Wei Wan",
      "Jianrong Lu",
      "Shengshan Hu",
      "Junyu Shi",
      "Leo Yu Zhang",
      "Man Zhou",
      "Yifeng Zheng"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.01437"
  },
  {
    "id": "arXiv:2210.01438",
    "title": "Complementary consistency semi-supervised learning for 3D left atrial  image segmentation",
    "abstract": "Complementary consistency semi-supervised learning for 3D left atrial  image segmentation",
    "descriptor": "",
    "authors": [
      "Hejun Huang",
      "Zuguo Chen",
      "Chaoyang Chen",
      "Ming Lu",
      "Ying Zou"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.01438"
  },
  {
    "id": "arXiv:2210.01639",
    "title": "SecureFedYJ: a safe feature Gaussianization protocol for Federated  Learning",
    "abstract": "Comments: Accepted to Neurips2022",
    "descriptor": "\nComments: Accepted to Neurips2022\n",
    "authors": [
      "Tanguy Marchand",
      "Boris Muzellec",
      "Constance Beguier",
      "Jean Ogier du Terrail",
      "Mathieu Andreux"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01639"
  },
  {
    "id": "arXiv:2210.02144",
    "title": "SECOE: Alleviating Sensors Failure in Machine Learning-Coupled IoT  Systems",
    "abstract": "Comments: 8 pages, 10 figures, Accepted as a conference paper at IEEE International Conference on Machine Learning and Applications (ICMLA 2022)",
    "descriptor": "\nComments: 8 pages, 10 figures, Accepted as a conference paper at IEEE International Conference on Machine Learning and Applications (ICMLA 2022)\n",
    "authors": [
      "Yousef AlShehri",
      "Lakshmish Ramaswamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.02144"
  },
  {
    "id": "arXiv:2210.02271",
    "title": "Extending Conformal Prediction to Hidden Markov Models with Exact  Validity via de Finetti's Theorem for Markov Chains",
    "abstract": "Extending Conformal Prediction to Hidden Markov Models with Exact  Validity via de Finetti's Theorem for Markov Chains",
    "descriptor": "",
    "authors": [
      "Buddhika Nettasinghe",
      "Samrat Chatterjee",
      "Ramakrishna Tipireddy",
      "Mahantesh Halappanavar"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.02271"
  },
  {
    "id": "arXiv:2210.02544",
    "title": "Deep learning for ECoG brain-computer interface: end-to-end vs.  hand-crafted features",
    "abstract": "Comments: Replaced duplicated plot in figure 7",
    "descriptor": "\nComments: Replaced duplicated plot in figure 7\n",
    "authors": [
      "Maciej \u015aliwowski",
      "Matthieu Martin",
      "Antoine Souloumiac",
      "Pierre Blanchart",
      "Tetiana Aksenova"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02544"
  },
  {
    "id": "arXiv:2210.03304",
    "title": "Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD  Coding",
    "abstract": "Comments: Accepted by Findings of EMNLP 2022, code is available at this https URL",
    "descriptor": "\nComments: Accepted by Findings of EMNLP 2022, code is available at this https URL\n",
    "authors": [
      "Zhichao Yang",
      "Shufan Wang",
      "Bhanu Pratap Singh Rawat",
      "Avijit Mitra",
      "Hong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.03304"
  },
  {
    "id": "arXiv:2210.03485",
    "title": "Gradient-based optimisation of the conditional-value-at-risk using the  multi-level Monte Carlo method",
    "abstract": "Comments: 26 pages, 18 figures, 1 table, Related to arXiv:2208.07252, Data available at this https URL",
    "descriptor": "\nComments: 26 pages, 18 figures, 1 table, Related to arXiv:2208.07252, Data available at this https URL\n",
    "authors": [
      "Sundar Ganesh",
      "Fabio Nobile"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.03485"
  },
  {
    "id": "arXiv:2210.03529",
    "title": "Mesh-Tension Driven Expression-Based Wrinkles for Synthetic Faces",
    "abstract": "Comments: In Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
    "descriptor": "\nComments: In Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n",
    "authors": [
      "Chirag Raman",
      "Charlie Hewitt",
      "Erroll Wood",
      "Tadas Baltrusaitis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.03529"
  },
  {
    "id": "arXiv:2210.03919",
    "title": "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features  for a Disentangled, Interpretable, and Controllable Text-Guided Image  Manipulation",
    "abstract": "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features  for a Disentangled, Interpretable, and Controllable Text-Guided Image  Manipulation",
    "descriptor": "",
    "authors": [
      "Chenliang Zhou",
      "Fangcheng Zhong",
      "Cengiz Oztireli"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.03919"
  },
  {
    "id": "arXiv:2210.04045",
    "title": "The FBHHRBNRSSSHK-Algorithm for Multiplication in  $\\mathbb{Z}_2^{5\\times5}$ is still not the end of the story",
    "abstract": "The FBHHRBNRSSSHK-Algorithm for Multiplication in  $\\mathbb{Z}_2^{5\\times5}$ is still not the end of the story",
    "descriptor": "",
    "authors": [
      "Manuel Kauers",
      "Jakob Moosbauer"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.04045"
  },
  {
    "id": "arXiv:2210.04092",
    "title": "Advancing Model Pruning via Bi-level Optimization",
    "abstract": "Comments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Yihua Zhang",
      "Yuguang Yao",
      "Parikshit Ram",
      "Pu Zhao",
      "Tianlong Chen",
      "Mingyi Hong",
      "Yanzhi Wang",
      "Sijia Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04092"
  },
  {
    "id": "arXiv:2210.04137",
    "title": "Few-Shot Continual Active Learning by a Robot",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Ali Ayub",
      "Carter Fendley"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.04137"
  },
  {
    "id": "arXiv:2210.04574",
    "title": "ARUBA: An Architecture-Agnostic Balanced Loss for Aerial Object  Detection",
    "abstract": "ARUBA: An Architecture-Agnostic Balanced Loss for Aerial Object  Detection",
    "descriptor": "",
    "authors": [
      "Rebbapragada V C Sairam",
      "Monish Keswani",
      "Uttaran Sinha",
      "Nishit Shah",
      "Vineeth N Balasubramanian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.04574"
  },
  {
    "id": "arXiv:2210.04756",
    "title": "Metaphorical Paraphrase Generation: Feeding Metaphorical Language Models  with Literal Texts",
    "abstract": "Comments: 14 pages, 2 figures",
    "descriptor": "\nComments: 14 pages, 2 figures\n",
    "authors": [
      "Giorgio Ottolina",
      "John Pavlopoulos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04756"
  },
  {
    "id": "arXiv:2210.04757",
    "title": "On the Performance of Gradient Tracking with Local Updates",
    "abstract": "Comments: 8 pages, 1 figure, submitted to ACC",
    "descriptor": "\nComments: 8 pages, 1 figure, submitted to ACC\n",
    "authors": [
      "Edward Duc Hien Nguyen",
      "Sulaiman A. Alghunaim",
      "Kun Yuan",
      "C\u00e9sar A. Uribe"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.04757"
  },
  {
    "id": "arXiv:2210.04797",
    "title": "DeepVol: Volatility Forecasting from High-Frequency Data with Dilated  Causal Convolutions",
    "abstract": "Comments: typos corrected",
    "descriptor": "\nComments: typos corrected\n",
    "authors": [
      "Fernando Moreno-Pino",
      "Stefan Zohren"
    ],
    "subjectives": [
      "Risk Management (q-fin.RM)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)"
    ],
    "url": "https://arxiv.org/abs/2210.04797"
  },
  {
    "id": "arXiv:2210.04847",
    "title": "NerfAcc: A General NeRF Acceleration Toolbox",
    "abstract": "Comments: Webpage: this https URL",
    "descriptor": "\nComments: Webpage: this https URL\n",
    "authors": [
      "Ruilong Li",
      "Matthew Tancik",
      "Angjoo Kanazawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.04847"
  },
  {
    "id": "arXiv:2210.04885",
    "title": "What the DAAM: Interpreting Stable Diffusion Using Cross Attention",
    "abstract": "Comments: 5 pages, 5 figures",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Raphael Tang",
      "Akshat Pandey",
      "Zhiying Jiang",
      "Gefei Yang",
      "Karun Kumar",
      "Jimmy Lin",
      "Ferhan Ture"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04885"
  },
  {
    "id": "arXiv:2210.04958",
    "title": "Mining Causality from Continuous-time Dynamics Models: An Application to  Tsunami Forecasting",
    "abstract": "Mining Causality from Continuous-time Dynamics Models: An Application to  Tsunami Forecasting",
    "descriptor": "",
    "authors": [
      "Fan Wu",
      "Sanghyun Hong",
      "Donsub Rim",
      "Noseong Park",
      "Kookjin Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.04958"
  },
  {
    "id": "arXiv:2210.05020",
    "title": "Spectral Sparsification for Communication-Efficient Collaborative  Rotation and Translation Estimation",
    "abstract": "Comments: Technical report (9 figures, 3 tables)",
    "descriptor": "\nComments: Technical report (9 figures, 3 tables)\n",
    "authors": [
      "Yulun Tian",
      "Jonathan P. How"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05020"
  },
  {
    "id": "arXiv:2210.05055",
    "title": "Smart Hybrid Beamforming and Pilot Assignment for 6G Cell-Free Massive  MIMO",
    "abstract": "Smart Hybrid Beamforming and Pilot Assignment for 6G Cell-Free Massive  MIMO",
    "descriptor": "",
    "authors": [
      "Carles Diaz-Vilor",
      "Alexei Ashikhmin",
      "Hong Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05055"
  },
  {
    "id": "arXiv:2210.05128",
    "title": "On fast greedy block Kaczmarz methods for solving large consistent  linear systems",
    "abstract": "Comments: 7 pages, 1 figure",
    "descriptor": "\nComments: 7 pages, 1 figure\n",
    "authors": [
      "Aqin Xiao",
      "Junfeng Yin",
      "Ning Zheng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05128"
  },
  {
    "id": "arXiv:2210.05225",
    "title": "A Formalisation of a Fast Fourier Transform",
    "abstract": "A Formalisation of a Fast Fourier Transform",
    "descriptor": "",
    "authors": [
      "Laurent Th\u00e9ry"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05225"
  },
  {
    "id": "arXiv:2210.05391",
    "title": "PP-StructureV2: A Stronger Document Analysis System",
    "abstract": "PP-StructureV2: A Stronger Document Analysis System",
    "descriptor": "",
    "authors": [
      "Chenxia Li",
      "Ruoyu Guo",
      "Jun Zhou",
      "Mengtao An",
      "Yuning Du",
      "Lingfeng Zhu",
      "Yi Liu",
      "Xiaoguang Hu",
      "Dianhai Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05391"
  },
  {
    "id": "arXiv:2210.05423",
    "title": "Learning to Locate Visual Answer in Video Corpus Using Question",
    "abstract": "Comments: 4 pages, 2 figures and 3 tables",
    "descriptor": "\nComments: 4 pages, 2 figures and 3 tables\n",
    "authors": [
      "Bin Li",
      "Yixuan Weng",
      "Bin Sun",
      "Shutao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05423"
  },
  {
    "id": "arXiv:2210.05639",
    "title": "Discovered Policy Optimisation",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Chris Lu",
      "Jakub Grudzien Kuba",
      "Alistair Letcher",
      "Luke Metz",
      "Christian Schroeder de Witt",
      "Jakob Foerster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05639"
  },
  {
    "id": "arXiv:2210.05657",
    "title": "The Unreasonable Effectiveness of Fully-Connected Layers for Low-Data  Regimes",
    "abstract": "Comments: Accepted to NeurIPS 2022, Homepage: this https URL 24 pages, 14 figures, 12 tables",
    "descriptor": "\nComments: Accepted to NeurIPS 2022, Homepage: this https URL 24 pages, 14 figures, 12 tables\n",
    "authors": [
      "Peter Kocsis",
      "Peter S\u00faken\u00edk",
      "Guillem Bras\u00f3",
      "Matthias Nie\u00dfner",
      "Laura Leal-Taix\u00e9",
      "Ismail Elezi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05657"
  },
  {
    "id": "arXiv:2210.05675",
    "title": "Transformers generalize differently from information stored in context  vs in weights",
    "abstract": "Transformers generalize differently from information stored in context  vs in weights",
    "descriptor": "",
    "authors": [
      "Stephanie C.Y. Chan",
      "Ishita Dasgupta",
      "Junkyung Kim",
      "Dharshan Kumaran",
      "Andrew K. Lampinen",
      "Felix Hill"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05675"
  },
  {
    "id": "arXiv:2210.05714",
    "title": "Visual Language Maps for Robot Navigation",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Chenguang Huang",
      "Oier Mees",
      "Andy Zeng",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05714"
  },
  {
    "id": "arXiv:2210.05769",
    "title": "Vote'n'Rank: Revision of Benchmarking with Social Choice Theory",
    "abstract": "Vote'n'Rank: Revision of Benchmarking with Social Choice Theory",
    "descriptor": "",
    "authors": [
      "Mark Rofin",
      "Vladislav Mikhailov",
      "Mikhail Florinskiy",
      "Andrey Kravchenko",
      "Elena Tutubalina",
      "Tatiana Shavrina",
      "Daniel Karabekyan",
      "Ekaterina Artemova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05769"
  },
  {
    "id": "arXiv:2210.05811",
    "title": "Deep Counterfactual Estimation with Categorical Background Variables",
    "abstract": "Comments: Proceedings of the NeurIPS 2022 Conference",
    "descriptor": "\nComments: Proceedings of the NeurIPS 2022 Conference\n",
    "authors": [
      "Edward De Brouwer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.05811"
  },
  {
    "id": "arXiv:2210.05916",
    "title": "Hate-CLIPper: Multimodal Hateful Meme Classification based on  Cross-modal Interaction of CLIP Features",
    "abstract": "Comments: Accepted at EMNLP 2022 Workshop on NLP for Positive Impact",
    "descriptor": "\nComments: Accepted at EMNLP 2022 Workshop on NLP for Positive Impact\n",
    "authors": [
      "Gokul Karthik Kumar",
      "Karthik Nandakumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.05916"
  },
  {
    "id": "arXiv:2210.05953",
    "title": "Classification by estimating the cumulative distribution function for  small data",
    "abstract": "Comments: 39 pages, 34 figures, references added",
    "descriptor": "\nComments: 39 pages, 34 figures, references added\n",
    "authors": [
      "Meng-Xian Zhu",
      "Yuan-Hai Shao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05953"
  },
  {
    "id": "arXiv:2210.05992",
    "title": "Fast Convergence to Unanimity in Dense Erd\u0151s-R\u00e9nyi Graphs",
    "abstract": "Comments: The introduction has been edited. arXiv admin note: text overlap with arXiv:2104.04996",
    "descriptor": "\nComments: The introduction has been edited. arXiv admin note: text overlap with arXiv:2104.04996\n",
    "authors": [
      "Ran Tamir"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.05992"
  },
  {
    "id": "arXiv:2210.06032",
    "title": "Modular Flows: Differential Molecular Generation",
    "abstract": "Comments: Accepted to NeurIPS 2022. More info at: this https URL",
    "descriptor": "\nComments: Accepted to NeurIPS 2022. More info at: this https URL\n",
    "authors": [
      "Yogesh Verma",
      "Samuel Kaski",
      "Markus Heinonen",
      "Vikas Garg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06032"
  },
  {
    "id": "arXiv:2210.06048",
    "title": "AIMY: An Open-source Table Tennis Ball Launcher for Versatile and  High-fidelity Trajectory Generation",
    "abstract": "AIMY: An Open-source Table Tennis Ball Launcher for Versatile and  High-fidelity Trajectory Generation",
    "descriptor": "",
    "authors": [
      "Alexander Dittrich",
      "Jan Schneider",
      "Simon Guist",
      "Bernhard Sch\u00f6lkopf",
      "Dieter B\u00fcchler"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Hardware Architecture (cs.AR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06048"
  },
  {
    "id": "arXiv:2210.06061",
    "title": "Non-smooth and H\u00f6lder-smooth Submodular Maximization",
    "abstract": "Non-smooth and H\u00f6lder-smooth Submodular Maximization",
    "descriptor": "",
    "authors": [
      "Duksang Lee",
      "Nam Ho-Nguyen",
      "Dabeen Lee"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06061"
  },
  {
    "id": "arXiv:2210.06091",
    "title": "Summary on the ISCSLP 2022 Chinese-English Code-Switching ASR Challenge",
    "abstract": "Comments: accepted by ISCSLP 2022",
    "descriptor": "\nComments: accepted by ISCSLP 2022\n",
    "authors": [
      "Shuhao Deng",
      "Chengfei Li",
      "Jinfeng Bai",
      "Qingqing Zhang",
      "Wei-Qiang Zhang",
      "Runyan Yang",
      "Gaofeng Cheng",
      "Pengyuan Zhang",
      "Yonghong Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06091"
  },
  {
    "id": "arXiv:2210.06295",
    "title": "Transfer Learning on Electromyography (EMG) Tasks: Approaches and Beyond",
    "abstract": "Transfer Learning on Electromyography (EMG) Tasks: Approaches and Beyond",
    "descriptor": "",
    "authors": [
      "Di Wu",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06295"
  },
  {
    "id": "arXiv:2210.06300",
    "title": "Generalised Mutual Information for Discriminative Clustering",
    "abstract": "Comments: To be published in Neural Information Processing Systems 2022",
    "descriptor": "\nComments: To be published in Neural Information Processing Systems 2022\n",
    "authors": [
      "Louis Ohl",
      "Pierre-Alexandre Mattei",
      "Charles Bouveyron",
      "Warith Harchaoui",
      "Micka\u00ebl Leclercq",
      "Arnaud Droit",
      "Frederic Preciosio"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.06300"
  },
  {
    "id": "arXiv:2210.06301",
    "title": "FontTransformer: Few-shot High-resolution Chinese Glyph Image Synthesis  via Stacked Transformers",
    "abstract": "Comments: 23 pages, 14 Figures",
    "descriptor": "\nComments: 23 pages, 14 Figures\n",
    "authors": [
      "Yitian Liu",
      "Zhouhui Lian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06301"
  },
  {
    "id": "arXiv:2210.06340",
    "title": "Improving Radiology Report Generation Systems by Removing Hallucinated  References to Non-existent Priors",
    "abstract": "Comments: 13 pages, 1 figure, 11 tables",
    "descriptor": "\nComments: 13 pages, 1 figure, 11 tables\n",
    "authors": [
      "Vignav Ramesh",
      "Nathan Andrew Chi",
      "Pranav Rajpurkar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06340"
  },
  {
    "id": "arXiv:2210.06381",
    "title": "Good Intentions, Bad Inventions: How Employees Judge Pervasive  Technologies in the Workplace",
    "abstract": "Comments: 9 pages, 2 figures, 3 tables",
    "descriptor": "\nComments: 9 pages, 2 figures, 3 tables\n",
    "authors": [
      "Marios Constantinides",
      "Daniele Quercia"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06381"
  },
  {
    "id": "arXiv:2210.06384",
    "title": "GMP*: Well-Tuned Global Magnitude Pruning Can Outperform Most  BERT-Pruning Methods",
    "abstract": "GMP*: Well-Tuned Global Magnitude Pruning Can Outperform Most  BERT-Pruning Methods",
    "descriptor": "",
    "authors": [
      "Eldar Kurtic",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06384"
  },
  {
    "id": "arXiv:2210.06418",
    "title": "Relational Graph Convolutional Neural Networks for Multihop Reasoning: A  Comparative Study",
    "abstract": "Comments: 8 pages + 2 pages references, 3 figures, 3 tables",
    "descriptor": "\nComments: 8 pages + 2 pages references, 3 figures, 3 tables\n",
    "authors": [
      "Ieva Stali\u016bnait\u0117",
      "Philip John Gorinski",
      "Ignacio Iacobacci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06418"
  },
  {
    "id": "arXiv:2210.06432",
    "title": "InfoCSE: Information-aggregated Contrastive Learning of Sentence  Embeddings",
    "abstract": "Comments: EMNLP 2022. arXiv admin note: text overlap with arXiv:2204.10298 by other authors",
    "descriptor": "\nComments: EMNLP 2022. arXiv admin note: text overlap with arXiv:2204.10298 by other authors\n",
    "authors": [
      "Xing Wu",
      "Chaochen Gao",
      "Zijia Lin",
      "Jizhong Han",
      "Zhongyuan Wang",
      "Songlin Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06432"
  },
  {
    "id": "arXiv:2210.06464",
    "title": "Predictive Querying for Autoregressive Neural Sequence Models",
    "abstract": "Comments: Presented at the Conference on Neural Information Processing Systems (NeurIPs 2022)",
    "descriptor": "\nComments: Presented at the Conference on Neural Information Processing Systems (NeurIPs 2022)\n",
    "authors": [
      "Alex Boyd",
      "Sam Showalter",
      "Stephan Mandt",
      "Padhraic Smyth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06464"
  }
]