[
  {
    "id": "arXiv:2210.04891",
    "title": "An SIE Formulation with Triangular Discretization and Loop Analysis for  Parameter Extraction of Arbitrarily Shaped Interconnects",
    "abstract": "A surface integral equation (SIE) formulation under the magneto-quasi-static\nassumption is proposed to efficiently and accurately model arbitrarily shaped\ninterconnects in packages. Through decently transferring all electromagnetic\nquantities into circuit elements, the loop analysis is used to carefully\nconstruct matrix equations with an independent and complete set of unknowns\nbased on graph theory. In addition, an efficient preconditioner is developed,\nand the proposed formulation is accelerated by the pre-corrected Fast Fourier\nTransform (pFFT). Four practical examples, including a rectangular metallic\ninterconnect, bounding wire arrays, interconnects in a real-life circuit and\nthe power distribution network (PDN) used in packages, are carried out to\nvalidate its accuracy, efficiency and scalability. Results show that the\nproposed formulation is accurate, efficient and flexible to model complex\ninterconnects in packages.",
    "descriptor": "",
    "authors": [
      "Zekun Zhu",
      "Zhizhang Chen",
      "Shunchuan Yang"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.04891"
  },
  {
    "id": "arXiv:2210.04892",
    "title": "A Coupled Hybridizable Discontinuous Galerkin and Boundary Integral  Method for Analyzing Electromagnetic Scattering",
    "abstract": "A coupled hybridizable discontinuous Galerkin (HDG) and boundary integral\n(BI) method is proposed to efficiently analyze electromagnetic scattering from\ninhomogeneous/composite objects. The coupling between the HDG and the BI\nequations is realized using the numerical flux operating on the equivalent\ncurrent and the global unknown of the HDG. This approach yields sparse coupling\nmatrices upon discretization. Inclusion of the BI equation ensures that the\nonly error in enforcing the radiation conditions is the discretization.\nHowever, the discretization of this equation yields a dense matrix, which\nprohibits the use of a direct matrix solver on the overall coupled system as\noften done with traditional HDG schemes. To overcome this bottleneck, a\n``hybrid'' method is developed. This method uses an iterative scheme to solve\nthe overall coupled system but within the matrix-vector multiplication\nsubroutine of the iterations, the inverse of the HDG matrix is efficiently\naccounted for using a sparse direct matrix solver. The same subroutine also\nuses the multilevel fast multipole algorithm to accelerate the multiplication\nof the guess vector with the dense BI matrix. The numerical results demonstrate\nthe accuracy, the efficiency, and the applicability of the proposed HDG-BI\nsolver.",
    "descriptor": "",
    "authors": [
      "Ran Zhao",
      "Ming Dong",
      "Liang Chen",
      "Hakan Bagci",
      "Jun Hu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.04892"
  },
  {
    "id": "arXiv:2210.04894",
    "title": "A Computationally Efficient, Robust Methodology for Evaluating Chemical  Timescales with Detailed Chemical Kinetics",
    "abstract": "Turbulent reacting flows occur in a variety of engineering applications such\nas chemical reactors and power generating equipment (gas turbines and internal\ncombustion engines). Turbulent reacting flows are characterized by two main\ntimescales, namely, flow timescales and chemical (or reaction) timescales.\nUnderstanding the relative timescales of flow and reaction kinetics plays an\nimportant role, not only in the choice of models required for the accurate\nsimulation of these devices but also their design/optimization studies. There\nare several definitions of chemical timescales, which can largely be classified\nas algebraic or eigenvalue-based methods. The computational complexity (and\nhence cost) depends on the method of evaluation of the chemical timescales and\nsize of the chemical reaction mechanism. The computational cost and robustness\nof the methodology of evaluating the reaction times scales is an important\nconsideration in large-scale multi-dimensional simulations using detailed\nchemical mechanisms. In this work, we present a computational efficient and\nrobust methodology to evaluate chemical timescales based on the algebraic\nmethod. Comparison of this novel methodology with other traditional methods is\npresented for a range of fuel-air mixtures, pressures and temperatures\nconditions. Additionally, chemical timescales are also presented for fuel-air\nmixtures at conditions of relevance to power generating equipment. The proposed\nmethod showed the same temporal characteristics as the eigenvalue-based methods\nwith no additional computational cost for all the 1cases studied. The proposed\nmethod thus has the potential for use with multidimensional turbulent reacting\nflow simulations which require the computation of the Damkohler number.",
    "descriptor": "",
    "authors": [
      "S. M. Aithal"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.04894"
  },
  {
    "id": "arXiv:2210.04895",
    "title": "The 'Problematic Paper Screener' automatically selects suspect  publications for post-publication (re)assessment",
    "abstract": "Post publication assessment remains necessary to check erroneous or\nfraudulent scientific publications. We present an online platform, the\n'Problematic Paper Screener'\n(https://www.irit.fr/~Guillaume.Cabanac/problematic-paper-screener) that\nleverages both automatic machine detection and human assessment to identify and\nflag already published problematic articles. We provide a new effective tool to\ncurate the scientific literature.",
    "descriptor": "\nComments: Presented at WCRI 2022: 7th World Conference on Research Integrity, Cape Town, South Africa. 29 May -- 1 June 2022\n",
    "authors": [
      "Guillaume Cabanac",
      "Cyril Labb\u00e9",
      "Alexander Magazinov"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.04895"
  },
  {
    "id": "arXiv:2210.04897",
    "title": "Robust Adaptive Neural Network Control of Time-Varying State Constrained  Nonlinear Systems",
    "abstract": "This paper deals with the tracking control problem for a very simple class of\nunknown nonlinear systems. In this paper, we presents a design strategy for\ntracking control of time-varying state constrained nonlinear systems in an\nadaptive framework. The controller is designed using the backstepping method.\nWhile designing it, Barrier Lyapunov Function (BLF) is used so that the state\nvariables do not contravene its constraints. In order to cope with the unknown\ndynamics of the system, an online approximator is designed using a neural\nnetwork with a novel adaptive law for its weight update. To make the controller\nrobust and computationally inexpensive, a disturbance observer is proposed to\ncope with the disturbance along with neural network approximation error and the\ntime derivative of virtual control input. The effectiveness of the proposed\napproach is demonstrated through a simulation study.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2210.04211\n",
    "authors": [
      "Pankaj Kumar Mishra",
      "Nishchal K Verma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.04897"
  },
  {
    "id": "arXiv:2210.04909",
    "title": "Meta-Principled Family of Hyperparameter Scaling Strategies",
    "abstract": "In this note, we first derive a one-parameter family of hyperparameter\nscaling strategies that interpolates between the neural-tangent scaling and\nmean-field/maximal-update scaling. We then calculate the scalings of dynamical\nobservables -- network outputs, neural tangent kernels, and differentials of\nneural tangent kernels -- for wide and deep neural networks. These calculations\nin turn reveal a proper way to scale depth with width such that resultant\nlarge-scale models maintain their representation-learning ability. Finally, we\nobserve that various infinite-width limits examined in the literature\ncorrespond to the distinct corners of the interconnected web spanned by\neffective theories for finite-width neural networks, with their training\ndynamics ranging from being weakly-coupled to being strongly-coupled.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Sho Yaida"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.04909"
  },
  {
    "id": "arXiv:2210.04929",
    "title": "Batch Exchanges with Constant Function Market Makers: Axioms,  Equilibria, and Computation",
    "abstract": "Batch trading systems and constant function market makers (CFMMs) are two\ndistinct market design innovations that have recently come to prominence as\nways to address some of the shortcomings of decentralized trading systems.\nHowever, different deployments have chosen substantially different methods for\nintegrating the two innovations.\nWe show here from a minimal set of axioms describing the beneficial\nproperties of each innovation that there is in fact only one, unique method for\nintegrating CFMMs into batch trading schemes that preserves all the beneficial\nproperties of both. Deployment of a batch trading schemes trading many assets\nsimultaneously requires a reliable algorithm for approximating equilibria in\nArrow-Debreu exchange markets. We study this problem when batches contain limit\norders and CFMMs. Specifically, we find that CFMM design affects the asymptotic\ncomplexity of the problem, give an easily-checkable criterion to validate that\na user-submitted CFMM is computationally tractable in a batch, and give a\nconvex program that computes equilibria on batches of limit orders and CFMMs.\nEquivalently, this convex program computes equilibria of Arrow-Debreu exchange\nmarkets when every agent's demand response satisfies weak gross\nsubstitutability and every agent has utility for only two types of assets. This\nconvex program has rational solutions when run on many (but not all) natural\nclasses of widely-deployed CFMMs.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Mohak Goyal",
      "Geoffrey Ramseyer",
      "Ashish Goel",
      "David Mazi\u00e8res"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.04929"
  },
  {
    "id": "arXiv:2210.04932",
    "title": "NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills  using Neural Radiance Fields",
    "abstract": "We present a system for applying sim2real approaches to \"in the wild\" scenes\nwith realistic visuals, and to policies which rely on active perception using\nRGB cameras. Given a short video of a static scene collected using a generic\nphone, we learn the scene's contact geometry and a function for novel view\nsynthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering\nof the static scene by overlaying the rendering of other dynamic objects (e.g.\nthe robot's own body, a ball). A simulation is then created using the rendering\nengine in a physics simulator which computes contact dynamics from the static\nscene geometry (estimated from the NeRF volume density) and the dynamic\nobjects' geometry and physical properties (assumed known). We demonstrate that\nwe can use this simulation to learn vision-based whole body navigation and ball\npushing policies for a 20 degrees of freedom humanoid robot with an actuated\nhead-mounted RGB camera, and we successfully transfer these policies to a real\nrobot. Project video is available at\nhttps://sites.google.com/view/nerf2real/home",
    "descriptor": "",
    "authors": [
      "Arunkumar Byravan",
      "Jan Humplik",
      "Leonard Hasenclever",
      "Arthur Brussee",
      "Francesco Nori",
      "Tuomas Haarnoja",
      "Ben Moran",
      "Steven Bohez",
      "Fereshteh Sadeghi",
      "Bojan Vujatovic",
      "Nicolas Heess"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04932"
  },
  {
    "id": "arXiv:2210.04933",
    "title": "An Action Is Worth Multiple Words: Handling Ambiguity in Action  Recognition",
    "abstract": "Precisely naming the action depicted in a video can be a challenging and\noftentimes ambiguous task. In contrast to object instances represented as nouns\n(e.g. dog, cat, chair, etc.), in the case of actions, human annotators\ntypically lack a consensus as to what constitutes a specific action (e.g.\njogging versus running). In practice, a given video can contain multiple valid\npositive annotations for the same action. As a result, video datasets often\ncontain significant levels of label noise and overlap between the atomic action\nclasses. In this work, we address the challenge of training multi-label action\nrecognition models from only single positive training labels. We propose two\napproaches that are based on generating pseudo training examples sampled from\nsimilar instances within the train set. Unlike other approaches that use\nmodel-derived pseudo-labels, our pseudo-labels come from human annotations and\nare selected based on feature similarity. To validate our approaches, we create\na new evaluation benchmark by manually annotating a subset of\nEPIC-Kitchens-100's validation set with multiple verb labels. We present\nresults on this new test set along with additional results on a new version of\nHMDB-51, called Confusing-HMDB-102, where we outperform existing methods in\nboth cases. Data and code are available at\nhttps://github.com/kiyoon/verb_ambiguity",
    "descriptor": "\nComments: BMVC 2022\n",
    "authors": [
      "Kiyoon Kim",
      "Davide Moltisanti",
      "Oisin Mac Aodha",
      "Laura Sevilla-Lara"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.04933"
  },
  {
    "id": "arXiv:2210.04935",
    "title": "Deep Insights of Learning based Micro Expression Recognition: A  Perspective on Promises, Challenges and Research Needs",
    "abstract": "Micro expression recognition (MER) is a very challenging area of research due\nto its intrinsic nature and fine-grained changes. In the literature, the\nproblem of MER has been solved through handcrafted/descriptor-based techniques.\nHowever, in recent times, deep learning (DL) based techniques have been adopted\nto gain higher performance for MER. Also, rich survey articles on MER are\navailable by summarizing the datasets, experimental settings, conventional and\ndeep learning methods. In contrast, these studies lack the ability to convey\nthe impact of network design paradigms and experimental setting strategies for\nDL-based MER. Therefore, this paper aims to provide a deep insight into the\nDL-based MER frameworks with a perspective on promises in network model\ndesigning, experimental strategies, challenges, and research needs. Also, the\ndetailed categorization of available MER frameworks is presented in various\naspects of model design and technical characteristics. Moreover, an empirical\nanalysis of the experimental and validation protocols adopted by MER methods is\npresented. The challenges mentioned earlier and network design strategies may\nassist the affective computing research community in forging ahead in MER\nresearch. Finally, we point out the future directions, research needs, and draw\nour conclusions.",
    "descriptor": "",
    "authors": [
      "Monu Verma",
      "Santosh Kumar Vipparthi",
      "Girdhari Singh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.04935"
  },
  {
    "id": "arXiv:2210.04936",
    "title": "EarthNets: Empowering AI in Earth Observation",
    "abstract": "Earth observation, aiming at monitoring the state of planet Earth using\nremote sensing data, is critical for improving our daily lives and living\nenvironment. With an increasing number of satellites in orbit, more and more\ndatasets with diverse sensors and research domains are published to facilitate\nthe research of the remote sensing community. In this paper, for the first\ntime, we present a comprehensive review of more than 400 publicly published\ndatasets, including applications like, land use/cover, change/disaster\nmonitoring, scene understanding, agriculture, climate change and weather\nforecasting. We systemically analyze these Earth observation datasets from five\naspects, including the volume, bibliometric analysis, research domains and the\ncorrelation between datasets. Based on the dataset attributes, we propose to\nmeasure, rank and select datasets to build a new benchmark for model\nevaluation. Furthermore, a new platform for Earth observation, termed\nEarthNets, is released towards a fair and consistent evaluation of deep\nlearning methods on remote sensing data. EarthNets supports standard dataset\nlibraries and cutting-edge deep learning models to bridge the gap between\nremote sensing and the machine learning community. Based on the EarthNets\nplatform, extensive deep learning methods are evaluated on the new benchmark.\nThe insightful results are beneficial to future research. The platform, dataset\ncollections are publicly available at https://earthnets.nicepage.io.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Zhitong Xiong",
      "Fahong Zhang",
      "Yi Wang",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.04936"
  },
  {
    "id": "arXiv:2210.04941",
    "title": "SLURP! Spectroscopy of Liquids Using Robot Pre-Touch Sensing",
    "abstract": "Liquids and granular media are pervasive throughout human environments. Their\nfree-flowing nature causes people to constrain them into containers. We do so\nwith thousands of different types of containers made out of different materials\nwith varying sizes, shapes, and colors. In this work, we present a\nstate-of-the-art sensing technique for robots to perceive what liquid is inside\nof an unknown container. We do so by integrating Visible to Near Infrared\n(VNIR) reflectance spectroscopy into a robot's end effector. We introduce a\nhierarchical model for inferring the material classes of both containers and\ninternal contents given spectral measurements from two integrated\nspectrometers. To train these inference models, we capture and open source a\ndataset of spectral measurements from over 180 different combinations of\ncontainers and liquids. Our technique demonstrates over 85% accuracy in\nidentifying 13 different liquids and granular media contained within 13\ndifferent containers. The sensitivity of our spectral readings allow our model\nto also identify the material composition of the containers themselves with 96%\naccuracy. Overall, VNIR spectroscopy presents a promising method to give\nhousehold robots a general-purpose ability to infer the liquids inside of\ncontainers, without needing to open or manipulate the containers.",
    "descriptor": "",
    "authors": [
      "Nathaniel Hanson",
      "Wesley Lewis",
      "Kavya Puthuveetil",
      "Donelle Furline",
      "Akhil Padmanabha",
      "Ta\u015fk\u0131n Pad\u0131r",
      "Zackory Erickson"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.04941"
  },
  {
    "id": "arXiv:2210.04942",
    "title": "Unveiling Hidden DNN Defects with Decision-Based Metamorphic Testing",
    "abstract": "Contemporary DNN testing works are frequently conducted using metamorphic\ntesting (MT). In general, de facto MT frameworks mutate DNN input images using\nsemantics-preserving mutations and determine if DNNs can yield consistent\npredictions. Nevertheless, we find that DNNs may rely on erroneous decisions\n(certain components on the DNN inputs) to make predictions, which may still\nretain the outputs by chance. Such DNN defects would be neglected by existing\nMT frameworks. Erroneous decisions, however, would likely result in successive\nmis-predictions over diverse images that may exist in real-life scenarios.\nThis research aims to unveil the pervasiveness of hidden DNN defects caused\nby incorrect DNN decisions (but retaining consistent DNN predictions). To do\nso, we tailor and optimize modern eXplainable AI (XAI) techniques to identify\nvisual concepts that represent regions in an input image upon which the DNN\nmakes predictions. Then, we extend existing MT-based DNN testing frameworks to\ncheck the consistency of DNN decisions made over a test input and its mutated\ninputs. Our evaluation shows that existing MT frameworks are oblivious to a\nconsiderable number of DNN defects caused by erroneous decisions. We conduct\nhuman evaluations to justify the validity of our findings and to elucidate\ntheir characteristics. Through the lens of DNN decision-based metamorphic\nrelations, we re-examine the effectiveness of metamorphic transformations\nproposed by existing MT frameworks. We summarize lessons from this study, which\ncan provide insights and guidelines for future DNN testing.",
    "descriptor": "\nComments: The extended version of a paper to appear in the Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, (ASE '22), 13 pages\n",
    "authors": [
      "Yuanyuan Yuan",
      "Qi Pang",
      "Shuai Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.04942"
  },
  {
    "id": "arXiv:2210.04944",
    "title": "Masked Autoencoders for Low dose CT denoising",
    "abstract": "Low-dose computed tomography (LDCT) reduces the X-ray radiation but\ncompromises image quality with more noises and artifacts. A plethora of\ntransformer models have been developed recently to improve LDCT image quality.\nHowever, the success of a transformer model relies on a large amount of paired\nnoisy and clean data, which is often unavailable in clinical applications. In\ncomputer vision and natural language processing fields, masked autoencoders\n(MAE) have been proposed as an effective label-free self-pretraining method for\ntransformers, due to its excellent feature representation ability. Here, we\nredesign the classical encoder-decoder learning model to match the denoising\ntask and apply it to LDCT denoising problem. The MAE can leverage the unlabeled\ndata and facilitate structural preservation for the LDCT denoising model when\nground truth data are missing. Experiments on the Mayo dataset validate that\nthe MAE can boost the transformer's denoising performance and relieve the\ndependence on the ground truth data.",
    "descriptor": "",
    "authors": [
      "Dayang Wang",
      "Yongshun Xu",
      "Shuo Han",
      "Hengyong Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04944"
  },
  {
    "id": "arXiv:2210.04946",
    "title": "Reaching Goals is Hard: Settling the Sample Complexity of the Stochastic  Shortest Path",
    "abstract": "We study the sample complexity of learning an $\\epsilon$-optimal policy in\nthe Stochastic Shortest Path (SSP) problem. We first derive sample complexity\nbounds when the learner has access to a generative model. We show that there\nexists a worst-case SSP instance with $S$ states, $A$ actions, minimum cost\n$c_{\\min}$, and maximum expected cost of the optimal policy over all states\n$B_{\\star}$, where any algorithm requires at least\n$\\Omega(SAB_{\\star}^3/(c_{\\min}\\epsilon^2))$ samples to return an\n$\\epsilon$-optimal policy with high probability. Surprisingly, this implies\nthat whenever $c_{\\min}=0$ an SSP problem may not be learnable, thus revealing\nthat learning in SSPs is strictly harder than in the finite-horizon and\ndiscounted settings. We complement this result with lower bounds when prior\nknowledge of the hitting time of the optimal policy is available and when we\nrestrict optimality by competing against policies with bounded hitting time.\nFinally, we design an algorithm with matching upper bounds in these cases. This\nsettles the sample complexity of learning $\\epsilon$-optimal polices in SSP\nwith generative models.\nWe also initiate the study of learning $\\epsilon$-optimal policies without\naccess to a generative model (i.e., the so-called best-policy identification\nproblem), and show that sample-efficient learning is impossible in general. On\nthe other hand, efficient learning can be made possible if we assume the agent\ncan directly reach the goal state from any state by paying a fixed cost. We\nthen establish the first upper and lower bounds under this assumption.\nFinally, using similar analytic tools, we prove that horizon-free regret is\nimpossible in SSPs under general costs, resolving an open problem in\n(Tarbouriech et al., 2021c).",
    "descriptor": "",
    "authors": [
      "Liyu Chen",
      "Andrea Tirinzoni",
      "Matteo Pirotta",
      "Alessandro Lazaric"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.04946"
  },
  {
    "id": "arXiv:2210.04949",
    "title": "A Hybrid Active-Passive Approach to Imbalanced Nonstationary Data Stream  Classification",
    "abstract": "In real-world applications, the process generating the data might suffer from\nnonstationary effects (e.g., due to seasonality, faults affecting sensors or\nactuators, and changes in the users' behaviour). These changes, often called\nconcept drift, might induce severe (potentially catastrophic) impacts on\ntrained learning models that become obsolete over time, and inadequate to solve\nthe task at hand. Learning in presence of concept drift aims at designing\nmachine and deep learning models that are able to track and adapt to concept\ndrift. Typically, techniques to handle concept drift are either active or\npassive, and traditionally, these have been considered to be mutually\nexclusive. Active techniques use an explicit drift detection mechanism, and\nre-train the learning algorithm when concept drift is detected. Passive\ntechniques use an implicit method to deal with drift, and continually update\nthe model using incremental learning. Differently from what present in the\nliterature, we propose a hybrid alternative which merges the two approaches,\nhence, leveraging on their advantages. The proposed method called\nHybrid-Adaptive REBAlancing (HAREBA) significantly outperforms strong baselines\nand state-of-the-art methods in terms of learning quality and speed; we\nexperiment how it is effective under severe class imbalance levels too.",
    "descriptor": "\nComments: Keywords: incremental learning, concept drift, class imbalance, data streams, nonstationary environments\n",
    "authors": [
      "Kleanthis Malialis",
      "Manuel Roveri",
      "Cesare Alippi",
      "Christos G. Panayiotou",
      "Marios M. Polycarpou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04949"
  },
  {
    "id": "arXiv:2210.04951",
    "title": "Ecovisor: A Virtual Energy System for Carbon-Efficient Applications",
    "abstract": "Cloud platforms' rapid growth is raising significant concerns about their\ncarbon emissions. To reduce emissions, future cloud platforms will need to\nincrease their reliance on renewable energy sources, such as solar and wind,\nwhich have zero emissions but are highly unreliable. Unfortunately, today's\nenergy systems effectively mask this unreliability in hardware, which prevents\napplications from optimizing their carbon-efficiency, or work done per kilogram\nof carbon emitted. To address this problem, we design an \"ecovisor\", which\nvirtualizes the energy system and exposes software-defined control of it to\napplications. An ecovisor enables each application to handle clean energy's\nunreliability in software based on its own specific requirements. We implement\na small-scale ecovisor prototype that virtualizes a physical energy system to\nenable software-based application-level i) visibility into variable grid\ncarbon-intensity and renewable generation and ii) control of server power usage\nand battery charging/discharging. We evaluate the ecovisor approach by showing\nhow multiple applications can concurrently exercise their virtual energy system\nin different ways to better optimize carbon-efficiency based on their specific\nrequirements compared to a general system-wide policy.",
    "descriptor": "",
    "authors": [
      "Abel Souza",
      "Noman Bashir",
      "Jorge Murillo",
      "Walid Hanafy",
      "Qianlin Liang",
      "David Irwin",
      "Prashant Shenoy"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.04951"
  },
  {
    "id": "arXiv:2210.04954",
    "title": "Using Immersive Virtual Reality to Enhance Social Interaction among  Older Adults: A Multi-site Study",
    "abstract": "Research examining older adults interactions with Virtual Reality (VR) and\nthe impact of social VR experiences on outcomes such as social engagement has\nbeen limited, especially among older adults. This multi-site pilot study\nevaluated the feasibility and acceptability of a novel social virtual reality\n(VR) program that paired older adults from different geographic locations (New\nYork City, Tallahassee, and Ithaca, N.Y) who engaged in virtual travel and\nproductive engagement activities together. The sample included 36 individuals\naged 60 and older, 25 percent of whom had cognitive impairment (CI). Older\nadults with and without CI reported high levels of engagement in the VR\nenvironment and perceived the social VR program to be enjoyable and usable.\nPerceived Spatial Presence was a central driver of the positive outcomes. Most\nalso indicated a willingness to reconnect with their VR partner in the future.\nThe data also identified important areas for improvement in the program, such\nas the use of more realistic and responsive avatars, controllers with larger\ncontrols, and more time for training. Overall, these findings suggest that VR\nsocial applications may foster social engagement among older adults.",
    "descriptor": "",
    "authors": [
      "Saleh Kalantari",
      "Tong Bill Xu",
      "Armin Mostafavi",
      "Andrew Dilanchian",
      "Benjamin Kim",
      "Walter Boot",
      "Sara Czaja"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.04954"
  },
  {
    "id": "arXiv:2210.04955",
    "title": "f-DM: A Multi-stage Diffusion Model via Progressive Signal  Transformation",
    "abstract": "Diffusion models (DMs) have recently emerged as SoTA tools for generative\nmodeling in various domains. Standard DMs can be viewed as an instantiation of\nhierarchical variational autoencoders (VAEs) where the latent variables are\ninferred from input-centered Gaussian distributions with fixed scales and\nvariances. Unlike VAEs, this formulation limits DMs from changing the latent\nspaces and learning abstract representations. In this work, we propose f-DM, a\ngeneralized family of DMs which allows progressive signal transformation. More\nprecisely, we extend DMs to incorporate a set of (hand-designed or learned)\ntransformations, where the transformed input is the mean of each diffusion\nstep. We propose a generalized formulation and derive the corresponding\nde-noising objective with a modified sampling algorithm. As a demonstration, we\napply f-DM in image generation tasks with a range of functions, including\ndown-sampling, blurring, and learned transformations based on the encoder of\npretrained VAEs. In addition, we identify the importance of adjusting the noise\nlevels whenever the signal is sub-sampled and propose a simple rescaling\nrecipe. f-DM can produce high-quality samples on standard image generation\nbenchmarks like FFHQ, AFHQ, LSUN, and ImageNet with better efficiency and\nsemantic interpretation.",
    "descriptor": "\nComments: 28 pages, 21 figures, work in progress\n",
    "authors": [
      "Jiatao Gu",
      "Shuangfei Zhai",
      "Yizhe Zhang",
      "Miguel Angel Bautista",
      "Josh Susskind"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04955"
  },
  {
    "id": "arXiv:2210.04956",
    "title": "A Monte Carlo Method for 3D Radiative Transfer Equations with  Multifractional Singular Kernels",
    "abstract": "We propose in this work a Monte Carlo method for three dimensional scalar\nradiative transfer equations with non-integrable, space-dependent scattering\nkernels. Such kernels typically account for long-range statistical features,\nand arise for instance in the context of wave propagation in turbulent\natmosphere, geophysics, and medical imaging in the peaked-forward regime. In\ncontrast to the classical case where the scattering cross section is\nintegrable, which results in a non-zero mean free time, the latter here\nvanishes. This creates numerical difficulties as standard Monte Carlo methods\nbased on a naive regularization exhibit large jump intensities and an increased\ncomputational cost. We propose a method inspired by the finance literature\nbased on a small jumps - large jumps decomposition, allowing us to treat the\nsmall jumps efficiently and reduce the computational burden. We demonstrate the\nperformance of the approach with numerical simulations and provide a complete\nerror analysis. The multifractional terminology refers to the fact that the\nhigh frequency contribution of the scattering operator is a fractional\nLaplace-Beltrami operator on the unit sphere with space-dependent index.",
    "descriptor": "\nComments: 33 pages, 18 figures\n",
    "authors": [
      "Christophe Gomez",
      "Olivier Pinaud"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.04956"
  },
  {
    "id": "arXiv:2210.04958",
    "title": "Mining Causality from Continuous-time Dynamics Models: An Application to  Tsunami Forecasting",
    "abstract": "Continuous-time dynamics models, such as neural ordinary differential\nequations, have enabled the modeling of underlying dynamics in time-series data\nand accurate forecasting. However, parameterization of dynamics using a neural\nnetwork makes it difficult for humans to identify causal structures in the\ndata. In consequence, this opaqueness hinders the use of these models in the\ndomains where capturing causal relationships carries the same importance as\naccurate predictions, e.g., tsunami forecasting. In this paper, we address this\nchallenge by proposing a mechanism for mining causal structures from\ncontinuous-time models. We train models to capture the causal structure by\nenforcing sparsity in the weights of the input layers of the dynamics models.\nWe first verify the effectiveness of our method in the scenario where the exact\ncausal-structures of time-series are known as a priori. We next apply our\nmethod to a real-world problem, namely tsunami forecasting, where the exact\ncausal-structures are difficult to characterize. Experimental results show that\nthe proposed method is effective in learning physically-consistent causal\nrelationships while achieving high forecasting accuracy.",
    "descriptor": "",
    "authors": [
      "Fan Wu",
      "Sanghyun Hong",
      "Dobsub Rim",
      "Noseong Park",
      "Kookjin Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.04958"
  },
  {
    "id": "arXiv:2210.04959",
    "title": "Characterization of anomalous diffusion through convolutional  transformers",
    "abstract": "The results of the Anomalous Diffusion Challenge (AnDi Challenge) have shown\nthat machine learning methods can outperform classical statistical methodology\nat the characterization of anomalous diffusion in both the inference of the\nanomalous diffusion exponent alpha associated with each trajectory (Task 1),\nand the determination of the underlying diffusive regime which produced such\ntrajectories (Task 2). Furthermore, of the five teams that finished in the top\nthree across both tasks of the AnDi challenge, three of those teams used\nrecurrent neural networks (RNNs). While RNNs, like the long short-term memory\n(LSTM) network, are effective at learning long-term dependencies in sequential\ndata, their key disadvantage is that they must be trained sequentially. In\norder to facilitate training with larger data sets, by training in parallel, we\npropose a new transformer based neural network architecture for the\ncharacterization of anomalous diffusion. Our new architecture, the\nConvolutional Transformer (ConvTransformer) uses a bi-layered convolutional\nneural network to extract features from our diffusive trajectories that can be\nthought of as being words in a sentence. These features are then fed to two\ntransformer encoding blocks that perform either regression or classification.\nTo our knowledge, this is the first time transformers have been used for\ncharacterizing anomalous diffusion. Moreover, this may be the first time that a\ntransformer encoding block has been used with a convolutional neural network\nand without the need for a transformer decoding block or positional encoding.\nApart from being able to train in parallel, we show that the ConvTransformer is\nable to outperform the previous state of the art at determining the underlying\ndiffusive regime in short trajectories (length 10-50 steps), which are the most\nimportant for experimental researchers.",
    "descriptor": "\nComments: 20 pages, 13 figures\n",
    "authors": [
      "Nicol\u00e1s Firbas",
      "\u00d2scar Garibo-i-Orts",
      "Miguel \u00c1ngel Garcia-March",
      "J. Alberto Conejero"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.04959"
  },
  {
    "id": "arXiv:2210.04962",
    "title": "Domain-Specific Word Embeddings with Structure Prediction",
    "abstract": "Complementary to finding good general word embeddings, an important question\nfor representation learning is to find dynamic word embeddings, e.g., across\ntime or domain. Current methods do not offer a way to use or predict\ninformation on structure between sub-corpora, time or domain and dynamic\nembeddings can only be compared after post-alignment. We propose novel word\nembedding methods that provide general word representations for the whole\ncorpus, domain-specific representations for each sub-corpus, sub-corpus\nstructure, and embedding alignment simultaneously. We present an empirical\nevaluation on New York Times articles and two English Wikipedia datasets with\narticles on science and philosophy. Our method, called Word2Vec with Structure\nPrediction (W2VPred), provides better performance than baselines in terms of\nthe general analogy tests, domain-specific analogy tests, and multiple specific\nword embedding evaluations as well as structure prediction performance when no\nstructure is given a priori. As a use case in the field of Digital Humanities\nwe demonstrate how to raise novel research questions for high literature from\nthe German Text Archive.",
    "descriptor": "\nComments: accepted at TACL 13 pages, 4 figures\n",
    "authors": [
      "Stephanie Brandl",
      "David Lassner",
      "Anne Baillot",
      "Shinichi Nakajima"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04962"
  },
  {
    "id": "arXiv:2210.04963",
    "title": "Every word counts: A multilingual analysis of individual human alignment  with model attention",
    "abstract": "Human fixation patterns have been shown to correlate strongly with\nTransformer-based attention. Those correlation analyses are usually carried out\nwithout taking into account individual differences between participants and are\nmostly done on monolingual datasets making it difficult to generalise findings.\nIn this paper, we analyse eye-tracking data from speakers of 13 different\nlanguages reading both in their native language (L1) and in English as language\nlearners (L2). We find considerable differences between languages but also that\nindividual reading behaviour such as skipping rate, total reading time and\nvocabulary knowledge (LexTALE) influence the alignment between humans and\nmodels to an extent that should be considered in future studies.",
    "descriptor": "\nComments: short paper, accepted at AACL 2022\n",
    "authors": [
      "Stephanie Brandl",
      "Nora Hollenstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04963"
  },
  {
    "id": "arXiv:2210.04964",
    "title": "Generating Executable Action Plans with Environmentally-Aware Language  Models",
    "abstract": "Large Language Models (LLMs) trained using massive text datasets have\nrecently shown promise in generating action plans for robotic agents from high\nlevel text queries. However, these models typically do not consider the robot's\nenvironment, resulting in generated plans that may not actually be executable\ndue to ambiguities in the planned actions or environmental constraints. In this\npaper, we propose an approach to generate environmentally-aware action plans\nthat can be directly mapped to executable agent actions. Our approach involves\nintegrating environmental objects and object relations as additional inputs\ninto LLM action plan generation to provide the system with an awareness of its\nsurroundings, resulting in plans where each generated action is mapped to\nobjects present in the scene. We also design a novel scoring function that,\nalong with generating the action steps and associating them with objects, helps\nthe system disambiguate among object instances and take into account their\nstates. We evaluate our approach using the VirtualHome simulator and the\nActivityPrograms knowledge base. Our results show that the action plans\ngenerated from our system outperform prior work in terms of their correctness\nand executability by 5.3% and 8.9% respectively.",
    "descriptor": "",
    "authors": [
      "Maitrey Gramopadhye",
      "Daniel Szafir"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04964"
  },
  {
    "id": "arXiv:2210.04971",
    "title": "Multi-step Planning for Automated Hyperparameter Optimization with  OptFormer",
    "abstract": "As machine learning permeates more industries and models become more\nexpensive and time consuming to train, the need for efficient automated\nhyperparameter optimization (HPO) has never been more pressing. Multi-step\nplanning based approaches to hyperparameter optimization promise improved\nefficiency over myopic alternatives by more effectively balancing out\nexploration and exploitation. However, the potential of these approaches has\nnot been fully realized due to their technical complexity and computational\nintensity. In this work, we leverage recent advances in Transformer-based,\nnatural-language-interfaced hyperparameter optimization to circumvent these\nbarriers. We build on top of the recently proposed OptFormer which casts both\nhyperparameter suggestion and target function approximation as autoregressive\ngeneration thus making planning via rollouts simple and efficient. We conduct\nextensive exploration of different strategies for performing multi-step\nplanning on top of the OptFormer model to highlight its potential for use in\nconstructing non-myopic HPO strategies.",
    "descriptor": "\nComments: 8 pages, 7 figures\n",
    "authors": [
      "Lucio M. Dery",
      "Abram L. Friesen",
      "Nando De Freitas",
      "Marc'Aurelio Ranzato",
      "Yutian Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04971"
  },
  {
    "id": "arXiv:2210.04975",
    "title": "MACARONS: Open hardware for vertical farming",
    "abstract": "The Modular Automated Crop Array Online System (MACARONS) is an extensible,\nscalable, open hardware system for plant transport in automated horticulture\nsystems such as vertical farms. It is specified to move trays of plants up to\n1060mm x 630mm and 12.5kg at a rate of 100mm/s along the guide rails and\n33.3mm/s up the lifts, such as between stations for monitoring and actuating\nplants. The cost for the construction of one grow unit of MACARONS is 144.96USD\nwhich equates to 128.85USD/m$^2$ of grow area. The designs are released and\nmeets the requirements of CERN-OSH-W, which includes step-by-step graphical\nbuild instructions and can be built by a typical technical person in one day at\na cost of 1535.50 USD. Integrated tests are included in the build instructions\nare used to validate against the specifications, and we report on a successful\nbuild. Through a simple analysis, we demonstrate that MACARONS can operate at a\nrate sufficient to automate tray loading/unloading, to reduce labour costs in a\nvertical farm.",
    "descriptor": "",
    "authors": [
      "Vijja Wichitwechkarn",
      "Charles Fox"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.04975"
  },
  {
    "id": "arXiv:2210.04976",
    "title": "Optimal wireless rate and power control in the presence of jammers using  reinforcement learning",
    "abstract": "Future wireless networks require high throughput and energy efficiency. This\npaper studies using Reinforcement Learning (RL) to do transmission rate and\npower control for maximizing a joint reward function consisting of both\nthroughput and energy consumption. We design the system state to include\nfactors that reflect packet queue length, interference from other nodes,\nquality of the wireless channel, battery status, etc. The reward function is\nnormalized and does not involve unit conversion. It can be used to train three\ndifferent types of agents: throughput-critical, energy-critical, and throughput\nand energy balanced. Using the NS-3 network simulation software, we implement\nand train these agents in an 802.11ac network with the presence of a jammer. We\nthen test the agents with two jamming nodes interfering with the packets\nreceived at the receiver. We compare the performance of our RL optimal policies\nwith the popular Minstrel rate adaptation algorithm: our approach can achieve\n(i) higher throughput when using the throughput-critical reward function; (ii)\nlower energy consumption when using the energy-critical reward function; and\n(iii) higher throughput and slightly higher energy when using the throughput\nand energy balanced reward function. Although our discussion is focused on\n802.11ac networks, our method is readily applicable to other types of wireless\nnetworks.",
    "descriptor": "\nComments: Published in International Telecommunication Union, Journal on Future and Evolving Technologies (ITU J-FET) 2022\n",
    "authors": [
      "Fadlullah Raji",
      "Lei Miao"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.04976"
  },
  {
    "id": "arXiv:2210.04977",
    "title": "Domain-guided data augmentation for deep learning on medical imaging",
    "abstract": "While domain-specific data augmentation can be useful in training neural\nnetworks for medical imaging tasks, such techniques have not been widely used\nto date. Here, we test whether domain-specific data augmentation is useful for\nmedical imaging using a well-benchmarked task: view classification on fetal\nultrasound FETAL-125 and OB-125 datasets. We found that using a\ncontext-preserving cut-paste strategy, we could create valid training data as\nmeasured by performance of the resulting trained model on the benchmark test\ndataset. When used in an online fashion, models trained on this data performed\nsimilarly to those trained using traditional data augmentation (FETAL-125\nF-score 85.33+/-0.24 vs 86.89+/-0.60, p-value 0.0139; OB-125 F-score\n74.60+/-0.11 vs 72.43+/-0.62, p-value 0.0039). Furthermore, the ability to\nperform augmentations during training time, as well as the ability to apply\nchosen augmentations equally across data classes, are important considerations\nin designing a bespoke data augmentation. Finally, we provide open-source code\nto facilitate running bespoke data augmentations in an online fashion. Taken\ntogether, this work expands the ability to design and apply domain-guided data\naugmentations for medical imaging tasks.",
    "descriptor": "\nComments: 18 pages, 6 Tables, 3 Figures\n",
    "authors": [
      "Chinmayee Athalye",
      "Rima Arnaout"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04977"
  },
  {
    "id": "arXiv:2210.04979",
    "title": "Label-free segmentation from cardiac ultrasound using self-supervised  learning",
    "abstract": "Background: Segmentation and measurement of cardiac chambers is critical in\nechocardiography but is also laborious and poorly reproducible. Neural networks\ncan assist, but supervised approaches require the same laborious manual\nannotations, while unsupervised approaches have fared poorly in ultrasound to\ndate. Objectives: We built a pipeline for self-supervised (no manual labels\nrequired) segmentation of cardiac chambers, combining computer vision, clinical\ndomain knowledge, and deep learning. Methods: We trained on 450 echocardiograms\n(145,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean\nage 61 years, 51% female), using the resulting segmentations to calculate\nstructural and functional measurements. We also tested our pipeline against\nexternal images from an additional 10,030 patients (20,060 images) with\navailable manual tracings of the left ventricle. Results: r2 between clinically\nmeasured and pipeline-predicted measurements were similar to reported\ninter-clinician variation for LVESV and LVEDV (pipeline vs. clinical r2= 0.74\nand r2=0.65, respectively), LVEF and LV mass (r2= 0.46 and r2=0.54), left and\nright atrium volumes (r2=0.7 and r2=0.6), and right ventricle area (r2=0.47).\nWhen binarized into normal vs. abnormal categories, average accuracy was 0.81\n(range 0.71-0.95). A subset of the test echocardiograms (n=553) had\ncorresponding cardiac MRI; correlation between pipeline and CMR measurements\nwas similar to that between clinical echocardiogram and CMR. Finally, in the\nexternal dataset, our pipeline accurately segments the left ventricle with an\naverage Dice score of 0.83 (95% CI 0.83). Conclusions: Our results demonstrate\na human-label-free, valid, and scalable method for segmentation from\nultrasound, a noisy but globally important imaging modality.",
    "descriptor": "\nComments: 48 pages, 5 Tables, 9 Figures\n",
    "authors": [
      "Danielle L. Ferreira",
      "Zaynaf Salaymang",
      "Rima Arnaout"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04979"
  },
  {
    "id": "arXiv:2210.04981",
    "title": "Hybrid DoF: Ray-Traced and Post-Processed Hybrid Depth of Field Effect  for Real-Time Rendering",
    "abstract": "Depth of Field (DoF) in games is usually achieved as a post-process effect by\nblurring pixels in the sharp rasterized image based on the defined focus plane.\nThis paper describes a novel real-time DoF technique that uses ray tracing with\nimage filtering to achieve more accurate partial occlusion semi-transparencies\non edges of blurry foreground geometry. This hybrid rendering technique\nleverages ray tracing hardware acceleration as well as spatio-temporal\nreconstruction techniques to achieve interactive frame rates.",
    "descriptor": "",
    "authors": [
      "Yu Wei Tan",
      "Nicholas Chua",
      "Nathan Biette",
      "Anand Bhojan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.04981"
  },
  {
    "id": "arXiv:2210.04982",
    "title": "REV: Information-Theoretic Evaluation of Free-Text Rationales",
    "abstract": "Free-text rationales are a promising step towards explainable AI, yet their\nevaluation remains an open research problem. While existing metrics have mostly\nfocused on measuring the direct association between the rationale and a given\nlabel, we argue that an ideal metric should also be able to focus on the new\ninformation uniquely provided in the rationale that is otherwise not provided\nin the input or the label. We investigate this research problem from an\ninformation-theoretic perspective using the conditional V-information. More\nconcretely, we propose a metric called REV (Rationale Evaluation with\nconditional V-information), that can quantify the new information in a\nrationale supporting a given label beyond the information already available in\nthe input or the label. Experiments on reasoning tasks across four benchmarks,\nincluding few-shot prompting with GPT-3, demonstrate the effectiveness of REV\nin evaluating different types of rationale-label pairs, compared to existing\nmetrics. Through several quantitative comparisons, we demonstrate the\ncapability of REV in providing more sensitive measurements of new information\nin free-text rationales with respect to a label. Furthermore, REV is consistent\nwith human judgments on rationale evaluations. Overall, when used alongside\ntraditional performance metrics, REV provides deeper insights into a models'\nreasoning and prediction processes.",
    "descriptor": "",
    "authors": [
      "Hanjie Chen",
      "Faeze Brahman",
      "Xiang Ren",
      "Yangfeng Ji",
      "Yejin Choi",
      "Swabha Swayamdipta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04982"
  },
  {
    "id": "arXiv:2210.04988",
    "title": "Simulating Coverage Path Planning with Roomba",
    "abstract": "Coverage Path Planning involves visiting every unoccupied state in an\nenvironment with obstacles. In this paper, we explore this problem in\nenvironments which are initially unknown to the agent, for purposes of\nsimulating the task of a vacuum cleaning robot. A survey of prior work reveals\nsparse effort in applying learning to solve this problem. In this paper, we\nexplore modeling a Cover Path Planning problem using Deep Reinforcement\nLearning, and compare it with the performance of the built-in algorithm of the\nRoomba, a popular vacuum cleaning robot.",
    "descriptor": "",
    "authors": [
      "Robert Chuchro"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04988"
  },
  {
    "id": "arXiv:2210.04989",
    "title": "On Designing Day Ahead and Same Day Ridership Level Prediction Models  for City-Scale Transit Networks Using Noisy APC Data",
    "abstract": "The ability to accurately predict public transit ridership demand benefits\npassengers and transit agencies. Agencies will be able to reallocate buses to\nhandle under or over-utilized bus routes, improving resource utilization, and\npassengers will be able to adjust and plan their schedules to avoid overcrowded\nbuses and maintain a certain level of comfort. However, accurately predicting\noccupancy is a non-trivial task. Various reasons such as heterogeneity,\nevolving ridership patterns, exogenous events like weather, and other\nstochastic variables, make the task much more challenging. With the progress of\nbig data, transit authorities now have access to real-time passenger occupancy\ninformation for their vehicles. The amount of data generated is staggering.\nWhile there is no shortage in data, it must still be cleaned, processed,\naugmented, and merged before any useful information can be generated. In this\npaper, we propose the use and fusion of data from multiple sources, cleaned,\nprocessed, and merged together, for use in training machine learning models to\npredict transit ridership. We use data that spans a 2-year period (2020-2022)\nincorporating transit, weather, traffic, and calendar data. The resulting data,\nwhich equates to 17 million observations, is used to train separate models for\nthe trip and stop level prediction. We evaluate our approach on real-world\ntransit data provided by the public transit agency of Nashville, TN. We\ndemonstrate that the trip level model based on Xgboost and the stop level model\nbased on LSTM outperform the baseline statistical model across the entire\ntransit service day.",
    "descriptor": "\nComments: 9 pages, 11 figures\n",
    "authors": [
      "Jose Paolo Talusan",
      "Ayan Mukhopadhyay",
      "Dan Freudberg",
      "Abhishek Dubey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04989"
  },
  {
    "id": "arXiv:2210.04992",
    "title": "Extracting or Guessing? Improving Faithfulness of Event Temporal  Relation Extraction",
    "abstract": "In this paper, we seek to improve the faithfulness of \\temprel extraction\nmodels from two perspectives. The first perspective is to extract genuinely\nbased on contextual description. To achieve this, we propose to conduct\ncounterfactual analysis to attenuate the effects of two significant types of\ntraining biases: the event trigger bias and the frequent label bias. We also\nadd tense information into event representations to explicitly place an\nemphasis on the contextual description. The second perspective is to provide\nproper uncertainty estimation and abstain from extraction when no relation is\ndescribed in the text. By parameterization of Dirichlet Prior over the\nmodel-predicted categorical distribution, we improve the model estimates of the\ncorrectness likelihood and make TempRel predictions more selective. We also\nemploy temperature scaling to recalibrate the model confidence measure after\nbias mitigation. Through experimental analysis on MATRES, MATRES-DS, and\nTDDiscourse, we demonstrate that our model extracts TempRel and timelines more\nfaithfully compared to SOTA methods, especially under distribution shifts.",
    "descriptor": "",
    "authors": [
      "Haoyu Wang",
      "Hongming Zhang",
      "Yuqian Deng",
      "Jacob R. Gardner",
      "Muhao Chen",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04992"
  },
  {
    "id": "arXiv:2210.04993",
    "title": "Learning with an Evolving Class Ontology",
    "abstract": "Lifelong learners must recognize concept vocabularies that evolve over time.\nA common yet underexplored scenario is learning with class labels over time\nthat refine/expand old classes. For example, humans learn to recognize ${\\tt\ndog}$ before dog breeds. In practical settings, dataset $\\textit{versioning}$\noften introduces refinement to ontologies, such as autonomous vehicle\nbenchmarks that refine a previous ${\\tt vehicle}$ class into ${\\tt school-bus}$\nas autonomous operations expand to new cities. This paper formalizes a protocol\nfor studying the problem of $\\textit{Learning with Evolving Class Ontology}$\n(LECO). LECO requires learning classifiers in distinct time periods (TPs); each\nTP introduces a new ontology of \"fine\" labels that refines old ontologies of\n\"coarse\" labels (e.g., dog breeds that refine the previous ${\\tt dog}$). LECO\nexplores such questions as whether to annotate new data or relabel the old, how\nto leverage coarse labels, and whether to finetune the previous TP's model or\ntrain from scratch. To answer these questions, we leverage insights from\nrelated problems such as class-incremental learning. We validate them under the\nLECO protocol through the lens of image classification (CIFAR and iNaturalist)\nand semantic segmentation (Mapillary). Our experiments lead to surprising\nconclusions; while the current status quo is to relabel existing datasets with\nnew ontologies (such as COCO-to-LVIS or Mapillary1.2-to-2.0), LECO demonstrates\nthat a far better strategy is to annotate $\\textit{new}$ data with the new\nontology. However, this produces an aggregate dataset with inconsistent\nold-vs-new labels, complicating learning. To address this challenge, we adopt\nmethods from semi-supervised and partial-label learning. Such strategies can\nsurprisingly be made near-optimal, approaching an \"oracle\" that learns on the\naggregate dataset exhaustively labeled with the newest ontology.",
    "descriptor": "\nComments: NeurIPS 2022; Website: this https URL\n",
    "authors": [
      "Zhiqiu Lin",
      "Deepak Pathak",
      "Yu-Xiong Wang",
      "Deva Ramanan",
      "Shu Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04993"
  },
  {
    "id": "arXiv:2210.04995",
    "title": "FEAMOE: Fair, Explainable and Adaptive Mixture of Experts",
    "abstract": "Three key properties that are desired of trustworthy machine learning models\ndeployed in high-stakes environments are fairness, explainability, and an\nability to account for various kinds of \"drift\". While drifts in model\naccuracy, for example due to covariate shift, have been widely investigated,\ndrifts in fairness metrics over time remain largely unexplored. In this paper,\nwe propose FEAMOE, a novel \"mixture-of-experts\" inspired framework aimed at\nlearning fairer, more explainable/interpretable models that can also rapidly\nadjust to drifts in both the accuracy and the fairness of a classifier. We\nillustrate our framework for three popular fairness measures and demonstrate\nhow drift can be handled with respect to these fairness constraints.\nExperiments on multiple datasets show that our framework as applied to a\nmixture of linear experts is able to perform comparably to neural networks in\nterms of accuracy while producing fairer models. We then use the large-scale\nHMDA dataset and show that while various models trained on HMDA demonstrate\ndrift with respect to both accuracy and fairness, FEAMOE can ably handle these\ndrifts with respect to all the considered fairness measures and maintain model\naccuracy as well. We also prove that the proposed framework allows for\nproducing fast Shapley value explanations, which makes computationally\nefficient feature attribution based explanations of model decisions readily\navailable via FEAMOE.",
    "descriptor": "",
    "authors": [
      "Shubham Sharma",
      "Jette Henderson",
      "Joydeep Ghosh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.04995"
  },
  {
    "id": "arXiv:2210.04996",
    "title": "Graph2Vid: Flow graph to Video Grounding forWeakly-supervised Multi-Step  Localization",
    "abstract": "In this work, we consider the problem of weakly-supervised multi-step\nlocalization in instructional videos. An established approach to this problem\nis to rely on a given list of steps. However, in reality, there is often more\nthan one way to execute a procedure successfully, by following the set of steps\nin slightly varying orders. Thus, for successful localization in a given video,\nrecent works require the actual order of procedure steps in the video, to be\nprovided by human annotators at both training and test times. Instead, here, we\nonly rely on generic procedural text that is not tied to a specific video. We\nrepresent the various ways to complete the procedure by transforming the list\nof instructions into a procedure flow graph which captures the partial order of\nsteps. Using the flow graphs reduces both training and test time annotation\nrequirements. To this end, we introduce the new problem of flow graph to video\ngrounding. In this setup, we seek the optimal step ordering consistent with the\nprocedure flow graph and a given video. To solve this problem, we propose a new\nalgorithm - Graph2Vid - that infers the actual ordering of steps in the video\nand simultaneously localizes them. To show the advantage of our proposed\nformulation, we extend the CrossTask dataset with procedure flow graph\ninformation. Our experiments show that Graph2Vid is both more efficient than\nthe baselines and yields strong step localization results, without the need for\nstep order annotation.",
    "descriptor": "\nComments: ECCV'22, oral\n",
    "authors": [
      "Nikita Dvornik",
      "Isma Hadji",
      "Hai Pham",
      "Dhaivat Bhatt",
      "Brais Martinez",
      "Afsaneh Fazly",
      "Allan D. Jepson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04996"
  },
  {
    "id": "arXiv:2210.05000",
    "title": "A Hierarchical Grouping Algorithm for the Multi-Vehicle Dial-a-Ride  Problem",
    "abstract": "Ride-sharing is an essential aspect of modern urban mobility. In this paper,\nwe consider a classical problem in ride-sharing - the Multi-Vehicle Dial-a-Ride\nProblem (Multi-Vehicle DaRP). Given a fleet of vehicles with a fixed capacity\nstationed at various locations and a set of ride requests specified by origins\nand destinations, the goal is to serve all requests such that no vehicle is\nassigned more passengers than its capacity at any point along its trip. We\npropose an algorithm HRA, which is the first non-trivial approximation\nalgorithm for the Multi-Vehicle DaRP. The main technical contribution is to\nreduce the Multi-Vehicle DaRP to a certain capacitated partitioning problem,\nwhich we solve using a novel hierarchical grouping algorithm. Experimental\nresults show that the vehicle routes produced by our algorithm not only exhibit\nless total travel distance compared to state-of-the-art baselines, but also\nenjoy a small in-transit latency, which crucially relates to riders' traveling\ntimes. This suggests that HRA enhances rider experience while being\nenergy-efficient.",
    "descriptor": "",
    "authors": [
      "Kelin Luo",
      "Alexandre M. Florio",
      "Syamantak Das",
      "Xiangyu Guo"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.05000"
  },
  {
    "id": "arXiv:2210.05001",
    "title": "Social Media Personal Event Notifier Using NLP and Machine Learning",
    "abstract": "Social media apps have become very promising and omnipresent in daily life.\nMost social media apps are used to deliver vital information to those nearby\nand far away. As our lives become more hectic, many of us strive to limit our\nusage of social media apps because they are too addictive, and the majority of\nus have gotten preoccupied with our daily lives. Because of this, we frequently\noverlook crucial information, such as invitations to weddings, interviews,\nbirthday parties, etc., or find ourselves unable to attend the event. In most\ncases, this happens because users are more likely to discover the invitation or\ninformation only before the event, giving them little time to prepare. To solve\nthis issue, in this study, we created a system that will collect social media\nchat and filter it using Natural Language Processing (NLP) methods like\nTokenization, Stop Words Removal, Lemmatization, Segmentation, and Named Entity\nRecognition (NER). Also, Machine Learning Algorithms such as K-Nearest Neighbor\n(KNN) Algorithm are implemented to prioritize the received invitation and to\nsort the level of priority. Finally, a customized notification will be\ndelivered to the users where they acknowledge the upcoming event. So, the\nchances of missing the event are less or can be planned.",
    "descriptor": "\nComments: 4 pages, 5 figures\n",
    "authors": [
      "Pavithiran G",
      "Sharan Padmanabhan",
      "Ashwin Kumar BR",
      "Vetriselvi A"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05001"
  },
  {
    "id": "arXiv:2210.05006",
    "title": "Energy-Efficient Deployment of Machine Learning Workloads on  Neuromorphic Hardware",
    "abstract": "As the technology industry is moving towards implementing tasks such as\nnatural language processing, path planning, image classification, and more on\nsmaller edge computing devices, the demand for more efficient implementations\nof algorithms and hardware accelerators has become a significant area of\nresearch. In recent years, several edge deep learning hardware accelerators\nhave been released that specifically focus on reducing the power and area\nconsumed by deep neural networks (DNNs). On the other hand, spiking neural\nnetworks (SNNs) which operate on discrete time-series data, have been shown to\nachieve substantial power reductions over even the aforementioned edge DNN\naccelerators when deployed on specialized neuromorphic event-based/asynchronous\nhardware. While neuromorphic hardware has demonstrated great potential for\naccelerating deep learning tasks at the edge, the current space of algorithms\nand hardware is limited and still in rather early development. Thus, many\nhybrid approaches have been proposed which aim to convert pre-trained DNNs into\nSNNs. In this work, we provide a general guide to converting pre-trained DNNs\ninto SNNs while also presenting techniques to improve the deployment of\nconverted SNNs on neuromorphic hardware with respect to latency, power, and\nenergy. Our experimental results show that when compared against the Intel\nNeural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to\n27x less power and 5x less energy in the tested image classification tasks by\nusing our SNN improvement techniques.",
    "descriptor": "",
    "authors": [
      "Peyton Chandarana",
      "Mohammadreza Mohammadi",
      "James Seekings",
      "Ramtin Zand"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Emerging Technologies (cs.ET)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.05006"
  },
  {
    "id": "arXiv:2210.05008",
    "title": "Fast Hierarchical Learning for Few-Shot Object Detection",
    "abstract": "Transfer learning based approaches have recently achieved promising results\non the few-shot detection task. These approaches however suffer from\n``catastrophic forgetting'' issue due to finetuning of base detector, leading\nto sub-optimal performance on the base classes. Furthermore, the slow\nconvergence rate of stochastic gradient descent (SGD) results in high latency\nand consequently restricts real-time applications. We tackle the aforementioned\nissues in this work. We pose few-shot detection as a hierarchical learning\nproblem, where the novel classes are treated as the child classes of existing\nbase classes and the background class. The detection heads for the novel\nclasses are then trained using a specialized optimization strategy, leading to\nsignificantly lower training times compared to SGD. Our approach obtains\ncompetitive novel class performance on few-shot MS-COCO benchmark, while\ncompletely retaining the performance of the initial model on the base classes.\nWe further demonstrate the application of our approach to a new class-refined\nfew-shot detection task.",
    "descriptor": "\nComments: 8 pages, 5 figures, accepted by IROS2022\n",
    "authors": [
      "Yihang She",
      "Goutam Bhat",
      "Martin Danelljan",
      "Fisher Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05008"
  },
  {
    "id": "arXiv:2210.05014",
    "title": "Exploring Adaptive MCTS with TD Learning in miniXCOM",
    "abstract": "In recent years, Monte Carlo tree search (MCTS) has achieved widespread\nadoption within the game community. Its use in conjunction with deep\nreinforcement learning has produced success stories in many applications. While\nthese approaches have been implemented in various games, from simple board\ngames to more complicated video games such as StarCraft, the use of deep neural\nnetworks requires a substantial training period. In this work, we explore\non-line adaptivity in MCTS without requiring pre-training. We present MCTS-TD,\nan adaptive MCTS algorithm improved with temporal difference learning. We\ndemonstrate our new approach on the game miniXCOM, a simplified version of\nXCOM, a popular commercial franchise consisting of several turn-based tactical\ngames, and show how adaptivity in MCTS-TD allows for improved performances\nagainst opponents.",
    "descriptor": "",
    "authors": [
      "Kimiya Saadat",
      "Richard Zhao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05014"
  },
  {
    "id": "arXiv:2210.05015",
    "title": "Generalized Optimality Guarantees for Solving Continuous Observation  POMDPs through Particle Belief MDP Approximation",
    "abstract": "Partially observable Markov decision processes (POMDPs) provide a flexible\nrepresentation for real-world decision and control problems. However, POMDPs\nare notoriously difficult to solve, especially when the state and observation\nspaces are continuous or hybrid, which is often the case for physical systems.\nWhile recent online sampling-based POMDP algorithms that plan with observation\nlikelihood weighting have shown practical effectiveness, a general theory\nbounding the approximation error of the particle filtering techniques that\nthese algorithms use has not previously been proposed. Our main contribution is\nto formally justify that optimality guarantees in a finite sample particle\nbelief MDP (PB-MDP) approximation of a POMDP/belief MDP yields optimality\nguarantees in the original POMDP as well. This fundamental bridge between\nPB-MDPs and POMDPs allows us to adapt any sampling-based MDP algorithm of\nchoice to a POMDP by solving the corresponding particle belief MDP\napproximation and preserve the convergence guarantees in the POMDP.\nPractically, this means additionally assuming access to the observation density\nmodel, and simply swapping out the state transition generative model with a\nparticle filtering-based model, which only increases the computational\ncomplexity by a factor of $\\mathcal{O}(C)$, with $C$ the number of particles in\na particle belief state. In addition to our theoretical contribution, we\nperform five numerical experiments on benchmark POMDPs to demonstrate that a\nsimple MDP algorithm adapted using PB-MDP approximation, Sparse-PFT, achieves\nperformance competitive with other leading continuous observation POMDP\nsolvers.",
    "descriptor": "",
    "authors": [
      "Michael H. Lim",
      "Tyler J. Becker",
      "Mykel J. Kochenderfer",
      "Claire J. Tomlin",
      "Zachary N. Sunberg"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05015"
  },
  {
    "id": "arXiv:2210.05018",
    "title": "LidarNAS: Unifying and Searching Neural Architectures for 3D Point  Clouds",
    "abstract": "Developing neural models that accurately understand objects in 3D point\nclouds is essential for the success of robotics and autonomous driving.\nHowever, arguably due to the higher-dimensional nature of the data (as compared\nto images), existing neural architectures exhibit a large variety in their\ndesigns, including but not limited to the views considered, the format of the\nneural features, and the neural operations used. Lack of a unified framework\nand interpretation makes it hard to put these designs in perspective, as well\nas systematically explore new ones. In this paper, we begin by proposing a\nunified framework of such, with the key idea being factorizing the neural\nnetworks into a series of view transforms and neural layers. We demonstrate\nthat this modular framework can reproduce a variety of existing works while\nallowing a fair comparison of backbone designs. Then, we show how this\nframework can easily materialize into a concrete neural architecture search\n(NAS) space, allowing a principled NAS-for-3D exploration. In performing\nevolutionary NAS on the 3D object detection task on the Waymo Open Dataset, not\nonly do we outperform the state-of-the-art models, but also report the\ninteresting finding that NAS tends to discover the same macro-level\narchitecture concept for both the vehicle and pedestrian classes.",
    "descriptor": "\nComments: ECCV 2022\n",
    "authors": [
      "Chenxi Liu",
      "Zhaoqi Leng",
      "Pei Sun",
      "Shuyang Cheng",
      "Charles R. Qi",
      "Yin Zhou",
      "Mingxing Tan",
      "Dragomir Anguelov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05018"
  },
  {
    "id": "arXiv:2210.05019",
    "title": "Parameterized Approaches to Orthogonal Compaction",
    "abstract": "Orthogonal graph drawings are used in applications such as UML diagrams, VLSI\nlayout, cable plans, and metro maps. We focus on drawing planar graphs and\nassume that we are given an \\emph{orthogonal representation} that describes the\ndesired shape, but not the exact coordinates of a drawing. Our aim is to\ncompute an orthogonal drawing on the grid that has minimum area among all grid\ndrawings that adhere to the given orthogonal representation.\nThis problem is called orthogonal compaction (OC) and is known to be NP-hard,\neven for orthogonal representations of cycles [Evans et al., 2022]. We\ninvestigate the complexity of OC with respect to several parameters. Among\nothers, we show that OC is fixed-parameter tractable with respect to the most\nnatural of these parameters, namely, the number of \\emph{kitty corners} of the\northogonal representation: the presence of pairs of kitty corners in an\northogonal representation makes the OC problem hard. Informally speaking, a\npair of kitty corners is a pair of reflex corners of a face that point at each\nother. Accordingly, the number of kitty corners is the number of corners that\nare involved in some pair of kitty corners.",
    "descriptor": "\nComments: 40 pages, 26 figures, to appear in Proc. SOFSEM 2023\n",
    "authors": [
      "Walter Didimo",
      "Siddharth Gupta",
      "Philipp Kindermann",
      "Giuseppe Liotta",
      "Alexander Wolff",
      "Meirav Zehavi"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.05019"
  },
  {
    "id": "arXiv:2210.05020",
    "title": "Spectral Sparsification for Communication-Efficient Collaborative  Rotation and Translation Estimation",
    "abstract": "We propose fast and communication-efficient distributed algorithms for\nrotation averaging and translation recovery problems that arise from\nmulti-robot simultaneous localization and mapping (SLAM) and distributed camera\nnetwork localization applications. Our methods are based on theoretical\nrelations between the Hessians of the underlying Riemannian optimization\nproblems and the Laplacians of suitably weighted graphs. We leverage these\nresults to design a distributed solver that performs approximate second-order\noptimization by solving a Laplacian system at each iteration. Crucially, our\nalgorithms permit robots to employ spectral sparsification to sparsify\nintermediate dense matrices before communication, and hence provide a mechanism\nto trade off accuracy with communication efficiency with provable guarantees.\nWe perform rigorous theoretical analysis of our methods and prove that they\nenjoy (local) linear rate of convergence on the problems of interest. Numerical\nexperiments show that the proposed methods converge to high-precision solutions\nin a few iterations and that they are significantly more\ncommunication-efficient compared to baseline second-order solvers.",
    "descriptor": "\nComments: Technical report (9 figures, 3 tables)\n",
    "authors": [
      "Yulun Tian",
      "Jonathan P. How"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05020"
  },
  {
    "id": "arXiv:2210.05021",
    "title": "The good, the bad and the ugly sides of data augmentation: An implicit  spectral regularization perspective",
    "abstract": "Data augmentation (DA) is a powerful workhorse for bolstering performance in\nmodern machine learning. Specific augmentations like translations and scaling\nin computer vision are traditionally believed to improve generalization by\ngenerating new (artificial) data from the same distribution. However, this\ntraditional viewpoint does not explain the success of prevalent augmentations\nin modern machine learning (e.g. randomized masking, cutout, mixup), that\ngreatly alter the training data distribution. In this work, we develop a new\ntheoretical framework to characterize the impact of a general class of DA on\nunderparameterized and overparameterized linear model generalization. Our\nframework reveals that DA induces implicit spectral regularization through a\ncombination of two distinct effects: a) manipulating the relative proportion of\neigenvalues of the data covariance matrix in a training-data-dependent manner,\nand b) uniformly boosting the entire spectrum of the data covariance matrix\nthrough ridge regression. These effects, when applied to popular augmentations,\ngive rise to a wide variety of phenomena, including discrepancies in\ngeneralization between over-parameterized and under-parameterized regimes and\ndifferences between regression and classification tasks. Our framework\nhighlights the nuanced and sometimes surprising impacts of DA on\ngeneralization, and serves as a testbed for novel augmentation design.",
    "descriptor": "\nComments: 75 pages, 9 figures\n",
    "authors": [
      "Chi-Heng Lin",
      "Chiraag Kaushik",
      "Eva L. Dyer",
      "Vidya Muthukumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05021"
  },
  {
    "id": "arXiv:2210.05022",
    "title": "Dynamic Gap: Formal Guarantees for Safe Gap-based Navigation in Dynamic  Environments",
    "abstract": "This paper extends the family of gap-based local planners to unknown dynamic\nenvironments through generating provable collision-free properties for\nhierarchical navigation systems. Existing perception-informed local planners\nthat operate in dynamic environments rely on emergent or empirical robustness\nfor collision avoidance as opposed to providing formal guarantees for safety.\nIn addition to this, the obstacle tracking that is performed in these existent\nplanners is often achieved with respect to a global inertial frame, subjecting\nsuch tracking estimates to transformation errors from odometry drift. The\nproposed local planner, called dynamic gap, shifts the tracking paradigm to\nmodeling how the free space, represented as gaps, evolves over time. Gap\ncrossing and closing conditions are developed to aid in determining the\nfeasibility of passage through gaps, and Bezier curves are used to define a\nsafe navigable gap that encapsulates both local environment dynamics and\nego-robot reachability. Artificial Harmonic Potential Field (AHPF) methods that\nguarantee collision-free convergence to the goal are then leveraged to generate\nsafe local trajectories. Monte Carlo benchmarking experiments are run in\nstructured simulation worlds with dynamic agents to showcase the benefits that\nsuch formal safety guarantees provide.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Max Asselmeier",
      "Ye Zhao",
      "Patricio A. Vela"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05022"
  },
  {
    "id": "arXiv:2210.05025",
    "title": "Revisiting Connotations of Digital Humanists: Exploratory Interviews",
    "abstract": "This ongoing study revisits the connotations of \"digital humanists\" and\nexplores the reasons why a researcher does or does not self-identify as a\ndigital humanist. Building on semi-structured interview data collected from\nfourteen researchers and practitioners engaging in digital humanities (DH)\nprojects, this poster illustrates researchers' various understandings of\n\"digital humanist\" as a term and research identity and highlights the\ncomplexity of \"digital humanists\" as a research community. This study\ncontributes to DH scholarship with insights into the collective imaginations of\nthe digital humanist as a research community one decade after the early\nattempts. Findings of this research study also facilitate a more thorough,\ntimely, and dynamic discussion of the major workforce in digital humanities,\npotentially paving the way for future research on labor and collaboration in\nthe DH research domain.",
    "descriptor": "",
    "authors": [
      "Rongqian Ma"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.05025"
  },
  {
    "id": "arXiv:2210.05027",
    "title": "Probabilities of Causation: Adequate Size of Experimental and  Observational Samples",
    "abstract": "The probabilities of causation are commonly used to solve decision-making\nproblems. Tian and Pearl derived sharp bounds for the probability of necessity\nand sufficiency (PNS), the probability of sufficiency (PS), and the probability\nof necessity (PN) using experimental and observational data. The assumption is\nthat one is in possession of a large enough sample to permit an accurate\nestimation of the experimental and observational distributions. In this study,\nwe present a method for determining the sample size needed for such estimation,\nwhen a given confidence interval (CI) is specified. We further show by\nsimulation that the proposed sample size delivered stable estimations of the\nbounds of PNS.",
    "descriptor": "",
    "authors": [
      "Ang Li",
      "Ruirui Mao",
      "Judea Pearl"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.05027"
  },
  {
    "id": "arXiv:2210.05030",
    "title": "Unit Selection: Case Study and Comparison with A/B Test Heuristic",
    "abstract": "The unit selection problem defined by Li and Pearl identifies individuals who\nhave desired counterfactual behavior patterns, for example, individuals who\nwould respond positively if encouraged and would not otherwise. Li and Pearl\nshowed by example that their unit selection model is beyond the A/B test\nheuristics. In this paper, we reveal the essence of the A/B test heuristics,\nwhich are exceptional cases of the benefit function defined by Li and Pearl.\nFurthermore, We provided more simulated use cases of Li-Pearl's unit selection\nmodel to help decision-makers apply their model correctly, explaining that A/B\ntest heuristics are generally problematic.",
    "descriptor": "",
    "authors": [
      "Ang Li",
      "Judea Pearl"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.05030"
  },
  {
    "id": "arXiv:2210.05031",
    "title": "A smoothing analysis for multigrid methods applied to tempered  fractional problems",
    "abstract": "We consider the numerical solution of time-dependent space tempered\nfractional diffusion equations. The use of Crank-Nicolson in time and of\nsecond-order accurate tempered weighted and shifted Gr\\\"unwald difference in\nspace leads to dense (multilevel) Toeplitz-like linear systems. By exploiting\nthe related structure, we design an ad-hoc multigrid solver and multigrid-based\npreconditioners, all with weighted Jacobi as smoother. A new smoothing analysis\nis provided, which refines state-of-the-art results expanding the set of the\nsuitable Jacobi weights. Furthermore, we prove that if a multigrid method is\neffective in the non-tempered case, then the same multigrid method is effective\nalso in the tempered one. The numerical results confirm the theoretical\nanalysis, showing that the resulting multigrid-based solvers are\ncomputationally effective for tempered fractional diffusion equations.",
    "descriptor": "",
    "authors": [
      "D. Ahmad",
      "M. Donatelli",
      "M. Mazza",
      "S. Serra-Capizzano",
      "K. Trotti"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05031"
  },
  {
    "id": "arXiv:2210.05033",
    "title": "Multilingual Representation Distillation with Contrastive Learning",
    "abstract": "Multilingual sentence representations from large models can encode semantic\ninformation from two or more languages and can be used for different\ncross-lingual information retrieval tasks. In this paper, we integrate\ncontrastive learning into multilingual representation distillation and use it\nfor quality estimation of parallel sentences (find semantically similar\nsentences that can be used as translations of each other). We validate our\napproach with multilingual similarity search and corpus filtering tasks.\nExperiments across different low-resource languages show that our method\nsignificantly outperforms previous sentence encoders such as LASER, LASER3, and\nLaBSE.",
    "descriptor": "",
    "authors": [
      "Weiting Tan",
      "Kevin Heffernan",
      "Holger Schwenk",
      "Philipp Koehn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05033"
  },
  {
    "id": "arXiv:2210.05034",
    "title": "Real-Time Dynamic Map with Crowdsourcing Vehicles in Edge Computing",
    "abstract": "Autonomous driving perceives surroundings with line-of-sight sensors that are\ncompromised under environmental uncertainties. To achieve real time global\ninformation in high definition map, we investigate to share perception\ninformation among connected and automated vehicles. However, it is challenging\nto achieve real time perception sharing under varying network dynamics in\nautomotive edge computing. In this paper, we propose a novel real time dynamic\nmap, named LiveMap to detect, match, and track objects on the road. We design\nthe data plane of LiveMap to efficiently process individual vehicle data with\nmultiple sequential computation components, including detection, projection,\nextraction, matching and combination. We design the control plane of LiveMap to\nachieve adaptive vehicular offloading with two new algorithms (central and\ndistributed) to balance the latency and coverage performance based on deep\nreinforcement learning techniques. We conduct extensive evaluation through both\nrealistic experiments on a small-scale physical testbed and network simulations\non an edge network simulator. The results suggest that LiveMap significantly\noutperforms existing solutions in terms of latency, coverage, and accuracy.",
    "descriptor": "\nComments: This paper is accepted by IEEE Transactions on Intelligent Vehicles\n",
    "authors": [
      "Qiang Liu",
      "Tao Han",
      "Jiang",
      "BaekGyu Kim"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.05034"
  },
  {
    "id": "arXiv:2210.05035",
    "title": "Not All Errors are Equal: Learning Text Generation Metrics using  Stratified Error Synthesis",
    "abstract": "Is it possible to build a general and automatic natural language generation\n(NLG) evaluation metric? Existing learned metrics either perform\nunsatisfactorily or are restricted to tasks where large human rating data is\nalready available. We introduce SESCORE, a model-based metric that is highly\ncorrelated with human judgements without requiring human annotation, by\nutilizing a novel, iterative error synthesis and severity scoring pipeline.\nThis pipeline applies a series of plausible errors to raw text and assigns\nseverity labels by simulating human judgements with entailment. We evaluate\nSESCORE against existing metrics by comparing how their scores correlate with\nhuman ratings. SESCORE outperforms all prior unsupervised metrics on multiple\ndiverse NLG tasks including machine translation, image captioning, and WebNLG\ntext generation. For WMT 20/21 En-De and Zh-En, SESCORE improve the average\nKendall correlation with human judgement from 0.154 to 0.195. SESCORE even\nachieves comparable performance to the best supervised metric COMET, despite\nreceiving no human-annotated training data.",
    "descriptor": "",
    "authors": [
      "Wenda Xu",
      "Yilin Tuan",
      "Yujie Lu",
      "Michael Saxon",
      "Lei Li",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05035"
  },
  {
    "id": "arXiv:2210.05036",
    "title": "Spatial Name System",
    "abstract": "The development of emerging classes of hardware such as Internet of Thing\ndevices and Augmented Reality headsets has outpaced the development of Internet\ninfrastructure. We identify problems with latency, security and privacy in the\nglobal hierarchical distributed Domain Name System. To remedy this, we propose\nthe Spatial Name System, an alternative network architecture that relies on the\ninnate physicality of this paradigm. Utilizing a device's pre-existing unique\nidentifier, its location, allows us to identify devices locally based on their\nphysical presence. A naming system tailored to the physical world for\nubiquitous computing can enable reliable, low latency, secure and private\ncommunication.",
    "descriptor": "\nComments: 45 pages, 21 figures\n",
    "authors": [
      "Ryan Thomas Gibb"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.05036"
  },
  {
    "id": "arXiv:2210.05037",
    "title": "Automated Audio Captioning via Fusion of Low- and High- Dimensional  Features",
    "abstract": "Automated audio captioning (AAC) aims to describe the content of an audio\nclip using simple sentences. Existing AAC methods are developed based on an\nencoder-decoder architecture that success is attributed to the use of a\npre-trained CNN10 called PANNs as the encoder to learn rich audio\nrepresentations. AAC is a highly challenging task due to its high-dimensional\ntalent space involves audio of various scenarios. Existing methods only use the\nhigh-dimensional representation of the PANNs as the input of the decoder.\nHowever, the low-dimension representation may retain as much audio information\nas the high-dimensional representation may be neglected. In addition, although\nthe high-dimensional approach may predict the audio captions by learning from\nexisting audio captions, which lacks robustness and efficiency. To deal with\nthese challenges, a fusion model which integrates low- and high-dimensional\nfeatures AAC framework is proposed. In this paper, a new encoder-decoder\nframework is proposed called the Low- and High-Dimensional Feature Fusion\n(LHDFF) model for AAC. Moreover, in LHDFF, a new PANNs encoder is proposed\ncalled Residual PANNs (RPANNs) by fusing the low-dimensional feature from the\nintermediate convolution layer output and the high-dimensional feature from the\nfinal layer output of PANNs. To fully explore the information of the low- and\nhigh-dimensional fusion feature and high-dimensional feature respectively, we\nproposed dual transformer decoder structures to generate the captions in\nparallel. Especially, a probabilistic fusion approach is proposed that can\nensure the overall performance of the system is improved by concentrating on\nthe respective advantages of the two transformer decoders. Experimental results\nshow that LHDFF achieves the best performance on the Clotho and AudioCaps\ndatasets compared with other existing models",
    "descriptor": "",
    "authors": [
      "Jianyuan Sun",
      "Xubo Liu",
      "Xinhao Mei",
      "Mark D. Plumbley",
      "Volkan Kilic",
      "Wenwu Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05037"
  },
  {
    "id": "arXiv:2210.05038",
    "title": "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video  Retrieval Benchmarks",
    "abstract": "Searching vast troves of videos with textual descriptions is a core\nmultimodal retrieval task. Owing to the lack of a purpose-built dataset for\ntext-to-video retrieval, video captioning datasets have been re-purposed to\nevaluate models by (1) treating captions as positive matches to their\nrespective videos and (2) all other videos as negatives. However, this\nmethodology leads to a fundamental flaw during evaluation: since captions are\nmarked as relevant only to their original video, many alternate videos also\nmatch the caption, which creates false-negative caption-video pairs. We show\nthat when these false negatives are corrected, a recent state-of-the-art model\ngains 25% recall points -- a difference that threatens the validity of the\nbenchmark itself. To diagnose and mitigate this issue, we annotate and release\n683K additional caption-video pairs. Using these, we recompute effectiveness\nscores for three models on two standard benchmarks (MSR-VTT and MSVD). We find\nthat (1) the recomputed metrics are up to 25% recall points higher for the best\nmodels, (2) these benchmarks are nearing saturation for Recall@10, (3) caption\nlength (generality) is related to the number of positives, and (4) annotation\ncosts can be mitigated by choosing evaluation sizes corresponding to desired\neffect size to detect. We recommend retiring these benchmarks in their current\nform and make recommendations for future text-to-video retrieval benchmarks.",
    "descriptor": "",
    "authors": [
      "Pedro Rodriguez",
      "Mahmoud Azab",
      "Becka Silvert",
      "Renato Sanchez",
      "Linzy Labson",
      "Hardik Shah",
      "Seungwhan Moon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05038"
  },
  {
    "id": "arXiv:2210.05039",
    "title": "Contrastive Video-Language Learning with Fine-grained Frame Sampling",
    "abstract": "Despite recent progress in video and language representation learning, the\nweak or sparse correspondence between the two modalities remains a bottleneck\nin the area. Most video-language models are trained via pair-level loss to\npredict whether a pair of video and text is aligned. However, even in paired\nvideo-text segments, only a subset of the frames are semantically relevant to\nthe corresponding text, with the remainder representing noise; where the ratio\nof noisy frames is higher for longer videos. We propose FineCo (Fine-grained\nContrastive Loss for Frame Sampling), an approach to better learn video and\nlanguage representations with a fine-grained contrastive objective operating on\nvideo frames. It helps distil a video by selecting the frames that are\nsemantically equivalent to the text, improving cross-modal correspondence.\nBuilding on the well established VideoCLIP model as a starting point, FineCo\nachieves state-of-the-art performance on YouCookII, a text-video retrieval\nbenchmark with long videos. FineCo also achieves competitive results on\ntext-video retrieval (MSR-VTT), and video question answering datasets (MSR-VTT\nQA and MSR-VTT MC) with shorter videos.",
    "descriptor": "\nComments: AACL-IJCNLP 2022\n",
    "authors": [
      "Zixu Wang",
      "Yujie Zhong",
      "Yishu Miao",
      "Lin Ma",
      "Lucia Specia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05039"
  },
  {
    "id": "arXiv:2210.05043",
    "title": "Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling",
    "abstract": "Ensembling BERT models often significantly improves accuracy, but at the cost\nof significantly more computation and memory footprint. In this work, we\npropose Multi-CLS BERT, a novel ensembling method for CLS-based prediction\ntasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses\nmultiple CLS tokens with a parameterization and objective that encourages their\ndiversity. Thus instead of fine-tuning each BERT model in an ensemble (and\nrunning them all at test time), we need only fine-tune our single Multi-CLS\nBERT model (and run the one model at test time, ensembling just the multiple\nfinal CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on\ntop of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and\nRudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS\nBERT reliably improves both overall accuracy and confidence estimation. When\nonly 100 training samples are available in GLUE, the Multi-CLS BERT_Base model\ncan even outperform the corresponding BERT_Large model. We analyze the behavior\nof our Multi-CLS BERT, showing that it has many of the same characteristics and\nbehavior as a typical BERT 5-way ensemble, but with nearly 4-times less\ncomputation and memory.",
    "descriptor": "",
    "authors": [
      "Haw-Shiuan Chang",
      "Ruei-Yao Sun",
      "Kathryn Ricci",
      "Andrew McCallum"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05043"
  },
  {
    "id": "arXiv:2210.05047",
    "title": "Improving Retrieval Augmented Neural Machine Translation by Controlling  Source and Fuzzy-Match Interactions",
    "abstract": "We explore zero-shot adaptation, where a general-domain model has access to\ncustomer or domain specific parallel data at inference time, but not during\ntraining. We build on the idea of Retrieval Augmented Translation (RAT) where\ntop-k in-domain fuzzy matches are found for the source sentence, and\ntarget-language translations of those fuzzy-matched sentences are provided to\nthe translation model at inference time. We propose a novel architecture to\ncontrol interactions between a source sentence and the top-k fuzzy\ntarget-language matches, and compare it to architectures from prior work. We\nconduct experiments in two language pairs (En-De and En-Fr) by training models\non WMT data and testing them with five and seven multi-domain datasets,\nrespectively. Our approach consistently outperforms the alternative\narchitectures, improving BLEU across language pair, domain, and number k of\nfuzzy matches.",
    "descriptor": "",
    "authors": [
      "Cuong Hoang",
      "Devendra Sachan",
      "Prashant Mathur",
      "Brian Thompson",
      "Marcello Federico"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05047"
  },
  {
    "id": "arXiv:2210.05050",
    "title": "Neurosymbolic Programming for Science",
    "abstract": "Neurosymbolic Programming (NP) techniques have the potential to accelerate\nscientific discovery across fields. These models combine neural and symbolic\ncomponents to learn complex patterns and representations from data, using\nhigh-level concepts or known constraints. As a result, NP techniques can\ninterface with symbolic domain knowledge from scientists, such as prior\nknowledge and experimental context, to produce interpretable outputs. Here, we\nidentify opportunities and challenges between current NP models and scientific\nworkflows, with real-world examples from behavior analysis in science. We\ndefine concrete next steps to move the NP for science field forward, to enable\nits use broadly for workflows across the natural and social sciences.",
    "descriptor": "\nComments: Neural Information Processing Systems 2022\n",
    "authors": [
      "Jennifer J. Sun",
      "Megan Tjandrasuwita",
      "Atharva Sehgal",
      "Armando Solar-Lezama",
      "Swarat Chaudhuri",
      "Yisong Yue",
      "Omar Costilla-Reyes"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05050"
  },
  {
    "id": "arXiv:2210.05052",
    "title": "Risk Automatic Prediction for Social Economy Companies using Camels",
    "abstract": "Governments have to supervise and inspect social economy enterprises (SEEs).\nHowever, inspecting all SEEs is not possible due to the large number of SEEs\nand the low number of inspectors in general. We proposed a prediction model\nbased on a machine learning approach. The method was trained with the random\nforest algorithm with historical data provided by each SEE. Three consecutive\nperiods of data were concatenated. The proposed method uses these periods as\ninput data and predicts the risk of each SEE in the fourth period. The model\nachieved 76\\% overall accuracy. In addition, it obtained good accuracy in\npredicting the high risk of a SEE. We found that the legal nature and the\nvariation of the past-due portfolio are good predictors of the future risk of a\nSEE. Thus, the risk of a SEE in a future period can be predicted by a\nsupervised machine learning method. Predicting the high risk of a SEE improves\nthe daily work of each inspector by focusing only on high-risk SEEs.",
    "descriptor": "\nComments: 8 pages, 1 figure\n",
    "authors": [
      "Joseph Gallego-Mejia",
      "Daniela Martin-Vega",
      "Fabio Gonzalez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.05052"
  },
  {
    "id": "arXiv:2210.05055",
    "title": "Smart Hybrid Beamforming and Pilot Assignment for 6G Cell-Free Massive  MIMO",
    "abstract": "This paper investigates Cell-Free massive MIMO networks, where each access\npoint (AP) is equipped with a hybrid transceiver, reducing the complexity and\ncost compared to a fully digital transceiver. Asymptotic approximations for the\nspectral efficiency are derived for uplink and downlink. Capitalizing on these\nexpressions, a max-min problem is formulated to optimize the (i) analog\nbeamformer at the APs and (ii) pilot assignment. Simulations show that the\noptimization of these variables substantially increases the network\nperformance.",
    "descriptor": "",
    "authors": [
      "Carles Diaz-Vilor",
      "Alexei Ashikhmin",
      "Hong Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05055"
  },
  {
    "id": "arXiv:2210.05059",
    "title": "Improving Robustness of Retrieval Augmented Translation via Shuffling of  Suggestions",
    "abstract": "Several recent studies have reported dramatic performance improvements in\nneural machine translation (NMT) by augmenting translation at inference time\nwith fuzzy-matches retrieved from a translation memory (TM). However, these\nstudies all operate under the assumption that the TMs available at test time\nare highly relevant to the testset. We demonstrate that for existing retrieval\naugmented translation methods, using a TM with a domain mismatch to the test\nset can result in substantially worse performance compared to not using a TM at\nall. We propose a simple method to expose fuzzy-match NMT systems during\ntraining and show that it results in a system that is much more tolerant\n(regaining up to 5.8 BLEU) to inference with TMs with domain mismatch. Also,\nthe model is still competitive to the baseline when fed with suggestions from\nrelevant TMs.",
    "descriptor": "",
    "authors": [
      "Cuong Hoang",
      "Devendra Sachan",
      "Prashant Mathur",
      "Brian Thompson",
      "Marcello Federico"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05059"
  },
  {
    "id": "arXiv:2210.05060",
    "title": "AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio  Visual Event Localization",
    "abstract": "An audio-visual event (AVE) is denoted by the correspondence of the visual\nand auditory signals in a video segment. Precise localization of the AVEs is\nvery challenging since it demands effective multi-modal feature correspondence\nto ground the short and long range temporal interactions. Existing approaches\nstruggle in capturing the different scales of multi-modal interaction due to\nineffective multi-modal training strategies. To overcome this limitation, we\nintroduce AVE-CLIP, a novel framework that integrates the AudioCLIP pre-trained\non large-scale audio-visual data with a multi-window temporal transformer to\neffectively operate on different temporal scales of video frames. Our\ncontributions are three-fold: (1) We introduce a multi-stage training framework\nto incorporate AudioCLIP pre-trained with audio-image pairs into the AVE\nlocalization task on video frames through contrastive fine-tuning, effective\nmean video feature extraction, and multi-scale training phases. (2) We propose\na multi-domain attention mechanism that operates on both temporal and feature\ndomains over varying timescales to fuse the local and global feature\nvariations. (3) We introduce a temporal refining scheme with event-guided\nattention followed by a simple-yet-effective post processing step to handle\nsignificant variations of the background over diverse events. Our method\nachieves state-of-the-art performance on the publicly available AVE dataset\nwith 5.9% mean accuracy improvement which proves its superiority over existing\napproaches.",
    "descriptor": "\nComments: Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023 (10 Pages, 5 Figures)\n",
    "authors": [
      "Tanvir Mahmud",
      "Diana Marculescu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05060"
  },
  {
    "id": "arXiv:2210.05061",
    "title": "InQMAD: Incremental Quantum Measurement Anomaly Detection",
    "abstract": "Streaming anomaly detection refers to the problem of detecting anomalous data\nsamples in streams of data. This problem poses challenges that classical and\ndeep anomaly detection methods are not designed to cope with, such as\nconceptual drift and continuous learning. State-of-the-art flow anomaly\ndetection methods rely on fixed memory using hash functions or nearest\nneighbors that may not be able to constrain high-frequency values as in a\nmoving average or remove seamless outliers and cannot be trained in an\nend-to-end deep learning architecture. We present a new incremental anomaly\ndetection method that performs continuous density estimation based on random\nFourier features and the mechanism of quantum measurements and density matrices\nthat can be viewed as an exponential moving average density. It can process\npotentially endless data and its update complexity is constant $O(1)$. A\nsystematic evaluation against 12 state-of-the-art streaming anomaly detection\nalgorithms using 12 streaming datasets is presented.",
    "descriptor": "\nComments: 15 pages, 3 figures\n",
    "authors": [
      "Joseph Gallego-Mejia",
      "Oscar Bustos-Brinez",
      "Fabio Gonzalez"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05061"
  },
  {
    "id": "arXiv:2210.05062",
    "title": "Relational Attention: Generalizing Transformers for Graph-Structured  Tasks",
    "abstract": "Transformers flexibly operate over sets of real-valued vectors representing\ntask-specific entities and their attributes, where each vector might encode one\nword-piece token and its position in a sequence, or some piece of information\nthat carries no position at all. But as set processors, transformers are at a\ndisadvantage in reasoning over more general graph-structured data where nodes\nrepresent entities and edges represent relations between entities. To address\nthis shortcoming, we generalize transformer attention to consider and update\nedge vectors in each transformer layer. We evaluate this relational transformer\non a diverse array of graph-structured tasks, including the large and\nchallenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically\noutperforms state-of-the-art graph neural networks expressly designed to reason\nover graph-structured data. Our analysis demonstrates that these gains are\nattributable to relational attention's inherent ability to leverage the greater\nexpressivity of graphs over sets.",
    "descriptor": "",
    "authors": [
      "Cameron Diao",
      "Ricky Loynd"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05062"
  },
  {
    "id": "arXiv:2210.05063",
    "title": "Improving Dense Contrastive Learning with Dense Negative Pairs",
    "abstract": "Many contrastive representation learning methods learn a single global\nrepresentation of an entire image. However, dense contrastive representation\nlearning methods such as DenseCL [19] can learn better representations for\ntasks requiring stronger spatial localization of features, such as multi-label\nclassification, detection, and segmentation. In this work, we study how to\nimprove the quality of the representations learned by DenseCL by modifying the\ntraining scheme and objective function, and propose DenseCL++. We also conduct\nseveral ablation studies to better understand the effects of: (i) various\ntechniques to form dense negative pairs among augmentations of different\nimages, (ii) cross-view dense negative and positive pairs, and (iii) an\nauxiliary reconstruction task. Our results show 3.5% and 4% mAP improvement\nover SimCLR [3] and DenseCL in COCO multi-label classification. In COCO and VOC\nsegmentation tasks, we achieve 1.8% and 0.7% mIoU improvements over SimCLR,\nrespectively.",
    "descriptor": "",
    "authors": [
      "Berk Iskender",
      "Zhenlin Xu",
      "Simon Kornblith",
      "Enhung Chu",
      "Maryam Khademi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05063"
  },
  {
    "id": "arXiv:2210.05064",
    "title": "VER: Scaling On-Policy RL Leads to the Emergence of Navigation in  Embodied Rearrangement",
    "abstract": "We present Variable Experience Rollout (VER), a technique for efficiently\nscaling batched on-policy reinforcement learning in heterogenous environments\n(where different environments take vastly different times to generate rollouts)\nto many GPUs residing on, potentially, many machines. VER combines the\nstrengths of and blurs the line between synchronous and asynchronous on-policy\nRL methods (SyncOnRL and AsyncOnRL, respectively). VER learns from on-policy\nexperience (like SyncOnRL) and has no synchronization points (like AsyncOnRL).\nVER leads to significant and consistent speed-ups across a broad range of\nembodied navigation and mobile manipulation tasks in photorealistic 3D\nsimulation environments. Specifically, for PointGoal navigation and ObjectGoal\nnavigation in Habitat 1.0, VER is 60-100% faster (1.6-2x speedup) than DD-PPO,\nthe current state of art distributed SyncOnRL, with similar sample efficiency.\nFor mobile manipulation tasks (open fridge/cabinet, pick/place objects) in\nHabitat 2.0 VER is 150% faster (2.5x speedup) on 1 GPU and 170% faster (2.7x\nspeedup) on 8 GPUs than DD-PPO. Compared to SampleFactory (the current\nstate-of-the-art AsyncOnRL), VER matches its speed on 1 GPU, and is 70% faster\n(1.7x speedup) on 8 GPUs with better sample efficiency.\nWe leverage these speed-ups to train chained skills for GeometricGoal\nrearrangement tasks in the Home Assistant Benchmark (HAB). We find a surprising\nemergence of navigation in skills that do not ostensible require any\nnavigation. Specifically, the Pick skill involves a robot picking an object\nfrom a table. During training the robot was always spawned close to the table\nand never needed to navigate. However, we find that if base movement is part of\nthe action space, the robot learns to navigate then pick an object in new\nenvironments with 50% success, demonstrating surprisingly high\nout-of-distribution generalization.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Erik Wijmans",
      "Irfan Essa",
      "Dhruv Batra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05064"
  },
  {
    "id": "arXiv:2210.05068",
    "title": "In-Hand Gravitational Pivoting Using Tactile Sensing",
    "abstract": "We study gravitational pivoting, a constrained version of in-hand\nmanipulation, where we aim to control the rotation of an object around the grip\npoint of a parallel gripper. To achieve this, instead of controlling the\ngripper to avoid slip, we embrace slip to allow the object to rotate in-hand.\nWe collect two real-world datasets, a static tracking dataset and a\ncontroller-in-the loop dataset, both annotated with object angle and angular\nvelocity labels. Both datasets contain force-based tactile information on ten\ndifferent household objects. We train an LSTM model to predict the angular\nposition and velocity of the held object from purely tactile data. We integrate\nthis model with a controller that opens and closes the gripper allowing the\nobject to rotate to desired relative angles. We conduct real-world experiments\nwhere the robot is tasked to achieve a relative target angle. We show that our\napproach outperforms a sliding-window based MLP in a zero-shot generalization\nsetting with unseen objects. Furthermore, we show a 16.6% improvement in\nperformance when the LSTM model is fine-tuned on a small set of data collected\nwith both the LSTM model and the controller in-the-loop. Code and videos are\navailable at https://rhys-newbury.github.io/projects/pivoting/",
    "descriptor": "\nComments: Accepted as poster presentation to Conference on Robot Learning (CoRL) 2022\n",
    "authors": [
      "Jason Toskov",
      "Rhys Newbury",
      "Mustafa Mukadam",
      "Dana Kuli\u0107",
      "Akansel Cosgun"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05068"
  },
  {
    "id": "arXiv:2210.05075",
    "title": "Reflection of Thought: Inversely Eliciting Numerical Reasoning in  Language Models via Solving Linear Systems",
    "abstract": "Numerical reasoning over natural language has been a long-standing goal for\nthe research community. However, cutting-edge language models have proven\ndifficult to reliably generalize to a broad range of numbers, although they\nhave shown proficiency in reasoning over common and simple numbers. In this\npaper, we propose a novel method to elicit and exploit the numerical reasoning\nknowledge hidden in pre-trained language models using simple anchor numbers.\nConcretely, we first leverage simple numbers as anchors to probe the implicitly\ninferred arithmetic expressions from language models, and then explicitly apply\nthe expressions on complex numbers to get corresponding answers. To inversely\nelicit arithmetic expressions, we transform and formulate the task as an\nanalytically solvable linear system. Experimental results on several numerical\nreasoning benchmarks demonstrate that our approach significantly improves\nnumerical reasoning capabilities of existing LMs. More importantly, our\napproach is training-free and simply works in the inference phase, making it\nhighly portable and achieving consistent performance benefits across a variety\nof language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and\nfine-tuning scenarios.",
    "descriptor": "",
    "authors": [
      "Fan Zhou",
      "Haoyu Dong",
      "Qian Liu",
      "Zhoujun Cheng",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05075"
  },
  {
    "id": "arXiv:2210.05076",
    "title": "ConchShell: A Generative Adversarial Networks that Turns Pictures into  Piano Music",
    "abstract": "We present ConchShell, a multi-modal generative adversarial framework that\ntakes pictures as input to the network and generates piano music samples that\nmatch the picture context. Inspired by I3D, we introduce a novel image feature\nrepresentation method: time-convolutional neural network (TCNN), which is used\nto forge features for images in the temporal dimension. Although our image data\nconsists of only six categories, our proposed framework will be innovative and\ncommercially meaningful. The project will provide technical ideas for work such\nas 3D game voice overs, short-video soundtracks, and real-time generation of\nmetaverse background music.We have also released a new dataset, the\nBeach-Ocean-Piano Dataset (BOPD) 1, which contains more than 3,000 images and\nmore than 1,500 piano pieces. This dataset will support multimodal\nimage-to-music research.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Wanpeng Fan",
      "Yuanzhi Su",
      "Yuxin Huang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Information Retrieval (cs.IR)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05076"
  },
  {
    "id": "arXiv:2210.05078",
    "title": "Joint Human Orientation-Activity Recognition Using WiFi Signals for  Human-Machine Interaction",
    "abstract": "WiFi sensing is an important part of the new WiFi 802.11bf standard, which\ncan detect motion and measure distances. In recent years, some machine learning\nmethods have been proposed for human activity recognition from WiFi signals.\nHowever, to the best of our knowledge, none of these methods have explored\norientation prediction of the user using WiFi signals. Orientation prediction\nis particularly critical for human-machine interaction in an environment with\nmultiple smart devices. In this paper, we propose a data collection setup and\nmachine learning models for joint human orientation and activity recognition\nusing WiFi signals from a single access point (AP) or multiple APs. The results\nshow feasibility of joint orientation-activity recognition in an indoor\nenvironment with a high accuracy.",
    "descriptor": "",
    "authors": [
      "Hojjat Salehinejad",
      "Navid Hasanzadeh",
      "Radomir Djogo",
      "Shahrokh Valaee"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.05078"
  },
  {
    "id": "arXiv:2210.05083",
    "title": "Convergence of Bi-Virus Epidemic Models with Non-Linear Rates on  Networks -- A Monotone Dynamical Systems Approach",
    "abstract": "We study convergence properties of competing epidemic models of the\nSusceptible-Infected-Susceptible (SIS) type. The SIS epidemic model has seen\nwidespread popularity in modelling the spreading dynamics of contagions such as\nviruses, infectious diseases, or even rumors/opinions over contact networks\n(graphs).We analyze the case of two such viruses spreading on overlaid graphs,\nwith non-linear rates of infection spread and recovery. We call this the\nnon-linear bi-virus model and, building upon recent results, obtain precise\nconditions for global convergence of the solutions to a trichotomy of possible\noutcomes: a virus-free state, a single-virus state, and to a coexistence state.\nOur techniques are based on the theory of monotone dynamical systems (MDS), in\ncontrast to Lyapunov based techniques that have only seen partial success in\ndetermining convergence properties in the setting of competing epidemics. We\ndemonstrate how the existing works have been unsuccessful in characterizing a\nlarge subset of the model parameter space for bi-virus epidemics, including all\nscenarios leading to coexistence of the epidemics. To the best of our\nknowledge, our results are the first in providing complete convergence analysis\nfor the bi-virus system with nonlinear infection and recovery rates on general\ngraphs.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2104.10872\n",
    "authors": [
      "Vishwaraj Doshi",
      "Shailaja Mallick",
      "Do Young eun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05083"
  },
  {
    "id": "arXiv:2210.05084",
    "title": "Covert Communication Gains from Adversary's Uncertainty of Phase Angles",
    "abstract": "This work investigates the phase gain of intelligent reflecting surface (IRS)\ncovert communication over complex-valued additive white Gaussian noise (AWGN)\nchannels. The transmitter Alice intends to transmit covert messages to the\nlegitimate receiver Bob via reflecting the broadcast signals from a radio\nfrequency (RF) source, while rendering the adversary Willie's detector\narbitrarily close to ineffective. Our analyses show that, compared to the\ncovert capacity for classical AWGN channels, we can achieve a covertness gain\nof value 2 by leveraging Willie's uncertainty of phase angles. This covertness\ngain is achieved when the number of possible phase angle pairs $N=2$. More\ninterestingly, our results show that the covertness gain will not further\nincrease with $N$ as long as $N \\ge 2$, even if it approaches infinity.",
    "descriptor": "",
    "authors": [
      "Sen Qiao",
      "Daming Cao",
      "Qiaosheng Zhang",
      "Yinfei Xu",
      "Guangjie Liu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05084"
  },
  {
    "id": "arXiv:2210.05087",
    "title": "Approximation of nearly-periodic symplectic maps via  structure-preserving neural networks",
    "abstract": "A continuous-time dynamical system with parameter $\\varepsilon$ is\nnearly-periodic if all its trajectories are periodic with nowhere-vanishing\nangular frequency as $\\varepsilon$ approaches 0. Nearly-periodic maps are\ndiscrete-time analogues of nearly-periodic systems, defined as\nparameter-dependent diffeomorphisms that limit to rotations along a circle\naction, and they admit formal $U(1)$ symmetries to all orders when the limiting\nrotation is non-resonant. For Hamiltonian nearly-periodic maps on exact\npresymplectic manifolds, the formal $U(1)$ symmetry gives rise to a\ndiscrete-time adiabatic invariant. In this paper, we construct a novel\nstructure-preserving neural network to approximate nearly-periodic symplectic\nmaps. This neural network architecture, which we call symplectic gyroceptron,\nensures that the resulting surrogate map is nearly-periodic and symplectic, and\nthat it gives rise to a discrete-time adiabatic invariant and a long-time\nstability. This new structure-preserving neural network provides a promising\narchitecture for surrogate modeling of non-dissipative dynamical systems that\nautomatically steps over short timescales without introducing spurious\ninstabilities.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Valentin Duruisseaux",
      "Joshua W. Burby",
      "Qi Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.05087"
  },
  {
    "id": "arXiv:2210.05092",
    "title": "The DKU-Tencent System for the VoxCeleb Speaker Recognition Challenge  2022",
    "abstract": "This paper is the system description of the DKU-Tencent System for the\nVoxCeleb Speaker Recognition Challenge 2022 (VoxSRC22). In this challenge, we\nfocus on track1 and track3. For track1, multiple backbone networks are adopted\nto extract frame-level features. Since track1 focus on the cross-age scenarios,\nwe adopt the cross-age trials and perform QMF to calibrate score. The\nmagnitude-based quality measures achieve a large improvement. For track3, the\nsemi-supervised domain adaptation task, the pseudo label method is adopted to\nmake domain adaptation. Considering the noise labels in clustering, the ArcFace\nis replaced by Sub-center ArcFace. The final submission achieves 0.107 mDCF in\ntask1 and 7.135% EER in task3.",
    "descriptor": "",
    "authors": [
      "Xiaoyi Qin",
      "Na Li",
      "Yuke Lin",
      "Yiwei Ding",
      "Chao Weng",
      "Dan Su",
      "Ming Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05092"
  },
  {
    "id": "arXiv:2210.05093",
    "title": "Crack Modeling via Minimum-Weight Surfaces in 3d Voronoi Diagrams",
    "abstract": "Shortest paths play an important role in mathematical modeling and image\nprocessing. Usually, shortest path problems are formulated on planar graphs\nthat consist of vertices and weighted arcs. In this context, one is interested\nin finding a path of minimum weight from a start vertex to an end vertex. The\nconcept of minimum-weight surfaces extends shortest paths to 3d. The\nminimum-weight surface problem is formulated on a cellular complex with\nweighted facets. A cycle on the arcs of the complex serves as input and one is\ninterested in finding a surface of minimum weight bounded by that cycle. In\npractice, minimum-weight surfaces can be used to segment 3d images. Vice versa,\nit is possible to use them as a modeling tool for geometric structures such as\ncracks. In this work, we present an approach for using minimum-weight surfaces\nin bounded Voronoi diagrams to generate synthetic 3d images of cracks.",
    "descriptor": "",
    "authors": [
      "Christian Jung",
      "Claudia Redenbach"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.05093"
  },
  {
    "id": "arXiv:2210.05096",
    "title": "Checks and Strategies for Enabling Code-Switched Machine Translation",
    "abstract": "Code-switching is a common phenomenon among multilingual speakers, where\nalternation between two or more languages occurs within the context of a single\nconversation. While multilingual humans can seamlessly switch back and forth\nbetween languages, multilingual neural machine translation (NMT) models are not\nrobust to such sudden changes in input. This work explores multilingual NMT\nmodels' ability to handle code-switched text. First, we propose checks to\nmeasure switching capability. Second, we investigate simple and effective data\naugmentation methods that can enhance an NMT model's ability to support\ncode-switching. Finally, by using a glass-box analysis of attention modules, we\ndemonstrate the effectiveness of these methods in improving robustness.",
    "descriptor": "",
    "authors": [
      "Thamme Gowda",
      "Mozhdeh Gheini",
      "Jonathan May"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.05096"
  },
  {
    "id": "arXiv:2210.05097",
    "title": "Repainting and Imitating Learning for Lane Detection",
    "abstract": "Current lane detection methods are struggling with the invisibility lane\nissue caused by heavy shadows, severe road mark degradation, and serious\nvehicle occlusion. As a result, discriminative lane features can be barely\nlearned by the network despite elaborate designs due to the inherent\ninvisibility of lanes in the wild. In this paper, we target at finding an\nenhanced feature space where the lane features are distinctive while\nmaintaining a similar distribution of lanes in the wild. To achieve this, we\npropose a novel Repainting and Imitating Learning (RIL) framework containing a\npair of teacher and student without any extra data or extra laborious labeling.\nSpecifically, in the repainting step, an enhanced ideal virtual lane dataset is\nbuilt in which only the lane regions are repainted while non-lane regions are\nkept unchanged, maintaining the similar distribution of lanes in the wild. The\nteacher model learns enhanced discriminative representation based on the\nvirtual data and serves as the guidance for a student model to imitate. In the\nimitating learning step, through the scale-fusing distillation module, the\nstudent network is encouraged to generate features that mimic the teacher model\nboth on the same scale and cross scales. Furthermore, the coupled adversarial\nmodule builds the bridge to connect not only teacher and student models but\nalso virtual and real data, adjusting the imitating learning process\ndynamically. Note that our method introduces no extra time cost during\ninference and can be plug-and-play in various cutting-edge lane detection\nnetworks. Experimental results prove the effectiveness of the RIL framework\nboth on CULane and TuSimple for four modern lane detection methods. The code\nand model will be available soon.",
    "descriptor": "",
    "authors": [
      "Yue He",
      "Minyue Jiang",
      "Xiaoqing Ye",
      "Liang Du",
      "Zhikang Zou",
      "Wei Zhang",
      "Xiao Tan",
      "Errui Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05097"
  },
  {
    "id": "arXiv:2210.05098",
    "title": "IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces",
    "abstract": "The ability to extract high-quality translation dictionaries from monolingual\nword embedding spaces depends critically on the geometric similarity of the\nspaces -- their degree of \"isomorphism.\" We address the root-cause of faulty\ncross-lingual mapping: that word embedding training resulted in the underlying\nspaces being non-isomorphic. We incorporate global measures of isomorphism\ndirectly into the skipgram loss function, successfully increasing the relative\nisomorphism of trained word embedding spaces and improving their ability to be\nmapped to a shared cross-lingual space. The result is improved bilingual\nlexicon induction in general data conditions, under domain mismatch, and with\ntraining algorithm dissimilarities. We release IsoVec at\nhttps://github.com/kellymarchisio/isovec.",
    "descriptor": "\nComments: EMNLP2022 Camera Ready\n",
    "authors": [
      "Kelly Marchisio",
      "Neha Verma",
      "Kevin Duh",
      "Philipp Koehn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05098"
  },
  {
    "id": "arXiv:2210.05102",
    "title": "COMBO: Pre-Training Representations of Binary Code Using Contrastive  Learning",
    "abstract": "Compiled software is delivered as executable binary code. Developers write\nsource code to express the software semantics, but the compiler converts it to\na binary format that the CPU can directly execute. Therefore, binary code\nanalysis is critical to applications in reverse engineering and computer\nsecurity tasks where source code is not available. However, unlike source code\nand natural language that contain rich semantic information, binary code is\ntypically difficult for human engineers to understand and analyze. While\nexisting work uses AI models to assist source code analysis, few studies have\nconsidered binary code. In this paper, we propose a COntrastive learning Model\nfor Binary cOde Analysis, or COMBO, that incorporates source code and comment\ninformation into binary code during representation learning. Specifically, we\npresent three components in COMBO: (1) a primary contrastive learning method\nfor cold-start pre-training, (2) a simplex interpolation method to incorporate\nsource code, comments, and binary code, and (3) an intermediate representation\nlearning algorithm to provide binary code embeddings. Finally, we evaluate the\neffectiveness of the pre-trained representations produced by COMBO using three\nindicative downstream tasks relating to binary code: algorithmic functionality\nclassification, binary code similarity, and vulnerability detection. Our\nexperimental results show that COMBO facilitates representation learning of\nbinary code visualized by distribution analysis, and improves the performance\non all three downstream tasks by 5.45% on average compared to state-of-the-art\nlarge-scale language representation models. To the best of our knowledge, COMBO\nis the first language representation model that incorporates source code,\nbinary code, and comments into contrastive code representation learning and\nunifies multiple tasks for binary code analysis.",
    "descriptor": "",
    "authors": [
      "Yifan Zhang",
      "Chen Huang",
      "Yueke Zhang",
      "Kevin Cao",
      "Scott Thomas Andersen",
      "Huajie Shao",
      "Kevin Leach",
      "Yu Huang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05102"
  },
  {
    "id": "arXiv:2210.05103",
    "title": "Leveraging Artificial Intelligence on Binary Code Comprehension",
    "abstract": "Understanding binary code is an essential but complex software engineering\ntask for reverse engineering, malware analysis, and compiler optimization.\nUnlike source code, binary code has limited semantic information, which makes\nit challenging for human comprehension. At the same time, compiling source to\nbinary code, or transpiling among different programming languages (PLs) can\nprovide a way to introduce external knowledge into binary comprehension. We\npropose to develop Artificial Intelligence (AI) models that aid human\ncomprehension of binary code. Specifically, we propose to incorporate domain\nknowledge from large corpora of source code (e.g., variable names, comments) to\nbuild AI models that capture a generalizable representation of binary code.\nLastly, we will investigate metrics to assess the performance of models that\napply to binary code by using human studies of comprehension.",
    "descriptor": "",
    "authors": [
      "Yifan Zhang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05103"
  },
  {
    "id": "arXiv:2210.05105",
    "title": "Distributed-Memory Randomized Algorithms for Sparse Tensor CP  Decomposition",
    "abstract": "Low-rank Candecomp / PARAFAC (CP) Decomposition is a powerful tool for the\nanalysis of sparse tensors, which can represent diverse datasets involving\ndiscrete-valued variables. Given a sparse tensor, producing a low-rank CP\ndecomposition is computation and memory intensive, typically involving several\nlarge, structured linear least-squares problems. Several recent works have\nprovided randomized sketching methods to reduce the cost of these least squares\nproblems, along with shared-memory prototypes of their algorithms.\nUnfortunately, these prototypes are slow compared to optimized non-randomized\ntensor decomposition. Furthermore, they do not scale to tensors that exceed the\nmemory capacity of a single shared-memory device.\nWe extend randomized algorithms for CP decomposition to the\ndistributed-memory setting and provide high-performance implementations\ncompetitive with state-of-the-art non-randomized libraries. These algorithms\nsample from a distribution of statistical leverage scores to reduce the cost of\nrepeated least squares solves required in the tensor decomposition. We show how\nto efficiently sample from an approximate leverage score distribution of the\nleft-hand side of each linear system when the CP factor matrices are\ndistributed by block rows among processors. In contrast to earlier works that\nonly communicate dense factor matrices in a Cartesian topology between\nprocessors, we use sampling to avoid expensive reduce-scatter collectives by\ncommunicating selected nonzeros from the sparse tensor and a small subset of\nfactor matrix rows. On the CPU partition of the NERSC Cray EX supercomputer\nPerlmutter, our high-performance implementations require just seconds to\ncompute low-rank approximations of real-world sparse tensors with billions of\nnonzeros.",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Vivek Bharadwaj",
      "Osman Asif Malik",
      "Riley Murray",
      "Aydin Bulu\u00e7",
      "James Demmel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05105"
  },
  {
    "id": "arXiv:2210.05109",
    "title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
    "abstract": "In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.",
    "descriptor": "\nComments: AACL 2022 (camera-ready)\n",
    "authors": [
      "Ajwad Akil",
      "Najrin Sultana",
      "Abhik Bhattacharjee",
      "Rifat Shahriyar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05109"
  },
  {
    "id": "arXiv:2210.05111",
    "title": "Deep learning model compression using network sensitivity and gradients",
    "abstract": "Deep learning model compression is an improving and important field for the\nedge deployment of deep learning models. Given the increasing size of the\nmodels and their corresponding power consumption, it is vital to decrease the\nmodel size and compute requirement without a significant drop in the model's\nperformance. In this paper, we present model compression algorithms for both\nnon-retraining and retraining conditions. In the first case where retraining of\nthe model is not feasible due to lack of access to the original data or absence\nof necessary compute resources while only having access to off-the-shelf\nmodels, we propose the Bin & Quant algorithm for compression of the deep\nlearning models using the sensitivity of the network parameters. This results\nin 13x compression of the speech command and control model and 7x compression\nof the DeepSpeech2 models. In the second case when the models can be retrained\nand utmost compression is required for the negligible loss in accuracy, we\npropose our novel gradient-weighted k-means clustering algorithm (GWK). This\nmethod uses the gradients in identifying the important weight values in a given\ncluster and nudges the centroid towards those values, thereby giving importance\nto sensitive weights. Our method effectively combines product quantization with\nthe EWGS[1] algorithm for sub-1-bit representation of the quantized models. We\ntest our GWK algorithm on the CIFAR10 dataset across a range of models such as\nResNet20, ResNet56, MobileNetv2 and show 35x compression on quantized models\nfor less than 2% absolute loss in accuracy compared to the floating-point\nmodels.",
    "descriptor": "",
    "authors": [
      "Madhumitha Sakthi",
      "Niranjan Yadla",
      "Raj Pawate"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05111"
  },
  {
    "id": "arXiv:2210.05112",
    "title": "HUE: Pretrained Model and Dataset for Understanding Hanja Documents of  Ancient Korea",
    "abstract": "Historical records in Korea before the 20th century were primarily written in\nHanja, an extinct language based on Chinese characters and not understood by\nmodern Korean or Chinese speakers. Historians with expertise in this time\nperiod have been analyzing the documents, but that process is very difficult\nand time-consuming, and language models would significantly speed up the\nprocess. Toward building and evaluating language models for Hanja, we release\nthe Hanja Understanding Evaluation dataset consisting of chronological\nattribution, topic classification, named entity recognition, and summary\nretrieval tasks. We also present BERT-based models continued training on the\ntwo major corpora from the 14th to the 19th centuries: the Annals of the Joseon\nDynasty and Diaries of the Royal Secretariats. We compare the models with\nseveral baselines on all tasks and show there are significant improvements\ngained by training on the two corpora. Additionally, we run zero-shot\nexperiments on the Daily Records of the Royal Court and Important Officials\n(DRRI). The DRRI dataset has not been studied much by the historians, and not\nat all by the NLP community.",
    "descriptor": "\nComments: Findings of NAACL 2022\n",
    "authors": [
      "Haneul Yoo",
      "Jiho Jin",
      "Juhee Son",
      "JinYeong Bak",
      "Kyunghyun Cho",
      "Alice Oh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05112"
  },
  {
    "id": "arXiv:2210.05118",
    "title": "Boosting Adversarial Robustness From The Perspective of Effective Margin  Regularization",
    "abstract": "The adversarial vulnerability of deep neural networks (DNNs) has been\nactively investigated in the past several years. This paper investigates the\nscale-variant property of cross-entropy loss, which is the most commonly used\nloss function in classification tasks, and its impact on the effective margin\nand adversarial robustness of deep neural networks. Since the loss function is\nnot invariant to logit scaling, increasing the effective weight norm will make\nthe loss approach zero and its gradient vanish while the effective margin is\nnot adequately maximized. On typical DNNs, we demonstrate that, if not properly\nregularized, the standard training does not learn large effective margins and\nleads to adversarial vulnerability. To maximize the effective margins and learn\na robust DNN, we propose to regularize the effective weight norm during\ntraining. Our empirical study on feedforward DNNs demonstrates that the\nproposed effective margin regularization (EMR) learns large effective margins\nand boosts the adversarial robustness in both standard and adversarial\ntraining. On large-scale models, we show that EMR outperforms basic adversarial\ntraining, TRADES and two regularization baselines with substantial improvement.\nMoreover, when combined with several strong adversarial defense methods (MART\nand MAIL), our EMR further boosts the robustness.",
    "descriptor": "\nComments: BMVC 2022\n",
    "authors": [
      "Ziquan Liu",
      "Antoni B. Chan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05118"
  },
  {
    "id": "arXiv:2210.05119",
    "title": "Exploring CNN-based models for image's aesthetic score prediction with  using ensemble",
    "abstract": "In this paper, we proposed a framework of constructing two types of the\nautomatic image aesthetics assessment models with different CNN architectures\nand improving the performance of the image's aesthetic score prediction by the\nensemble. Moreover, the attention regions of the models to the images are\nextracted to analyze the consistency with the subjects in the images. The\nexperimental results verify that the proposed method is effective for improving\nthe AS prediction. Moreover, it is found that the AS classification models\ntrained on XiheAA dataset seem to learn the latent photography principles,\nalthough it can't be said that they learn the aesthetic sense.",
    "descriptor": "",
    "authors": [
      "Ying Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05119"
  },
  {
    "id": "arXiv:2210.05121",
    "title": "Current injection and voltage insertion attacks against the VMG-KLJN  secure key exchanger",
    "abstract": "In this paper, the vulnerability of the Vadai, Mingesz and Gingl (VMG)-\nKirchhoff-Law-Johnson-Noise (KLJN) Key Exchanger (Nature, Science Report 5\n(2015) 13653) against two active attacks is demonstrated. The security\nvulnerability arises from the fact that the effective driving impedances are\ndifferent between the HL and LH cases for the VMG-KLJN scheme; whereas for the\nideal KLJN scheme they are same. Two defense schemes are shown against these\nattacks but each of them can protect against only one of the attack types; but\nnot against the two attacks simultaneously. The theoretical results are\nconfirmed by computer simulations.",
    "descriptor": "",
    "authors": [
      "Shahriar Ferdous",
      "Christiana Chamon",
      "Laszlo B. Kish"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05121"
  },
  {
    "id": "arXiv:2210.05125",
    "title": "Human-AI Coordination via Human-Regularized Search and Learning",
    "abstract": "We consider the problem of making AI agents that collaborate well with humans\nin partially observable fully cooperative environments given datasets of human\nbehavior. Inspired by piKL, a human-data-regularized search method that\nimproves upon a behavioral cloning policy without diverging far away from it,\nwe develop a three-step algorithm that achieve strong performance in\ncoordinating with real humans in the Hanabi benchmark. We first use a\nregularized search algorithm and behavioral cloning to produce a better human\nmodel that captures diverse skill levels. Then, we integrate the policy\nregularization idea into reinforcement learning to train a human-like best\nresponse to the human model. Finally, we apply regularized search on top of the\nbest response policy at test time to handle out-of-distribution challenges when\nplaying with humans. We evaluate our method in two large scale experiments with\nhumans. First, we show that our method outperforms experts when playing with a\ngroup of diverse human players in ad-hoc teams. Second, we show that our method\nbeats a vanilla best response to behavioral cloning baseline by having experts\nplay repeatedly with the two agents.",
    "descriptor": "",
    "authors": [
      "Hengyuan Hu",
      "David J Wu",
      "Adam Lerer",
      "Jakob Foerster",
      "Noam Brown"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.05125"
  },
  {
    "id": "arXiv:2210.05126",
    "title": "Tackling Instance-Dependent Label Noise with Dynamic Distribution  Calibration",
    "abstract": "Instance-dependent label noise is realistic but rather challenging, where the\nlabel-corruption process depends on instances directly. It causes a severe\ndistribution shift between the distributions of training and test data, which\nimpairs the generalization of trained models. Prior works put great effort into\ntackling the issue. Unfortunately, these works always highly rely on strong\nassumptions or remain heuristic without theoretical guarantees. In this paper,\nto address the distribution shift in learning with instance-dependent label\nnoise, a dynamic distribution-calibration strategy is adopted. Specifically, we\nhypothesize that, before training data are corrupted by label noise, each class\nconforms to a multivariate Gaussian distribution at the feature level. Label\nnoise produces outliers to shift the Gaussian distribution. During training, to\ncalibrate the shifted distribution, we propose two methods based on the mean\nand covariance of multivariate Gaussian distribution respectively. The\nmean-based method works in a recursive dimension-reduction manner for robust\nmean estimation, which is theoretically guaranteed to train a high-quality\nmodel against label noise. The covariance-based method works in a distribution\ndisturbance manner, which is experimentally verified to improve the model\nrobustness. We demonstrate the utility and effectiveness of our methods on\ndatasets with synthetic label noise and real-world unknown noise.",
    "descriptor": "\nComments: Accepted at ACM MM2022\n",
    "authors": [
      "Manyi Zhang",
      "Yuxin Ren",
      "Zihao Wang",
      "Chun Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05126"
  },
  {
    "id": "arXiv:2210.05128",
    "title": "On fast greedy block Kaczmarz methods for solving large consistent  linear systems",
    "abstract": "A class of fast greedy block Kaczmarz methods combined with general greedy\nstrategy and average technique are proposed for solving consistent linear\nsystems. Theoretical analysis of the convergence of the proposed method is\ngiven in detail. Numerical experiments show that the proposed methods are more\nefficient and faster than the existing methods.",
    "descriptor": "\nComments: 10 pages, 1 figure\n",
    "authors": [
      "Aqin Xiao",
      "Junfeng Yin",
      "Ning Zheng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05128"
  },
  {
    "id": "arXiv:2210.05129",
    "title": "Multi-Object Navigation with dynamically learned neural implicit  representations",
    "abstract": "Understanding and mapping a new environment are core abilities of any\nautonomously navigating agent. While classical robotics usually estimates maps\nin a stand-alone manner with SLAM variants, which maintain a topological or\nmetric representation, end-to-end learning of navigation keeps some form of\nmemory in a neural network. Networks are typically imbued with inductive\nbiases, which can range from vectorial representations to birds-eye metric\ntensors or topological structures. In this work, we propose to structure neural\nnetworks with two neural implicit representations, which are learned\ndynamically during each episode and map the content of the scene: (i) the\nSemantic Finder predicts the position of a previously seen queried object; (ii)\nthe Occupancy and Exploration Implicit Representation encapsulates information\nabout explored area and obstacles, and is queried with a novel global read\nmechanism which directly maps from function space to a usable embedding space.\nBoth representations are leveraged by an agent trained with Reinforcement\nLearning (RL) and learned online during each episode. We evaluate the agent on\nMulti-Object Navigation and show the high impact of using neural implicit\nrepresentations as a memory source.",
    "descriptor": "",
    "authors": [
      "Pierre Marza",
      "Laetitia Matignon",
      "Olivier Simonin",
      "Christian Wolf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05129"
  },
  {
    "id": "arXiv:2210.05130",
    "title": "ACRNet: Attention Cube Regression Network for Multi-view Real-time 3D  Human Pose Estimation in Telemedicine",
    "abstract": "Human pose estimation (HPE) for 3D skeleton reconstruction in telemedicine\nhas long received attention. Although the development of deep learning has made\nHPE methods in telemedicine simpler and easier to use, addressing low accuracy\nand high latency remains a big challenge. In this paper, we propose a novel\nmulti-view Attention Cube Regression Network (ACRNet), which regresses the 3D\nposition of joints in real time by aggregating informative attention points on\neach cube surface. More specially, a cube whose each surface contains uniformly\ndistributed attention points with specific coordinate values is first created\nto wrap the target from the main view. Then, our network regresses the 3D\nposition of each joint by summing and averaging the coordinates of attention\npoints on each surface after being weighted. To verify our method, we first\ntested ACRNet on the open-source ITOP dataset; meanwhile, we collected a new\nmulti-view upper body movement dataset (UBM) on the trunk support trainer\n(TruST) to validate the capability of our model in real rehabilitation\nscenarios. Experimental results demonstrate the superiority of ACRNet compared\nwith other state-of-the-art methods. We also validate the efficacy of each\nmodule in ACRNet. Furthermore, Our work analyzes the performance of ACRNet\nunder the medical monitoring indicator. Because of the high accuracy and\nrunning speed, our model is suitable for real-time telemedicine settings. The\nsource code is available at https://github.com/BoceHu/ACRNet",
    "descriptor": "",
    "authors": [
      "Boce Hu",
      "Chenfei Zhu",
      "Xupeng Ai",
      "Sunil K. Agrawal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05130"
  },
  {
    "id": "arXiv:2210.05135",
    "title": "X-NeRF: Explicit Neural Radiance Field for Multi-Scene 360$^{\\circ} $  Insufficient RGB-D Views",
    "abstract": "Neural Radiance Fields (NeRFs), despite their outstanding performance on\nnovel view synthesis, often need dense input views. Many papers train one model\nfor each scene respectively and few of them explore incorporating multi-modal\ndata into this problem. In this paper, we focus on a rarely discussed but\nimportant setting: can we train one model that can represent multiple scenes,\nwith 360$^\\circ $ insufficient views and RGB-D images? We refer insufficient\nviews to few extremely sparse and almost non-overlapping views. To deal with\nit, X-NeRF, a fully explicit approach which learns a general scene completion\nprocess instead of a coordinate-based mapping, is proposed. Given a few\ninsufficient RGB-D input views, X-NeRF first transforms them to a sparse point\ncloud tensor and then applies a 3D sparse generative Convolutional Neural\nNetwork (CNN) to complete it to an explicit radiance field whose volumetric\nrendering can be conducted fast without running networks during inference. To\navoid overfitting, besides common rendering loss, we apply perceptual loss as\nwell as view augmentation through random rotation on point clouds. The proposed\nmethodology significantly out-performs previous implicit methods in our\nsetting, indicating the great potential of proposed problem and approach. Codes\nand data are available at https://github.com/HaoyiZhu/XNeRF.",
    "descriptor": "",
    "authors": [
      "Haoyi Zhu",
      "Hao-Shu Fang",
      "Cewu Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05135"
  },
  {
    "id": "arXiv:2210.05142",
    "title": "A Design Method of Distributed Algorithms via Discrete-time Blended  Dynamics Theorem",
    "abstract": "We develop a discrete-time version of the blended dynamics theorem for the\nuse of designing distributed computation algorithms. The blended dynamics\ntheorem enables to predict the behavior of heterogeneous multi-agent systems.\nTherefore, once we get a blended dynamics for a particular computational task,\ndesign idea of node dynamics for individual heterogeneous agents can easily\noccur. In the continuous-time case, prediction by blended dynamics was enabled\nby high coupling gain among neighboring agents. In the discrete-time case, we\npropose an equivalent action, which we call multi-step coupling in this paper.\nCompared to the continuous-time case, the blended dynamics can have more\nvariety depending on the coupling matrix. This benefit is demonstrated with\nthree applications; distributed estimation of network size, distributed\ncomputation of the PageRank, and distributed computation of the degree sequence\nof a graph, which correspond to the coupling by doubly-stochastic,\ncolumn-stochastic, and row-stochastic matrices, respectively.",
    "descriptor": "",
    "authors": [
      "Jeong Woo Kim",
      "Jin Gyu Lee",
      "Donggil Lee",
      "Hyungbo Shim"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05142"
  },
  {
    "id": "arXiv:2210.05143",
    "title": "Time-aware topic identification in social media with pre-trained  language models: A case study of electric vehicles",
    "abstract": "Recent extensively competitive business environment makes companies to keep\ntheir eyes on social media, as there is a growing recognition over customer\nlanguages (e.g., needs, interests, and complaints) as source of future\nopportunities. This research avenue analysing social media data has received\nmuch attention in academia, but their utilities are limited as most of methods\nprovide retrospective results. Moreover, the increasing number of\ncustomer-generated contents and rapidly varying topics have made the necessity\nof time-aware topic evolution analyses. Recently, several researchers have\nshowed the applicability of pre-trained semantic language models to social\nmedia as an input feature, but leaving limitations in understanding evolving\ntopics. In this study, we propose a time-aware topic identification approach\nwith pre-trained language models. The proposed approach consists of two stages:\nthe dynamics-focused function for tracking time-varying topics with language\nmodels and the emergence-scoring function to examine future promising topics.\nHere we apply the proposed approach to reddit data on electric vehicles, and\nour findings highlight the feasibility of capturing emerging customer topics\nfrom voluminous social media in a time-aware manner.",
    "descriptor": "",
    "authors": [
      "Byeongki Jeong",
      "Janghyeok Yoon",
      "Jaewoong Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.05143"
  },
  {
    "id": "arXiv:2210.05144",
    "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token",
    "abstract": "Mixture-of-Experts (MoE) networks have been proposed as an efficient way to\nscale up model capacity and implement conditional computing. However, the study\nof MoE components mostly focused on the feedforward layer in Transformer\narchitecture. This paper proposes the Mixture of Attention Heads (MoA), a new\narchitecture that combines multi-head attention with the MoE mechanism. MoA\nincludes a set of attention heads that each has its own set of parameters.\nGiven an input, a router dynamically selects a subset of $k$ attention heads\nper token. This conditional computation schema allows MoA to achieve stronger\nperformance than the standard multi-head attention layer. Furthermore, the\nsparsely gated MoA can easily scale up the number of attention heads and the\nnumber of parameters while preserving computational efficiency. In addition to\nthe performance improvements, MoA also automatically differentiates heads'\nutilities, providing a new perspective to discuss the model's interpretability.\nWe conducted experiments on several important tasks, including Machine\nTranslation and Masked Language Modeling. Experiments have shown promising\nresults on several tasks against strong baselines that involve large and very\ndeep models.",
    "descriptor": "\nComments: accepted in EMNLP 2022\n",
    "authors": [
      "Xiaofeng Zhang",
      "Yikang Shen",
      "Zeyu Huang",
      "Jie Zhou",
      "Wenge Rong",
      "Zhang Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05144"
  },
  {
    "id": "arXiv:2210.05145",
    "title": "Retrieval Augmentation for T5 Re-ranker using External Sources",
    "abstract": "Retrieval augmentation has shown promising improvements in different tasks.\nHowever, whether such augmentation can assist a large language model based\nre-ranker remains unclear. We investigate how to augment T5-based re-rankers\nusing high-quality information retrieved from two external corpora -- a\ncommercial web search engine and Wikipedia. We empirically demonstrate how\nretrieval augmentation can substantially improve the effectiveness of T5-based\nre-rankers for both in-domain and zero-shot out-of-domain re-ranking tasks.",
    "descriptor": "",
    "authors": [
      "Kai Hui",
      "Tao Chen",
      "Zhen Qin",
      "Honglei Zhuang",
      "Fernando Diaz",
      "Mike Bendersky",
      "Don Metzler"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05145"
  },
  {
    "id": "arXiv:2210.05146",
    "title": "CSS: Combining Self-training and Self-supervised Learning for Few-shot  Dialogue State Tracking",
    "abstract": "Few-shot dialogue state tracking (DST) is a realistic problem that trains the\nDST model with limited labeled data. Existing few-shot methods mainly transfer\nknowledge learned from external labeled dialogue data (e.g., from question\nanswering, dialogue summarization, machine reading comprehension tasks, etc.)\ninto DST, whereas collecting a large amount of external labeled data is\nlaborious, and the external data may not effectively contribute to the\nDST-specific task. In this paper, we propose a few-shot DST framework called\nCSS, which Combines Self-training and Self-supervised learning methods. The\nunlabeled data of the DST task is incorporated into the self-training\niterations, where the pseudo labels are predicted by a DST model trained on\nlimited labeled data in advance. Besides, a contrastive self-supervised method\nis used to learn better representations, where the data is augmented by the\ndropout operation to train the model. Experimental results on the MultiWOZ\ndataset show that our proposed CSS achieves competitive performance in several\nfew-shot scenarios.",
    "descriptor": "\nComments: Accepted to AACL 2022\n",
    "authors": [
      "Haoning Zhang",
      "Junwei Bao",
      "Haipeng Sun",
      "Huaishao Luo",
      "Wenye Li",
      "Shuguang Cui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05146"
  },
  {
    "id": "arXiv:2210.05147",
    "title": "Markup-to-Image Diffusion Models with Scheduled Sampling",
    "abstract": "Building on recent advances in image generation, we present a fully\ndata-driven approach to rendering markup into images. The approach is based on\ndiffusion models, which parameterize the distribution of data using a sequence\nof denoising operations on top of a Gaussian noise distribution. We view the\ndiffusion denoising process as a sequential decision making process, and show\nthat it exhibits compounding errors similar to exposure bias issues in\nimitation learning problems. To mitigate these issues, we adapt the scheduled\nsampling algorithm to diffusion training. We conduct experiments on four markup\ndatasets: mathematical formulas (LaTeX), table layouts (HTML), sheet music\n(LilyPond), and molecular images (SMILES). These experiments each verify the\neffectiveness of the diffusion process and the use of scheduled sampling to fix\ngeneration issues. These results also show that the markup-to-image task\npresents a useful controlled compositional setting for diagnosing and analyzing\ngenerative image models.",
    "descriptor": "",
    "authors": [
      "Yuntian Deng",
      "Noriyuki Kojima",
      "Alexander M. Rush"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05147"
  },
  {
    "id": "arXiv:2210.05148",
    "title": "DiffRoll: Diffusion-based Generative Music Transcription with  Unsupervised Pretraining Capability",
    "abstract": "In this paper we propose a novel generative approach, DiffRoll, to tackle\nautomatic music transcription (AMT). Instead of treating AMT as a\ndiscriminative task in which the model is trained to convert spectrograms into\npiano rolls, we think of it as a conditional generative task where we train our\nmodel to generate realistic looking piano rolls from pure Gaussian noise\nconditioned on spectrograms. This new AMT formulation enables DiffRoll to\ntranscribe, generate and even inpaint music. Due to the classifier-free nature,\nDiffRoll is also able to be trained on unpaired datasets where only piano rolls\nare available. Our experiments show that DiffRoll outperforms its\ndiscriminative counterpart by 17.9 percentage points (ppt.) and our ablation\nstudies also indicate that it outperforms similar existing methods by 3.70 ppt.",
    "descriptor": "",
    "authors": [
      "Kin Wai Cheuk",
      "Ryosuke Sawata",
      "Toshimitsu Uesaka",
      "Naoki Murata",
      "Naoya Takahashi",
      "Shusuke Takahashi",
      "Dorien Herremans",
      "Yuki Mitsufuji"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05148"
  },
  {
    "id": "arXiv:2210.05150",
    "title": "DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical  Reinforcement Learning",
    "abstract": "Hierarchical Reinforcement Learning (HRL) has made notable progress in\ncomplex control tasks by leveraging temporal abstraction. However, previous HRL\nalgorithms often suffer from serious data inefficiency as environments get\nlarge. The extended components, $i.e.$, goal space and length of episodes,\nimpose a burden on either one or both high-level and low-level policies since\nboth levels share the total horizon of the episode. In this paper, we present a\nmethod of Decoupling Horizons Using a Graph in Hierarchical Reinforcement\nLearning (DHRL) which can alleviate this problem by decoupling the horizons of\nhigh-level and low-level policies and bridging the gap between the length of\nboth horizons using a graph. DHRL provides a freely stretchable high-level\naction interval, which facilitates longer temporal abstraction and faster\ntraining in complex tasks. Our method outperforms state-of-the-art HRL\nalgorithms in typical HRL environments. Moreover, DHRL achieves long and\ncomplex locomotion and manipulation tasks.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Seungjae Lee",
      "Jigang Kim",
      "Inkyu Jang",
      "H. Jin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05150"
  },
  {
    "id": "arXiv:2210.05151",
    "title": "UGformer for Robust Left Atrium and Scar Segmentation Across Scanners",
    "abstract": "Thanks to the capacity for long-range dependencies and robustness to\nirregular shapes, vision transformers and deformable convolutions are emerging\nas powerful vision techniques of segmentation.Meanwhile, Graph Convolution\nNetworks (GCN) optimize local features based on global topological relationship\nmodeling. Particularly, they have been proved to be effective in addressing\nissues in medical imaging segmentation tasks including multi-domain\ngeneralization for low-quality images. In this paper, we present a novel,\neffective, and robust framework for medical image segmentation, namely,\nUGformer. It unifies novel transformer blocks, GCN bridges, and convolution\ndecoders originating from U-Net to predict left atriums (LAs) and LA scars. We\nhave identified two appealing findings of the proposed UGformer: 1). an\nenhanced transformer module with deformable convolutions to improve the\nblending of the transformer information with convolutional information and help\npredict irregular LAs and scar shapes. 2). Using a bridge incorporating GCN to\nfurther overcome the difficulty of capturing condition inconsistency across\ndifferent Magnetic Resonance Images scanners with various inconsistent domain\ninformation. The proposed UGformer model exhibits outstanding ability to\nsegment the left atrium and scar on the LAScarQS 2022 dataset, outperforming\nseveral recent state-of-the-arts.",
    "descriptor": "",
    "authors": [
      "Tianyi Liu",
      "Size Hou",
      "Jiayuan Zhu",
      "Zilong Zhao",
      "Haochuan Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05151"
  },
  {
    "id": "arXiv:2210.05152",
    "title": "TriangleNet: Edge Prior Augmented Network for Semantic Segmentation  through Cross-Task Consistency",
    "abstract": "Semantic segmentation is a classic computer vision problem dedicated to\nlabeling each pixel with its corresponding category. As a basic task for\nadvanced tasks such as industrial quality inspection, remote sensing\ninformation extraction, medical diagnostic aid, and autonomous driving,\nsemantic segmentation has been developed for a long time in combination with\ndeep learning, and a lot of work has been accumulated. However, neither the\nclassic FCN-based works nor the popular Transformer-based works have attained\nfine-grained localization of pixel labels, which remains the main challenge in\nthis field. Recently, with the popularity of autonomous driving, the\nsegmentation of road scenes has received more and more attention. Based on the\ncross-task consistency theory, we incorporate edge priors into semantic\nsegmentation tasks to obtain better results. The main contribution is that we\nprovide a model-agnostic method that improves the accuracy of semantic\nsegmentation models with zero extra inference runtime overhead, verified on the\ndatasets of road and non-road scenes. From our experimental results, our method\ncan effectively improve semantic segmentation accuracy.",
    "descriptor": "\nComments: 22 pages, 3 figures\n",
    "authors": [
      "Dan Zhang",
      "Rui Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05152"
  },
  {
    "id": "arXiv:2210.05153",
    "title": "Understanding the Failure of Batch Normalization for Transformers in NLP",
    "abstract": "Batch Normalization (BN) is a core and prevalent technique in accelerating\nthe training of deep neural networks and improving the generalization on\nComputer Vision (CV) tasks. However, it fails to defend its position in Natural\nLanguage Processing (NLP), which is dominated by Layer Normalization (LN). In\nthis paper, we are trying to answer why BN usually performs worse than LN in\nNLP tasks with Transformer models. We find that the inconsistency between\ntraining and inference of BN is the leading cause that results in the failure\nof BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively\nmeasure this inconsistency and reveal that TID can indicate BN's performance,\nsupported by extensive experiments, including image classification, neural\nmachine translation, language modeling, sequence labeling, and text\nclassification tasks. We find that BN can obtain much better test performance\nthan LN when TID keeps small through training. To suppress the explosion of\nTID, we propose Regularized BN (RBN) that adds a simple regularization term to\nnarrow the gap between batch statistics and population statistics of BN. RBN\nimproves the performance of BN consistently and outperforms or is on par with\nLN on 17 out of 20 settings, involving ten datasets and two common variants of\nTransformer\\footnote{Our code is available at\n\\url{https://github.com/wjxts/RegularizedBN}}.",
    "descriptor": "\nComments: 17 pages, 11 figures, NeurIPS 2022\n",
    "authors": [
      "Jiaxi Wang",
      "Ji Wu",
      "Lei Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05153"
  },
  {
    "id": "arXiv:2210.05155",
    "title": "Contrastive Trajectory Similarity Learning with Dual-Feature Attention",
    "abstract": "Trajectory similarity measures act as query predicates in trajectory\ndatabases, making them the key player in determining the query results. They\nalso have a heavy impact on the query efficiency. An ideal measure should have\nthe capability to accurately evaluate the similarity between any two\ntrajectories in a very short amount of time. However, existing heuristic\nmeasures are mainly based on pointwise comparisons following hand-crafted\nrules, thus resulting in either poor quality results or low efficiency in many\ncases. Although several deep learning-based measures have recently aimed at\nthese problems, their improvements are limited by the difficulties to learn the\nfine-grained spatial patterns of trajectories.\nTo address these issues, we propose a contrastive learning-based trajectory\nmodelling method named TrajCL, which is robust in application scenarios where\nthe data set contains low-quality trajectories. Specifically, we present four\ntrajectory augmentation methods and a novel dual-feature self-attention-based\ntrajectory backbone encoder. The resultant model can jointly learn both the\nspatial and the structural patterns of trajectories. Our model does not involve\nany recurrent structures and thus has a high efficiency. Besides, our\npre-trained backbone encoder can be fine-tuned towards other computationally\nexpensive measures with minimal supervision data. Experimental results show\nthat TrajCL is consistently and significantly more accurate and faster than the\nstate-of-the-art trajectory similarity measures. After fine-tuning, i.e., when\nbeing used as an estimator for heuristic measures, TrajCL can even outperform\nthe state-of-the-art supervised method by up to 32% in the accuracy for\nprocessing trajectory similarity queries.",
    "descriptor": "",
    "authors": [
      "Yanchuan Chang",
      "Jianzhong Qi",
      "Yuxuan Liang",
      "Egemen Tanin"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05155"
  },
  {
    "id": "arXiv:2210.05156",
    "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for  Open-Domain Question Answering",
    "abstract": "Given its effectiveness on knowledge-intensive natural language processing\ntasks, dense retrieval models have become increasingly popular. Specifically,\nthe de-facto architecture for open-domain question answering uses two\nisomorphic encoders that are initialized from the same pretrained model but\nseparately parameterized for questions and passages. This bi-encoder\narchitecture is parameter-inefficient in that there is no parameter sharing\nbetween encoders. Further, recent studies show that such dense retrievers\nunderperform BM25 in various settings. We thus propose a new architecture,\nTask-aware Specialization for dense Retrieval (TASER), which enables parameter\nsharing by interleaving shared and specialized blocks in a single encoder. Our\nexperiments on five question answering datasets show that \\ourmodel\\ can\nachieve superior accuracy, surpassing BM25, while using about 60% of the\nparameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER\nis also empirically more robust than bi-encoder dense retrievers.",
    "descriptor": "",
    "authors": [
      "Hao Cheng",
      "Hao Fang",
      "Xiaodong Liu",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05156"
  },
  {
    "id": "arXiv:2210.05158",
    "title": "ConserWeightive Behavioral Cloning for Reliable Offline Reinforcement  Learning",
    "abstract": "The goal of offline reinforcement learning (RL) is to learn near-optimal\npolicies from static logged datasets, thus sidestepping expensive online\ninteractions. Behavioral cloning (BC) provides a straightforward solution to\noffline RL by mimicking offline trajectories via supervised learning. Recent\nadvances (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021) have\nshown that by conditioning on desired future returns, BC can perform\ncompetitively to their value-based counterparts, while enjoying much more\nsimplicity and training stability. However, the distribution of returns in the\noffline dataset can be arbitrarily skewed and suboptimal, which poses a unique\nchallenge for conditioning BC on expert returns at test time. We propose\nConserWeightive Behavioral Cloning (CWBC), a simple and effective method for\nimproving the performance of conditional BC for offline RL with two key\ncomponents: trajectory weighting and conservative regularization. Trajectory\nweighting addresses the bias-variance tradeoff in conditional BC and provides a\nprincipled mechanism to learn from both low return trajectories (typically\nplentiful) and high return trajectories (typically few). Further, we analyze\nthe notion of conservatism in existing BC methods, and propose a novel\nconservative regularize that explicitly encourages the policy to stay close to\nthe data distribution. The regularizer helps achieve more reliable performance,\nand removes the need for ad-hoc tuning of the conditioning value during\nevaluation. We instantiate CWBC in the context of Reinforcement Learning via\nSupervised Learning (RvS) (Emmons et al., 2021) and Decision Transformer (DT)\n(Chen et al., 2021), and empirically show that it significantly boosts the\nperformance and stability of prior methods on various offline RL benchmarks.\nCode is available at https://github.com/tung-nd/cwbc.",
    "descriptor": "",
    "authors": [
      "Tung Nguyen",
      "Qinqing Zheng",
      "Aditya Grover"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05158"
  },
  {
    "id": "arXiv:2210.05159",
    "title": "Can Language Models Be Specific? How?",
    "abstract": "A good speaker not only needs to be correct, but also has the ability to be\nspecific when desired, and so are language models. In this paper, we propose to\nmeasure how specific the language of pre-trained language models (PLMs) is. To\nachieve this, we introduce a novel approach to build a benchmark for\nspecificity testing by forming masked token prediction tasks with prompts. For\ninstance, given ``J. K. Rowling was born in [MASK].'', we want to test whether\na more specific answer will be better filled in by PLMs, e.g., Yate instead of\nEngland. From our evaluations, we show that existing PLMs have only a slight\npreference for more specific answers. We identify underlying factors affecting\nthe specificity and design two prompt-based methods to improve the specificity.\nResults show that the specificity of the models can be improved by the proposed\nmethods without additional training. We believe this work can provide new\ninsights for language modeling and encourage the research community to further\nexplore this important but understudied problem.",
    "descriptor": "",
    "authors": [
      "Jie Huang",
      "Kevin Chen-Chuan Chang",
      "Jinjun Xiong",
      "Wen-mei Hwu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05159"
  },
  {
    "id": "arXiv:2210.05168",
    "title": "LARF: Two-level Attention-based Random Forests with a Mixture of  Contamination Models",
    "abstract": "New models of the attention-based random forests called LARF (Leaf\nAttention-based Random Forest) are proposed. The first idea behind the models\nis to introduce a two-level attention, where one of the levels is the \"leaf\"\nattention and the attention mechanism is applied to every leaf of trees. The\nsecond level is the tree attention depending on the \"leaf\" attention. The\nsecond idea is to replace the softmax operation in the attention with the\nweighted sum of the softmax operations with different parameters. It is\nimplemented by applying a mixture of the Huber's contamination models and can\nbe regarded as an analog of the multi-head attention with \"heads\" defined by\nselecting a value of the softmax parameter. Attention parameters are simply\ntrained by solving the quadratic optimization problem. To simplify the tuning\nprocess of the models, it is proposed to make the tuning contamination\nparameters to be training and to compute them by solving the quadratic\noptimization problem. Many numerical experiments with real datasets are\nperformed for studying LARFs. The code of proposed algorithms can be found in\nhttps://github.com/andruekonst/leaf-attention-forest.",
    "descriptor": "",
    "authors": [
      "Andrei V. Konstantinov",
      "Lev V. Utkin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05168"
  },
  {
    "id": "arXiv:2210.05170",
    "title": "Constructions of cyclic codes and extended primitive cyclic codes with  their applications",
    "abstract": "Linear codes with a few weights have many nice applications including\ncombinatorial design, distributed storage system, secret sharing schemes and so\non. In this paper, we construct two families of linear codes with a few weights\nbased on special polynomials over finite fields. The first family of linear\ncodes are extended primitive cyclic codes which are affine-invariant. The\nsecond family of linear codes are reducible cyclic codes. The parameters of\nthese codes and their duals are determined. As the first application, we prove\nthat these two families of linear codes hold $t$-designs, where $t=2,3$. As the\nsecond application, the minimum localities of the codes are also determined and\noptimal locally recoverable codes are derived.",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Ziling Heng",
      "Xinran Wang",
      "Xiaoru Li"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05170"
  },
  {
    "id": "arXiv:2210.05171",
    "title": "Deep Fourier Up-Sampling",
    "abstract": "Existing convolutional neural networks widely adopt spatial down-/up-sampling\nfor multi-scale modeling. However, spatial up-sampling operators (\\emph{e.g.},\ninterpolation, transposed convolution, and un-pooling) heavily depend on local\npixel attention, incapably exploring the global dependency. In contrast, the\nFourier domain obeys the nature of global modeling according to the spectral\nconvolution theorem. Unlike the spatial domain that performs up-sampling with\nthe property of local similarity, up-sampling in the Fourier domain is more\nchallenging as it does not follow such a local property. In this study, we\npropose a theoretically sound Deep Fourier Up-Sampling (FourierUp) to solve\nthese issues. We revisit the relationships between spatial and Fourier domains\nand reveal the transform rules on the features of different resolutions in the\nFourier domain, which provide key insights for FourierUp's designs. FourierUp\nas a generic operator consists of three key components: 2D discrete Fourier\ntransform, Fourier dimension increase rules, and 2D inverse Fourier transform,\nwhich can be directly integrated with existing networks. Extensive experiments\nacross multiple computer vision tasks, including object detection, image\nsegmentation, image de-raining, image dehazing, and guided image\nsuper-resolution, demonstrate the consistent performance gains obtained by\nintroducing our FourierUp.",
    "descriptor": "\nComments: This paper was accepted by NeurIPS 2022. Project Paper:this https URL\n",
    "authors": [
      "Man Zhou",
      "Hu Yu",
      "Jie Huang",
      "Feng Zhao",
      "Jinwei Gu",
      "Chen Change Loy",
      "Deyu Meng",
      "Chongyi Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05171"
  },
  {
    "id": "arXiv:2210.05173",
    "title": "On Explainability in AI-Solutions: A Cross-Domain Survey",
    "abstract": "Artificial Intelligence (AI) increasingly shows its potential to outperform\npredicate logic algorithms and human control alike. In automatically deriving a\nsystem model, AI algorithms learn relations in data that are not detectable for\nhumans. This great strength, however, also makes use of AI methods dubious. The\nmore complex a model, the more difficult it is for a human to understand the\nreasoning for the decisions. As currently, fully automated AI algorithms are\nsparse, every algorithm has to provide a reasoning for human operators. For\ndata engineers, metrics such as accuracy and sensitivity are sufficient.\nHowever, if models are interacting with non-experts, explanations have to be\nunderstandable. This work provides an extensive survey of literature on this\ntopic, which, to a large part, consists of other surveys. The findings are\nmapped to ways of explaining decisions and reasons for explaining decisions. It\nshows that the heterogeneity of reasons and methods of and for explainability\nlead to individual explanatory frameworks.",
    "descriptor": "",
    "authors": [
      "Simon Daniel Duque Anton",
      "Daniel Schneider",
      "Hans Dieter Schotten"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05173"
  },
  {
    "id": "arXiv:2210.05174",
    "title": "BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised  Instance Segmentation",
    "abstract": "Labeling objects with pixel-wise segmentation requires a huge amount of human\nlabor compared to bounding boxes. Most existing methods for weakly supervised\ninstance segmentation focus on designing heuristic losses with priors from\nbounding boxes. While, we find that box-supervised methods can produce some\nfine segmentation masks and we wonder whether the detectors could learn from\nthese fine masks while ignoring low-quality masks. To answer this question, we\npresent BoxTeacher, an efficient and end-to-end training framework for\nhigh-performance weakly supervised instance segmentation, which leverages a\nsophisticated teacher to generate high-quality masks as pseudo labels.\nConsidering the massive noisy masks hurt the training, we present a mask-aware\nconfidence score to estimate the quality of pseudo masks, and propose the\nnoise-aware pixel loss and noise-reduced affinity loss to adaptively optimize\nthe student with pseudo masks. Extensive experiments can demonstrate\neffectiveness of the proposed BoxTeacher. Without bells and whistles,\nBoxTeacher remarkably achieves $34.4$ mask AP and $35.4$ mask AP with ResNet-50\nand ResNet-101 respectively on the challenging MS-COCO dataset, which\noutperforms the previous state-of-the-art methods by a significant margin. The\ncode and models are available at \\url{https://github.com/hustvl/BoxTeacher}.",
    "descriptor": "\nComments: Preprint. Work in progress. Code and models: this https URL\n",
    "authors": [
      "Tianheng Cheng",
      "Xinggang Wang",
      "Shaoyu Chen",
      "Qian Zhang",
      "Wenyu Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05174"
  },
  {
    "id": "arXiv:2210.05175",
    "title": "Variability Matters : Evaluating inter-rater variability in  histopathology for robust cell detection",
    "abstract": "Large annotated datasets have been a key component in the success of deep\nlearning. However, annotating medical images is challenging as it requires\nexpertise and a large budget. In particular, annotating different types of\ncells in histopathology suffer from high inter- and intra-rater variability due\nto the ambiguity of the task. Under this setting, the relation between\nannotators' variability and model performance has received little attention. We\npresent a large-scale study on the variability of cell annotations among 120\nboard-certified pathologists and how it affects the performance of a deep\nlearning model. We propose a method to measure such variability, and by\nexcluding those annotators with low variability, we verify the trade-off\nbetween the amount of data and its quality. We found that naively increasing\nthe data size at the expense of inter-rater variability does not necessarily\nlead to better-performing models in cell detection. Instead, decreasing the\ninter-rater variability with the expense of decreasing dataset size increased\nthe model performance. Furthermore, models trained from data annotated with\nlower inter-labeler variability outperform those from higher inter-labeler\nvariability. These findings suggest that the evaluation of the annotators may\nhelp tackle the fundamental budget issues in the histopathology domain",
    "descriptor": "",
    "authors": [
      "Cholmin Kang",
      "Chunggi Lee",
      "Heon Song",
      "Minuk Ma",
      "S ergio Pereira"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05175"
  },
  {
    "id": "arXiv:2210.05176",
    "title": "Fine-Grained Image Style Transfer with Visual Transformers",
    "abstract": "With the development of the convolutional neural network, image style\ntransfer has drawn increasing attention. However, most existing approaches\nadopt a global feature transformation to transfer style patterns into content\nimages (e.g., AdaIN and WCT). Such a design usually destroys the spatial\ninformation of the input images and fails to transfer fine-grained style\npatterns into style transfer results. To solve this problem, we propose a novel\nSTyle TRansformer (STTR) network which breaks both content and style images\ninto visual tokens to achieve a fine-grained style transformation.\nSpecifically, two attention mechanisms are adopted in our STTR. We first\npropose to use self-attention to encode content and style tokens such that\nsimilar tokens can be grouped and learned together. We then adopt\ncross-attention between content and style tokens that encourages fine-grained\nstyle transformations. To compare STTR with existing approaches, we conduct\nuser studies on Amazon Mechanical Turk (AMT), which are carried out with 50\nhuman subjects with 1,000 votes in total. Extensive evaluations demonstrate the\neffectiveness and efficiency of the proposed STTR in generating visually\npleasing style transfer results.",
    "descriptor": "\nComments: 24 pages, 15 figures\n",
    "authors": [
      "Jianbo Wang",
      "Huan Yang",
      "Jianlong Fu",
      "Toshihiko Yamasaki",
      "Baining Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05176"
  },
  {
    "id": "arXiv:2210.05177",
    "title": "Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation  Approach",
    "abstract": "Deep neural networks often suffer from poor generalization caused by complex\nand non-convex loss landscapes. One of the popular solutions is Sharpness-Aware\nMinimization (SAM), which smooths the loss landscape via minimizing the\nmaximized change of training loss when adding a perturbation to the weight.\nHowever, we find the indiscriminate perturbation of SAM on all parameters is\nsuboptimal, which also results in excessive computation, i.e., double the\noverhead of common optimizers like Stochastic Gradient Descent (SGD). In this\npaper, we propose an efficient and effective training scheme coined as Sparse\nSAM (SSAM), which achieves sparse perturbation by a binary mask. To obtain the\nsparse mask, we provide two solutions which are based onFisher information and\ndynamic sparse training, respectively. In addition, we theoretically prove that\nSSAM can converge at the same rate as SAM, i.e., $O(\\log T/\\sqrt{T})$. Sparse\nSAM not only has the potential for training acceleration but also smooths the\nloss landscape effectively. Extensive experimental results on CIFAR10,\nCIFAR100, and ImageNet-1K confirm the superior efficiency of our method to SAM,\nand the performance is preserved or even better with a perturbation of merely\n50% sparsity. Code is availiable at\n\\url{https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization}.",
    "descriptor": "\nComments: 20 pages, 5figures, accepted by NeurIPS 2022\n",
    "authors": [
      "Peng Mi",
      "Li Shen",
      "Tianhe Ren",
      "Yiyi Zhou",
      "Xiaoshuai Sun",
      "Rongrong Ji",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05177"
  },
  {
    "id": "arXiv:2210.05178",
    "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks from a  Handful of Trials",
    "abstract": "Recent progress in deep learning highlights the tremendous potential of\nutilizing diverse datasets for achieving effective generalization and makes it\nenticing to consider leveraging broad datasets for attaining more robust\ngeneralization in robotic learning as well. However, in practice we likely will\nwant to learn a new skill in a new environment that is unlikely to be contained\nin the prior data. Therefore we ask: how can we leverage existing diverse\noffline datasets in combination with small amounts of task-specific data to\nsolve new tasks, while still enjoying the generalization benefits of training\non large amounts of data? In this paper, we demonstrate that end-to-end offline\nRL can be an effective approach for doing this, without the need for any\nrepresentation learning or vision-based pre-training. We present pre-training\nfor robots (PTR), a framework based on offline RL that attempts to effectively\nlearn new tasks by combining pre-training on existing robotic datasets with\nrapid fine-tuning on a new task, with as a few as 10 demonstrations. At its\ncore, PTR applies an existing offline RL method such as conservative Q-learning\n(CQL), but extends it to include several crucial design decisions that enable\nPTR to actually work and outperform a variety of prior methods. To the best of\nour knowledge, PTR is the first offline RL method that succeeds at learning new\ntasks in a new domain on a real WidowX robot with as few as 10 task\ndemonstrations, by effectively leveraging an existing dataset of diverse\nmulti-task robot data collected in a variety of toy kitchens. Our\nimplementation can be found at: https://github.com/Asap7772/PTR.",
    "descriptor": "",
    "authors": [
      "Aviral Kumar",
      "Anikait Singh",
      "Frederik Ebert",
      "Yanlai Yang",
      "Chelsea Finn",
      "Sergey Levine"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05178"
  },
  {
    "id": "arXiv:2210.05180",
    "title": "Neurosymbolic Motion and Task Planning for Linear Temporal Logic Tasks",
    "abstract": "This paper presents a neurosymbolic framework to solve motion planning\nproblems for mobile robots involving temporal goals. The temporal goals are\ndescribed using temporal logic formulas such as Linear Temporal Logic (LTL) to\ncapture complex tasks. The proposed framework trains Neural Network (NN)-based\nplanners that enjoy strong correctness guarantees when applying to unseen\ntasks, i.e., the exact task (including workspace, LTL formula, and dynamic\nconstraints of a robot) is unknown during the training of NNs. Our approach to\nachieving theoretical guarantees and computational efficiency is based on two\ninsights. First, we incorporate a symbolic model into the training of NNs such\nthat the resulting NN-based planner inherits the interpretability and\ncorrectness guarantees of the symbolic model. Moreover, the symbolic model\nserves as a discrete \"memory\", which is necessary for satisfying temporal logic\nformulas. Second, we train a library of neural networks offline and combine a\nsubset of the trained NNs into a single NN-based planner at runtime when a task\nis revealed. In particular, we develop a novel constrained NN training\nprocedure, named formal NN training, to enforce that each neural network in the\nlibrary represents a \"symbol\" in the symbolic model. As a result, our\nneurosymbolic framework enjoys the scalability and flexibility benefits of\nmachine learning and inherits the provable guarantees from control-theoretic\nand formal-methods techniques. We demonstrate the effectiveness of our\nframework in both simulations and on an actual robotic vehicle, and show that\nour framework can generalize to unknown tasks where state-of-the-art\nmeta-reinforcement learning techniques fail.",
    "descriptor": "",
    "authors": [
      "Xiaowu Sun",
      "Yasser Shoukry"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05180"
  },
  {
    "id": "arXiv:2210.05182",
    "title": "Edge-Cloud Cooperation for DNN Inference via Reinforcement Learning and  Supervised Learning",
    "abstract": "Deep Neural Networks (DNNs) have been widely applied in Internet of Things\n(IoT) systems for various tasks such as image classification and object\ndetection. However, heavyweight DNN models can hardly be deployed on edge\ndevices due to limited computational resources. In this paper, an edge-cloud\ncooperation framework is proposed to improve inference accuracy while\nmaintaining low inference latency. To this end, we deploy a lightweight model\non the edge and a heavyweight model on the cloud. A reinforcement learning\n(RL)-based DNN compression approach is used to generate the lightweight model\nsuitable for the edge from the heavyweight model. Moreover, a supervised\nlearning (SL)-based offloading strategy is applied to determine whether the\nsample should be processed on the edge or on the cloud. Our method is\nimplemented on real hardware and tested on multiple datasets. The experimental\nresults show that (1) The sizes of the lightweight models obtained by RL-based\nDNN compression are up to 87.6% smaller than those obtained by the baseline\nmethod; (2) SL-based offloading strategy makes correct offloading decisions in\nmost cases; (3) Our method reduces up to 78.8% inference latency and achieves\nhigher accuracy compared with the cloud-only strategy.",
    "descriptor": "\nComments: This paper appears in the Proceedings of 2022 IEEE International Conferences on Internet of Things (iThings)\n",
    "authors": [
      "Tinghao Zhang",
      "Zhijun Li",
      "Yongrui Chen",
      "Kwok-Yan Lam",
      "Jun Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05182"
  },
  {
    "id": "arXiv:2210.05185",
    "title": "Meta-Learning with Self-Improving Momentum Target",
    "abstract": "The idea of using a separately trained target model (or teacher) to improve\nthe performance of the student model has been increasingly popular in various\nmachine learning domains, and meta-learning is no exception; a recent discovery\nshows that utilizing task-wise target models can significantly boost the\ngeneralization performance. However, obtaining a target model for each task can\nbe highly expensive, especially when the number of tasks for meta-learning is\nlarge. To tackle this issue, we propose a simple yet effective method, coined\nSelf-improving Momentum Target (SiMT). SiMT generates the target model by\nadapting from the temporal ensemble of the meta-learner, i.e., the momentum\nnetwork. This momentum network and its task-specific adaptations enjoy a\nfavorable generalization performance, enabling self-improving of the\nmeta-learner through knowledge distillation. Moreover, we found that perturbing\nparameters of the meta-learner, e.g., dropout, further stabilize this\nself-improving process by preventing fast convergence of the distillation loss\nduring meta-training. Our experimental results demonstrate that SiMT brings a\nsignificant performance gain when combined with a wide range of meta-learning\nmethods under various applications, including few-shot regression, few-shot\nclassification, and meta-reinforcement learning. Code is available at\nhttps://github.com/jihoontack/SiMT.",
    "descriptor": "\nComments: Published as a conference proceeding for NeurIPS 2022\n",
    "authors": [
      "Jihoon Tack",
      "Jongjin Park",
      "Hankook Lee",
      "Jaeho Lee",
      "Jinwoo Shin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05185"
  },
  {
    "id": "arXiv:2210.05187",
    "title": "Broad-persistent Advice for Interactive Reinforcement Learning Scenarios",
    "abstract": "The use of interactive advice in reinforcement learning scenarios allows for\nspeeding up the learning process for autonomous agents. Current interactive\nreinforcement learning research has been limited to real-time interactions that\noffer relevant user advice to the current state only. Moreover, the information\nprovided by each interaction is not retained and instead discarded by the agent\nafter a single use. In this paper, we present a method for retaining and\nreusing provided knowledge, allowing trainers to give general advice relevant\nto more than just the current state. Results obtained show that the use of\nbroad-persistent advice substantially improves the performance of the agent\nwhile reducing the number of interactions required for the trainer.",
    "descriptor": "\nComments: Extended abstract accepted at the 2nd RL-CONFORM Workshop at IEEE/RSJ IROS'22 Conference. 5 pages, 7 figures. arXiv admin note: substantial text overlap with arXiv:2102.02441, arXiv:2110.08003\n",
    "authors": [
      "Francisco Cruz",
      "Adam Bignold",
      "Hung Son Nguyen",
      "Richard Dazeley",
      "Peter Vamplew"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05187"
  },
  {
    "id": "arXiv:2210.05188",
    "title": "Legal Element-oriented Modeling with Multi-view Contrastive Learning for  Legal Case Retrieval",
    "abstract": "Legal case retrieval, which aims to retrieve relevant cases given a query\ncase, plays an essential role in the legal system. While recent research\nefforts improve the performance of traditional ad-hoc retrieval models, legal\ncase retrieval is still challenging since queries are legal cases, which\ncontain hundreds of tokens. Legal cases are much longer and more complicated\nthan keywords queries. Apart from that, the definition of legal relevance is\nbeyond the general definition. In addition to general topical relevance, the\nrelevant cases also involve similar situations and legal elements, which can\nsupport the judgment of the current case. In this paper, we propose an\ninteraction-focused network for legal case retrieval with a multi-view\ncontrastive learning objective. The contrastive learning views, including\ncase-view and element-view, aim to overcome the above challenges. The case-view\ncontrastive learning minimizes the hidden space distance between relevant legal\ncase representations produced by a pre-trained language model (PLM) encoder.\nThe element-view builds positive and negative instances by changing legal\nelements of cases to help the network better compute legal relevance. To\nachieve this, we employ a legal element knowledge-aware indicator to detect\nlegal elements of cases. We conduct extensive experiments on the benchmark of\nrelevant case retrieval. Evaluation results indicate our proposed method\nobtains significant improvement over the existing methods.",
    "descriptor": "\nComments: accepted by the 2022 IEEE International Joint Conference on Neural Networks (IJCNN 2022)\n",
    "authors": [
      "Zhaowei Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05188"
  },
  {
    "id": "arXiv:2210.05189",
    "title": "Neural Networks are Decision Trees",
    "abstract": "In this manuscript, we show that any neural network having piece-wise linear\nactivation functions can be represented as a decision tree. The representation\nis equivalence and not an approximation, thus keeping the accuracy of the\nneural network exactly as is. This equivalence shows that neural networks are\nindeed interpretable by design and makes the \\textit{black-box} understanding\nobsolete. We share equivalent trees of some neural networks and show that\nbesides providing interpretability, tree representation can also achieve some\ncomputational advantages. The analysis holds both for fully connected and\nconvolutional networks, which may or may not also include skip connections\nand/or normalizations.",
    "descriptor": "",
    "authors": [
      "Caglar Aytekin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05189"
  },
  {
    "id": "arXiv:2210.05192",
    "title": "A Theory of Factors Affecting Continuous Experimentation (FACE)",
    "abstract": "Continuous experimentation (CE) is used by many companies with\ninternet-facing products to improve their software based on user data. Some\ncompanies deliberately adopt an experiment-driven approach to software\ndevelopment while some companies use CE in a more ad-hoc fashion. The goal of\nthe study is to identify factors that explain the variations in the utility and\nefficacy of CE between different companies. We conducted a multi-case study of\n12 companies involved with CE and performed 27 interviewees with practitioners\nat these companies. Based on that empirical data, we then built a theory of\nfactors at play in CE. We introduce a theory of Factors Affecting Continuous\nExperimentation (FACE). The theory includes three factors, namely 1) processes\nand infrastructure for CE, 2) the user problem complexity of the product\noffering, and 3) incentive structures for CE. It explains how these factors\naffect the effectiveness of CE and its ability to achieve problem-solution and\nproduct-market fit. Our theory can be used by practitioners to assess an\norganisation's potential for adopting CE, as well as, identifying factors which\npose challenges in gaining value from CE practices. Our results also provide a\nstarting point for further research on how contextual factors affect CE and how\nthese may be mitigated.",
    "descriptor": "",
    "authors": [
      "Rasmus Ros",
      "Elizabeth Bjarnason",
      "Per Runeson"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05192"
  },
  {
    "id": "arXiv:2210.05193",
    "title": "Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive  Machine Translation",
    "abstract": "Non-autoregressive models achieve significant decoding speedup in neural\nmachine translation but lack the ability to capture sequential dependency.\nDirected Acyclic Transformer (DA-Transformer) was recently proposed to model\nsequential dependency with a directed acyclic graph. Consequently, it has to\napply a sequential decision process at inference time, which harms the global\ntranslation accuracy. In this paper, we present a Viterbi decoding framework\nfor DA-Transformer, which guarantees to find the joint optimal solution for the\ntranslation and decoding path under any length constraint. Experimental results\ndemonstrate that our approach consistently improves the performance of\nDA-Transformer while maintaining a similar decoding speedup.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Chenze Shao",
      "Zhengrui Ma",
      "Yang Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05193"
  },
  {
    "id": "arXiv:2210.05194",
    "title": "New infinite families of near MDS codes holding $t$-designs and optimal  locally recoverable codes",
    "abstract": "Linear codes with parameters $[n,k,n-k]$ are called almost maximum distance\nseparable (AMDS for short) codes. AMDS codes whose duals are also AMDS are said\nto be near maximum distance separable (NMDS for short). In 1949, Golay\ndiscovered the first near MDS code holding 4-designs, i.e. the $[11,6,5]$\nternary Golay code. In the past 70 years after this discovery, only sporadic\nNMDS codes holding $t$-designs were found. In 2020, Ding and Tang made a\nbreakthrough in constructing the first two infinite families of NMDS codes\nholding $2$-designs or $3$-designs. Up to now, there are only a few known\ninfinite families of NMDS codes which hold $t$-designs for $t=2,3,4$ in the\nliterature. The objective of this paper is to construct new infinite families\nof NMDS codes holding $t$-designs. To this end, some special matrices over\nfinite fields are used as the generator matrices of the NMDS codes. We then\ndetermine the weight enumerators of the NMDS codes and prove that the NMDS\ncodes hold $2$-deigns or $3$-designs. Compared with known $t$-designs from NMDS\ncodes, ours have different parameters. Besides, several infinite families of\noptimal locally recoverable codes are also derived via the NMDS codes.",
    "descriptor": "\nComments: 42 pages\n",
    "authors": [
      "Ziling Heng",
      "Xinran Wang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05194"
  },
  {
    "id": "arXiv:2210.05195",
    "title": "Real-time Trajectory Optimization and Control for Ball Bumping with  Quadruped Robots",
    "abstract": "This paper studies real-time motion planning and control for ball bumping\nmotion with quadruped robots. To enable the quadruped to bump the flying ball\nwith different initializations, we develop a nonlinear trajectory\noptimization-based planning scheme that jointly identifies the take-off time\nand state to achieve accurate ball hitting during the flight phase. Such a\nplanning scheme employs a two-dimensional single rigid body model that achieves\na satisfactory balance between accuracy and efficiency for the highly\ntime-sensitive task. To precisely execute the planned motion, the tracking\ncontroller needs to incorporate the strict time-state constraint imposed on the\ntake-off and ball-hitting events. To this end, we develop an improved model\npredictive controller that respects the critical time-state constraints. The\nproposed planning and control framework is validated with a real Aliengo robot.\nExperiments show that the problem planning approach can be computed in\napproximately 60ms on average, enabling successful accomplishment of the ball\nbumping motion with various initializations in real time.",
    "descriptor": "",
    "authors": [
      "Qiayuan Liao",
      "Zhefeng Cao",
      "Hua Chen",
      "Wei Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05195"
  },
  {
    "id": "arXiv:2210.05196",
    "title": "DIGAT: Modeling News Recommendation with Dual-Graph Interaction",
    "abstract": "News recommendation (NR) is essential for online news services. Existing NR\nmethods typically adopt a news-user representation learning framework, facing\ntwo potential limitations. First, in news encoder, single candidate news\nencoding suffers from an insufficient semantic information problem. Second,\nexisting graph-based NR methods are promising but lack effective news-user\nfeature interaction, rendering the graph-based recommendation suboptimal. To\novercome these limitations, we propose dual-interactive graph attention\nnetworks (DIGAT) consisting of news- and user-graph channels. In the news-graph\nchannel, we enrich the semantics of single candidate news by incorporating the\nsemantically relevant news information with a semantic-augmented graph (SAG).\nIn the user-graph channel, multi-level user interests are represented with a\nnews-topic graph. Most notably, we design a dual-graph interaction process to\nperform effective feature interaction between the news and user graphs, which\nfacilitates accurate news-user representation matching. Experiment results on\nthe benchmark dataset MIND show that DIGAT outperforms existing news\nrecommendation methods. Further ablation studies and analyses validate the\neffectiveness of (1) semantic-augmented news graph modeling and (2) dual-graph\ninteraction.",
    "descriptor": "\nComments: Findings of EMNLP 2022. This paper was first submitted to ARR 2021 November (this https URL)\n",
    "authors": [
      "Zhiming Mao",
      "Jian Li",
      "Hongru Wang",
      "Xingshan Zeng",
      "Kam-Fai Wong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05196"
  },
  {
    "id": "arXiv:2210.05197",
    "title": "Mixed-modality Representation Learning and Pre-training for Joint  Table-and-Text Retrieval in OpenQA",
    "abstract": "Retrieving evidences from tabular and textual resources is essential for\nopen-domain question answering (OpenQA), which provides more comprehensive\ninformation. However, training an effective dense table-text retriever is\ndifficult due to the challenges of table-text discrepancy and data sparsity\nproblem. To address the above challenges, we introduce an optimized OpenQA\nTable-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences.\nFirstly, we propose to enhance mixed-modality representation learning via two\nmechanisms: modality-enhanced representation and mixed-modality negative\nsampling strategy. Secondly, to alleviate data sparsity problem and enhance the\ngeneral retrieval ability, we conduct retrieval-centric mixed-modality\nsynthetic pre-training. Experimental results demonstrate that OTTeR\nsubstantially improves the performance of table-and-text retrieval on the\nOTT-QA dataset. Comprehensive analyses examine the effectiveness of all the\nproposed mechanisms. Besides, equipped with OTTeR, our OpenQA system achieves\nthe state-of-the-art result on the downstream QA task, with 10.1\\% absolute\nimprovement in terms of the exact match over the previous best system.\n\\footnote{All the code and data are available at\n\\url{https://github.com/Jun-jie-Huang/OTTeR}.}",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022\n",
    "authors": [
      "Junjie Huang",
      "Wanjun Zhong",
      "Qian Liu",
      "Ming Gong",
      "Daxin Jiang",
      "Nan Duan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05197"
  },
  {
    "id": "arXiv:2210.05200",
    "title": "CTC Alignments Improve Autoregressive Translation",
    "abstract": "Connectionist Temporal Classification (CTC) is a widely used approach for\nautomatic speech recognition (ASR) that performs conditionally independent\nmonotonic alignment. However for translation, CTC exhibits clear limitations\ndue to the contextual and non-monotonic nature of the task and thus lags behind\nattentional decoder approaches in terms of translation quality. In this work,\nwe argue that CTC does in fact make sense for translation if applied in a joint\nCTC/attention framework wherein CTC's core properties can counteract several\nkey weaknesses of pure-attention models during training and decoding. To\nvalidate this conjecture, we modify the Hybrid CTC/Attention model originally\nproposed for ASR to support text-to-text translation (MT) and speech-to-text\ntranslation (ST). Our proposed joint CTC/attention models outperform\npure-attention baselines across six benchmark translation tasks.",
    "descriptor": "",
    "authors": [
      "Brian Yan",
      "Siddharth Dalmia",
      "Yosuke Higuchi",
      "Graham Neubig",
      "Florian Metze",
      "Alan W Black",
      "Shinji Watanabe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05200"
  },
  {
    "id": "arXiv:2210.05204",
    "title": "A review of cuspidal serial and parallel manipulators",
    "abstract": "Cuspidal robots can move from one inverse or direct kinematic solution to\nanother without ever passing through a singularity. These robots have remained\nunknown because almost all industrial robots do not have this feature. However,\nin fact, industrial robots are the exceptions. Some robots appeared recently in\nthe industrial market can be shown to be cuspidal but, surprisingly, almost\nnobody knows it and robot users meet difficulties in planning trajectories with\nthese robots. This paper proposes a review on the fundamental and application\naspects of cuspidal robots. It addresses the important issues raised by these\nrobots for the design and planning of trajectories. The identification of all\ncuspidal robots is still an open issue. This paper recalls in details the case\nof serial robots with three joints but it also addresses robots with more\ncomplex architectures such as 6-revolute-jointed robot and parallel robots. We\nhope that this paper will help disseminate more widely knowledge on cuspidal\nrobots.",
    "descriptor": "",
    "authors": [
      "Philippe Wenger",
      "Damien Chablat"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05204"
  },
  {
    "id": "arXiv:2210.05206",
    "title": "Kernelized multi-graph matching",
    "abstract": "Multigraph matching is a recent variant of the graph matching problem. In\nthis framework, the optimization procedure considers several graphs and\nenforces the consistency of the matches along the graphs. This constraint can\nbe formalized as a cycle consistency across the pairwise permutation matrices,\nwhich implies the definition of a universe of\nvertex~\\citep{pachauri2013solving}. The label of each vertex is encoded by a\nsparse vector and the dimension of this space corresponds to the rank of the\nbulk permutation matrix, the matrix built from the aggregation of all the\npairwise permutation matrices. The matching problem can then be formulated as a\nnon-convex quadratic optimization problem (QAP) under constraints imposed on\nthe rank and the permutations. In this paper, we introduce a novel kernelized\nmultigraph matching technique that handles vectors of attributes on both the\nvertices and edges of the graphs, while maintaining a low memory usage. We\nsolve the QAP problem using a projected power optimization approach and propose\nseveral projectors leading to improved stability of the results. We provide\nseveral experiments showing that our method is competitive against other\nunsupervised methods.",
    "descriptor": "",
    "authors": [
      "Fran\u00e7ois-Xavier Dup\u00e9",
      "Rohit Yadav",
      "Guillaume Auzias",
      "S. Takerkart"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05206"
  },
  {
    "id": "arXiv:2210.05208",
    "title": "How Well Do Multi-hop Reading Comprehension Models Understand Date  Information?",
    "abstract": "Several multi-hop reading comprehension datasets have been proposed to\nresolve the issue of reasoning shortcuts by which questions can be answered\nwithout performing multi-hop reasoning. However, the ability of multi-hop\nmodels to perform step-by-step reasoning when finding an answer to a comparison\nquestion remains unclear. It is also unclear how questions about the internal\nreasoning process are useful for training and evaluating question-answering\n(QA) systems. To evaluate the model precisely in a hierarchical manner, we\nfirst propose a dataset, \\textit{HieraDate}, with three probing tasks in\naddition to the main question: extraction, reasoning, and robustness. Our\ndataset is created by enhancing two previous multi-hop datasets, HotpotQA and\n2WikiMultiHopQA, focusing on multi-hop questions on date information that\ninvolve both comparison and numerical reasoning. We then evaluate the ability\nof existing models to understand date information. Our experimental results\nreveal that the multi-hop models do not have the ability to subtract two dates\neven when they perform well in date comparison and number subtraction tasks.\nOther results reveal that our probing questions can help to improve the\nperformance of the models (e.g., by +10.3 F1) on the main QA task and our\ndataset can be used for data augmentation to improve the robustness of the\nmodels.",
    "descriptor": "\nComments: 10 pages, 2 figures, and 8 tables; Accepted to AACL-IJCNLP 2022\n",
    "authors": [
      "Xanh Ho",
      "Saku Sugawara",
      "Akiko Aizawa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05208"
  },
  {
    "id": "arXiv:2210.05210",
    "title": "Robust Human Matting via Semantic Guidance",
    "abstract": "Automatic human matting is highly desired for many real applications. We\ninvestigate recent human matting methods and show that common bad cases happen\nwhen semantic human segmentation fails. This indicates that semantic\nunderstanding is crucial for robust human matting. From this, we develop a fast\nyet accurate human matting framework, named Semantic Guided Human Matting\n(SGHM). It builds on a semantic human segmentation network and introduces a\nlight-weight matting module with only marginal computational cost. Unlike\nprevious works, our framework is data efficient, which requires a small amount\nof matting ground-truth to learn to estimate high quality object mattes. Our\nexperiments show that trained with merely 200 matting images, our method can\ngeneralize well to real-world datasets, and outperform recent methods on\nmultiple benchmarks, while remaining efficient. Considering the unbearable\nlabeling cost of matting data and widely available segmentation data, our\nmethod becomes a practical and effective solution for the task of human\nmatting. Source code is available at\nhttps://github.com/cxgincsu/SemanticGuidedHumanMatting.",
    "descriptor": "\nComments: ACCV 2022\n",
    "authors": [
      "Xiangguang Chen",
      "Ye Zhu",
      "Yu Li",
      "Bingtao Fu",
      "Lei Sun",
      "Ying Shan",
      "Shan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05210"
  },
  {
    "id": "arXiv:2210.05211",
    "title": "A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models",
    "abstract": "Despite the remarkable success of pre-trained language models (PLMs), they\nstill face two challenges: First, large-scale PLMs are inefficient in terms of\nmemory footprint and computation. Second, on the downstream tasks, PLMs tend to\nrely on the dataset bias and struggle to generalize to out-of-distribution\n(OOD) data. In response to the efficiency problem, recent studies show that\ndense PLMs can be replaced with sparse subnetworks without hurting the\nperformance. Such subnetworks can be found in three scenarios: 1) the\nfine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even\ninside 3) PLMs without any parameter fine-tuning. However, these results are\nonly obtained in the in-distribution (ID) setting. In this paper, we extend the\nstudy on PLMs subnetworks to the OOD setting, investigating whether sparsity\nand robustness to dataset bias can be achieved simultaneously. To this end, we\nconduct extensive experiments with the pre-trained BERT model on three natural\nlanguage understanding (NLU) tasks. Our results demonstrate that \\textbf{sparse\nand robust subnetworks (SRNets) can consistently be found in BERT}, across the\naforementioned three scenarios, using different training and compression\nmethods. Furthermore, we explore the upper bound of SRNets using the OOD\ninformation and show that \\textbf{there exist sparse and almost unbiased BERT\nsubnetworks}. Finally, we present 1) an analytical study that provides insights\non how to promote the efficiency of SRNets searching process and 2) a solution\nto improve subnetworks' performance at high sparsity. The code is available at\nhttps://github.com/llyx97/sparse-and-robust-PLM.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Yuanxin Liu",
      "Fandong Meng",
      "Zheng Lin",
      "Jiangnan Li",
      "Peng Fu",
      "Yanan Cao",
      "Weiping Wang",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05211"
  },
  {
    "id": "arXiv:2210.05212",
    "title": "On Scrambling Phenomena for Randomly Initialized Recurrent Networks",
    "abstract": "Recurrent Neural Networks (RNNs) frequently exhibit complicated dynamics, and\ntheir sensitivity to the initialization process often renders them notoriously\nhard to train. Recent works have shed light on such phenomena analyzing when\nexploding or vanishing gradients may occur, either of which is detrimental for\ntraining dynamics. In this paper, we point to a formal connection between RNNs\nand chaotic dynamical systems and prove a qualitatively stronger phenomenon\nabout RNNs than what exploding gradients seem to suggest. Our main result\nproves that under standard initialization (e.g., He, Xavier etc.), RNNs will\nexhibit \\textit{Li-Yorke chaos} with \\textit{constant} probability\n\\textit{independent} of the network's width. This explains the experimentally\nobserved phenomenon of \\textit{scrambling}, under which trajectories of nearby\npoints may appear to be arbitrarily close during some timesteps, yet will be\nfar away in future timesteps. In stark contrast to their feedforward\ncounterparts, we show that chaotic behavior in RNNs is preserved under small\nperturbations and that their expressive power remains exponential in the number\nof feedback iterations. Our technical arguments rely on viewing RNNs as random\nwalks under non-linear activations, and studying the existence of certain types\nof higher-order fixed points called \\textit{periodic points} that lead to phase\ntransitions from order to chaos.",
    "descriptor": "\nComments: Accepted for publication, Neurips 2022\n",
    "authors": [
      "Vaggos Chatziafratis",
      "Ioannis Panageas",
      "Clayton Sanford",
      "Stelios Andrew Stavroulakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05212"
  },
  {
    "id": "arXiv:2210.05217",
    "title": "Abstract interpretation of Michelson smart-contracts",
    "abstract": "Static analysis of smart-contracts is becoming more widespread on blockchain\nplatforms. Analyzers rely on techniques like symbolic execution or model\nchecking, but few of them can provide strong soundness properties and guarantee\nthe analysis termination at the same time. As smart-contracts often manipulate\neconomic assets, proving numerical properties beyond the absence of runtime\nerrors is also desirable. Smart-contract execution models differ considerably\nfrom mainstream programming languages and vary from one blockchain to another,\nmaking state-of-the-art analyses hard to adapt. For instance, smart-contract\ncalls may modify a persistent storage impacting subsequent calls. This makes it\ndifficult for tools to infer invariants required to formally ensure the absence\nof exploitable vulnerabilities. The Michelson smart-contract language, used in\nthe Tezos blockchain, is strongly typed, stack-based, and has a strict\nexecution model leaving few opportunities for implicit runtime errors. We\npresent a work in progress static analyzer for Michelson based on Abstract\nInterpretation and implemented within MOPSA, a modular static analyzer. Our\ntool supports the Michelson semantic features, including inner calls to\nexternal contracts. It can prove the absence of runtime errors and infer\ninvariants on the persistent storage over an unbounded number of calls. It is\nalso being extended to prove high-level numerical and security properties. CCS\nConcepts: $\\bullet$ Security and privacy $\\rightarrow$ Logic and verification;\n$\\bullet$ Software and its engineering $\\rightarrow$ Automated static analysis.",
    "descriptor": "",
    "authors": [
      "Guillaume Bau",
      "Antoine Min\u00e9",
      "Vincent Botbol",
      "Mehdi Bouaziz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Programming Languages (cs.PL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05217"
  },
  {
    "id": "arXiv:2210.05220",
    "title": "Solving forward and inverse problems in a non-linear 3D PDE via an  asymptotic expansion based approach",
    "abstract": "This paper is concerned with the usage of the asymptotic expansion method for\nefficiently solving forward and inverse problems in a non-linear singularly\nperturbed time-dependent reaction-diffusion-advection equation. By using the\nasymptotic expansion with the local coordinates in the transition layer region,\nwe prove the existence and uniqueness of a smooth solution with a sharp\ntransition layer for a three-dimensional PDE. Moreover, with the help of\nasymptotic expansion, a simplified model is derived for the corresponding\ninverse source problem, which is close to the original inverse problem over the\nentire region except for a narrow transition layer. We show that such\nsimplification does not reduce the accuracy of the inversion result when the\nmeasurement data contains noise. Based on this simpler inversion model, an\nasymptotic expansion regularization algorithm is proposed for solving the\ninverse source problem in the three-dimensional case. Various numerical\nexamples for both forward and inverse problems are given to show the efficiency\nof the proposed numerical approach.",
    "descriptor": "",
    "authors": [
      "Dmitrii Chaikovskii",
      "Ye Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05220"
  },
  {
    "id": "arXiv:2210.05221",
    "title": "CHAE: Fine-Grained Controllable Story Generation with Characters,  Actions and Emotions",
    "abstract": "Story generation has emerged as an interesting yet challenging NLP task in\nrecent years. Some existing studies aim at generating fluent and coherent\nstories from keywords and outlines; while others attempt to control the global\nfeatures of the story, such as emotion, style and topic. However, these works\nfocus on coarse-grained control on the story, neglecting control on the details\nof the story, which is also crucial for the task. To fill the gap, this paper\nproposes a model for fine-grained control on the story, which allows the\ngeneration of customized stories with characters, corresponding actions and\nemotions arbitrarily assigned. Extensive experimental results on both automatic\nand human manual evaluations show the superiority of our method. It has strong\ncontrollability to generate stories according to the fine-grained personalized\nguidance, unveiling the effectiveness of our methodology. Our code is available\nat https://github.com/victorup/CHAE.",
    "descriptor": "\nComments: Accepted by COLING 2022\n",
    "authors": [
      "Xinpeng Wang",
      "Han Jiang",
      "Zhihua Wei",
      "Shanlin Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05221"
  },
  {
    "id": "arXiv:2210.05222",
    "title": "Regret Analysis of the Stochastic Direct Search Method for Blind  Resource Allocation",
    "abstract": "Motivated by programmatic advertising optimization, we consider the task of\nsequentially allocating budget across a set of resources. At every time step, a\nfeasible allocation is chosen and only a corresponding random return is\nobserved. The goal is to maximize the cumulative expected sum of returns. This\nis a realistic model for budget allocation across subdivisions of marketing\ncampaigns, when the objective is to maximize the number of conversions. We\nstudy direct search (aka pattern search) methods for linearly constrained and\nderivative-free optimization in the presence of noise. Those algorithms are\neasy to implement and particularly suited to constrained optimization. They\nhave not yet been analyzed from the perspective of cumulative regret. We\nprovide a regret upper-bound of the order of T 2/3 in the general case. Our\nmathematical analysis also establishes, as a by-product, time-independent\nregret bounds in the deterministic, unconstrained case. We also propose an\nimproved version of the method relying on sequential tests to accelerate the\nidentification of descent directions.",
    "descriptor": "",
    "authors": [
      "Juliette Achddou",
      "Olivier Cappe",
      "Aur\u00e9lien Garivier"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.05222"
  },
  {
    "id": "arXiv:2210.05225",
    "title": "A Formalisation of a Fast Fourier Transform",
    "abstract": "This notes explains how a standard algorithm that constructs the discrete\nFourier transform has been formalised and proved correct in the Coq proof\nassistant using the SSReflect extension.",
    "descriptor": "",
    "authors": [
      "Laurent Th\u00e9ry"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05225"
  },
  {
    "id": "arXiv:2210.05226",
    "title": "Detecting Hidden Attackers in Photovoltaic Systems Using Machine  Learning",
    "abstract": "In modern smart grids, the proliferation of communication-enabled distributed\nenergy resource (DER) systems has increased the surface of possible\ncyber-physical attacks. Attacks originating from the distributed edge devices\nof DER system, such as photovoltaic (PV) system, is often difficult to detect.\nAn attacker may change the control configurations or various setpoints of the\nPV inverters to destabilize the power grid, damage devices, or for the purpose\nof economic gain. A more powerful attacker may even manipulate the PV system\nmetering data transmitted for remote monitoring, so that (s)he can remain\nhidden. In this paper, we consider a case where PV systems operating in\ndifferent control modes can be simultaneously attacked and the attacker has the\nability to manipulate individual PV bus measurements to avoid detection. We\nshow that even in such a scenario, with just the aggregated measurements (that\nthe attacker cannot manipulate), machine learning (ML) techniques are able to\ndetect the attack in a fast and accurate manner. We use a standard radial\ndistribution network, together with real smart home electricity consumption\ndata and solar power data in our experimental setup. We test the performance of\nseveral ML algorithms to detect attacks on the PV system. Our detailed\nevaluations show that the proposed intrusion detection system (IDS) is highly\neffective and efficient in detecting attacks on PV inverter control modes.",
    "descriptor": "",
    "authors": [
      "Suman Sourav",
      "Partha P. Biswas",
      "Binbin Chen",
      "Daisuke Mashima"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05226"
  },
  {
    "id": "arXiv:2210.05229",
    "title": "Deep Learning-Aided Delay-Tolerant Zero-Forcing Precoding in Cell-Free  Massive MIMO",
    "abstract": "In the context of cell-free massive multi-input multi-output (CFmMIMO),\nzero-forcing precoding (ZFP) is superior in terms of spectral efficiency.\nHowever, it suffers from channel aging owing to fronthaul and processing\ndelays. In this paper, we propose a robust scheme coined delay-tolerant\nzero-forcing precoding (DT-ZFP), which exploits deep learning-aided channel\nprediction to alleviate the effect of outdated channel state information (CSI).\nA predictor consisting of a bank of user-specific predictive modules is\nspecifically designed for such a multi-user scenario. Leveraging the degree of\nfreedom brought by the prediction horizon, the delivery of CSI and precoded\ndata through a fronthaul network and the transmission of user data and pilots\nover an air interface can be parallelized. Therefore, DT-ZFP not only\neffectively combats channel aging but also avoids the inefficient Stop-and-Wait\nmechanism of the canonical ZFP in CFmMIMO.",
    "descriptor": "\nComments: 2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall), London and Beijing, 26-29 September 2022\n",
    "authors": [
      "Wei Jiang",
      "Hans D. Schotten"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05229"
  },
  {
    "id": "arXiv:2210.05230",
    "title": "From Mimicking to Integrating: Knowledge Integration for Pre-Trained  Language Models",
    "abstract": "Investigating better ways to reuse the released pre-trained language models\n(PLMs) can significantly reduce the computational cost and the potential\nenvironmental side-effects. This paper explores a novel PLM reuse paradigm,\nKnowledge Integration (KI). Without human annotations available, KI aims to\nmerge the knowledge from different teacher-PLMs, each of which specializes in a\ndifferent classification problem, into a versatile student model. To achieve\nthis, we first derive the correlation between virtual golden supervision and\nteacher predictions. We then design a Model Uncertainty--aware Knowledge\nIntegration (MUKI) framework to recover the golden supervision for the student.\nSpecifically, MUKI adopts Monte-Carlo Dropout to estimate model uncertainty for\nthe supervision integration. An instance-wise re-weighting mechanism based on\nthe margin of uncertainty scores is further incorporated, to deal with the\npotential conflicting supervision from teachers. Experimental results\ndemonstrate that MUKI achieves substantial improvements over baselines on\nbenchmark datasets. Further analysis shows that MUKI can generalize well for\nmerging teacher models with heterogeneous architectures, and even teachers\nmajor in cross-lingual datasets.",
    "descriptor": "\nComments: EMNLP 2022 (Findings), an improved version of arXiv:2112.07327. Code will be available at this https URL\n",
    "authors": [
      "Lei Li",
      "Yankai Lin",
      "Xuancheng Ren",
      "Guangxiang Zhao",
      "Peng Li",
      "Jie Zhou",
      "Xu Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05230"
  },
  {
    "id": "arXiv:2210.05232",
    "title": "DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation",
    "abstract": "Establishment of point correspondence between camera and object coordinate\nsystems is a promising way to solve 6D object poses. However, surrogate\nobjectives of correspondence learning in 3D space are a step away from the true\nones of object pose estimation, making the learning suboptimal for the end\ntask. In this paper, we address this shortcoming by introducing a new method of\nDeep Correspondence Learning Network for direct 6D object pose estimation,\nshortened as DCL-Net. Specifically, DCL-Net employs dual newly proposed Feature\nDisengagement and Alignment (FDA) modules to establish, in the feature space,\npartial-to-partial correspondence and complete-to-complete one for partial\nobject observation and its complete CAD model, respectively, which result in\naggregated pose and match feature pairs from two coordinate systems; these two\nFDA modules thus bring complementary advantages. The match feature pairs are\nused to learn confidence scores for measuring the qualities of deep\ncorrespondence, while the pose feature pairs are weighted by confidence scores\nfor direct object pose regression. A confidence-based pose refinement network\nis also proposed to further improve pose precision in an iterative manner.\nExtensive experiments show that DCL-Net outperforms existing methods on three\nbenchmarking datasets, including YCB-Video, LineMOD, and Oclussion-LineMOD;\nablation studies also confirm the efficacy of our novel designs.",
    "descriptor": "",
    "authors": [
      "Hongyang Li",
      "Jiehong Lin",
      "Kui Jia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05232"
  },
  {
    "id": "arXiv:2210.05233",
    "title": "Estimation of Doubly-Dispersive Channels in Linearly Precoded  Multicarrier Systems Using Smoothness Regularization",
    "abstract": "In this paper, we propose a novel channel estimation scheme for pulse-shaped\nmulticarrier systems using smoothness regularization for ultra-reliable\nlow-latency communication (URLLC). It can be applied to any multicarrier system\nwith or without linear precoding to estimate challenging doubly-dispersive\nchannels. A recently proposed modulation scheme using orthogonal precoding is\northogonal time-frequency and space modulation (OTFS). In OTFS, pilot and data\nsymbols are placed in delay-Doppler (DD) domain and are jointly precoded to the\ntime-frequency (TF) domain. On the one hand, such orthogonal precoding\nincreases the achievable channel estimation accuracy and enables high TF\ndiversity at the receiver. On the other hand, it introduces leakage effects\nwhich requires extensive leakage suppression when the piloting is jointly\nprecoded with the data. To avoid this, we propose to precode the data symbols\nonly, place pilot symbols without precoding into the TF domain, and estimate\nthe channel coefficients by interpolating smooth functions from the pilot\nsamples. Furthermore, we present a piloting scheme enabling a smooth control of\nthe number and position of the pilot symbols. Our numerical results suggest\nthat the proposed scheme provides accurate channel estimation with reduced\nsignaling overhead compared to standard estimators using Wiener filtering in\nthe discrete DD domain.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Andreas Pfadler",
      "Tom Szollmann",
      "Peter Jung",
      "Slawomir Stanczak"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05233"
  },
  {
    "id": "arXiv:2210.05234",
    "title": "It Takes Two: Masked Appearance-Motion Modeling for Self-supervised  Video Transformer Pre-training",
    "abstract": "Self-supervised video transformer pre-training has recently benefited from\nthe mask-and-predict pipeline. They have demonstrated outstanding effectiveness\non downstream video tasks and superior data efficiency on small datasets.\nHowever, temporal relation is not fully exploited by these methods. In this\nwork, we explicitly investigate motion cues in videos as extra prediction\ntarget and propose our Masked Appearance-Motion Modeling (MAM2) framework.\nSpecifically, we design an encoder-regressor-decoder pipeline for this task.\nThe regressor separates feature encoding and pretext tasks completion, such\nthat the feature extraction process is completed adequately by the encoder. In\norder to guide the encoder to fully excavate spatial-temporal features, two\nseparate decoders are used for two pretext tasks of disentangled appearance and\nmotion prediction. We explore various motion prediction targets and figure out\nRGB-difference is simple yet effective. As for appearance prediction, VQGAN\ncodes are leveraged as prediction target. With our pre-training pipeline,\nconvergence can be remarkably speed up, e.g., we only require half of epochs\nthan state-of-the-art VideoMAE (400 v.s. 800) to achieve the competitive\nperformance. Extensive experimental results prove that our method learns\ngeneralized video representations. Notably, our MAM2 with ViT-B achieves 82.3%\non Kinects-400, 71.3% on Something-Something V2, 91.5% on UCF101, and 62.5% on\nHMDB51.",
    "descriptor": "",
    "authors": [
      "Yuxin Song",
      "Min Yang",
      "Wenhao Wu",
      "Dongliang He",
      "Fu Li",
      "Jingdong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05234"
  },
  {
    "id": "arXiv:2210.05236",
    "title": "Planning Assembly Sequence with Graph Transformer",
    "abstract": "Assembly sequence planning (ASP) is the essential process for modern\nmanufacturing, proven to be NP-complete thus its effective and efficient\nsolution has been a challenge for researchers in the field. In this paper, we\npresent a graph-transformer based framework for the ASP problem which is\ntrained and demonstrated on a self-collected ASP database. The ASP database\ncontains a self-collected set of LEGO models. The LEGO model is abstracted to a\nheterogeneous graph structure after a thorough analysis of the original\nstructure and feature extraction. The ground truth assembly sequence is first\ngenerated by brute-force search and then adjusted manually to in line with\nhuman rational habits. Based on this self-collected ASP dataset, we propose a\nheterogeneous graph-transformer framework to learn the latent rules for\nassembly planning. We evaluated the proposed framework in a series of\nexperiment. The results show that the similarity of the predicted and ground\ntruth sequences can reach 0.44, a medium correlation measured by Kendall's\n$\\tau$. Meanwhile, we compared the different effects of node features and edge\nfeatures and generated a feasible and reasonable assembly sequence as a\nbenchmark for further research. Our data set and code is available on\nhttps://github.com/AIR-DISCOVER/ICRA\\_ASP.",
    "descriptor": "\nComments: Submitted to ICRA2023\n",
    "authors": [
      "Lin Ma",
      "Jiangtao Gong",
      "Hao Xu",
      "Hao Chen",
      "Hao Zhao",
      "Wenbing Huang",
      "Guyue Zhou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05236"
  },
  {
    "id": "arXiv:2210.05237",
    "title": "Fair and Efficient Multi-Resource Allocation for Cloud Computing",
    "abstract": "We study the problem of allocating multiple types of resources to agents with\nLeontief preferences. The classic Dominant Resource Fairness (DRF) mechanism\nsatisfies several desired fairness and incentive properties, but is known to\nhave poor performance in terms of social welfare approximation ratio. In this\nwork, we propose a new approximation ratio measure, called \\emph{\\fratio},\nwhich is defined as the worst-case ratio between the optimal social welfare\n(resp. utilization) among all \\emph{fair} allocations and that by the\nmechanism, allowing us to break the lower bound barrier under the classic\napproximation ratio. We then generalize DRF and present several new mechanisms\nwith two and multiple types of resources that satisfy the same set of\nproperties as DRF but with better social welfare and utilization guarantees\nunder the new benchmark. We also demonstrate the effectiveness of these\nmechanisms through experiments on both synthetic and real-world datasets.",
    "descriptor": "\nComments: Accepted to WINE 2022\n",
    "authors": [
      "Xiaohui Bei",
      "Zihao Li",
      "Junjie Luo"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.05237"
  },
  {
    "id": "arXiv:2210.05238",
    "title": "Minimum distances of binary optimal LCD codes of dimension five are  completely determined",
    "abstract": "Let $t \\in \\{2,8,10,12,14,16,18\\}$ and $n=31s+t\\geq 14$, $d_{a}(n,5)$ and\n$d_{l}(n,5)$ be distances of binary $[n,5]$ optimal linear codes and optimal\nlinear complementary dual (LCD) codes, respectively. We show that an\n$[n,5,d_{a}(n,5)]$ optimal linear code is not an LCD code, there is an\n$[n,5,d_{l}(n,5)]=[n,5,d_{a}(n,5)-1]$ optimal LCD code if $t\\neq 16$, and an\noptimal $[n,5,d_{l}(n,5)]$ optimal LCD code has $d_{l}(n,5)=16s+6=d_{a}(n,5)-2$\nfor $t=16$. Combined with known results on optimal LCD code,\n$d_{l}(n,5)$ of all $[n,5]$ LCD codes are completely determined.",
    "descriptor": "\nComments: 15pages,7tables\n",
    "authors": [
      "Yang Liu",
      "Ruihu Li",
      "Qiang Fu",
      "Hao Song"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05238"
  },
  {
    "id": "arXiv:2210.05240",
    "title": "Multi-site Diagnostic Classification Of Schizophrenia Using 3D CNN On  Aggregated Task-based fMRI Data",
    "abstract": "In spite of years of research, the mechanisms that underlie the development\nof schizophrenia, as well as its relapse, symptomatology, and treatment,\ncontinue to be a mystery. The absence of appropriate analytic tools to deal\nwith the variable and complicated nature of schizophrenia may be one of the\nfactors that contribute to the development of this disorder. Deep learning is a\nsubfield of artificial intelligence that was inspired by the nervous system. In\nrecent years, deep learning has made it easier to model and analyse\ncomplicated, high-dimensional, and nonlinear systems. Research on schizophrenia\nis one of the many areas of study that has been revolutionised as a result of\nthe outstanding accuracy that deep learning algorithms have demonstrated in\nclassification and prediction tasks. Deep learning has the potential to become\na powerful tool for understanding the mechanisms that are at the root of\nschizophrenia. In addition, a growing variety of techniques aimed at improving\nmodel interpretability and causal reasoning are contributing to this trend.\nUsing multi-site fMRI data and a variety of deep learning approaches, this\nstudy seeks to identify different types of schizophrenia. Our proposed method\nof temporal aggregation of the 4D fMRI data outperforms existing work. In\naddition, this study aims to shed light on the strength of connections between\nvarious brain areas in schizophrenia individuals.",
    "descriptor": "",
    "authors": [
      "Vigneshwaran Shankaran",
      "Bhaskaran V"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05240"
  },
  {
    "id": "arXiv:2210.05241",
    "title": "STSC-SNN: Spatio-Temporal Synaptic Connection with Temporal Convolution  and Attention for Spiking Neural Networks",
    "abstract": "Spiking Neural Networks (SNNs), as one of the algorithmic models in\nneuromorphic computing, have gained a great deal of research attention owing to\ntemporal information processing capability, low power consumption, and high\nbiological plausibility. The potential to efficiently extract spatio-temporal\nfeatures makes it suitable for processing the event streams. However, existing\nsynaptic structures in SNNs are almost full-connections or spatial 2D\nconvolution, neither of which can extract temporal dependencies adequately. In\nthis work, we take inspiration from biological synapses and propose a\nspatio-temporal synaptic connection SNN (STSC-SNN) model, to enhance the\nspatio-temporal receptive fields of synaptic connections, thereby establishing\ntemporal dependencies across layers. Concretely, we incorporate temporal\nconvolution and attention mechanisms to implement synaptic filtering and gating\nfunctions. We show that endowing synaptic models with temporal dependencies can\nimprove the performance of SNNs on classification tasks. In addition, we\ninvestigate the impact of performance vias varied spatial-temporal receptive\nfields and reevaluate the temporal modules in SNNs. Our approach is tested on\nneuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST,\nCIFAR10-DVS (image classification), and SHD (speech digit recognition). The\nresults show that the proposed model outperforms the state-of-the-art accuracy\non nearly all datasets.",
    "descriptor": "",
    "authors": [
      "Chengting Yu",
      "Zheming Gu",
      "Da Li",
      "Gaoang Wang",
      "Aili Wang",
      "Erping Li"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05241"
  },
  {
    "id": "arXiv:2210.05242",
    "title": "Leveraging the Video-level Semantic Consistency of Event for  Audio-visual Event Localization",
    "abstract": "Audio-visual event localization has attracted much attention in recent years.\nMost existing methods are often limited to independently encoding and\nclassifying each video segment separated from the full video (which can be\nregarded as the segment-level representations of events). However, they ignore\nthe semantic consistency of the event within the same full video (which can be\nconsidered as the video-level representations of events). In contrast to\nexisting methods, we propose a novel video-level semantic consistency guidance\nnetwork for the AVE task. Specifically, we propose an event semantic\nconsistency modeling (ESCM) module to explore the video-level semantic\nconsistency of events. It consists of two components: cross-modal event\nrepresentation extractor (CERE) and intra-modal semantic consistency enhancer\n(ISCE). CERE is proposed to obtain the event semantic representation at the\nvideo level including, audio and visual modules. Furthermore, ISCE takes the\nvideo-level event semantic representation as the prior knowledge to guide the\nmodel to focus on the semantic continuity of the event within each modality.\nMoreover, we propose a new negative pair filter loss to encourage the network\nto filter out the irrelevant segment pairs and a new smooth loss to further\nincrease the gap between different categories of events under the\nweakly-supervised setting. We perform extensive experiments on the public AVE\ndataset and outperform the state-of-the-art methods in both fully and weakly\nsupervised settings, thus verifying the effectiveness of our method.",
    "descriptor": "",
    "authors": [
      "Yuanyuan Jiang",
      "Jianqin Yin",
      "Yonghao Dang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05242"
  },
  {
    "id": "arXiv:2210.05243",
    "title": "Cross-modal Search Method of Technology Video based on Adversarial  Learning and Feature Fusion",
    "abstract": "Technology videos contain rich multi-modal information. In cross-modal\ninformation search, the data features of different modalities cannot be\ncompared directly, so the semantic gap between different modalities is a key\nproblem that needs to be solved. To address the above problems, this paper\nproposes a novel Feature Fusion based Adversarial Cross-modal Retrieval method\n(FFACR) to achieve text-to-video matching, ranking and searching. The proposed\nmethod uses the framework of adversarial learning to construct a video\nmultimodal feature fusion network and a feature mapping network as generator, a\nmodality discrimination network as discriminator. Multi-modal features of\nvideos are obtained by the feature fusion network. The feature mapping network\nprojects multi-modal features into the same semantic space based on semantics\nand similarity. The modality discrimination network is responsible for\ndetermining the original modality of features. Generator and discriminator are\ntrained alternately based on adversarial learning, so that the data obtained by\nthe feature mapping network is semantically consistent with the original data\nand the modal features are eliminated, and finally the similarity is used to\nrank and obtain the search results in the semantic space. Experimental results\ndemonstrate that the proposed method performs better in text-to-video search\nthan other existing methods, and validate the effectiveness of the method on\nthe self-built datasets of technology videos.",
    "descriptor": "",
    "authors": [
      "Xiangbin Liu",
      "Junping Du",
      "Meiyu Liang",
      "Ang Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05243"
  },
  {
    "id": "arXiv:2210.05244",
    "title": "Dataloader Parameter Tuner: An Automated Dataloader Parameter Tuner for  Deep Learning Models",
    "abstract": "Deep learning has recently become one of the most compute/data-intensive\nmethods and is widely used in many research areas and businesses. One of the\ncritical challenges of deep learning is that it has many parameters that can be\nadjusted, and the optimal value may need to be determined for faster operation\nand high accuracy. The focus of this paper is the adjustable parameters of the\ndataloader. The dataloader in a system mainly groups the data appropriately and\nloads it to the main memory for the deep learning model to use. We introduce an\nautomated framework called Dataloader Parameter Tuner (DPT) that determines the\noptimal value for the parameters required for the dataloader. This framework\ndiscovers the optimal values for the number of dataloader's subprocesses (i.e.,\nworker) and prefetch factor through grid search to accelerate the data transfer\nfor machine learning systems.",
    "descriptor": "",
    "authors": [
      "JooYoung Park",
      "DoangJoo Synn",
      "XinYu Piao",
      "Jong-Kook Kim"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.05244"
  },
  {
    "id": "arXiv:2210.05245",
    "title": "PatternRank: Leveraging Pretrained Language Models and Part of Speech  for Unsupervised Keyphrase Extraction",
    "abstract": "Keyphrase extraction is the process of automatically selecting a small set of\nmost relevant phrases from a given text. Supervised keyphrase extraction\napproaches need large amounts of labeled training data and perform poorly\noutside the domain of the training data (Bennani-Smires et al., 2018). In this\npaper, we present PatternRank, which leverages pretrained language models and\npart-of-speech for unsupervised keyphrase extraction from single documents. Our\nexperiments show PatternRank achieves higher precision, recall and F1 -scores\nthan previous state-of-the-art approaches. In addition, we present the\nKeyphraseVectorizers package, which allows easy modification of part-of-speech\npatterns for candidate keyphrase selection, and hence adaptation of our\napproach to any domain.",
    "descriptor": "\nComments: Accepted to 14th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - KDIR\n",
    "authors": [
      "Tim Schopf",
      "Simon Klimek",
      "Florian Matthes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05245"
  },
  {
    "id": "arXiv:2210.05246",
    "title": "Cluster-level pseudo-labelling for source-free cross-domain facial  expression recognition",
    "abstract": "Automatically understanding emotions from visual data is a fundamental task\nfor human behaviour understanding. While models devised for Facial Expression\nRecognition (FER) have demonstrated excellent performances on many datasets,\nthey often suffer from severe performance degradation when trained and tested\non different datasets due to domain shift. In addition, as face images are\nconsidered highly sensitive data, the accessibility to large-scale datasets for\nmodel training is often denied. In this work, we tackle the above-mentioned\nproblems by proposing the first Source-Free Unsupervised Domain Adaptation\n(SFUDA) method for FER. Our method exploits self-supervised pretraining to\nlearn good feature representations from the target data and proposes a novel\nand robust cluster-level pseudo-labelling strategy that accounts for in-cluster\nstatistics. We validate the effectiveness of our method in four adaptation\nsetups, proving that it consistently outperforms existing SFUDA methods when\napplied to FER, and is on par with methods addressing FER in the UDA setting.",
    "descriptor": "\nComments: Accepted at BMVC2022, 13 pages, 4 figures, code is available at this https URL\n",
    "authors": [
      "Alessandro Conti",
      "Paolo Rota",
      "Yiming Wang",
      "Elisa Ricci"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05246"
  },
  {
    "id": "arXiv:2210.05247",
    "title": "Efficient debiasing with contrastive weight pruning",
    "abstract": "Neural networks are often biased to spuriously correlated features that\nprovide misleading statistical evidence that does not generalize. This raises a\nfundamental question: \"Does an optimal unbiased functional subnetwork exist in\na severely biased network? If so, how to extract such subnetwork?\" While few\nstudies have revealed the existence of such optimal subnetworks with the\nguidance of ground-truth unbiased samples, the way to discover the optimal\nsubnetworks with biased training dataset is still unexplored in practice. To\naddress this, here we first present our theoretical insight that alerts\npotential limitations of existing algorithms in exploring unbiased subnetworks\nin the presence of strong spurious correlations. We then further elucidate the\nimportance of bias-conflicting samples on structure learning. Motivated by\nthese observations, we propose a Debiased Contrastive Weight Pruning (DCWP)\nalgorithm, which probes unbiased subnetworks without expensive group\nannotations. Experimental results demonstrate that our approach significantly\noutperforms state-of-the-art debiasing methods despite its considerable\nreduction in the number of parameters.",
    "descriptor": "",
    "authors": [
      "Geon Yeong Park",
      "Sangmin Lee",
      "Sang Wan Lee",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05247"
  },
  {
    "id": "arXiv:2210.05248",
    "title": "Self-supervised debiasing using low rank regularization",
    "abstract": "Spurious correlations can cause strong biases in deep neural networks,\nimpairing generalization ability. While most of existing debiasing methods\nrequire full supervisions on either spurious attributes or target labels,\ntraining a debiased model from a limited amount of both annotations is still an\nopen issue. To overcome such limitations, we first examined an interesting\nphenomenon by the spectral analysis of latent representations: spuriously\ncorrelated, easy-to-learn attributes make neural networks inductively biased\ntowards encoding lower effective rank representations. We also show that a rank\nregularization can amplify this bias in a way that encourages highly correlated\nfeatures. Motivated by these observations, we propose a self-supervised\ndebiasing framework that is potentially compatible with unlabeled samples. We\nfirst pretrain a biased encoder in a self-supervised manner with the rank\nregularization, serving as a semantic bottleneck to enforce the encoder to\nlearn the spuriously correlated attributes. This biased encoder is then used to\ndiscover and upweight bias-conflicting samples in a downstream task, serving as\na boosting to effectively debias the main model. Remarkably, the proposed\ndebiasing framework significantly improves the generalization performance of\nself-supervised learning baselines and, in some cases, even outperforms\nstate-of-the-art supervised debiasing approaches.",
    "descriptor": "",
    "authors": [
      "Geon Yeong Park",
      "Chanyong Jung",
      "Jong Chul Ye",
      "Sang Wan Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05248"
  },
  {
    "id": "arXiv:2210.05250",
    "title": "Digital twins for city simulation: Automatic, efficient, and robust mesh  generation for large-scale city modeling and simulation",
    "abstract": "The concept of creating digital twins, connected digital models of physical\nsystems, is gaining increasing attention for modeling and simulation of whole\ncities. The basis for building a digital twin of a city is the generation of a\n3D city model, often represented as a mesh. Creating and updating such models\nis a tedious process that requires manual work and considerable effort,\nespecially in the modeling of building geometries. In the current paper, we\npresent a novel algorithm and implementation for automatic, efficient, and\nrobust mesh generation for large-scale city modeling and simulation. The\nalgorithm relies on standard, publicly available data, in particular 2D\ncadastral maps (building footprints) and 3D point clouds obtained from aerial\nscanning. The algorithm generates LoD1.2 city models in the form of both\ntriangular surface meshes, suitable for visualisation, and high-quality\ntetrahedral volume meshes, suitable for simulation. Our tests demonstrate good\nperformance and scaling and indicate good avenues for further optimization\nbased on parallelisation. The long-term goal is a generic digital twin of\ncities volume mesh generator that provides (nearly) real-time mesh manipulation\nin LoD2.x.",
    "descriptor": "",
    "authors": [
      "Vasilis Naserentin",
      "Anders Logg"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05250"
  },
  {
    "id": "arXiv:2210.05252",
    "title": "Graph Neural Network Policies and Imitation Learning for Multi-Domain  Task-Oriented Dialogues",
    "abstract": "Task-oriented dialogue systems are designed to achieve specific goals while\nconversing with humans. In practice, they may have to handle simultaneously\nseveral domains and tasks. The dialogue manager must therefore be able to take\ninto account domain changes and plan over different domains/tasks in order to\ndeal with multidomain dialogues. However, learning with reinforcement in such\ncontext becomes difficult because the state-action dimension is larger while\nthe reward signal remains scarce. Our experimental results suggest that\nstructured policies based on graph neural networks combined with different\ndegrees of imitation learning can effectively handle multi-domain dialogues.\nThe reported experiments underline the benefit of structured policies over\nstandard policies.",
    "descriptor": "",
    "authors": [
      "Thibault Cordier",
      "Tanguy Urvoy",
      "Fabrice Lef\u00e8vre",
      "Lina M. Rojas-Barahona"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05252"
  },
  {
    "id": "arXiv:2210.05253",
    "title": "Constrained Deployment Optimization in Integrated Access and Backhaul  Networks",
    "abstract": "Integrated access and backhaul (IAB) is one of the promising techniques for\n5G networks and beyond (6G), in which the same node/hardware is used to provide\nboth backhaul and cellular services in a multi-hop fashion. Due to the\nsensitivity of the backhaul links with high rate/reliability demands, proper\nnetwork planning is needed to make the IAB network performing appropriately and\nas good as possible. In this paper, we study the effect of deployment\noptimization on the coverage of IAB networks. We concentrate on the cases\nwhere, due to either geographical or interference management limitations,\nunconstrained IAB node placement is not feasible in some areas. To that end, we\npropose various millimeter wave (mmWave) blocking-aware constrained deployment\noptimization approaches. Our results indicate that, even with limitations on\ndeployment optimization, network planning boosts the coverage of IAB networks\nconsiderably.",
    "descriptor": "\nComments: Submitted to IEEE Wireless Communications and Networking Conference (WCNC)'2023, Glasgow, Scotland, United Kingdom\n",
    "authors": [
      "Charitha Madapatha",
      "Behrooz Makki",
      "Hao Guo",
      "Tommy Svensson"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05253"
  },
  {
    "id": "arXiv:2210.05254",
    "title": "Deep Spectro-temporal Artifacts for Detecting Synthesized Speech",
    "abstract": "The Audio Deep Synthesis Detection (ADD) Challenge has been held to detect\ngenerated human-like speech. With our submitted system, this paper provides an\noverall assessment of track 1 (Low-quality Fake Audio Detection) and track 2\n(Partially Fake Audio Detection). In this paper, spectro-temporal artifacts\nwere detected using raw temporal signals, spectral features, as well as deep\nembedding features. To address track 1, low-quality data augmentation, domain\nadaptation via finetuning, and various complementary feature information fusion\nwere aggregated in our system. Furthermore, we analyzed the clustering\ncharacteristics of subsystems with different features by visualization method\nand explained the effectiveness of our proposed greedy fusion strategy. As for\ntrack 2, frame transition and smoothing were detected using self-supervised\nlearning structure to capture the manipulation of PF attacks in the time\ndomain. We ranked 4th and 5th in track 1 and track 2, respectively.",
    "descriptor": "\nComments: 7 pages, 1 figures, Accecpted by Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia\n",
    "authors": [
      "Xiaohui Liu",
      "Meng Liu",
      "Lin Zhang",
      "Linjuan Zhang",
      "Chang Zeng",
      "Kai Li",
      "Nan Li",
      "Kong Aik Lee",
      "Longbiao Wang",
      "Jianwu Dang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05254"
  },
  {
    "id": "arXiv:2210.05257",
    "title": "Rethinking the Event Coding Pipeline with Prompt Entailment",
    "abstract": "For monitoring crises, political events are extracted from the news. The\nlarge amount of unstructured full-text event descriptions makes a case-by-case\nanalysis unmanageable, particularly for low-resource humanitarian aid\norganizations. This creates a demand to classify events into event types, a\ntask referred to as event coding. Typically, domain experts craft an event type\nontology, annotators label a large dataset and technical experts develop a\nsupervised coding system. In this work, we propose PR-ENT, a new event coding\napproach that is more flexible and resource-efficient, while maintaining\ncompetitive accuracy: first, we extend an event description such as \"Military\ninjured two civilians'' by a template, e.g. \"People were [Z]\" and prompt a\npre-trained (cloze) language model to fill the slot Z. Second, we select answer\ncandidates Z* = {\"injured'', \"hurt\"...} by treating the event description as\npremise and the filled templates as hypothesis in a textual entailment task.\nThis allows domain experts to draft the codebook directly as labeled prompts\nand interpretable answer candidates. This human-in-the-loop process is guided\nby our interactive codebook design tool. We evaluate PR-ENT in several\nrobustness checks: perturbing the event description and prompt template,\nrestricting the vocabulary and removing contextual information.",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Lefebvre",
      "Niklas Stoehr"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05257"
  },
  {
    "id": "arXiv:2210.05261",
    "title": "Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair  Modeling",
    "abstract": "Transformer-based models have achieved great success on sentence pair\nmodeling tasks, such as answer selection and natural language inference (NLI).\nThese models generally perform cross-attention over input pairs, leading to\nprohibitive computational costs. Recent studies propose dual-encoder and late\ninteraction architectures for faster computation. However, the balance between\nthe expressive of cross-attention and computation speedup still needs better\ncoordinated. To this end, this paper introduces a novel paradigm MixEncoder for\nefficient sentence pair modeling. MixEncoder involves a light-weight\ncross-attention mechanism. It conducts query encoding only once while modeling\nthe query-candidate interaction in parallel. Extensive experiments conducted on\nfour tasks demonstrate that our MixEncoder can speed up sentence pairing by\nover 113x while achieving comparable performance as the more expensive\ncross-attention models.",
    "descriptor": "",
    "authors": [
      "Yuanhang Yang",
      "shiyi qi",
      "Cuiyun Gao",
      "Zenglin Xu",
      "Yulan He",
      "Qifan Wang",
      "Chuanyi Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05261"
  },
  {
    "id": "arXiv:2210.05264",
    "title": "Integrated Graphene Patch Antenna for Communications at THz Frequencies",
    "abstract": "Graphene is an attractive material for communications in the THz range due to\nits ability to support surface plasmon polaritons. This enables a graphene\nantenna to be smaller in size than its metallic counterpart. In addition, the\npossibility to control the graphene conductivity during operation by an applied\nbias leads to the tunability of the resonant frequency of graphene antennas.\nGraphene-based antennas integrated into transceivers working at THz frequencies\nmay lead to faster and more efficient devices. In this work, we design and\nsimulate a graphene patch antenna that can be integrated into transceivers by\nthrough-substrate vias. The tuning of the resonant frequency is also studied by\nsimulations.",
    "descriptor": "\nComments: Published in: 2022 47th International Conference on Infrared, Millimeter and Terahertz Waves (IRMMW-THz)\n",
    "authors": [
      "E. P. de Santana",
      "A. K. Wigger",
      "Z. Wang",
      "K. Wang",
      "M. Lemme",
      "S. Abadal",
      "P. H. Bolivar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05264"
  },
  {
    "id": "arXiv:2210.05265",
    "title": "MFCCA:Multi-Frame Cross-Channel attention for multi-speaker ASR in  Multi-party meeting scenario",
    "abstract": "Recently cross-channel attention, which better leverages multi-channel\nsignals from microphone array, has shown promising results in the multi-party\nmeeting scenario. Cross-channel attention focuses on either learning global\ncorrelations between sequences of different channels or exploiting fine-grained\nchannel-wise information effectively at each time step. Considering the delay\nof microphone array receiving sound, we propose a multi-frame cross-channel\nattention, which models cross-channel information between adjacent frames to\nexploit the complementarity of both frame-wise and channel-wise knowledge.\nBesides, we also propose a multi-layer convolutional mechanism to fuse the\nmulti-channel output and a channel masking strategy to combat the channel\nnumber mismatch problem between training and inference. Experiments on the\nAliMeeting, a real-world corpus, reveal that our proposed model outperforms\nsingle-channel model by 31.7\\% and 37.0\\% CER reduction on Eval and Test sets.\nMoreover, with comparable model parameters and training data, our proposed\nmodel achieves a new SOTA performance on the AliMeeting corpus, as compared\nwith the top ranking systems in the ICASSP2022 M2MeT challenge, a recently held\nmulti-channel multi-speaker ASR challenge.",
    "descriptor": "\nComments: Accepted by SLT 2022\n",
    "authors": [
      "Fan Yu",
      "Shiliang Zhang",
      "Pengcheng Guo",
      "Yuhao Liang",
      "Zhihao Du",
      "Yuxiao Lin",
      "Lei Xie"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05265"
  },
  {
    "id": "arXiv:2210.05267",
    "title": "Simulating Structural Plasticity of the Brain more Scalable than  Expected",
    "abstract": "Structural plasticity of the brain describes the creation of new and the\ndeletion of old synapses over time. Rinke et al. (JPDC 2018) introduced a\nscalable algorithm that simulates structural plasticity for up to one billion\nneurons on current hardware using a variant of the Barnes--Hut algorithm. They\ndemonstrate good scalability and prove a runtime complexity of $O(n \\log^2 n)$.\nIn this comment paper, we show that with careful consideration of the\nalgorithm, the theoretical runtime can even be classified as $O(n \\log n)$.",
    "descriptor": "",
    "authors": [
      "Fabian Czappa",
      "Alexander Gei\u00df",
      "Felix Wolf"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2210.05267"
  },
  {
    "id": "arXiv:2210.05268",
    "title": "Component-Wise Natural Gradient Descent -- An Efficient Neural Network  Optimization",
    "abstract": "Natural Gradient Descent (NGD) is a second-order neural network training that\npreconditions the gradient descent with the inverse of the Fisher Information\nMatrix (FIM). Although NGD provides an efficient preconditioner, it is not\npracticable due to the expensive computation required when inverting the FIM.\nThis paper proposes a new NGD variant algorithm named Component-Wise Natural\nGradient Descent (CW-NGD). CW-NGD is composed of 2 steps. Similar to several\nexisting works, the first step is to consider the FIM matrix as a\nblock-diagonal matrix whose diagonal blocks correspond to the FIM of each\nlayer's weights. In the second step, unique to CW-NGD, we analyze the layer's\nstructure and further decompose the layer's FIM into smaller segments whose\nderivatives are approximately independent. As a result, individual layers' FIMs\nare approximated in a block-diagonal form that trivially supports the\ninversion. The segment decomposition strategy is varied by layer structure.\nSpecifically, we analyze the dense and convolutional layers and design their\ndecomposition strategies appropriately. In an experiment of training a network\ncontaining these 2 types of layers, we empirically prove that CW-NGD requires\nfewer iterations to converge compared to the state-of-the-art first-order and\nsecond-order methods.",
    "descriptor": "",
    "authors": [
      "Tran Van Sang",
      "Mhd Irvan",
      "Rie Shigetomi Yamaguchi",
      "Toshiyuki Nakata"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05268"
  },
  {
    "id": "arXiv:2210.05271",
    "title": "GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from  Diffusion Models",
    "abstract": "We propose AudioStyleGAN (ASGAN), a new generative adversarial network (GAN)\nfor unconditional speech synthesis. As in the StyleGAN family of image\nsynthesis models, ASGAN maps sampled noise to a disentangled latent vector\nwhich is then mapped to a sequence of audio features so that signal aliasing is\nsuppressed at every layer. To successfully train ASGAN, we introduce a number\nof new techniques, including a modification to adaptive discriminator\naugmentation to probabilistically skip discriminator updates. ASGAN achieves\nstate-of-the-art results in unconditional speech synthesis on the Google Speech\nCommands dataset. It is also substantially faster than the top-performing\ndiffusion models. Through a design that encourages disentanglement, ASGAN is\nable to perform voice conversion and speech editing without being explicitly\ntrained to do so. ASGAN demonstrates that GANs are still highly competitive\nwith diffusion models. Code, models, samples:\nhttps://github.com/RF5/simple-asgan/.",
    "descriptor": "\nComments: 6 pages, 2 figures, 2 tables. Accepted at IEEE SLT 2022\n",
    "authors": [
      "Matthew Baas",
      "Herman Kamper"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05271"
  },
  {
    "id": "arXiv:2210.05274",
    "title": "Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design",
    "abstract": "Fragment-based drug discovery has been an effective paradigm in early-stage\ndrug development. An open challenge in this area is designing linkers between\ndisconnected molecular fragments of interest to obtain chemically-relevant\ncandidate drug molecules. In this work, we propose DiffLinker, an\nE(3)-equivariant 3D-conditional diffusion model for molecular linker design.\nGiven a set of disconnected fragments, our model places missing atoms in\nbetween and designs a molecule incorporating all the initial fragments. Unlike\nprevious approaches that are only able to connect pairs of molecular fragments,\nour method can link an arbitrary number of fragments. Additionally, the model\nautomatically determines the number of atoms in the linker and its attachment\npoints to the input fragments. We demonstrate that DiffLinker outperforms other\nmethods on the standard datasets generating more diverse and\nsynthetically-accessible molecules. Besides, we experimentally test our method\nin real-world applications, showing that it can successfully generate valid\nlinkers conditioned on target protein pockets.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Ilia Igashov",
      "Hannes St\u00e4rk",
      "Cl\u00e9ment Vignac",
      "Victor Garcia Satorras",
      "Pascal Frossard",
      "Max Welling",
      "Michael Bronstein",
      "Bruno Correia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2210.05274"
  },
  {
    "id": "arXiv:2210.05276",
    "title": "RoHNAS: A Neural Architecture Search Framework with Conjoint  Optimization for Adversarial Robustness and Hardware Efficiency of  Convolutional and Capsule Networks",
    "abstract": "Neural Architecture Search (NAS) algorithms aim at finding efficient Deep\nNeural Network (DNN) architectures for a given application under given system\nconstraints. DNNs are computationally-complex as well as vulnerable to\nadversarial attacks. In order to address multiple design objectives, we propose\nRoHNAS, a novel NAS framework that jointly optimizes for adversarial-robustness\nand hardware-efficiency of DNNs executed on specialized hardware accelerators.\nBesides the traditional convolutional DNNs, RoHNAS additionally accounts for\ncomplex types of DNNs such as Capsule Networks. For reducing the exploration\ntime, RoHNAS analyzes and selects appropriate values of adversarial\nperturbation for each dataset to employ in the NAS flow. Extensive evaluations\non multi - Graphics Processing Unit (GPU) - High Performance Computing (HPC)\nnodes provide a set of Pareto-optimal solutions, leveraging the tradeoff\nbetween the above-discussed design objectives. For example, a Pareto-optimal\nDNN for the CIFAR-10 dataset exhibits 86.07% accuracy, while having an energy\nof 38.63 mJ, a memory footprint of 11.85 MiB, and a latency of 4.47 ms.",
    "descriptor": "\nComments: Accepted for publication at IEEE Access\n",
    "authors": [
      "Alberto Marchisio",
      "Vojtech Mrazek",
      "Andrea Massa",
      "Beatrice Bussolino",
      "Maurizio Martina",
      "Muhammad Shafique"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05276"
  },
  {
    "id": "arXiv:2210.05278",
    "title": "EnsembleMOT: A Step towards Ensemble Learning of Multiple Object  Tracking",
    "abstract": "Multiple Object Tracking (MOT) has rapidly progressed in recent years.\nExisting works tend to design a single tracking algorithm to perform both\ndetection and association. Though ensemble learning has been exploited in many\ntasks, i.e, classification and object detection, it hasn't been studied in the\nMOT task, which is mainly caused by its complexity and evaluation metrics. In\nthis paper, we propose a simple but effective ensemble method for MOT, called\nEnsembleMOT, which merges multiple tracking results from various trackers with\nspatio-temporal constraints. Meanwhile, several post-processing procedures are\napplied to filter out abnormal results. Our method is model-independent and\ndoesn't need the learning procedure. What's more, it can easily work in\nconjunction with other algorithms, e.g., tracklets interpolation. Experiments\non the MOT17 dataset demonstrate the effectiveness of the proposed method.\nCodes are available at https://github.com/dyhBUPT/EnsembleMOT.",
    "descriptor": "\nComments: 5 pages, 1 figure\n",
    "authors": [
      "Yunhao Du",
      "Zihang Liu",
      "Fei Su"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05278"
  },
  {
    "id": "arXiv:2210.05279",
    "title": "Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity",
    "abstract": "$\\ell_0$ constrained optimization is prevalent in machine learning,\nparticularly for high-dimensional problems, because it is a fundamental\napproach to achieve sparse learning. Hard-thresholding gradient descent is a\ndominant technique to solve this problem. However, first-order gradients of the\nobjective function may be either unavailable or expensive to calculate in a lot\nof real-world problems, where zeroth-order (ZO) gradients could be a good\nsurrogate. Unfortunately, whether ZO gradients can work with the\nhard-thresholding operator is still an unsolved problem. To solve this puzzle,\nin this paper, we focus on the $\\ell_0$ constrained black-box stochastic\noptimization problems, and propose a new stochastic zeroth-order gradient\nhard-thresholding (SZOHT) algorithm with a general ZO gradient estimator\npowered by a novel random support sampling. We provide the convergence analysis\nof SZOHT under standard assumptions. Importantly, we reveal a conflict between\nthe deviation of ZO estimators and the expansivity of the hard-thresholding\noperator, and provide a theoretical minimal value of the number of random\ndirections in ZO gradients. In addition, we find that the query complexity of\nSZOHT is independent or weakly dependent on the dimensionality under different\nsettings. Finally, we illustrate the utility of our method on a portfolio\noptimization problem as well as black-box adversarial attacks.",
    "descriptor": "\nComments: Accepted for publication at NeurIPS 2022\n",
    "authors": [
      "William de Vazelhes",
      "Hualin Zhang",
      "Huimin Wu",
      "Xiao-Tong Yuan",
      "Bin Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05279"
  },
  {
    "id": "arXiv:2210.05280",
    "title": "ME-D2N: Multi-Expert Domain Decompositional Network for Cross-Domain  Few-Shot Learning",
    "abstract": "Recently, Cross-Domain Few-Shot Learning (CD-FSL) which aims at addressing\nthe Few-Shot Learning (FSL) problem across different domains has attracted\nrising attention. The core challenge of CD-FSL lies in the domain gap between\nthe source and novel target datasets. Though many attempts have been made for\nCD-FSL without any target data during model training, the huge domain gap makes\nit still hard for existing CD-FSL methods to achieve very satisfactory results.\nAlternatively, learning CD-FSL models with few labeled target domain data which\nis more realistic and promising is advocated in previous\nwork~\\cite{fu2021meta}. Thus, in this paper, we stick to this setting and\ntechnically contribute a novel Multi-Expert Domain Decompositional Network\n(ME-D2N). Concretely, to solve the data imbalance problem between the source\ndata with sufficient examples and the auxiliary target data with limited\nexamples, we build our model under the umbrella of multi-expert learning. Two\nteacher models which can be considered to be experts in their corresponding\ndomain are first trained on the source and the auxiliary target sets,\nrespectively. Then, the knowledge distillation technique is introduced to\ntransfer the knowledge from two teachers to a unified student model. Taking a\nstep further, to help our student model learn knowledge from different domain\nteachers simultaneously, we further present a novel domain decomposition module\nthat learns to decompose the student model into two domain-related sub parts.\nThis is achieved by a novel domain-specific gate that learns to assign each\nfilter to only one specific domain in a learnable way. Extensive experiments\ndemonstrate the effectiveness of our method. Codes and models are available at\nhttps://github.com/lovelyqian/ME-D2N_for_CDFSL.",
    "descriptor": "\nComments: Accepted by ACM Multimedia 2022\n",
    "authors": [
      "Yuqian Fu",
      "Yu Xie",
      "Yanwei Fu",
      "Jingjing Chen",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05280"
  },
  {
    "id": "arXiv:2210.05282",
    "title": "Computer Vision based inspection on post-earthquake with UAV synthetic  dataset",
    "abstract": "The area affected by the earthquake is vast and often difficult to entirely\ncover, and the earthquake itself is a sudden event that causes multiple defects\nsimultaneously, that cannot be effectively traced using traditional, manual\nmethods. This article presents an innovative approach to the problem of\ndetecting damage after sudden events by using an interconnected set of deep\nmachine learning models organized in a single pipeline and allowing for easy\nmodification and swapping models seamlessly. Models in the pipeline were\ntrained with a synthetic dataset and were adapted to be further evaluated and\nused with unmanned aerial vehicles (UAVs) in real-world conditions. Thanks to\nthe methods presented in the article, it is possible to obtain high accuracy in\ndetecting buildings defects, segmenting constructions into their components and\nestimating their technical condition based on a single drone flight.",
    "descriptor": "\nComments: 15 pages, 8 figures, published version, software available from this https URL\n",
    "authors": [
      "Mateusz \u017barski",
      "Bartosz W\u00f3jcik",
      "Jaros\u0142aw A. Miszczak",
      "Bart\u0142omiej Blachowski",
      "Mariusz Ostrowski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05282"
  },
  {
    "id": "arXiv:2210.05287",
    "title": "Revisiting and Advancing Chinese Natural Language Understanding with  Accelerated Heterogeneous Knowledge Pre-training",
    "abstract": "Recently, knowledge-enhanced pre-trained language models (KEPLMs) improve\ncontext-aware representations via learning from structured relations in\nknowledge graphs, and/or linguistic knowledge from syntactic or dependency\nanalysis. Unlike English, there is a lack of high-performing open-source\nChinese KEPLMs in the natural language processing (NLP) community to support\nvarious language understanding applications. In this paper, we revisit and\nadvance the development of Chinese natural language understanding with a series\nof novel Chinese KEPLMs released in various parameter sizes, namely CKBERT\n(Chinese knowledge-enhanced BERT).Specifically, both relational and linguistic\nknowledge is effectively injected into CKBERT based on two novel pre-training\ntasks, i.e., linguistic-aware masked language modeling and contrastive\nmulti-hop relation modeling. Based on the above two pre-training paradigms and\nour in-house implemented TorchAccelerator, we have pre-trained base (110M),\nlarge (345M) and huge (1.3B) versions of CKBERT efficiently on GPU clusters.\nExperiments demonstrate that CKBERT outperforms strong baselines for Chinese\nover various benchmark NLP tasks and in terms of different model sizes.",
    "descriptor": "\nComments: EMNLP 2022 industry track\n",
    "authors": [
      "Taolin Zhang",
      "Junwei DOng",
      "Jianing Wang",
      "Chengyu Wang",
      "Ang Wang",
      "Yinghui Liu",
      "Jun Huang",
      "Yong Li",
      "Xiaofeng He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05287"
  },
  {
    "id": "arXiv:2210.05289",
    "title": "A numerical study of the spectral properties of Isogeometric collocation  matrices for acoustic wave problems",
    "abstract": "This paper focuses on the spectral properties of the mass and stiffness\nmatrices for acoustic wave problems discretized with Isogeometric analysis\n(IGA) collocation methods in space and Newmark methods in time. Extensive\nnumerical results are reported for the eigenvalues and condition numbers of the\nacoustic mass and stiffness matrices in the reference square domain with\nDirichlet, Neumann and absorbing boundary conditions, varying the polynomial\ndegree $p$, mesh size $h$, regularity $k$, of the IGA discretization and the\ntime step $\\Delta t$ and parameter $\\beta$ of the Newmark method. Results on\nthe sparsity of the matrices and the eigenvalue distribution with respect to\nthe degrees of freedom d.o.f. and the number of nonzero entries nz are also\nreported. The results are comparable with the available spectral estimates for\nIGA Galerkin matrices associated to the Poisson problem with Dirichlet boundary\nconditions, and in some cases the IGA collocation results are better than the\ncorresponding IGA Galerkin estimates.",
    "descriptor": "",
    "authors": [
      "Elena Zampieri",
      "Luca Franco Pavarino"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05289"
  },
  {
    "id": "arXiv:2210.05291",
    "title": "On the Use of Semantically-Aligned Speech Representations for Spoken  Language Understanding",
    "abstract": "In this paper we examine the use of semantically-aligned speech\nrepresentations for end-to-end spoken language understanding (SLU). We employ\nthe recently-introduced SAMU-XLSR model, which is designed to generate a single\nembedding that captures the semantics at the utterance level, semantically\naligned across different languages. This model combines the acoustic\nframe-level speech representation learning model (XLS-R) with the Language\nAgnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the\nSAMU-XLSR model instead of the initial XLS-R model improves significantly the\nperformance in the framework of end-to-end SLU. Finally, we present the\nbenefits of using this model towards language portability in SLU.",
    "descriptor": "\nComments: Accepted in IEEE SLT 2022. This work was performed using HPC resources from GENCI/IDRIS (grant 2022 AD011012565) and received funding from the EU H2020 research and innovation programme under the Marie Sklodowska-Curie ESPERANTO project (grant agreement No 101007666), through the SELMA project (grant No 957017) and from the French ANR through the AISSPER project (ANR-19-CE23-0004)\n",
    "authors": [
      "Ga\u00eblle Laperri\u00e8re",
      "Valentin Pelloin",
      "Micka\u00ebl Rouvier",
      "Themos Stafylakis",
      "Yannick Est\u00e8ve"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05291"
  },
  {
    "id": "arXiv:2210.05294",
    "title": "Evaluation of the Quality of Exercises for the Data Structures'  eTextbook and Find the Difficult Topics Using Item Response Theory and Logged  Data Analysis",
    "abstract": "The growing dependence on eTextbooks and Massive Open Online Courses (MOOCs)\nhas led to an increase in the amount of students' learning data. By carefully\nanalyzing this data, educators can identify difficult exercises, and evaluate\nthe quality of the exercises when teaching a particular topic. In this study,\nan analysis of log data from the use of a semester of the OpenDSA eTextbook was\noffered to identify the most difficult data structure course exercises and to\nevaluate the quality of the course exercises. Our study is based on analyzing\nstudents' responses to the course exercises. To identify the difficult\nexercises, we applied two different approaches, the first of which involved\nanalyzing student responses to exercises using item response theory (IRT)\nanalysis and a latent trait model (LTM) technique, and the second involved\ndetermining which exercises were more difficult based on how students\ninteracted with them. We computed different measures for every exercise such\nthat difficulty level, trial and error, and hint ratio. We generated an item\ncharacteristics curve, item information curve, and test information function\nfor each exercise. To evaluate the quality of the exercises, we applied the IRT\nanalysis to the students' responses to the exercises and, we computed the\ndifficulty and discrimination index for each exercise. We classified whether\nthe exercise is good or poor based on these two measures. Our findings showed\nthat the exercises that related to algorithm analysis topics represented most\nof the difficult exercises that students struggle with, and there existing six\nexercises out of 56 exercises are classified as poor exercises which could be\nrejected or improved. Some of these poor exercises do not differentiate between\nstudents with different abilities; the others give preference to low-ability\nstudents to answer these exercises over high-ability students.",
    "descriptor": "\nComments: 23 pages,20 figures\n",
    "authors": [
      "Ahmed Abd Elrahman",
      "Taysir Hassan A Soliman",
      "Mohammed F. Farghally",
      "Ahmed I. Taloba"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05294"
  },
  {
    "id": "arXiv:2210.05296",
    "title": "Natural Language Processing for Cognitive Analysis of Emotions",
    "abstract": "Emotion analysis in texts suffers from two major limitations: annotated\ngold-standard corpora are mostly small and homogeneous, and emotion\nidentification is often simplified as a sentence-level classification problem.\nTo address these issues, we introduce a new annotation scheme for exploring\nemotions and their causes, along with a new French dataset composed of\nautobiographical accounts of an emotional scene. The texts were collected by\napplying the Cognitive Analysis of Emotions developed by A. Finkel to help\npeople improve on their emotion management. The method requires the manual\nanalysis of an emotional event by a coach trained in Cognitive Analysis. We\npresent a rule-based approach to automatically annotate emotions and their\nsemantic roles (e.g. emotion causes) to facilitate the identification of\nrelevant aspects by the coach. We investigate future directions for emotion\nanalysis using graph structures.",
    "descriptor": "",
    "authors": [
      "Gustave Cortal",
      "Alain Finkel",
      "Patrick Paroubek",
      "Lina Ye"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05296"
  },
  {
    "id": "arXiv:2210.05298",
    "title": "Weakly-Supervised Optical Flow Estimation for Time-of-Flight",
    "abstract": "Indirect Time-of-Flight (iToF) cameras are a widespread type of 3D sensor,\nwhich perform multiple captures to obtain depth values of the captured scene.\nWhile recent approaches to correct iToF depths achieve high performance when\nremoving multi-path-interference and sensor noise, little research has been\ndone to tackle motion artifacts. In this work we propose a training algorithm,\nwhich allows to supervise Optical Flow (OF) networks directly on the\nreconstructed depth, without the need of having ground truth flows. We\ndemonstrate that this approach enables the training of OF networks to align raw\niToF measurements and compensate motion artifacts in the iToF depth images. The\napproach is evaluated for both single- and multi-frequency sensors as well as\nmulti-tap sensors, and is able to outperform other motion compensation\ntechniques.",
    "descriptor": "\nComments: accepted at WACV 2023\n",
    "authors": [
      "Michael Schelling",
      "Pedro Hermosilla",
      "Timo Ropinski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.05298"
  },
  {
    "id": "arXiv:2210.05301",
    "title": "Intrinsic Dimension for Large-Scale Geometric Learning",
    "abstract": "The concept of dimension is essential to grasp the complexity of data. A\nnaive approach to determine the dimension of a dataset is based on the number\nof attributes. More sophisticated methods derive a notion of intrinsic\ndimension (ID) that employs more complex feature functions, e.g., distances\nbetween data points. Yet, many of these approaches are based on empirical\nobservations, cannot cope with the geometric character of contemporary\ndatasets, and do lack an axiomatic foundation. A different approach was\nproposed by V. Pestov, who links the intrinsic dimension axiomatically to the\nmathematical concentration of measure phenomenon. First methods to compute this\nand related notions for ID were computationally intractable for large-scale\nreal-world datasets. In the present work, we derive a computationally feasible\nmethod for determining said axiomatic ID functions. Moreover, we demonstrate\nhow the geometric properties of complex data are accounted for in our modeling.\nIn particular, we propose a principle way to incorporate neighborhood\ninformation, as in graph data, into the ID. This allows for new insights into\ncommon graph learning procedures, which we illustrate by experiments on the\nOpen Graph Benchmark.",
    "descriptor": "\nComments: 16 pages, 3 tables, 2 figures\n",
    "authors": [
      "Maximilian Stubbemann",
      "Tom Hanika",
      "Friedrich Martin Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05301"
  },
  {
    "id": "arXiv:2210.05302",
    "title": "Towards Structure-aware Paraphrase Identification with Phrase Alignment  Using Sentence Encoders",
    "abstract": "Previous works have demonstrated the effectiveness of utilising pre-trained\nsentence encoders based on their sentence representations for meaning\ncomparison tasks. Though such representations are shown to capture hidden\nsyntax structures, the direct similarity comparison between them exhibits weak\nsensitivity to word order and structural differences in given sentences. A\nsingle similarity score further makes the comparison process hard to interpret.\nTherefore, we here propose to combine sentence encoders with an alignment\ncomponent by representing each sentence as a list of predicate-argument spans\n(where their span representations are derived from sentence encoders), and\ndecomposing the sentence-level meaning comparison into the alignment between\ntheir spans for paraphrase identification tasks. Empirical results show that\nthe alignment component brings in both improved performance and\ninterpretability for various sentence encoders. After closer investigation, the\nproposed approach indicates increased sensitivity to structural difference and\nenhanced ability to distinguish non-paraphrases with high lexical overlap.",
    "descriptor": "\nComments: COLING 2022 Oral\n",
    "authors": [
      "Qiwei Peng",
      "David Weir",
      "Julie Weeds"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05302"
  },
  {
    "id": "arXiv:2210.05304",
    "title": "Learning Control Policies for Region Stabilization in Stochastic Systems",
    "abstract": "We consider the problem of learning control policies in stochastic systems\nwhich guarantee that the system stabilizes within some specified stabilization\nregion with probability $1$. Our approach is based on the novel notion of\nstabilizing ranking supermartingales (sRSMs) that we introduce in this work.\nOur sRSMs overcome the limitation of methods proposed in previous works whose\napplicability is restricted to systems in which the stabilizing region cannot\nbe left once entered under any control policy. We present a learning procedure\nthat learns a control policy together with an sRSM that formally certifies\nprobability-$1$ stability, both learned as neural networks. Our experimental\nevaluation shows that our learning procedure can successfully learn provably\nstabilizing policies in practice.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.09495\n",
    "authors": [
      "Matin Ansaripour",
      "Mathias Lechner",
      "\u0110or\u0111e \u017dikeli\u0107",
      "Krishnendu Chatterjee",
      "Thomas A. Henzinger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05304"
  },
  {
    "id": "arXiv:2210.05308",
    "title": "Learning Control Policies for Stochastic Systems with Reach-avoid  Guarantees",
    "abstract": "We study the problem of learning controllers for discrete-time non-linear\nstochastic dynamical systems with formal reach-avoid guarantees. This work\npresents the first method for providing formal reach-avoid guarantees, which\ncombine and generalize stability and safety guarantees, with a tolerable\nprobability threshold $p\\in[0,1]$ over the infinite time horizon. Our method\nleverages advances in machine learning literature and it represents formal\ncertificates as neural networks. In particular, we learn a certificate in the\nform of a reach-avoid supermartingale (RASM), a novel notion that we introduce\nin this work. Our RASMs provide reachability and avoidance guarantees by\nimposing constraints on what can be viewed as a stochastic extension of level\nsets of Lyapunov functions for deterministic systems. Our approach solves\nseveral important problems -- it can be used to learn a control policy from\nscratch, to verify a reach-avoid specification for a fixed control policy, or\nto fine-tune a pre-trained policy if it does not satisfy the reach-avoid\nspecification. We validate our approach on $3$ stochastic non-linear\nreinforcement learning tasks.",
    "descriptor": "",
    "authors": [
      "\u0110or\u0111e \u017dikeli\u0107",
      "Mathias Lechner",
      "Thomas A. Henzinger",
      "Krishnendu Chatterjee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05308"
  },
  {
    "id": "arXiv:2210.05311",
    "title": "CD-FSOD: A Benchmark for Cross-domain Few-shot Object Detection",
    "abstract": "Although few-shot object detection (FSOD) has attracted great research\nattention, no work yet exists that studies FSOD across the different domains\nseen in real-world scenarios. In this paper, we propose a new study of the\ncross-domain few-shot object detection (CD-FSOD) benchmark, consisting of image\ndata from a diverse data domain. On the proposed benchmark, we evaluate\nstate-of-art FSOD approaches, and analyze the impact of detection models and\npre-training datasets on performance. The results reveal several key findings:\n(1) the existing FSOD approaches tend to fall, and even underperform the naive\nfine-tuning model; 2) the pre-training datasets and detection architectures\nplay an important role, and the right choice can boost the performance of the\ntarget tasks significantly. Besides, we also analyze the reasons for existing\nFSOD approaches' failure, and introduce a strong baseline that uses a\nmutually-beneficial manner to alleviate the overfitting problem. Our approach\nis remarkably superior to existing approaches by significant margins (\\%2.3 on\naverage) on the proposed benchmark and also achieves competitive performance on\nthe FSOD benchmark.",
    "descriptor": "",
    "authors": [
      "Wuti Xiong",
      "Li Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05311"
  },
  {
    "id": "arXiv:2210.05313",
    "title": "Memory transformers for full context and high-resolution 3D Medical  Segmentation",
    "abstract": "Transformer models achieve state-of-the-art results for image segmentation.\nHowever, achieving long-range attention, necessary to capture global context,\nwith high-resolution 3D images is a fundamental challenge. This paper\nintroduces the Full resolutIoN mEmory (FINE) transformer to overcome this\nissue. The core idea behind FINE is to learn memory tokens to indirectly model\nfull range interactions while scaling well in both memory and computational\ncosts. FINE introduces memory tokens at two levels: the first one allows full\ninteraction between voxels within local image regions (patches), the second one\nallows full interactions between all regions of the 3D volume. Combined, they\nallow full attention over high resolution images, e.g. 512 x 512 x 256 voxels\nand above. Experiments on the BCV image segmentation dataset shows better\nperformances than state-of-the-art CNN and transformer baselines, highlighting\nthe superiority of our full attention mechanism compared to recent transformer\nbaselines, e.g. CoTr, and nnFormer.",
    "descriptor": "",
    "authors": [
      "Loic Themyr",
      "Cl\u00e9ment Rambour",
      "Nicolas Thome",
      "Toby Collins",
      "Alexandre Hostettler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05313"
  },
  {
    "id": "arXiv:2210.05314",
    "title": "Client Error Clustering Approaches in Content Delivery Networks (CDN)",
    "abstract": "Content delivery networks (CDNs) are the backbone of the Internet and are key\nin delivering high quality video on demand (VoD), web content and file services\nto billions of users. CDNs usually consist of hierarchically organized content\nservers positioned as close to the customers as possible. CDN operators face a\nsignificant challenge when analyzing billions of web server and proxy logs\ngenerated by their systems. The main objective of this study was to analyze the\napplicability of various clustering methods in CDN error log analysis. We\nworked with real-life CDN proxy logs, identified key features included in the\nlogs (e.g., content type, HTTP status code, time-of-day, host) and clustered\nthe log lines corresponding to different host types offering live TV, video on\ndemand, file caching and web content. Our experiments were run on a dataset\nconsisting of proxy logs collected over a 7-day period from a single, physical\nCDN server running multiple types of services (VoD, live TV, file). The dataset\nconsisted of 2.2 billion log lines. Our analysis showed that CDN error\nclustering is a viable approach towards identifying recurring errors and\nimproving overall quality of service.",
    "descriptor": "",
    "authors": [
      "Ermiyas Birihanu",
      "Jiyan Mahmud",
      "P\u00e9ter Kiss",
      "Adolf Kamuzora",
      "Wadie Skaf",
      "Tom\u00e1\u0161 Horv\u00e1th",
      "Tam\u00e1s Jursonovics",
      "Peter Pogrzeba",
      "Imre Lend\u00e1k"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05314"
  },
  {
    "id": "arXiv:2210.05316",
    "title": "Sizing up the Batteries: Modelling of Energy-Harvesting Sensor Nodes in  a Delay Tolerant Network",
    "abstract": "For energy-harvesting sensor nodes, rechargeable batteries play a critical\nrole in sensing and transmissions. By coupling two simple Markovian queue\nmodels in a delay-tolerant networking setting, we consider the problem of\nbattery sizing for these sensor nodes to operate effectively: given the\nintended energy depletion and overflow probabilities, how to decide the minimal\nbattery capacity that is required to ensure opportunistic data exchange despite\nthe inherent intermittency of renewable energy generation.",
    "descriptor": "\nComments: 13 pages, 5 figures. To appear in Festschrift for Professor Martin Purvis, University of Otago\n",
    "authors": [
      "Jeremiah D. Deng"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2210.05316"
  },
  {
    "id": "arXiv:2210.05318",
    "title": "CASAPose: Class-Adaptive and Semantic-Aware Multi-Object Pose Estimation",
    "abstract": "Applications in the field of augmented reality or robotics often require\njoint localisation and 6d pose estimation of multiple objects. However, most\nalgorithms need one network per object class to be trained in order to provide\nthe best results. Analysing all visible objects demands multiple inferences,\nwhich is memory and time-consuming. We present a new single-stage architecture\ncalled CASAPose that determines 2D-3D correspondences for pose estimation of\nmultiple different objects in RGB images in one pass. It is fast and memory\nefficient, and achieves high accuracy for multiple objects by exploiting the\noutput of a semantic segmentation decoder as control input to a keypoint\nrecognition decoder via local class-adaptive normalisation. Our new\ndifferentiable regression of keypoint locations significantly contributes to a\nfaster closing of the domain gap between real test and synthetic training data.\nWe apply segmentation-aware convolutions and upsampling operations to increase\nthe focus inside the object mask and to reduce mutual interference of occluding\nobjects. For each inserted object, the network grows by only one output\nsegmentation map and a negligible number of parameters. We outperform\nstate-of-the-art approaches in challenging multi-object scenes with\ninter-object occlusion and synthetic training.",
    "descriptor": "\nComments: Accepted at BMVC 2022 (this submission includes the paper and supplementary material)\n",
    "authors": [
      "Niklas Gard",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05318"
  },
  {
    "id": "arXiv:2210.05320",
    "title": "Synthetic Model Combination: An Instance-wise Approach to Unsupervised  Ensemble Learning",
    "abstract": "Consider making a prediction over new test data without any opportunity to\nlearn from a training set of labelled data - instead given access to a set of\nexpert models and their predictions alongside some limited information about\nthe dataset used to train them. In scenarios from finance to the medical\nsciences, and even consumer practice, stakeholders have developed models on\nprivate data they either cannot, or do not want to, share. Given the value and\nlegislation surrounding personal information, it is not surprising that only\nthe models, and not the data, will be released - the pertinent question\nbecoming: how best to use these models? Previous work has focused on global\nmodel selection or ensembling, with the result of a single final model across\nthe feature space. Machine learning models perform notoriously poorly on data\noutside their training domain however, and so we argue that when ensembling\nmodels the weightings for individual instances must reflect their respective\ndomains - in other words models that are more likely to have seen information\non that instance should have more attention paid to them. We introduce a method\nfor such an instance-wise ensembling of models, including a novel\nrepresentation learning step for handling sparse high-dimensional domains.\nFinally, we demonstrate the need and generalisability of our method on\nclassical machine learning tasks as well as highlighting a real world use case\nin the pharmacological setting of vancomycin precision dosing.",
    "descriptor": "",
    "authors": [
      "Alex J. Chan",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05320"
  },
  {
    "id": "arXiv:2210.05321",
    "title": "Image Segmentation Semantic Communication over Internet of Vehicles",
    "abstract": "In this paper, the problem of semantic-based efficient image transmission is\nstudied over the Internet of Vehicles (IoV). In the considered model, a vehicle\nshares massive amount of visual data perceived by its visual sensors to assist\nother vehicles in making driving decisions. However, it is hard to maintain a\nhigh reliable visual data transmission due to the limited spectrum resources.\nTo tackle this problem, a semantic communication approach is introduced to\nreduce the transmission data amount while ensuring the semantic-level accuracy.\nParticularly, an image segmentation semantic communication (ISSC) system is\nproposed, which can extract the semantic features from the perceived images and\ntransmit the features to the receiving vehicle that reconstructs the image\nsegmentations. The ISSC system consists of an encoder and a decoder at the\ntransmitter and the receiver, respectively. To accurately extract the image\nsemantic features, the ISSC system encoder employs a Swin Transformer based\nmulti-scale semantic feature extractor. Then, to resist the wireless noise and\nreconstruct the image segmentation, a semantic feature decoder and a\nreconstructor are designed at the receiver. Simulation results show that the\nproposed ISSC system can reconstruct the image segmentation accurately with a\nhigh compression ratio, and can achieve robust transmission performance against\nchannel noise, especially at the low signal-to-noise ratio (SNR). In terms of\nmean Intersection over Union (mIoU), the ISSC system can achieve an increase by\n75%, compared to the baselines using traditional coding method",
    "descriptor": "",
    "authors": [
      "Qiang Pan",
      "Haonan Tong",
      "Jie Lv",
      "Tao Luo",
      "Zhilong Zhang",
      "Changchuan Yin",
      "Jianfeng Li"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.05321"
  },
  {
    "id": "arXiv:2210.05325",
    "title": "Modeling and Performance Analysis for Movable Antenna Enabled Wireless  Communications",
    "abstract": "In this paper, we propose a novel antenna architecture called movable antenna\n(MA) to improve the performance of wireless communication systems. Different\nfrom conventional fixed-position antennas (FPAs), the MAs with the capability\nof flexible movement can be deployed at positions with more favorable channel\nconditions to achieve higher spatial diversity gains. To characterize the\ngeneral multi-path channel in a given region, a field-response based channel\nmodel is developed under the far-field condition. In the deterministic channel\ncase, we show the periodic behavior of the multi-path channel gain in a given\nspatial field, which can be exploited for analyzing the maximum channel gain of\nthe MA. In the case of stochastic channels, the expected value of an upper\nbound on the maximum channel gain of the MA in the receive region is derived\nfor different numbers of channel paths. Moreover, our results reveal that\nhigher performance gains by the MA over the FPA can be acquired when the number\nof channel paths increases due to more pronounced small-scale fading effects in\nthe spatial domain. Simulation results demonstrate that the MA system can reap\nconsiderable performance gains over the conventional FPA, and even achieve\ncomparable performance to the single-input multiple-output beamforming system.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Lipeng Zhu",
      "Wenyan Ma",
      "Rui Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05325"
  },
  {
    "id": "arXiv:2210.05327",
    "title": "A Causal Analysis of Harm",
    "abstract": "As autonomous systems rapidly become ubiquitous, there is a growing need for\na legal and regulatory framework to address when and how such a system harms\nsomeone. There have been several attempts within the philosophy literature to\ndefine harm, but none of them has proven capable of dealing with with the many\nexamples that have been presented, leading some to suggest that the notion of\nharm should be abandoned and \"replaced by more well-behaved notions\". As harm\nis generally something that is caused, most of these definitions have involved\ncausality at some level. Yet surprisingly, none of them makes use of causal\nmodels and the definitions of actual causality that they can express. In this\npaper we formally define a qualitative notion of harm that uses causal models\nand is based on a well-known definition of actual causality (Halpern, 2016).\nThe key novelty of our definition is that it is based on contrastive causation\nand uses a default utility to which the utility of actual outcomes is compared.\nWe show that our definition is able to handle the examples from the literature,\nand illustrate its importance for reasoning about situations involving\nautonomous systems.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Sander Beckers",
      "Hana Chockler",
      "Joseph Y. Halpern"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05327"
  },
  {
    "id": "arXiv:2210.05328",
    "title": "Reciprocity in Directed Hypergraphs: Measures, Findings, and Generators",
    "abstract": "Group interactions are prevalent in a variety of areas. Many of them,\nincluding email exchanges, chemical reactions, and bitcoin transactions, are\ndirectional, and thus they are naturally modeled as directed hypergraphs, where\neach hyperarc consists of the set of source nodes and the set of destination\nnodes. For directed graphs, which are a special case of directed hypergraphs,\nreciprocity has played a key role as a fundamental graph statistic in revealing\norganizing principles of graphs and in solving graph learning tasks. For\ngeneral directed hypergraphs, however, even no systematic measure of\nreciprocity has been developed. In this work, we investigate the reciprocity of\n11 real-world hypergraphs. To this end, we first introduce eight axioms that\nany reasonable measure of reciprocity should satisfy. Second, we propose\nHyperRec, a principled measure of hypergraph reciprocity that satisfies all the\naxioms. Third, we develop Ferret, a fast and exact algorithm for computing the\nmeasure, whose search space is up to 10^{147}x smaller than that of naive\ncomputation. Fourth, using them, we examine 11 real-world hypergraphs and\ndiscover patterns that distinguish them from random hypergraphs. Lastly, we\npropose ReDi, an intuitive generative model for directed hypergraphs exhibiting\nthe patterns.",
    "descriptor": "\nComments: The extended version of the ICDM 2022 paper with the same title. 31 pages, 8 figures\n",
    "authors": [
      "Sunwoo Kim",
      "Minyoung Choe",
      "Jaemin Yoo",
      "Kijung Shin"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.05328"
  },
  {
    "id": "arXiv:2210.05330",
    "title": "Label Noise-Robust Learning using a Confidence-Based Sieving Strategy",
    "abstract": "In learning tasks with label noise, boosting model robustness against\noverfitting is a pivotal challenge because the model eventually memorizes\nlabels including the noisy ones. Identifying the samples with corrupted labels\nand preventing the model from learning them is a promising approach to address\nthis challenge. Per-sample training loss is a previously studied metric that\nconsiders samples with small loss as clean samples on which the model should be\ntrained. In this work, we first demonstrate the ineffectiveness of this\nsmall-loss trick. Then, we propose a novel discriminator metric called\nconfidence error and a sieving strategy called CONFES to effectively\ndifferentiate between the clean and noisy samples. We experimentally illustrate\nthe superior performance of our proposed approach compared to recent studies on\nvarious settings such as synthetic and real-world label noise.",
    "descriptor": "",
    "authors": [
      "Reihaneh Torkzadehmahani",
      "Reza Nasirigerdeh",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05330"
  },
  {
    "id": "arXiv:2210.05331",
    "title": "Generalization Analysis on Learning with a Concurrent Verifier",
    "abstract": "Machine learning technologies have been used in a wide range of practical\nsystems. In practical situations, it is natural to expect the input-output\npairs of a machine learning model to satisfy some requirements. However, it is\ndifficult to obtain a model that satisfies requirements by just learning from\nexamples. A simple solution is to add a module that checks whether the\ninput-output pairs meet the requirements and then modifies the model's outputs.\nSuch a module, which we call a {\\em concurrent verifier} (CV), can give a\ncertification, although how the generalizability of the machine learning model\nchanges using a CV is unclear. This paper gives a generalization analysis of\nlearning with a CV. We analyze how the learnability of a machine learning model\nchanges with a CV and show a condition where we can obtain a guaranteed\nhypothesis using a verifier only in the inference time. We also show that\ntypical error bounds based on Rademacher complexity will be no larger than that\nof the original model when using a CV in multi-class classification and\nstructured prediction settings.",
    "descriptor": "",
    "authors": [
      "Masaaki Nishino",
      "Kengo Nakamura",
      "Norihito Yasuda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05331"
  },
  {
    "id": "arXiv:2210.05332",
    "title": "Gender Stereotyping Impact in Facial Expression Recognition",
    "abstract": "Facial Expression Recognition (FER) uses images of faces to identify the\nemotional state of users, allowing for a closer interaction between humans and\nautonomous systems. Unfortunately, as the images naturally integrate some\ndemographic information, such as apparent age, gender, and race of the subject,\nthese systems are prone to demographic bias issues. In recent years, machine\nlearning-based models have become the most popular approach to FER. These\nmodels require training on large datasets of facial expression images, and\ntheir generalization capabilities are strongly related to the characteristics\nof the dataset. In publicly available FER datasets, apparent gender\nrepresentation is usually mostly balanced, but their representation in the\nindividual label is not, embedding social stereotypes into the datasets and\ngenerating a potential for harm. Although this type of bias has been overlooked\nso far, it is important to understand the impact it may have in the context of\nFER. To do so, we use a popular FER dataset, FER+, to generate derivative\ndatasets with different amounts of stereotypical bias by altering the gender\nproportions of certain labels. We then proceed to measure the discrepancy\nbetween the performance of the models trained on these datasets for the\napparent gender groups. We observe a discrepancy in the recognition of certain\nemotions between genders of up to $29 \\%$ under the worst bias conditions. Our\nresults also suggest a safety range for stereotypical bias in a dataset that\ndoes not appear to produce stereotypical bias in the resulting model. Our\nfindings support the need for a thorough bias analysis of public datasets in\nproblems like FER, where a global balance of demographic representation can\nstill hide other types of bias that harm certain demographic groups.",
    "descriptor": "\nComments: Presented at SoGood 2022, The 7th Workshop on Data Science for Social Good, held in conjunction with ECML PKDD 2022, in September 2022, at Grenoble, France\n",
    "authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Mikel Galar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05332"
  },
  {
    "id": "arXiv:2210.05333",
    "title": "Routing Schemes for Hybrid Communication Networks in Unit-Disk Graphs",
    "abstract": "Hybrid communication networks provide multiple modes of communication with\nvarying characteristics. The $\\mathsf{HYRBID}$ model was introduced to unlock\ntheoretical study of such networks. It combines two fundamentally different\ncommunication modes. A \\emph{local} mode, which restricts communication among\nnodes to a given graph and a \\emph{global} mode where any two nodes may\ncommunicate in principle, but only very little such communication can take\nplace per unit of time. We are interested in the fast computation of\n\\emph{routing schemes}, where nodes have to compute small labels and routing\ntables that allow for efficient routing of messages in the local network, which\ntypically offers the majority of the throughput. Recent work has shown that\nusing the $\\mathsf{HYRBID}$ model admits a tremendous speed-up compared to what\nwould be possible if either communication mode were used in isolation. However,\nit was also shown that the computation of routing schemes takes polynomial\nrounds in the $\\mathsf{HYRBID}$ model if general graphs are used as local graph\neven when large labels are allowed. Coy et al.\\ [OPODIS'21] bypass this lower\nbound by restricting the local graph to unit-disc-graphs, and solve the problem\ndeterministically with running time, label size, and size of routing tables all\nin $O(\\log n)$. However they assume that the unit disk graph has no ``radio\nholes'', which makes the graph much simpler (topologically similar to a tree).\nIn this work we show how to generalize the algorithm to \\emph{any} unit disk\ngraph with running time $O(h^2 \\!+\\! \\log n)$ where $h$ is the number of radio\nholes, which preserves the prior guarantees if $h$ is small.",
    "descriptor": "",
    "authors": [
      "Sam Coy",
      "Artur Czumaj",
      "Christian Scheideler",
      "Philipp Schneider",
      "Julian Werthmann"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.05333"
  },
  {
    "id": "arXiv:2210.05335",
    "title": "MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training  Model",
    "abstract": "Multimodal semantic understanding often has to deal with uncertainty, which\nmeans the obtained message tends to refer to multiple targets. Such uncertainty\nis problematic for our interpretation, including intra-modal and inter-modal\nuncertainty. Little effort studies the modeling of this uncertainty,\nparticularly in pre-training on unlabeled datasets and fine-tuning in\ntask-specific downstream tasks. To address this, we project the representations\nof all modalities as probabilistic distributions via a Probability Distribution\nEncoder (PDE) by utilizing rich multimodal semantic information. Furthermore,\nwe integrate uncertainty modeling with popular pre-training frameworks and\npropose suitable pre-training tasks: Distribution-based Vision-Language\nContrastive learning (D-VLC), Distribution-based Masked Language Modeling\n(D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned\nmodels are applied to challenging downstream tasks, including image-text\nretrieval, visual question answering, visual reasoning, and visual entailment,\nand achieve state-of-the-art results. Code is released at\nhttps://github.com/IIGROUP/MAP.",
    "descriptor": "",
    "authors": [
      "Yatai Ji",
      "Junjie Wang",
      "Yuan Gong",
      "Lin Zhang",
      "Yanru Zhu",
      "Hongfa Wang",
      "Jiaxing Zhang",
      "Tetsuya Sakai",
      "Yujiu Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.05335"
  },
  {
    "id": "arXiv:2210.05336",
    "title": "Negation-as-Failure in the Base-extension Semantics for Intuitionistic  Propositional Logic",
    "abstract": "Proof-theoretic semantics (P-tS) is the paradigm of semantics in which\nmeaning in logic is based on proof (as opposed to truth). A particular instance\nof P-tS for intuitionistic propositional logic (IPL) is its\n\\emph{base-extension semantics} (B-eS). In this paper, we prove completeness of\nIPL with respect to the B-eS through logic programming (LP). This reveals that\ninherent to P-tS is \\emph{proof-search} and the \\emph{negation-as-failure}\nprotocol. Traditionally, the denial of a proposition is understood as the\nassertion of its negation, which is indeed the case in the model-theoretic\nsemantics for IPL, but the connection to LP reveals that in P-tS the denial of\na proposition refers to the failure of finding a proof of it. In this way,\nassertion and denial are both prime concepts in P-tS.",
    "descriptor": "\nComments: submitted\n",
    "authors": [
      "Alexander V. Gheorghiu",
      "David J. Pym"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05336"
  },
  {
    "id": "arXiv:2210.05337",
    "title": "SGD with large step sizes learns sparse features",
    "abstract": "We showcase important features of the dynamics of the Stochastic Gradient\nDescent (SGD) in the training of neural networks. We present empirical\nobservations that commonly used large step sizes (i) lead the iterates to jump\nfrom one side of a valley to the other causing loss stabilization, and (ii)\nthis stabilization induces a hidden stochastic dynamics orthogonal to the\nbouncing directions that biases it implicitly toward simple predictors.\nFurthermore, we show empirically that the longer large step sizes keep SGD high\nin the loss landscape valleys, the better the implicit regularization can\noperate and find sparse representations. Notably, no explicit regularization is\nused so that the regularization effect comes solely from the SGD training\ndynamics influenced by the step size schedule. Therefore, these observations\nunveil how, through the step size schedules, both gradient and noise drive\ntogether the SGD dynamics through the loss landscape of neural networks. We\njustify these findings theoretically through the study of simple neural network\nmodels as well as qualitative arguments inspired from stochastic processes.\nFinally, this analysis allows to shed a new light on some common practice and\nobserved phenomena when training neural networks. The code of our experiments\nis available at https://github.com/tml-epfl/sgd-sparse-features.",
    "descriptor": "",
    "authors": [
      "Maksym Andriushchenko",
      "Aditya Varre",
      "Loucas Pillaud-Vivien",
      "Nicolas Flammarion"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05337"
  },
  {
    "id": "arXiv:2210.05338",
    "title": "FusionDeepMF: A Dual Embedding based Deep Fusion Model for  Recommendation",
    "abstract": "Traditional Collaborative Filtering (CF) based methods are applied to\nunderstand the personal preferences of users/customers for items or products\nfrom the rating matrix. Usually, the rating matrix is sparse in nature. So\nthere are some improved variants of the CF method that apply the increasing\namount of side information to handle the sparsity problem. Only linear kernel\nor only non-linear kernel is applied in most of the available\nrecommendation-related work to understand user-item latent feature embeddings\nfrom data. Only linear kernel or only non-linear kernel is not sufficient to\nlearn complex user-item features from side information of users. Recently, some\nresearchers have focused on hybrid models that learn some features with\nnon-linear kernels and some other features with linear kernels. But it is very\ndifficult to understand which features can be learned accurately with linear\nkernels or with non-linear kernels. To overcome this problem, we propose a\nnovel deep fusion model named FusionDeepMF and the novel attempts of this model\nare i) learning user-item rating matrix and side information through linear and\nnon-linear kernel simultaneously, ii) application of a tuning parameter\ndetermining the trade-off between the dual embeddings that are generated from\nlinear and non-linear kernels. Extensive experiments on online review datasets\nestablish that FusionDeepMF can be remarkably futuristic compared to other\nbaseline approaches. Empirical evidence also shows that FusionDeepMF achieves\nbetter performances compared to the linear kernels of Matrix Factorization (MF)\nand the non-linear kernels of Multi-layer Perceptron (MLP).",
    "descriptor": "",
    "authors": [
      "Supriyo Mandal",
      "Abyayananda Maiti"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05338"
  },
  {
    "id": "arXiv:2210.05342",
    "title": "Formalizing May's Theorem",
    "abstract": "This report presents a formalization of May's theorem in the proof assistant\nCoq. It describes how the theorem statement is first translated into Coq\ndefinitions, and how it is subsequently proved. Various aspects of the proof\nand related work are discussed. To the best of the author's knowledge, this\nproject is the first documented attempt in mechanizing May's Theorem.",
    "descriptor": "",
    "authors": [
      "Kwing Hei Li"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05342"
  },
  {
    "id": "arXiv:2210.05343",
    "title": "Printing variability of copy detection patterns",
    "abstract": "Copy detection pattern (CDP) is a novel solution for products' protection\nagainst counterfeiting, which gains its popularity in recent years. CDP\nattracts the anti-counterfeiting industry due to its numerous benefits in\ncomparison to alternative protection techniques. Besides its attractiveness,\nthere is an essential gap in the fundamental analysis of CDP authentication\nperformance in large-scale industrial applications. It concerns variability of\nCDP parameters under different production conditions that include a type of\nprinter, substrate, printing resolution, etc. Since digital off-set printing\nrepresents great flexibility in terms of product personalized in comparison\nwith traditional off-set printing, it looks very interesting to address the\nabove concerns for digital off-set printers that are used by several companies\nfor the CDP protection of physical objects. In this paper, we thoroughly\ninvestigate certain factors impacting CDP. The experimental results obtained\nduring our study reveal some previously unknown results and raise new and even\nmore challenging questions. The results prove that it is a matter of great\nimportance to choose carefully the substrate or printer for CDP production.\nThis paper presents a new dataset produced by two industrial HP Indigo\nprinters. The similarity between printed CDP and the digital templates, from\nwhich they have been produced, is chosen as a simple measure in our study. We\nfound several particularities that might be of interest for large-scale\nindustrial applications.",
    "descriptor": "",
    "authors": [
      "Roman Chaban",
      "Olga Taran",
      "Joakim Tutt",
      "Yury Belousov",
      "Brian Pulfer",
      "Taras Holotyak",
      "Slava Voloshynovskiy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05343"
  },
  {
    "id": "arXiv:2210.05344",
    "title": "From Proof-theoretic Validity to Base-extension Semantics for  Intuitionistic Propositional Logic",
    "abstract": "There are two major approaches to proof-theoretic semantics:\nproof-theor\\-etic validity (P-tV) and base(-extension) semantics (B-eS). In\nthis paper, we show that for intuitionistic propositional logic (IPL) the\nlatter may be understood as capturing the declarative content of the former,\nwhich proceeds operationally on proofs. In so doing we expose a central problem\naround negation in proof-theoretic semantics -- viz. the semantic content of\n\\emph{ex falso quodlibet} for IPL. In P-tV, the rule follows as a vacuous\nimplication from the rejection of valid arguments for absurdity, but by\naugmenting the notion of \\emph{base} underpinning proof-theoretic semantics one\nmay use the rule to define absurdity (i.e., absurdity follows when all things\nare valid). This analysis clarifies the connexions between the two major\napproaches to proof-theoretic semantics and gives insight into what some\nmathematical requirements are in general for such semantics.",
    "descriptor": "",
    "authors": [
      "Alexander V. Gheorghiu",
      "David J. Pym"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05344"
  },
  {
    "id": "arXiv:2210.05346",
    "title": "Stateful Realizers for Nonstandard Analysis",
    "abstract": "In this paper we propose a new approach to realizability interpretations for\nnonstandard arithmetic. We deal with nonstandard analysis in the context of\n(semi)intuitionistic realizability, focusing on the Lightstone-Robinson\nconstruction of a model for nonstandard analysis through an ultrapower. In\nparticular, we consider an extension of the $\\lambda$-calculus with a memory\ncell, that contains an integer (the state), in order to indicate in which slice\nof the ultrapower $\\cal{M}^{\\mathbb{N}}$ the computation is being done. We pay\nattention to the nonstandard principles (and their computational content)\nobtainable in this setting. In particular, we give non-trivial realizers to\nIdealization and a non-standard version of the LLPO principle. We then discuss\nhow to quotient this product to mimic the Lightstone-Robinson construction.",
    "descriptor": "",
    "authors": [
      "Bruno Dinis",
      "\u00c9tienne Miquey"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05346"
  },
  {
    "id": "arXiv:2210.05348",
    "title": "Semantical Analysis of the Logic of Bunched Implications",
    "abstract": "We give a novel approach to proving soundness and completeness for a logic\n(henceforth: the object-logic) that bypasses truth-in-a-model to work directly\nwith validity. Instead of working with specific worlds in specific models, we\nreason with eigenworlds (i.e., generic representatives of worlds) in an\narbitrary model. This reasoning is captured by a sequent calculus for a\nmeta-logic (in this case, first-order classical logic) expressive enough to\ncapture the semantics of the object-logic. Essentially, one has a calculus of\nvalidity for the object-logic. The method proceeds through the perspective of\nreductive logic (as opposed to the more traditional paradigm of deductive\nlogic), using the space of reductions as a medium for showing the behavioural\nequivalence of reduction in the sequent calculus for the object-logic and in\nthe validity calculus. Rather than study the technique in general, we\nillustrate it for the logic of Bunched Implications (BI), thus IPL and MILL\n(without negation) are also treated. Intuitively, BI is the free combination of\nintuitionistic propositional logic and multiplicative intuitionistic linear\nlogic, which renders its meta-theory is quite complex. The literature on BI\ncontains many similar, but ultimately different, algebraic structures and\nsatisfaction relations that either capture only fragments of the logic (albeit\nlarge ones) or have complex clauses for certain connectives (e.g., Beth's\nclause for disjunction instead of Kripke's). It is this complexity that\nmotivates us to use BI as a case-study for this approach to semantics.",
    "descriptor": "\nComments: accepted\n",
    "authors": [
      "Alexander V. Gheorghiu",
      "David J. Pym"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05348"
  },
  {
    "id": "arXiv:2210.05349",
    "title": "Planar Manipulation via Learning Regrasping",
    "abstract": "Regrasping is important for robots to reorient objects in planar manipulation\ntasks. Different placements of objects can provide robots with alternative\ngrasp configurations, which are used in complex planar manipulation tasks that\nrequire multiple pick-rotate-and-place steps due to the constraints of the\nenvironment and robot kinematic constraints. In this work, our goal is to\ngenerate diverse placements of objects on the plane using deep neural networks.\nWe propose a pipeline with the stages of orientation generation, position\nrefinement, and placement discrimination to obtain accurate and diverse stable\nplacements based on the perception of point clouds. A large-scale dataset is\ncreated for training, including simulated placements and contact information\nbetween objects and the plane. The simulation results show that our pipeline\noutperforms the start-of-the-art, achieving an accuracy rate of 90% and a\ndiversity rate of 80% in simulation on generated placements. Our pipeline is\nalso validated in real-robot experiments. With the generated placements,\nsequential pick-rotate-and-place steps are calculated for the robot to reorient\nobjects to goal poses that are not reachable within one step. Videos and\ndataset are available at https://sites.google.com/view/pmvlr2022/.",
    "descriptor": "",
    "authors": [
      "Peng Xu",
      "Zhiyuan Chen",
      "Jiankun Wang",
      "Max Q.-H. Meng"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05349"
  },
  {
    "id": "arXiv:2210.05350",
    "title": "A new perspective on Digital Twins: Imparting intelligence and agency to  entities",
    "abstract": "Despite the Digital Twin (DT) concept being in the industry for a long time,\nit remains ambiguous, unable to differentiate itself from information models,\ngeneral computing, and simulation technologies. Part of this confusion stems\nfrom previous studies overlooking the DT's bidirectional nature, that enables\nthe shift of agency (delegating control) from humans to physical elements,\nsomething that was not possible with earlier technologies. Thus, we present DTs\nin a new light by viewing them as a means of imparting intelligence and agency\nto entities, emphasizing that DTs are not just expert-centric tools but are\nactive systems that extend the capabilities of the entities being twinned. This\nnew perspective on DTs can help reduce confusion and humanize the concept by\nstarting discussions about how intelligent a DT should be, and its roles and\nresponsibilities, as well as setting a long-term direction for DTs.",
    "descriptor": "",
    "authors": [
      "Ashwin Agrawal",
      "Vishal Singh",
      "Martin Fischer"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05350"
  },
  {
    "id": "arXiv:2210.05352",
    "title": "The Neumann boundary condition for the two-dimensional Lax-Wendroff  scheme",
    "abstract": "We study the stability of the two-dimensional Lax-Wendroff scheme with a\nstabilizer that approximates solutions to the transport equation. The problem\nis first analyzed in the whole space in order to show that the so-called energy\nmethod yields an optimal stability criterion for this finite difference scheme.\nWe then deal with the case of a half-space when the transport operator is\noutgoing. At the numerical level, we enforce the Neumann extrapolation boundary\ncondition and show that the corresponding scheme is stable. Eventually we\nanalyze the case of a quarter-space when the transport operator is outgoing\nwith respect to both sides. We then enforce the Neumann extrapolation boundary\ncondition on each side of the boundary and propose an extrapolation boundary\ncondition at the numerical corner in order to maintain stability for the whole\nnumerical scheme.",
    "descriptor": "",
    "authors": [
      "Jean-Fran\u00e7ois Coulombel",
      "Antoine Benoit"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05352"
  },
  {
    "id": "arXiv:2210.05355",
    "title": "Multi-User Reinforcement Learning with Low Rank Rewards",
    "abstract": "In this work, we consider the problem of collaborative multi-user\nreinforcement learning. In this setting there are multiple users with the same\nstate-action space and transition probabilities but with different rewards.\nUnder the assumption that the reward matrix of the $N$ users has a low-rank\nstructure -- a standard and practically successful assumption in the offline\ncollaborative filtering setting -- the question is can we design algorithms\nwith significantly lower sample complexity compared to the ones that learn the\nMDP individually for each user. Our main contribution is an algorithm which\nexplores rewards collaboratively with $N$ user-specific MDPs and can learn\nrewards efficiently in two key settings: tabular MDPs and linear MDPs. When $N$\nis large and the rank is constant, the sample complexity per MDP depends\nlogarithmically over the size of the state-space, which represents an\nexponential reduction (in the state-space size) when compared to the standard\n``non-collaborative'' algorithms.",
    "descriptor": "",
    "authors": [
      "Naman Agarwal",
      "Prateek Jain",
      "Suhas Kowshik",
      "Dheeraj Nagaraj",
      "Praneeth Netrapalli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05355"
  },
  {
    "id": "arXiv:2210.05356",
    "title": "Multi-User Redirected Walking in Separate Physical Spaces for Online VR  Scenarios",
    "abstract": "With the recent rise of Metaverse, online multiplayer VR applications are\nbecoming increasingly prevalent worldwide. Allowing users to move easily in\nvirtual environments is crucial for high-quality experiences in such\ncollaborative VR applications. This paper focuses on redirected walking\ntechnology (RDW) to allow users to move beyond the confines of the limited\nphysical environments (PE). The existing RDW methods lack the scheme to\ncoordinate multiple users in different PEs, and thus have the issue of\ntriggering too many resets for all the users. We propose a novel multi-user RDW\nmethod that is able to significantly reduce the overall reset number and give\nusers a better immersive experience by providing a more continuous exploration.\nOur key idea is to first find out the \"bottleneck\" user that may cause all\nusers to be reset and estimate the time to reset, and then redirect all the\nusers to favorable poses during that maximized bottleneck time to ensure the\nsubsequent resets can be postponed as much as possible. More particularly, we\ndevelop methods to estimate the time of possibly encountering obstacles and the\nreachable area for a specific pose to enable the prediction of the next reset\ncaused by any user. Our experiments and user study found that our method\noutperforms existing RDW methods in online VR applications.",
    "descriptor": "",
    "authors": [
      "Sen-Zhe Xu",
      "Jia-Hong Liu",
      "Miao Wang",
      "Fang-Lue Zhang",
      "Song-Hai Zhang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.05356"
  },
  {
    "id": "arXiv:2210.05357",
    "title": "Neighbourhood Representative Sampling for Efficient End-to-end Video  Quality Assessment",
    "abstract": "The increased resolution of real-world videos presents a dilemma between\nefficiency and accuracy for deep Video Quality Assessment (VQA). On the one\nhand, keeping the original resolution will lead to unacceptable computational\ncosts. On the other hand, existing practices, such as resizing and cropping,\nwill change the quality of original videos due to the loss of details and\ncontents, and are therefore harmful to quality assessment. With the obtained\ninsight from the study of spatial-temporal redundancy in the human visual\nsystem and visual coding theory, we observe that quality information around a\nneighbourhood is typically similar, motivating us to investigate an effective\nquality-sensitive neighbourhood representatives scheme for VQA. In this work,\nwe propose a unified scheme, spatial-temporal grid mini-cube sampling (St-GMS)\nto get a novel type of sample, named fragments. Full-resolution videos are\nfirst divided into mini-cubes with preset spatial-temporal grids, then the\ntemporal-aligned quality representatives are sampled to compose the fragments\nthat serve as inputs for VQA. In addition, we design the Fragment Attention\nNetwork (FANet), a network architecture tailored specifically for fragments.\nWith fragments and FANet, the proposed efficient end-to-end FAST-VQA and\nFasterVQA achieve significantly better performance than existing approaches on\nall VQA benchmarks while requiring only 1/1612 FLOPs compared to the current\nstate-of-the-art. Codes, models and demos are available at\nhttps://github.com/timothyhtimothy/FAST-VQA-and-FasterVQA.",
    "descriptor": "",
    "authors": [
      "Haoning Wu",
      "Chaofeng Chen",
      "Liang Liao",
      "Jingwen Hou",
      "Wenxiu Sun",
      "Qiong Yan",
      "Jinwei Gu",
      "Weisi Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.05357"
  },
  {
    "id": "arXiv:2210.05359",
    "title": "Mind's Eye: Grounded Language Model Reasoning through Simulation",
    "abstract": "Successful and effective communication between humans and AI relies on a\nshared experience of the world. By training solely on written text, current\nlanguage models (LMs) miss the grounded experience of humans in the real-world\n-- their failure to relate language to the physical world causes knowledge to\nbe misrepresented and obvious mistakes in their reasoning. We present Mind's\nEye, a paradigm to ground language model reasoning in the physical world. Given\na physical reasoning question, we use a computational physics engine\n(DeepMind's MuJoCo) to simulate the possible outcomes, and then use the\nsimulation results as part of the input, which enables language models to\nperform reasoning. Experiments on 39 tasks in a physics alignment benchmark\ndemonstrate that Mind's Eye can improve reasoning ability by a large margin\n(27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average).\nSmaller language models armed with Mind's Eye can obtain similar performance to\nmodels that are 100x larger. Finally, we confirm the robustness of Mind's Eye\nthrough ablation studies.",
    "descriptor": "",
    "authors": [
      "Ruibo Liu",
      "Jason Wei",
      "Shixiang Shane Gu",
      "Te-Yen Wu",
      "Soroush Vosoughi",
      "Claire Cui",
      "Denny Zhou",
      "Andrew M. Dai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05359"
  },
  {
    "id": "arXiv:2210.05361",
    "title": "Uncertainty-Aware Unsupervised Image Deblurring with Deep Priors Guided  by Domain Knowledge",
    "abstract": "Non-blind deblurring methods achieve decent performance under the accurate\nblur kernel assumption. Since the kernel error is inevitable in practice,\nringing artifacts are often introduced by non-blind deblurring. Recently,\nsemi-blind deblurring methods can handle kernel uncertainty by introducing the\nprior of the kernel (or induced) error. However, how to design a suitable prior\nfor the kernel (or induced) error remains challenging. Hand-crafted prior,\nincorporating domain knowledge, generally performs well but may lead to poor\nperformance when kernel (or induced) error is complex. Data-driven prior, which\nexcessively depends on the diversity and abundance of training data, is\nvulnerable to out-of-distribution blurs and images. To address this challenge,\nwe suggest a data-free deep prior for the kernel induced error (termed as\nresidual) expressed by a customized untrained deep neural network, which allows\nus to flexibly adapt to different blurs and images in real scenarios. By\norganically integrating the respective strengths of deep priors and\nhand-crafted priors, we propose an unsupervised semi-blind deblurring model\nwhich recovers the latent image from the blurry image and inaccurate blur\nkernel. To tackle the formulated model, an efficient alternating minimization\nalgorithm is developed. Extensive experiments demonstrate the superiority of\nthe proposed method to both data-driven prior and hand-crafted prior based\nmethods in terms of the image quality and the robustness to the kernel error.",
    "descriptor": "",
    "authors": [
      "Xiaole Tang",
      "Xile Zhao",
      "Jun Liu",
      "Jianli Wang",
      "Yuchun Miao",
      "Tieyong Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05361"
  },
  {
    "id": "arXiv:2210.05364",
    "title": "Hybrid MBlur: Using Ray Tracing to Solve the Partial Occlusion Artifacts  in Real-Time Rendering of Motion Blur Effect",
    "abstract": "For a foreground object in motion, details of its background which would\notherwise be hidden are uncovered through its inner blur. This paper presents a\nnovel hybrid motion blur rendering technique combining post-process image\nfiltering and hardware-accelerated ray tracing. In each frame, we advance rays\nrecursively into the scene to retrieve background information for inner blur\nregions and apply a post-process filtering pass on the ray-traced background\nand rasterized colour before compositing them together. Our approach achieves\nmore accurate partial occlusion semi-transparencies for moving objects while\nmaintaining interactive frame rates.",
    "descriptor": "",
    "authors": [
      "Yu Wei Tan",
      "Xiaohan Cui",
      "Anand Bhojan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.05364"
  },
  {
    "id": "arXiv:2210.05365",
    "title": "Cloud-Assisted Hybrid Rendering for Thin-Client Games and VR  Applications",
    "abstract": "We introduce a novel distributed rendering approach to generate high-quality\ngraphics in thin-client games and VR applications. Many mobile devices have\nlimited computational power to achieve ray tracing in real-time. Hence,\nhardware-accelerated cloud servers can perform ray tracing instead and have\ntheir output streamed to clients in remote rendering. Applying the approach of\ndistributed hybrid rendering, we leverage the computational capabilities of\nboth the thin client and powerful server by performing rasterization locally\nwhile offloading ray tracing to the server. With advancements in 5G technology,\nthe server and client can communicate effectively over the network and work\ntogether to produce a high-quality output while maintaining interactive frame\nrates. Our approach can achieve better visuals as compared to local rendering\nbut faster performance as compared to remote rendering.",
    "descriptor": "",
    "authors": [
      "Yu Wei Tan",
      "Louiz Kim-Chan",
      "Anthony Halim",
      "Anand Bhojan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.05365"
  },
  {
    "id": "arXiv:2210.05366",
    "title": "Race Bias Analysis of Bona Fide Errors in face anti-spoofing",
    "abstract": "The study of bias in Machine Learning is receiving a lot of attention in\nrecent years, however, few only papers deal explicitly with the problem of race\nbias in face anti-spoofing. In this paper, we present a systematic study of\nrace bias in face anti-spoofing with three key characteristics: the focus is on\nanalysing potential bias in the bona fide errors, where significant ethical and\nlegal issues lie; the analysis is not restricted to the final binary outcomes\nof the classifier, but also covers the classifier's scalar responses and its\nlatent space; the threshold determining the operating point of the classifier\nis considered a variable. We demonstrate the proposed bias analysis process on\na VQ-VAE based face anti-spoofing algorithm, trained on the Replay Attack and\nthe Spoof in the Wild (SiW) databases, and analysed for bias on the SiW and\nRacial Faces in the Wild (RFW), databases. The results demonstrate that race\nbias is not necessarily the result of different mean response values among the\nvarious populations. Instead, it can be better understood as the combined\neffect of several possible characteristics of the response distributions:\ndifferent means; different variances; bimodal behaviour; existence of outliers.",
    "descriptor": "\nComments: 15 pages, 11 figures\n",
    "authors": [
      "Latifah Abduh",
      "Ioannis Ivrissimtzis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05366"
  },
  {
    "id": "arXiv:2210.05367",
    "title": "Learning Credit Assignment for Cooperative Reinforcement Learning",
    "abstract": "Cooperative multi-agent policy gradient (MAPG) algorithms have recently\nattracted wide attention and are regarded as a general scheme for the\nmulti-agent system. Credit assignment plays an important role in MAPG and can\ninduce cooperation among multiple agents. However, most MAPG algorithms cannot\nachieve good credit assignment because of the game-theoretic pathology known as\n\\textit{centralized-decentralized mismatch}. To address this issue, this paper\npresents a novel method, \\textit{\\underline{M}ulti-\\underline{A}gent\n\\underline{P}olarization \\underline{P}olicy \\underline{G}radient} (MAPPG).\nMAPPG takes a simple but efficient polarization function to transform the\noptimal consistency of joint and individual actions into easily realized\nconstraints, thus enabling efficient credit assignment in MAPG. Theoretically,\nwe prove that individual policies of MAPPG can converge to the global optimum.\nEmpirically, we evaluate MAPPG on the well-known matrix game and differential\ngame, and verify that MAPPG can converge to the global optimum for both\ndiscrete and continuous action spaces. We also evaluate MAPPG on a set of\nStarCraft II micromanagement tasks and demonstrate that MAPPG outperforms the\nstate-of-the-art MAPG algorithms.",
    "descriptor": "",
    "authors": [
      "Wubing Chen",
      "Wenbin Li",
      "Xiao Liu",
      "Shangdong Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05367"
  },
  {
    "id": "arXiv:2210.05370",
    "title": "DeepPerform: An Efficient Approach for Performance Testing of  Resource-Constrained Neural Networks",
    "abstract": "Today, an increasing number of Adaptive Deep Neural Networks (AdNNs) are\nbeing used on resource-constrained embedded devices. We observe that, similar\nto traditional software, redundant computation exists in AdNNs, resulting in\nconsiderable performance degradation. The performance degradation is dependent\non the input and is referred to as input-dependent performance bottlenecks\n(IDPBs). To ensure an AdNN satisfies the performance requirements of\nresource-constrained applications, it is essential to conduct performance\ntesting to detect IDPBs in the AdNN. Existing neural network testing methods\nare primarily concerned with correctness testing, which does not involve\nperformance testing. To fill this gap, we propose DeepPerform, a scalable\napproach to generate test samples to detect the IDPBs in AdNNs. We first\ndemonstrate how the problem of generating performance test samples detecting\nIDPBs can be formulated as an optimization problem. Following that, we\ndemonstrate how DeepPerform efficiently handles the optimization problem by\nlearning and estimating the distribution of AdNNs' computational consumption.\nWe evaluate DeepPerform on three widely used datasets against five popular AdNN\nmodels. The results show that DeepPerform generates test samples that cause\nmore severe performance degradation (FLOPs: increase up to 552\\%). Furthermore,\nDeepPerform is substantially more efficient than the baseline methods in\ngenerating test inputs(runtime overhead: only 6-10 milliseconds).",
    "descriptor": "\nComments: This paper is accepted to IEEE/ACM International Conference on Automated Software Engineering 2022\n",
    "authors": [
      "Simin Chen",
      "Mirazul Haque",
      "Cong Liu",
      "Wei Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05370"
  },
  {
    "id": "arXiv:2210.05371",
    "title": "A global analysis of global optimisation",
    "abstract": "Theoretical understanding of the training of deep neural networks has made\ngreat strides in recent years. In particular, it has been shown that sufficient\nwidth and sufficiently small learning rate suffice to guarantee that chain\nnetworks trained with the square cost converge to global minima close to\ninitialisation. However, this theory cannot apply to the cross-entropy cost,\nwhose global minima exit only at infinity. In this paper, we introduce a\ngeneral theoretical framework, designed for the study of optimisation, that\nencompasses ubiquitous architectural choices including batch normalisation,\nweight normalisation and skip connections. We use our framework to conduct a\nglobal analysis of the curvature and regularity properties of neural network\nloss landscapes, and give two applications. First, we give the first proof that\na class of deep neural networks can be trained using gradient descent to global\noptima even when such optima only exist at infinity. Second, we use the theory\nin an empirical analysis of the effect of residual connections on training\nspeed, which we verify with ResNets on MNIST, CIFAR10 and CIFAR100.",
    "descriptor": "",
    "authors": [
      "Lachlan Ewen MacDonald",
      "Hemanth Saratchandran",
      "Jack Valmadre",
      "Simon Lucey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05371"
  },
  {
    "id": "arXiv:2210.05372",
    "title": "DEPTWEET: A Typology for Social Media Texts to Detect Depression  Severities",
    "abstract": "Mental health research through data-driven methods has been hindered by a\nlack of standard typology and scarcity of adequate data. In this study, we\nleverage the clinical articulation of depression to build a typology for social\nmedia texts for detecting the severity of depression. It emulates the standard\nclinical assessment procedure Diagnostic and Statistical Manual of Mental\nDisorders (DSM-5) and Patient Health Questionnaire (PHQ-9) to encompass subtle\nindications of depressive disorders from tweets. Along with the typology, we\npresent a new dataset of 40191 tweets labeled by expert annotators. Each tweet\nis labeled as 'non-depressed' or 'depressed'. Moreover, three severity levels\nare considered for 'depressed' tweets: (1) mild, (2) moderate, and (3) severe.\nAn associated confidence score is provided with each label to validate the\nquality of annotation. We examine the quality of the dataset via representing\nsummary statistics while setting strong baseline results using attention-based\nmodels like BERT and DistilBERT. Finally, we extensively address the\nlimitations of the study to provide directions for further research.",
    "descriptor": "\nComments: 17 pages, 6 figures, 6 tables, Accepted in Computers in Human Behavior\n",
    "authors": [
      "Mohsinul Kabir",
      "Tasnim Ahmed",
      "Md. Bakhtiar Hasan",
      "Md Tahmid Rahman Laskar",
      "Tarun Kumar Joarder",
      "Hasan Mahmud",
      "Kamrul Hasan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05372"
  },
  {
    "id": "arXiv:2210.05373",
    "title": "Stable and Efficient Adversarial Training through Local Linearization",
    "abstract": "There has been a recent surge in single-step adversarial training as it shows\nrobustness and efficiency. However, a phenomenon referred to as ``catastrophic\noverfitting\" has been observed, which is prevalent in single-step defenses and\nmay frustrate attempts to use FGSM adversarial training. To address this issue,\nwe propose a novel method, Stable and Efficient Adversarial Training (SEAT),\nwhich mitigates catastrophic overfitting by harnessing on local properties that\ndistinguish a robust model from that of a catastrophic overfitted model. The\nproposed SEAT has strong theoretical justifications, in that minimizing the\nSEAT loss can be shown to favour smooth empirical risk, thereby leading to\nrobustness. Experimental results demonstrate that the proposed method\nsuccessfully mitigates catastrophic overfitting, yielding superior performance\namongst efficient defenses. Our single-step method can reach 51% robust\naccuracy for CIFAR-10 with $l_\\infty$ perturbations of radius $8/255$ under a\nstrong PGD-50 attack, matching the performance of a 10-step iterative\nadversarial training at merely 3% computational cost.",
    "descriptor": "",
    "authors": [
      "Zhuorong Li",
      "Daiwei Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05373"
  },
  {
    "id": "arXiv:2210.05375",
    "title": "A randomized operator splitting scheme inspired by stochastic  optimization methods",
    "abstract": "In this paper, we combine the operator splitting methodology for abstract\nevolution equations with that of stochastic methods for large-scale\noptimization problems. The combination results in a randomized splitting\nscheme, which in a given time step does not necessarily use all the parts of\nthe split operator. This is in contrast to deterministic splitting schemes\nwhich always use every part at least once, and often several times. As a\nresult, the computational cost can be significantly decreased in comparison to\nsuch methods. We rigorously define a randomized operator splitting scheme in an\nabstract setting and provide an error analysis where we prove that the temporal\nconvergence order of the scheme is at least 1/2. We illustrate the theory by\nnumerical experiments on both linear and quasilinear diffusion problems, using\na randomized domain decomposition approach. We conclude that choosing the\nrandomization in certain ways may improve the order to 1. This is as accurate\nas applying e.g. backward (implicit) Euler to the full problem, without\nsplitting.",
    "descriptor": "",
    "authors": [
      "Monika Eisenmann",
      "Tony Stillfjord"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05375"
  },
  {
    "id": "arXiv:2210.05382",
    "title": "Break the Wall Between Homophily and Heterophily for Graph  Representation Learning",
    "abstract": "Homophily and heterophily are intrinsic properties of graphs that describe\nwhether two linked nodes share similar properties. Although many Graph Neural\nNetwork (GNN) models have been proposed, it remains unclear how to design a\nmodel so that it can generalize well to the whole spectrum of homophily. This\nwork addresses the challenge by identifying three graph features, including the\nego node feature, the aggregated node feature, and the graph structure feature,\nthat are essential for graph representation learning. It further proposes a new\nGNN model called OGNN (Omnipotent Graph Neural Network) that extracts all three\ngraph features and adaptively fuses them to achieve generalizability across the\nwhole spectrum of homophily. Extensive experiments on both synthetic and real\ndatasets demonstrate the superiority (average rank 1.56) of our OGNN compared\nwith state-of-the-art methods.",
    "descriptor": "\nComments: 15 pages, 2 figures\n",
    "authors": [
      "Xiao Liu",
      "Lijun Zhang",
      "Hui Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05382"
  },
  {
    "id": "arXiv:2210.05384",
    "title": "Morphing Planar Graph Drawings Through 3D",
    "abstract": "In this paper, we investigate crossing-free 3D morphs between planar\nstraight-line drawings. We show that, for any two (not necessarily\ntopologically equivalent) planar straight-line drawings of an $n$-vertex planar\ngraph, there exists a piecewise-linear crossing-free 3D morph with $O(n^2)$\nsteps that transforms one drawing into the other. We also give some evidence\nwhy it is difficult to obtain a linear lower bound (which exists in 2D) for the\nnumber of steps of a crossing-free 3D morph.",
    "descriptor": "",
    "authors": [
      "Kevin Buchin",
      "Will Evans",
      "Fabrizio Frati",
      "Irina Kostitsyna",
      "Maarten L\u00f6ffler",
      "Tim Ophelders",
      "Alexander Wolff"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.05384"
  },
  {
    "id": "arXiv:2210.05385",
    "title": "Enhancing Branch-and-Bound for Multi-Objective 0-1 Programming",
    "abstract": "In the bi-objective branch-and-bound literature, a key ingredient is\nobjective branching, i.e. to create smaller and disjoint sub-problems in the\nobjective space, obtained from the partial dominance of the lower bound set by\nthe upper bound set. When considering three or more objective functions,\nhowever, applying objective branching becomes more complex, and its benefit has\nso far been unclear. In this paper, we investigate several ingredients which\nallow to better exploit objective branching in a multi-objective setting. We\nextend the idea of probing to multiple objectives, enhance it in several ways,\nand show that when coupled with objective branching, it results in significant\nspeed-ups in terms of CPU times. We also investigate cut generation based on\nthe objective branching constraints. Besides, we generalize the best-bound idea\nfor node selection to multiple objectives and we show that the proposed rules\noutperform the, in the multi-objective literature, commonly employed\ndepth-first and breadth-first strategies. We also analyze problem specific\nbranching rules. We test the proposed ideas on available benchmark instances\nfor three problem classes with three and four objectives, namely the\ncapacitated facility location problem, the uncapacitated facility location\nproblem, and the knapsack problem. Our enhanced multi-objective\nbranch-and-bound algorithm outperforms the best existing branch-and-bound based\napproach and is the first to obtain competitive and even slightly better\nresults than a state-of-the-art objective space search method on a subset of\nthe problem classes.",
    "descriptor": "",
    "authors": [
      "Nicolas Forget",
      "Sophie N. Parragh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.05385"
  },
  {
    "id": "arXiv:2210.05387",
    "title": "Sequential Ensembling for Semantic Segmentation",
    "abstract": "Ensemble approaches for deep-learning-based semantic segmentation remain\ninsufficiently explored despite the proliferation of competitive benchmarks and\ndownstream applications. In this work, we explore and benchmark the popular\nensembling approach of combining predictions of multiple,\nindependently-trained, state-of-the-art models at test time on popular\ndatasets. Furthermore, we propose a novel method inspired by boosting to\nsequentially ensemble networks that significantly outperforms the naive\nensemble baseline. Our approach trains a cascade of models conditioned on class\nprobabilities predicted by the previous model as an additional input. A key\nbenefit of this approach is that it allows for dynamic computation offloading,\nwhich helps deploy models on mobile devices. Our proposed novel ADaptive\nmodulatiON (ADON) block allows spatial feature modulation at various layers\nusing previous-stage probabilities. Our approach does not require sophisticated\nsample selection strategies during training and works with multiple neural\narchitectures. We significantly improve over the naive ensemble baseline on\nchallenging datasets such as Cityscapes, ADE-20K, COCO-Stuff, and\nPASCAL-Context and set a new state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Rawal Khirodkar",
      "Brandon Smith",
      "Siddhartha Chandra",
      "Amit Agrawal",
      "Antonio Criminisi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05387"
  },
  {
    "id": "arXiv:2210.05391",
    "title": "PP-StructureV2: A Stronger Document Analysis System",
    "abstract": "A large amount of document data exists in unstructured form such as raw\nimages without any text information. Designing a practical document image\nanalysis system is a meaningful but challenging task. In previous work, we\nproposed an intelligent document analysis system PP-Structure. In order to\nfurther upgrade the function and performance of PP-Structure, we propose\nPP-StructureV2 in this work, which contains two subsystems: Layout Information\nExtraction and Key Information Extraction. Firstly, we integrate Image\nDirection Correction module and Layout Restoration module to enhance the\nfunctionality of the system. Secondly, 8 practical strategies are utilized in\nPP-StructureV2 for better performance. For Layout Analysis model, we introduce\nultra light-weight detector PP-PicoDet and knowledge distillation algorithm FGD\nfor model lightweighting, which increased the inference speed by 11 times with\ncomparable mAP. For Table Recognition model, we utilize PP-LCNet, CSP-PAN and\nSLAHead to optimize the backbone module, feature fusion module and decoding\nmodule, respectively, which improved the table structure accuracy by 6\\% with\ncomparable inference speed. For Key Information Extraction model, we introduce\nVI-LayoutXLM which is a visual-feature independent LayoutXLM architecture,\nTB-YX sorting algorithm and U-DML knowledge distillation algorithm, which\nbrought 2.8\\% and 9.1\\% improvement respectively on the Hmean of Semantic\nEntity Recognition and Relation Extraction tasks. All the above mentioned\nmodels and code are open-sourced in the GitHub repository PaddleOCR.",
    "descriptor": "",
    "authors": [
      "Chenxia Li",
      "Ruoyu Guo",
      "Jun Zhou",
      "Mengtao An",
      "Yuning Du",
      "Lingfeng Zhu",
      "Yi Liu",
      "Xiaoguang Hu",
      "Dianhai Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05391"
  },
  {
    "id": "arXiv:2210.05392",
    "title": "TGDM: Target Guided Dynamic Mixup for Cross-Domain Few-Shot Learning",
    "abstract": "Given sufficient training data on the source domain, cross-domain few-shot\nlearning (CD-FSL) aims at recognizing new classes with a small number of\nlabeled examples on the target domain. The key to addressing CD-FSL is to\nnarrow the domain gap and transferring knowledge of a network trained on the\nsource domain to the target domain. To help knowledge transfer, this paper\nintroduces an intermediate domain generated by mixing images in the source and\nthe target domain. Specifically, to generate the optimal intermediate domain\nfor different target data, we propose a novel target guided dynamic mixup\n(TGDM) framework that leverages the target data to guide the generation of\nmixed images via dynamic mixup. The proposed TGDM framework contains a Mixup-3T\nnetwork for learning classifiers and a dynamic ratio generation network (DRGN)\nfor learning the optimal mix ratio. To better transfer the knowledge, the\nproposed Mixup-3T network contains three branches with shared parameters for\nclassifying classes in the source domain, target domain, and intermediate\ndomain. To generate the optimal intermediate domain, the DRGN learns to\ngenerate an optimal mix ratio according to the performance on auxiliary target\ndata. Then, the whole TGDM framework is trained via bi-level meta-learning so\nthat TGDM can rectify itself to achieve optimal performance on target data.\nExtensive experimental results on several benchmark datasets verify the\neffectiveness of our method.",
    "descriptor": "\nComments: accepted by ACM MM 2022\n",
    "authors": [
      "Linhai Zhuo",
      "Yuqian Fu",
      "Jingjing Chen",
      "Yixin Cao",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05392"
  },
  {
    "id": "arXiv:2210.05393",
    "title": "Scaling Directed Controller Synthesis via Reinforcement Learning",
    "abstract": "Directed Controller Synthesis technique finds solutions for the non-blocking\nproperty in discrete event systems by exploring a reduced portion of the\nexponentially big state space, using best-first search. Aiming to minimize the\nexplored states, it is currently guided by a domain-independent handcrafted\nheuristic, with which it reaches state-of-the-art performance. In this work, we\npropose a new method for obtaining heuristics based on Reinforcement Learning.\nThe synthesis algorithm is framed as an RL task with an unbounded action space\nand a modified version of DQN is used. With a simple and general set of\nfeatures, we show that it is possible to learn heuristics on small versions of\na problem in a way that generalizes to the larger instances. Our agents learn\nfrom scratch and outperform the existing heuristic overall, in instances unseen\nduring training.",
    "descriptor": "\nComments: 9 pages, 1 figure, pre-print. Comments are very welcome\n",
    "authors": [
      "Tom\u00e1s Delgado",
      "V\u00edctor Braberman",
      "Sebastian Uchitel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05393"
  },
  {
    "id": "arXiv:2210.05394",
    "title": "Computationally-efficient initialisation of GPs: The generalised  variogram method",
    "abstract": "We present a computationally-efficient strategy to find the hyperparameters\nof a Gaussian process (GP) avoiding the computation of the likelihood function.\nThe found hyperparameters can then be used directly for regression or passed as\ninitial conditions to maximum-likelihood (ML) training. Motivated by the fact\nthat training a GP via ML is equivalent (on average) to minimising the\nKL-divergence between the true and learnt model, we set to explore different\nmetrics/divergences among GPs that are computationally inexpensive and provide\nestimates close to those of ML. In particular, we identify the GP\nhyperparameters by matching the empirical covariance to a parametric candidate,\nproposing and studying various measures of discrepancy. Our proposal extends\nthe Variogram method developed by the geostatistics literature and thus is\nreferred to as the Generalised Variogram method (GVM). In addition to the\ntheoretical presentation of GVM, we provide experimental validation in terms of\naccuracy, consistency with ML and computational complexity for different\nkernels using synthetic and real-world data.",
    "descriptor": "",
    "authors": [
      "Felipe Tobar",
      "Elsa Cazelles",
      "Taco de Wolff"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05394"
  },
  {
    "id": "arXiv:2210.05396",
    "title": "MIMO Capacity Characterization for Movable Antenna Systems",
    "abstract": "In this paper, we propose a new multiple-input multiple-output (MIMO)\ncommunication system with movable antennas (MAs) to exploit the antenna\nposition optimization for enhancing the capacity. Different from conventional\nMIMO systems with fixed-position antennas (FPAs), the proposed system can\nflexibly change the positions of transmit/receive MAs, such that the MIMO\nchannel between them is reconfigured to achieve higher capacity. We aim to\ncharacterize the capacity of MA-enabled point-to-point MIMO communication\nsystems, by jointly optimizing the positions of transmit and receive MAs as\nwell as the covariance of transmit signals. First, we develop an efficient\nalternating optimization algorithm to find a locally optimal solution by\niteratively optimizing the transmit covariance matrix and the position of each\ntransmit/receive MA with the other variables being fixed. Next, we propose\nalternative algorithms of lower complexity for capacity maximization in the\nlow-SNR regime and for the multiple-input single-output (MISO) and single-input\nmultiple-output (SIMO) cases. Numerical results show that our proposed MA\nsystems significantly improve the MIMO channel capacity compared to traditional\nFPA systems as well as various benchmark schemes, and useful insights are drawn\ninto the capacity gains of MA systems.",
    "descriptor": "",
    "authors": [
      "Wenyan Ma",
      "Lipeng Zhu",
      "Rui Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05396"
  },
  {
    "id": "arXiv:2210.05397",
    "title": "Analysis of Expected Hitting Time for Designing Evolutionary Neural  Architecture Search Algorithms",
    "abstract": "Evolutionary computation-based neural architecture search (ENAS) is a popular\ntechnique for automating architecture design of deep neural networks. In recent\nyears, various ENAS algorithms have been proposed and shown promising\nperformance on diverse real-world applications. In contrast to these\ngroundbreaking applications, there is no theoretical guideline for assigning a\nreasonable running time (mainly affected by the generation number, population\nsize, and evolution operator) given both the anticipated performance and\nacceptable computation budget on ENAS problems. The expected hitting time\n(EHT), which refers to the average generations, is considered to analyze the\nrunning time of ENAS algorithms. This paper proposes a general framework for\nestimating the EHT of ENAS algorithms, which includes common configuration,\nsearch space partition, transition probability estimation, and hitting time\nanalysis. By exploiting the proposed framework, we consider the so-called\n($\\lambda$+$\\lambda$)-ENAS algorithms with different mutation operators and\nmanage to estimate the lower bounds of the EHT {which are critical for the\nalgorithm to find the global optimum}. Furthermore, we study the theoretical\nresults on the NAS-Bench-101 architecture searching problem, and the results\nshow that the one-bit mutation with \"bit-based fair mutation\" strategy needs\nless time than the \"offspring-based fair mutation\" strategy, and the bitwise\nmutation operator needs less time than the $q$-bit mutation operator. To the\nbest of our knowledge, this is the first work focusing on the theory of ENAS,\nand the above observation will be substantially helpful in designing efficient\nENAS algorithms.",
    "descriptor": "",
    "authors": [
      "Zeqiong Lv",
      "Chao Qian",
      "Gary G. Yen",
      "Yanan Sun"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.05397"
  },
  {
    "id": "arXiv:2210.05398",
    "title": "Continual Learning by Modeling Intra-Class Variation",
    "abstract": "It has been observed that neural networks perform poorly when the data or\ntasks are presented sequentially. Unlike humans, neural networks suffer greatly\nfrom catastrophic forgetting, making it impossible to perform life-long\nlearning. To address this issue, memory-based continual learning has been\nactively studied and stands out as one of the best-performing methods. We\nexamine memory-based continual learning and identify that large variation in\nthe representation space is crucial for avoiding catastrophic forgetting.\nMotivated by this, we propose to diversify representations by using two types\nof perturbations: model-agnostic variation (i.e., the variation is generated\nwithout the knowledge of the learned neural network) and model-based variation\n(i.e., the variation is conditioned on the learned neural network). We\ndemonstrate that enlarging representational variation serves as a general\nprinciple to improve continual learning. Finally, we perform empirical studies\nwhich demonstrate that our method, as a simple plug-and-play component, can\nconsistently improve a number of memory-based continual learning methods by a\nlarge margin.",
    "descriptor": "\nComments: Technical Report (23 pages, 13 figures)\n",
    "authors": [
      "Longhui Yu",
      "Tianyang Hu",
      "Lanqing Hong",
      "Zhen Liu",
      "Adrian Weller",
      "Weiyang Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05398"
  },
  {
    "id": "arXiv:2210.05401",
    "title": "Not Good Times for Lies: Misinformation Detection on the Russia-Ukraine  War, COVID-19, and Refugees",
    "abstract": "Misinformation spread in online social networks is an urgent-to-solve problem\nhaving harmful consequences that threaten human health, public safety,\neconomics, and so on. In this study, we construct a novel dataset, called\nMiDe-22, having 5,284 English and 5,064 Turkish tweets with their\nmisinformation labels under several recent events, including the Russia-Ukraine\nwar, COVID-19 pandemic, and Refugees. Moreover, we provide the user engagements\nto the tweets in terms of likes, replies, retweets, and quotes. We present a\ndetailed data analysis with descriptive statistics and temporal analysis, and\nprovide the experimental results of a benchmark evaluation for misinformation\ndetection on our novel dataset.",
    "descriptor": "",
    "authors": [
      "Cagri Toraman",
      "Oguzhan Ozcelik",
      "Furkan \u015eahinu\u00e7",
      "Fazli Can"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05401"
  },
  {
    "id": "arXiv:2210.05403",
    "title": "Hierarchical Categories in Colored Searching",
    "abstract": "In colored range counting (CRC), the input is a set of points where each\npoint is assigned a ``color'' (or a ``category'') and the goal is to store them\nin a data structure such that the number of distinct categories inside a given\nquery range can be counted efficiently. CRC has strong motivations as it allows\ndata structure to deal with categorical data. However, colors (i.e., the\ncategories) in the CRC problem do not have any internal structure, whereas this\nis not the case for many datasets in practice where hierarchical categories\nexists or where a single input belongs to multiple categories. Motivated by\nthese, we consider variants of the problem where such structures can be\nrepresented. We define two variants of the problem called hierarchical range\ncounting (HCC) and sub-category colored range counting (SCRC) and consider\nhierarchical structures that can either be a DAG or a tree. We show that the\ntwo problems on some special trees are in fact equivalent to other well-known\nproblems in the literature. Based on these, we also give efficient data\nstructures when the underlying hierarchy can be represented as a tree. We show\na conditional lower bound for the general case when the existing hierarchy can\nbe any DAG, through reductions from the orthogonal vectors problem.",
    "descriptor": "",
    "authors": [
      "Peyman Afshani",
      "Rasmus Killman",
      "Kasper Green Larsen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.05403"
  },
  {
    "id": "arXiv:2210.05404",
    "title": "Machine Translation between Spoken Languages and Signed Languages  Represented in SignWriting",
    "abstract": "This paper presents work on novel machine translation (MT) systems between\nspoken and signed languages, where signed languages are represented in\nSignWriting, a sign language writing system. Our work seeks to address the lack\nof out-of-the-box support for signed languages in current MT systems and is\nbased on the SignBank dataset, which contains pairs of spoken language text and\nSignWriting content. We introduce novel methods to parse, factorize, decode,\nand evaluate SignWriting, leveraging ideas from neural factored MT. In a\nbilingual setup--translating from American Sign Language to (American)\nEnglish--our method achieves over 30 BLEU, while in two multilingual\nsetups--translating in both directions between spoken languages and signed\nlanguages--we achieve over 20 BLEU. We find that common MT techniques used to\nimprove spoken language translation similarly affect the performance of sign\nlanguage translation. These findings validate our use of an intermediate text\nrepresentation for signed languages to include them in natural language\nprocessing research.",
    "descriptor": "",
    "authors": [
      "Zifan Jiang",
      "Amit Moryossef",
      "Mathias M\u00fcller",
      "Sarah Ebling"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05404"
  },
  {
    "id": "arXiv:2210.05405",
    "title": "From Earth to Space: A First Deployment of 5G Core Network on Satellite",
    "abstract": "Recent developments in the aerospace industry have led to a dramatic\nreduction in the manufacturing and launch costs of low Earth orbit satellites.\nThe new trend enables the paradigm shift of satellite-terrestrial integrated\nnetworks with global coverage. In particular, the integration of 5G\ncommunication systems and satellites has the potential to restructure\nnext-generation mobile networks. By leveraging the network function\nvirtualization and network slicing, the orbital 5G core networks will\nfacilitate the coordination and management of network functions in\nsatellite-terrestrial integrated networks. We are the first to deploy a\nlightweight 5G core network on a real-world satellite to investigate its\nfeasibility. We conducted experiments to validate the onboard 5G core network\nfunctions. The validated procedures include registration and session setup\nprocedures. The results show that the 5G core network can function normally and\ngenerate correct signaling.",
    "descriptor": "\nComments: This paper has been accepted by China Communications\n",
    "authors": [
      "Ruolin Xing",
      "Xiao Ma",
      "Ao Zhou",
      "Schahram Dustdar",
      "Shangguang Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.05405"
  },
  {
    "id": "arXiv:2210.05406",
    "title": "Code Librarian: A Software Package Recommendation System",
    "abstract": "The use of packaged libraries can significantly shorten the software\ndevelopment cycle by improving the quality and readability of code. In this\npaper, we present a recommendation engine called Librarian for open source\nlibraries. A candidate library package is recommended for a given context if:\n1) it has been frequently used with the imported libraries in the program; 2)\nit has similar functionality to the imported libraries in the program; 3) it\nhas similar functionality to the developer's implementation, and 4) it can be\nused efficiently in the context of the provided code. We apply the\nstate-of-the-art CodeBERT-based model for analysing the context of the source\ncode to deliver relevant library recommendations to users.",
    "descriptor": "",
    "authors": [
      "Lili Tao",
      "Alexandru-Petre Cazan",
      "Senad Ibraimoski",
      "Sean Moran"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05406"
  },
  {
    "id": "arXiv:2210.05408",
    "title": "Private Randomness Agreement and its Application in Quantum Key  Distribution Networks",
    "abstract": "We define a variation on the well-known problem of private message\ntransmission. This new problem called private randomness agreement (PRA) gives\ntwo participants access to a public, authenticated channel alongside the main\nchannels, and the 'message' is not fixed a priori.\nInstead, the participants' aim to agree on a random string completely unknown\nto a computationally unbounded adversary.\nWe define privacy and reliability, and show that PRA cannot be solved in a\nsingle round. We then show that it can be solved in three rounds, albeit with\nexponential cost, and give an efficient four-round protocol based on polynomial\nevaluation.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Ren\u00e9 B\u00f8dker Christensen",
      "Petar Popovski"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05408"
  },
  {
    "id": "arXiv:2210.05409",
    "title": "LECO: Learnable Episodic Count for Task-Specific Intrinsic Reward",
    "abstract": "Episodic count has been widely used to design a simple yet effective\nintrinsic motivation for reinforcement learning with a sparse reward. However,\nthe use of episodic count in a high-dimensional state space as well as over a\nlong episode time requires a thorough state compression and fast hashing, which\nhinders rigorous exploitation of it in such hard and complex exploration\nenvironments. Moreover, the interference from task-irrelevant observations in\nthe episodic count may cause its intrinsic motivation to overlook task-related\nimportant changes of states, and the novelty in an episodic manner can lead to\nrepeatedly revisit the familiar states across episodes. In order to resolve\nthese issues, in this paper, we propose a learnable hash-based episodic count,\nwhich we name LECO, that efficiently performs as a task-specific intrinsic\nreward in hard exploration problems. In particular, the proposed intrinsic\nreward consists of the episodic novelty and the task-specific modulation where\nthe former employs a vector quantized variational autoencoder to automatically\nobtain the discrete state codes for fast counting while the latter regulates\nthe episodic novelty by learning a modulator to optimize the task-specific\nextrinsic reward. The proposed LECO specifically enables the automatic\ntransition from exploration to exploitation during reinforcement learning. We\nexperimentally show that in contrast to the previous exploration methods LECO\nsuccessfully solves hard exploration problems and also scales to large state\nspaces through the most difficult tasks in MiniGrid and DMLab environments.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Daejin Jo",
      "Sungwoong Kim",
      "Daniel Wontae Nam",
      "Taehwan Kwon",
      "Seungeun Rho",
      "Jongmin Kim",
      "Donghoon Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05409"
  },
  {
    "id": "arXiv:2210.05411",
    "title": "Class-Specific Explainability for Deep Time Series Classifiers",
    "abstract": "Explainability helps users trust deep learning solutions for time series\nclassification. However, existing explainability methods for multi-class time\nseries classifiers focus on one class at a time, ignoring relationships between\nthe classes. Instead, when a classifier is choosing between many classes, an\neffective explanation must show what sets the chosen class apart from the rest.\nWe now formalize this notion, studying the open problem of class-specific\nexplainability for deep time series classifiers, a challenging and impactful\nproblem setting. We design a novel explainability method, DEMUX, which learns\nsaliency maps for explaining deep multi-class time series classifiers by\nadaptively ensuring that its explanation spotlights the regions in an input\ntime series that a model uses specifically to its predicted class. DEMUX adopts\na gradient-based approach composed of three interdependent modules that combine\nto generate consistent, class-specific saliency maps that remain faithful to\nthe classifier's behavior yet are easily understood by end users. Our\nexperimental study demonstrates that DEMUX outperforms nine state-of-the-art\nalternatives on five popular datasets when explaining two types of deep time\nseries classifiers. Further, through a case study, we demonstrate that DEMUX's\nexplanations indeed highlight what separates the predicted class from the\nothers in the eyes of the classifier. Our code is publicly available at\nhttps://github.com/rameshdoddaiah/DEMUX.",
    "descriptor": "\nComments: This paper is accepted in ICDM 2022\n",
    "authors": [
      "Ramesh Doddaiah",
      "Prathyush Parvatharaju",
      "Elke Rundensteiner",
      "Thomas Hartvigsen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05411"
  },
  {
    "id": "arXiv:2210.05417",
    "title": "A Perception-Driven Approach To Immersive Remote Telerobotics",
    "abstract": "Virtual Reality (VR) interfaces are increasingly used as remote visualization\nmedia in telerobotics. Remote environments captured through RGB-D cameras and\nvisualized using VR interfaces can enhance operators' situational awareness and\nsense of presence. However, this approach has strict requirements for the\nspeed, throughput, and quality of the visualized 3D data.Further, telerobotics\nrequires operators to focus on their tasks fully, requiring high perceptual and\ncognitive skills. This paper shows a work-in-progress framework to address\nthese challenges by taking the human visual system (HVS) as an inspiration.\nHuman eyes use attentional mechanisms to select and draw user engagement to a\nspecific place from the dynamic environment. Inspired by this, the framework\nimplements functionalities to draw users's engagement to a specific place while\nsimultaneously reducing latency and bandwidth requirements.",
    "descriptor": "",
    "authors": [
      "Y.T.Tefera",
      "D.Mazzanti",
      "S.Anastasi",
      "D.G. Caldwell",
      "P.Fiorini",
      "N.Deshpande"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05417"
  },
  {
    "id": "arXiv:2210.05419",
    "title": "Exploring Interactions and Regulations in Collaborative Learning: An  Interdisciplinary Multimodal Dataset",
    "abstract": "Collaborative learning is an educational approach that enhances learning\nthrough shared goals and working together. Interaction and regulation are two\nessential factors related to the success of collaborative learning. Since the\ninformation from various modalities can reflect the quality of collaboration, a\nnew multimodal dataset with cognitive and emotional triggers is introduced in\nthis paper to explore how regulations affect interactions during the\ncollaborative process. Specifically, a learning task with intentional\ninterventions is designed and assigned to high school students aged 15 years\nold (N=81) in average. Multimodal signals, including video, Kinect, audio, and\nphysiological data, are collected and exploited to study regulations in\ncollaborative learning in terms of individual-participant-single-modality,\nindividual-participant-multiple-modality, and\nmultiple-participant-multiple-modality. Analysis of annotated emotions, body\ngestures, and their interactions indicates that our multimodal dataset with\ndesigned treatments could effectively examine moments of regulation in\ncollaborative learning. In addition, preliminary experiments based on baseline\nmodels suggest that the dataset provides a challenging in-the-wild scenario,\nwhich could further contribute to the fields of education and affective\ncomputing.",
    "descriptor": "\nComments: 17 pages, 9 figures\n",
    "authors": [
      "Yante Li",
      "Yang Liu",
      "Kh\u00c1nh Nguyen",
      "Henglin Shi",
      "Eija Vuorenmaa",
      "Sanna Jarvela",
      "Guoying Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.05419"
  },
  {
    "id": "arXiv:2210.05420",
    "title": "Controlling unfolding in type theory",
    "abstract": "We present a novel mechanism for controlling the unfolding of definitions in\ndependent type theory. Traditionally, proof assistants let users specify\nwhether each definition can or cannot be unfolded in the remainder of a\ndevelopment; unfolding definitions is often necessary in order to reason about\nthem, but an excess of unfolding can result in brittle proofs and intractably\nlarge proof goals. In our system, definitions are by default not unfolded, but\nusers can selectively unfold them in a local manner. We justify our mechanism\nby means of elaboration to a core type theory with extension types, a\nconnective first introduced in the context of homotopy type theory. We prove a\nnormalization theorem for our core calculus and have implemented our system in\nthe cooltt proof assistant, providing both theoretical and practical evidence\nfor it.",
    "descriptor": "",
    "authors": [
      "Daniel Gratzer",
      "Jonathan Sterling",
      "Carlo Angiuli",
      "Thierry Coquand",
      "Lars Birkedal"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05420"
  },
  {
    "id": "arXiv:2210.05422",
    "title": "Word Sense Induction with Hierarchical Clustering and Mutual Information  Maximization",
    "abstract": "Word sense induction (WSI) is a difficult problem in natural language\nprocessing that involves the unsupervised automatic detection of a word's\nsenses (i.e. meanings). Recent work achieves significant results on the WSI\ntask by pre-training a language model that can exclusively disambiguate word\nsenses, whereas others employ previously pre-trained language models in\nconjunction with additional strategies to induce senses. In this paper, we\npropose a novel unsupervised method based on hierarchical clustering and\ninvariant information clustering (IIC). The IIC is used to train a small model\nto optimize the mutual information between two vector representations of a\ntarget word occurring in a pair of synthetic paraphrases. This model is later\nused in inference mode to extract a higher quality vector representation to be\nused in the hierarchical clustering. We evaluate our method on two WSI tasks\nand in two distinct clustering configurations (fixed and dynamic number of\nclusters). We empirically demonstrate that, in certain cases, our approach\noutperforms prior WSI state-of-the-art methods, while in others, it achieves a\ncompetitive performance.",
    "descriptor": "",
    "authors": [
      "Hadi Abdine",
      "Moussa Kamal Eddine",
      "Michalis Vazirgiannis",
      "Davide Buscaldi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05422"
  },
  {
    "id": "arXiv:2210.05423",
    "title": "Learning to Locate Visual Answer in Video Corpus Using Question",
    "abstract": "We introduce a novel task, named video corpus visual answer localization\n(VCVAL), which aims to locate the visual answer in a large collection of\nuntrimmed, unsegmented instructional videos using a natural language question.\nThis task requires a range of skills - the interaction between vision and\nlanguage, video retrieval, passage comprehension, and visual answer\nlocalization. To solve these, we propose a cross-modal contrastive global-span\n(CCGS) method for the VCVAL, jointly training the video corpus retrieval and\nvisual answer localization tasks. More precisely, we enhance the video\nquestion-answer semantic by adding element-wise visual information into the\npre-trained language model, and designing a novel global-span predictor through\nfusion information to locate the visual answer point. The Global-span\ncontrastive learning is adopted to differentiate the span point in the positive\nand negative samples with the global-span matrix. We have reconstructed a new\ndataset named MedVidCQA and benchmarked the VCVAL task, where the proposed\nmethod achieves state-of-the-art (SOTA) both in the video corpus retrieval and\nvisual answer localization tasks. Most importantly, we pave a new path for\nunderstanding the instructional videos, performing detailed analyses on\nextensive experiments, which ushers in further research.",
    "descriptor": "\nComments: 4 pages, 2 figures and 3 tables\n",
    "authors": [
      "Bin Li",
      "Yixuan Weng",
      "Bin Sun",
      "Shutao Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05423"
  },
  {
    "id": "arXiv:2210.05425",
    "title": "COVID-19-related Nepali Tweets Classification in a Low Resource Setting",
    "abstract": "Billions of people across the globe have been using social media platforms in\ntheir local languages to voice their opinions about the various topics related\nto the COVID-19 pandemic. Several organizations, including the World Health\nOrganization, have developed automated social media analysis tools that\nclassify COVID-19-related tweets into various topics. However, these tools that\nhelp combat the pandemic are limited to very few languages, making several\ncountries unable to take their benefit. While multi-lingual or low-resource\nlanguage-specific tools are being developed, they still need to expand their\ncoverage, such as for the Nepali language. In this paper, we identify the eight\nmost common COVID-19 discussion topics among the Twitter community using the\nNepali language, set up an online platform to automatically gather Nepali\ntweets containing the COVID-19-related keywords, classify the tweets into the\neight topics, and visualize the results across the period in a web-based\ndashboard. We compare the performance of two state-of-the-art multi-lingual\nlanguage models for Nepali tweet classification, one generic (mBERT) and the\nother Nepali language family-specific model (MuRIL). Our results show that the\nmodels' relative performance depends on the data size, with MuRIL doing better\nfor a larger dataset. The annotated data, models, and the web-based dashboard\nare open-sourced at https://github.com/naamiinepal/covid-tweet-classification.",
    "descriptor": "\nComments: Accepted at the 7th Social Media Mining for Health (#SMM4H) Workshop, co-located at Coling 2022\n",
    "authors": [
      "Rabin Adhikari",
      "Safal Thapaliya",
      "Nirajan Basnet",
      "Samip Poudel",
      "Aman Shakya",
      "Bishesh Khanal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05425"
  },
  {
    "id": "arXiv:2210.05433",
    "title": "On the Feasibility of Profiling Electric Vehicles through Charging Data",
    "abstract": "Electric vehicles (EVs) represent the long-term green substitute for\ntraditional fuel-based vehicles. To encourage EV adoption, the trust of the\nend-users must be assured. In this work, we focus on a recently emerging\nprivacy threat of profiling and identifying EVs via the analog electrical data\nexchanged during the EV charging process. The core focus of our work is to\ninvestigate the feasibility of such a threat at scale. To this end, we first\npropose an improved EV profiling approach that outperforms the state-of-the-art\nEV profiling techniques. Next, we exhaustively evaluate the performance of our\nimproved approach to profile EVs in real-world settings. In our evaluations, we\nconduct a series of experiments including 25032 charging sessions from 530 real\nEVs, sub-sampled datasets with different data distributions, etc. Our results\nshow that even with our improved approach, profiling and individually\nidentifying the growing number of EVs is not viable in practice; at least with\nthe analog charging data utilized throughout the literature. We believe that\nour findings from this work will further foster the trust of potential users in\nthe EV ecosystem, and consequently, encourage EV adoption.",
    "descriptor": "",
    "authors": [
      "Ankit Gangwal",
      "Aakash Jain",
      "Mauro Conti"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05433"
  },
  {
    "id": "arXiv:2210.05437",
    "title": "DPANET:Dual Pooling Attention Network for Semantic Segmentation",
    "abstract": "Image segmentation is a historic and significant computer vision task. With\nthe help of deep learning techniques, image semantic segmentation has made\ngreat progresses. Over recent years, based on guidance of attention mechanism\ncompared with CNN which overcomes the problems of lacking of interaction\nbetween different channels, and effective capturing and aggregating contextual\ninformation. However, the massive operations generated by the attention\nmechanism lead to its extremely high complexity and high demand for GPU memory.\nFor this purpose, we propose a lightweight and flexible neural network named\nDual Pool Attention Network(DPANet). The most important is that all modules in\nDPANet generate \\textbf{0} parameters. The first component is spatial pool\nattention module, we formulate an easy and powerful method densely to extract\ncontextual characteristics and reduce the amount of calculation and complexity\ndramatically.Meanwhile, it demonstrates the power of even and large kernel\nsize. The second component is channel pool attention module. It is known that\nthe computation process of CNN incorporates the information of spatial and\nchannel dimensions. So, the aim of this module is stripping them out, in order\nto construct relationship of all channels and heighten different channels\nsemantic information selectively. Moreover, we experiments on segmentation\ndatasets, which shows our method simple and effective with low parameters and\ncalculation complexity.",
    "descriptor": "",
    "authors": [
      "Dongwei Sun",
      "Zhuolin Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05437"
  },
  {
    "id": "arXiv:2210.05438",
    "title": "Parallel Augmentation and Dual Enhancement for Occluded Person  Re-identification",
    "abstract": "Occluded person re-identification (Re-ID), the task of searching for the same\nperson's images in occluded environments, has attracted lots of attention in\nthe past decades. Recent approaches concentrate on improving performance on\noccluded data by data/feature augmentation or using extra models to predict\nocclusions. However, they ignore the imbalance problem in the test set and not\nfully utilize the information from the training data. To alleviate the above\nproblems, we propose a simple but effective method with Parallel Augmentation\nand Dual Enhancement (PADE) that is robust on both occluded and non-occluded\ndata, and does not require any auxiliary clues. First, we design a parallel\naugmentation mechanism (PAM) for occluded Re-ID to generate more suitable\noccluded data to mitigate the negative effects of unbalanced data. Second, we\npropose the dual enhancement strategy (DES)for global and local features to\npromote the context information and details. Experimental results on widely\nused occluded datasets (OccludedDuke, Partial-REID, and Occluded-ReID) and\nnon-occluded datasets (Market-1501 and DukeMTMC-reID) validate the\neffectiveness of our method. The code will be available soon.",
    "descriptor": "\nComments: Submitted to AAAI\n",
    "authors": [
      "Zi wang",
      "Huaibo Huang",
      "Aihua Zheng",
      "Chenglong Li",
      "Ran He"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05438"
  },
  {
    "id": "arXiv:2210.05447",
    "title": "The Design and Regulation of Exchanges: A Formal Approach",
    "abstract": "We use formal methods to specify, design, and monitor continuous double\nauctions, which are widely used to match buyers and sellers at exchanges of\nforeign currencies, stocks, and commodities. We identify three natural\nproperties of such auctions and formally prove that these properties completely\ndetermine the input-output relationship. We then formally verify that a natural\nalgorithm satisfies these properties. All definitions, theorems, and proofs are\nformalized in an interactive theorem prover. We extract a verified program of\nour algorithm to build an automated checker that is guaranteed to detect errors\nin the trade logs of exchanges if they generate transactions that violate any\nof the natural properties.",
    "descriptor": "\nComments: 21 pages, FSTTCS 2022 (to appear)\n",
    "authors": [
      "Mohit Garg",
      "Suneel Sarswat"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2210.05447"
  },
  {
    "id": "arXiv:2210.05448",
    "title": "A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based  Policy Learning",
    "abstract": "Open ad hoc teamwork is the problem of training a single agent to efficiently\ncollaborate with an unknown group of teammates whose composition may change\nover time. A variable team composition creates challenges for the agent, such\nas the requirement to adapt to new team dynamics and dealing with changing\nstate vector sizes. These challenges are aggravated in real-world applications\nwhere the controlled agent has no access to the full state of the environment.\nIn this work, we develop a class of solutions for open ad hoc teamwork under\nfull and partial observability. We start by developing a solution for the fully\nobservable case that leverages graph neural network architectures to obtain an\noptimal policy based on reinforcement learning. We then extend this solution to\npartially observable scenarios by proposing different methodologies that\nmaintain belief estimates over the latent environment states and team\ncomposition. These belief estimates are combined with our solution for the\nfully observable case to compute an agent's optimal policy under partial\nobservability in open ad hoc teamwork. Empirical results demonstrate that our\napproach can learn efficient policies in open ad hoc teamwork in full and\npartially observable cases. Further analysis demonstrates that our methods'\nsuccess is a result of effectively learning the effects of teammates' actions\nwhile also inferring the inherent state of the environment under partial\nobservability",
    "descriptor": "",
    "authors": [
      "Arrasy Rahman",
      "Ignacio Carlucho",
      "Niklas H\u00f6pner",
      "Stefano V. Albrecht"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05448"
  },
  {
    "id": "arXiv:2210.05451",
    "title": "Enabling ISP-less Low-Power Computer Vision",
    "abstract": "In order to deploy current computer vision (CV) models on\nresource-constrained low-power devices, recent works have proposed in-sensor\nand in-pixel computing approaches that try to partly/fully bypass the image\nsignal processor (ISP) and yield significant bandwidth reduction between the\nimage sensor and the CV processing unit by downsampling the activation maps in\nthe initial convolutional neural network (CNN) layers. However, direct\ninference on the raw images degrades the test accuracy due to the difference in\ncovariance of the raw images captured by the image sensors compared to the\nISP-processed images used for training. Moreover, it is difficult to train deep\nCV models on raw images, because most (if not all) large-scale open-source\ndatasets consist of RGB images. To mitigate this concern, we propose to invert\nthe ISP pipeline, which can convert the RGB images of any dataset to its raw\ncounterparts, and enable model training on raw images. We release the raw\nversion of the COCO dataset, a large-scale benchmark for generic high-level\nvision tasks. For ISP-less CV systems, training on these raw images result in a\n7.1% increase in test accuracy on the visual wake works (VWW) dataset compared\nto relying on training with traditional ISP-processed RGB datasets. To further\nimprove the accuracy of ISP-less CV models and to increase the energy and\nbandwidth benefits obtained by in-sensor/in-pixel computing, we propose an\nenergy-efficient form of analog in-pixel demosaicing that may be coupled with\nin-pixel CNN computations. When evaluated on raw images captured by real\nsensors from the PASCALRAW dataset, our approach results in a 8.1% increase in\nmAP. Lastly, we demonstrate a further 20.5% increase in mAP by using a novel\napplication of few-shot learning with thirty shots each for the novel PASCALRAW\ndataset, constituting 3 classes.",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Gourav Datta",
      "Zeyu Liu",
      "Zihan Yin",
      "Linyu Sun",
      "Akhilesh R. Jaiswal",
      "Peter A. Beerel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.05451"
  },
  {
    "id": "arXiv:2210.05455",
    "title": "Unlabelled Sample Compression Schemes for Intersection-Closed Classes  and Extremal Classes",
    "abstract": "The sample compressibility of concept classes plays an important role in\nlearning theory, as a sufficient condition for PAC learnability, and more\nrecently as an avenue for robust generalisation in adaptive data analysis.\nWhether compression schemes of size $O(d)$ must necessarily exist for all\nclasses of VC dimension $d$ is unknown, but conjectured to be true by Warmuth.\nRecently Chalopin, Chepoi, Moran, and Warmuth (2018) gave a beautiful\nunlabelled sample compression scheme of size VC dimension for all maximum\nclasses: classes that meet the Sauer-Shelah-Perles Lemma with equality. They\nalso offered a counterexample to compression schemes based on a promising\napproach known as corner peeling. In this paper we simplify and extend their\nproof technique to deal with so-called extremal classes of VC dimension $d$\nwhich contain maximum classes of VC dimension $d-1$. A criterion is given which\nwould imply that all extremal classes admit unlabelled compression schemes of\nsize $d$. We also prove that all intersection-closed classes with VC dimension\n$d$ admit unlabelled compression schemes of size at most $11d$.",
    "descriptor": "\nComments: Appearing at NeurIPS2022\n",
    "authors": [
      "J. Hyam Rubinstein",
      "Benjamin I. P. Rubinstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.05455"
  },
  {
    "id": "arXiv:2210.05457",
    "title": "Are Pretrained Multilingual Models Equally Fair Across Languages?",
    "abstract": "Pretrained multilingual language models can help bridge the digital language\ndivide, enabling high-quality NLP models for lower resourced languages. Studies\nof multilingual models have so far focused on performance, consistency, and\ncross-lingual generalisation. However, with their wide-spread application in\nthe wild and downstream societal impact, it is important to put multilingual\nmodels under the same scrutiny as monolingual models. This work investigates\nthe group fairness of multilingual models, asking whether these models are\nequally fair across languages. To this end, we create a new four-way\nmultilingual dataset of parallel cloze test examples (MozArt), equipped with\ndemographic information (balanced with regard to gender and native tongue)\nabout the test participants. We evaluate three multilingual models on MozArt --\nmBERT, XLM-R, and mT5 -- and show that across the four target languages, the\nthree models exhibit different levels of group disparity, e.g., exhibiting\nnear-equal risk for Spanish, but high levels of disparity for German.",
    "descriptor": "",
    "authors": [
      "Laura Cabello Piqueras",
      "Anders S\u00f8gaard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05457"
  },
  {
    "id": "arXiv:2210.05461",
    "title": "FreGAN: Exploiting Frequency Components for Training GANs under Limited  Data",
    "abstract": "Training GANs under limited data often leads to discriminator overfitting and\nmemorization issues, causing divergent training. Existing approaches mitigate\nthe overfitting by employing data augmentations, model regularization, or\nattention mechanisms. However, they ignore the frequency bias of GANs and take\npoor consideration towards frequency information, especially high-frequency\nsignals that contain rich details. To fully utilize the frequency information\nof limited data, this paper proposes FreGAN, which raises the model's frequency\nawareness and draws more attention to producing high-frequency signals,\nfacilitating high-quality generation. In addition to exploiting both real and\ngenerated images' frequency information, we also involve the frequency signals\nof real images as a self-supervised constraint, which alleviates the GAN\ndisequilibrium and encourages the generator to synthesize adequate rather than\narbitrary frequency signals. Extensive results demonstrate the superiority and\neffectiveness of our FreGAN in ameliorating generation quality in the low-data\nregime (especially when training data is less than 100). Besides, FreGAN can be\nseamlessly applied to existing regularization and attention mechanism models to\nfurther boost the performance.",
    "descriptor": "\nComments: To appear in NeurIPS 2022, github:this https URL\n",
    "authors": [
      "Mengping Yang",
      "Zhe Wang",
      "Ziqiu Chi",
      "Yanbing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05461"
  },
  {
    "id": "arXiv:2210.05463",
    "title": "Large-to-small Image Resolution Asymmetry in Deep Metric Learning",
    "abstract": "Deep metric learning for vision is trained by optimizing a representation\nnetwork to map (non-)matching image pairs to (non-)similar representations.\nDuring testing, which typically corresponds to image retrieval, both database\nand query examples are processed by the same network to obtain the\nrepresentation used for similarity estimation and ranking. In this work, we\nexplore an asymmetric setup by light-weight processing of the query at a small\nimage resolution to enable fast representation extraction. The goal is to\nobtain a network for database examples that is trained to operate on large\nresolution images and benefits from fine-grained image details, and a second\nnetwork for query examples that operates on small resolution images but\npreserves a representation space aligned with that of the database network. We\nachieve this with a distillation approach that transfers knowledge from a fixed\nteacher network to a student via a loss that operates per image and solely\nrelies on coupled augmentations without the use of any labels. In contrast to\nprior work that explores such asymmetry from the point of view of different\nnetwork architectures, this work uses the same architecture but modifies the\nimage resolution. We conclude that resolution asymmetry is a better way to\noptimize the performance/efficiency trade-off than architecture asymmetry.\nEvaluation is performed on three standard deep metric learning benchmarks,\nnamely CUB200, Cars196, and SOP. Code: https://github.com/pavelsuma/raml",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Pavel Suma",
      "Giorgos Tolias"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05463"
  },
  {
    "id": "arXiv:2210.05470",
    "title": "Block Format Error Bounds and Optimal Block Size Selection",
    "abstract": "The amounts of data that need to be transmitted, processed, and stored by the\nmodern deep neural networks have reached truly enormous volumes in the last few\nyears calling for the invention of new paradigms both in hardware and software\ndevelopment. One of the most promising and rapidly advancing frontiers here is\nthe creation of new data formats. In this work we focus on the family of block\nfloating point numerical formats due to their combination of wide dynamic\nrange, numerical accuracy, and efficient hardware implementation of inner\nproducts using simple integer arithmetic. These formats are characterized by a\nblock of mantissas with a shared scale factor. The basic Block Floating Point\n(BFP) format quantizes the block scales into the nearest powers of two on the\nright. Its simple modification - Scaled BFP (SBFP) - stores the same scales in\nfull precision and thus allows higher accuracy. In this paper, we study the\nstatistical behavior of both these formats rigorously. We develop asymptotic\nbounds on the inner product error in SBFP- and BFP-quantized normally\ndistributed vectors. Next, we refine those asymptotic results to finite\ndimensional settings and derive high-dimensional tight bounds for the same\nerrors. Based on the obtained results we introduce a performance metric\nassessing accuracy of any block format. This metric allows us to determine the\noptimal parameters, such as the block size, yielding highest accuracy. In\nparticular, we show that if the precision of the BFP format is fixed at 4 bits,\nthe optimal block size becomes 64. All theoretical derivations are supported by\nnumerical experiments and studies on the weights of publicly available\npretrained neural networks.",
    "descriptor": "",
    "authors": [
      "Ilya Soloveychik",
      "Ilya Lyubomirsky",
      "Xin Wang",
      "Sudeep Bhoja"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05470"
  },
  {
    "id": "arXiv:2210.05471",
    "title": "Instance Regularization for Discriminative Language Model Pre-training",
    "abstract": "Discriminative pre-trained language models (PrLMs) can be generalized as\ndenoising auto-encoders that work with two procedures, ennoising and denoising.\nFirst, an ennoising process corrupts texts with arbitrary noising functions to\nconstruct training instances. Then, a denoising language model is trained to\nrestore the corrupted tokens. Existing studies have made progress by optimizing\nindependent strategies of either ennoising or denosing. They treat training\ninstances equally throughout the training process, with little attention on the\nindividual contribution of those instances. To model explicit signals of\ninstance contribution, this work proposes to estimate the complexity of\nrestoring the original sentences from corrupted ones in language model\npre-training. The estimations involve the corruption degree in the ennoising\ndata construction process and the prediction confidence in the denoising\ncounterpart. Experimental results on natural language understanding and reading\ncomprehension benchmarks show that our approach improves pre-training\nefficiency, effectiveness, and robustness. Code is publicly available at\nhttps://github.com/cooelf/InstanceReg",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Zhuosheng Zhang",
      "Hai Zhao",
      "Ming Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05471"
  },
  {
    "id": "arXiv:2210.05472",
    "title": "Tuning Rate of Strategy Revision in Population Games",
    "abstract": "We investigate a multi-agent decision problem in population games where each\nagent in a population makes a decision on strategy selection and revision to\nengage in repeated games with others. The strategy revision is subject to time\ndelay which represents the time it takes for an agent revising its strategy\nneeds to spend before it can adopt a new strategy and return back to the game.\nWe discuss the effect of the time delay on long-term behavior of the agents'\nstrategy revision. In particular, when the time delay is large, the strategy\nrevision would exhibit oscillation and the agents spend substantial time in\n\"transitioning\" between different strategies, which prevents the agents from\nattaining the Nash equilibrium of the game. As a main contribution of the\npaper, we propose an algorithm that tunes the rate of the agents' strategy\nrevision and show such tuning approach ensures convergence to the Nash\nequilibrium. We validate our analytical results using simulations.",
    "descriptor": "",
    "authors": [
      "Shinkyu Park"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05472"
  },
  {
    "id": "arXiv:2210.05476",
    "title": "Medha: Microcoded Hardware Accelerator for computing on Encrypted Data",
    "abstract": "Homomorphic encryption (HE) enables computation on encrypted data, and hence\nit has a great potential in privacy-preserving outsourcing of computations to\nthe cloud. Hardware acceleration of HE is crucial as software implementations\nare very slow. In this paper, we present design methodologies for building a\nprogrammable hardware accelerator for speeding up the cloud-side homomorphic\nevaluations on encrypted data. First, we propose a divide-and-conquer technique\nthat enables homomorphic evaluations in a large polynomial ring $R_{Q,2N}$ to\nuse a hardware accelerator that has been built for the smaller ring $R_{Q,N}$.\nThe technique makes it possible to use a single hardware accelerator flexibly\nfor supporting several HE parameter sets. Next, we present several\narchitectural design methods that we use to realize the flexible and\ninstruction-set accelerator architecture, which we call `Medha'. At every level\nof the implementation hierarchy, we explore possibilities for parallel\nprocessing. Starting from hardware-friendly parallel algorithms for the basic\nbuilding blocks, we gradually build heavily parallel RNS polynomial arithmetic\nunits. Next, many of these parallel units are interconnected elegantly so that\ntheir interconnections require the minimum number of nets, therefore making the\noverall architecture placement-friendly on the platform. For Medha, we take a\nmemory-conservative design approach and get rid of any off-chip memory access\nduring homomorphic evaluations. Finally, we implement Medha in a Xilinx Alveo\nU250 FPGA and measure timing performances of the microcoded homomorphic\naddition, multiplication, key-switching, and rescaling for the leveled HE\nscheme RNS-HEAAN at 200 MHz clock frequency. For two large parameter sets,\nMedha achieves accelerations by up to 68x and 78x times respectively compared\nto a highly optimized software implementation Microsoft SEAL running at 2.3\nGHz.",
    "descriptor": "\nComments: This paper will appear at IACR Transactions on Cryptographic Hardware and Embedded Systems 2023\n",
    "authors": [
      "Ahmet Can Mert",
      "Aikata",
      "Sunmin Kwon",
      "Youngsam Shin",
      "Donghoon Yoo",
      "Yongwoo Lee",
      "Sujoy Sinha Roy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.05476"
  },
  {
    "id": "arXiv:2210.05478",
    "title": "Aggregating Layers for Deepfake Detection",
    "abstract": "The increasing popularity of facial manipulation (Deepfakes) and synthetic\nface creation raises the need to develop robust forgery detection solutions.\nCrucially, most work in this domain assume that the Deepfakes in the test set\ncome from the same Deepfake algorithms that were used for training the network.\nThis is not how things work in practice. Instead, we consider the case where\nthe network is trained on one Deepfake algorithm, and tested on Deepfakes\ngenerated by another algorithm. Typically, supervised techniques follow a\npipeline of visual feature extraction from a deep backbone, followed by a\nbinary classification head. Instead, our algorithm aggregates features\nextracted across all layers of one backbone network to detect a fake. We\nevaluate our approach on two domains of interest - Deepfake detection and\nSynthetic image detection, and find that we achieve SOTA results.",
    "descriptor": "",
    "authors": [
      "Amir Jevnisek",
      "Shai Avidan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05478"
  },
  {
    "id": "arXiv:2210.05479",
    "title": "Frequency-Aware Self-Supervised Monocular Depth Estimation",
    "abstract": "We present two versatile methods to generally enhance self-supervised\nmonocular depth estimation (MDE) models. The high generalizability of our\nmethods is achieved by solving the fundamental and ubiquitous problems in\nphotometric loss function. In particular, from the perspective of spatial\nfrequency, we first propose Ambiguity-Masking to suppress the incorrect\nsupervision under photometric loss at specific object boundaries, the cause of\nwhich could be traced to pixel-level ambiguity. Second, we present a novel\nfrequency-adaptive Gaussian low-pass filter, designed to robustify the\nphotometric loss in high-frequency regions. We are the first to propose\nblurring images to improve depth estimators with an interpretable analysis.\nBoth modules are lightweight, adding no parameters and no need to manually\nchange the network structures. Experiments show that our methods provide\nperformance boosts to a large number of existing models, including those who\nclaimed state-of-the-art, while introducing no extra inference computation at\nall.",
    "descriptor": "\nComments: 8 pages, 5 figures, published to WACV2023\n",
    "authors": [
      "Xingyu Chen",
      "Thomas H. Li",
      "Ruonan Zhang",
      "Ge Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05479"
  },
  {
    "id": "arXiv:2210.05480",
    "title": "T5 for Hate Speech, Augmented Data and Ensemble",
    "abstract": "We conduct relatively extensive investigations of automatic hate speech (HS)\ndetection using different state-of-the-art (SoTA) baselines over 11 subtasks of\n6 different datasets. Our motivation is to determine which of the recent SoTA\nmodels is best for automatic hate speech detection and what advantage methods\nlike data augmentation and ensemble may have on the best model, if any. We\ncarry out 6 cross-task investigations. We achieve new SoTA on two subtasks -\nmacro F1 scores of 91.73% and 53.21% for subtasks A and B of the HASOC 2020\ndataset, where previous SoTA are 51.52% and 26.52%, respectively. We achieve\nnear-SoTA on two others - macro F1 scores of 81.66% for subtask A of the OLID\n2019 dataset and 82.54% for subtask A of the HASOC 2021 dataset, where SoTA are\n82.9% and 83.05%, respectively. We perform error analysis and use two\nexplainable artificial intelligence (XAI) algorithms (IG and SHAP) to reveal\nhow two of the models (Bi-LSTM and T5) make the predictions they do by using\nexamples. Other contributions of this work are 1) the introduction of a simple,\nnovel mechanism for correcting out-of-class (OOC) predictions in T5, 2) a\ndetailed description of the data augmentation methods, 3) the revelation of the\npoor data annotations in the HASOC 2021 dataset by using several examples and\nXAI (buttressing the need for better quality control), and 4) the public\nrelease of our model checkpoints and codes to foster transparency.",
    "descriptor": "\nComments: 15 pages, 18 figures\n",
    "authors": [
      "Tosin Adewumi",
      "Sana Sabah Sabry",
      "Nosheen Abid",
      "Foteini Liwicki",
      "Marcus Liwicki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05480"
  },
  {
    "id": "arXiv:2210.05481",
    "title": "Better Than Whitespace: Information Retrieval for Languages without  Custom Tokenizers",
    "abstract": "Tokenization is a crucial step in information retrieval, especially for\nlexical matching algorithms, where the quality of indexable tokens directly\nimpacts the effectiveness of a retrieval system. Since different languages have\nunique properties, the design of the tokenization algorithm is usually\nlanguage-specific and requires at least some lingustic knowledge. However, only\na handful of the 7000+ languages on the planet benefit from specialized,\ncustom-built tokenization algorithms, while the other languages are stuck with\na \"default\" whitespace tokenizer, which cannot capture the intricacies of\ndifferent languages. To address this challenge, we propose a different approach\nto tokenization for lexical matching retrieval algorithms (e.g., BM25): using\nthe WordPiece tokenizer, which can be built automatically from unsupervised\ndata. We test the approach on 11 typologically diverse languages in the MrTyDi\ncollection: results show that the mBERT tokenizer provides strong relevance\nsignals for retrieval \"out of the box\", outperforming whitespace tokenization\non most languages. In many cases, our approach also improves retrieval\neffectiveness when combined with existing custom-built tokenizers.",
    "descriptor": "",
    "authors": [
      "Odunayo Ogundepo",
      "Xinyu Zhang",
      "Jimmy Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05481"
  },
  {
    "id": "arXiv:2210.05482",
    "title": "Descriptive complexity of the generalized spectra of graphs",
    "abstract": "Two graphs are cospectral if their respective adjacency matrices have the\nsame multiset of eigenvalues, and generalized cospectral if they are cospectral\nand so are their complements. We study generalized cospectrality in relation to\nlogical definability. We show that any pair of graphs that are elementary\nequivalent with respect to the three-variable counting first-order logic $C^3$\nare generalized cospectral, and this is not the case with $C^2$, nor with any\nnumber of variables if we exclude counting quantifiers. Using this result we\nprovide a new characterization of the well-known class of distance-regular\ngraphs using the logic $C^3$. We also show that, for controllable graphs (it is\nknown that almost all graphs are controllable), the elementary equivalence in\n$C^2$ coincides with isomorphism.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Aida Abiad",
      "Anuj Dawar",
      "Octavio Zapata"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.05482"
  },
  {
    "id": "arXiv:2210.05484",
    "title": "Architectural Optimization over Subgroups for Equivariant Neural  Networks",
    "abstract": "Incorporating equivariance to symmetry groups as a constraint during neural\nnetwork training can improve performance and generalization for tasks\nexhibiting those symmetries, but such symmetries are often not perfectly nor\nexplicitly present. This motivates algorithmically optimizing the architectural\nconstraints imposed by equivariance. We propose the equivariance relaxation\nmorphism, which preserves functionality while reparameterizing a group\nequivariant layer to operate with equivariance constraints on a subgroup, as\nwell as the $[G]$-mixed equivariant layer, which mixes layers constrained to\ndifferent groups to enable within-layer equivariance optimization. We further\npresent evolutionary and differentiable neural architecture search (NAS)\nalgorithms that utilize these mechanisms respectively for equivariance-aware\narchitectural optimization. Experiments across a variety of datasets show the\nbenefit of dynamically constrained equivariance to find effective architectures\nwith approximate equivariance.",
    "descriptor": "",
    "authors": [
      "Kaitlin Maile",
      "Dennis G. Wilson",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05484"
  },
  {
    "id": "arXiv:2210.05487",
    "title": "Like a bilingual baby: The advantage of visually grounding a bilingual  language model",
    "abstract": "Unlike most neural language models, humans learn language in a rich,\nmulti-sensory and, often, multi-lingual environment. Current language models\ntypically fail to fully capture the complexities of multilingual language use.\nWe train an LSTM language model on images and captions in English and Spanish\nfrom MS-COCO-ES. We find that the visual grounding improves the model's\nunderstanding of semantic similarity both within and across languages and\nimproves perplexity. However, we find no significant advantage of visual\ngrounding for abstract words. Our results provide additional evidence of the\nadvantages of visually grounded language models and point to the need for more\nnaturalistic language data from multilingual speakers and multilingual datasets\nwith perceptual grounding.",
    "descriptor": "\nComments: Preprint, 7 pages, 2 tables, 1 figure\n",
    "authors": [
      "Khai-Nguyen Nguyen",
      "Zixin Tang",
      "Ankur Mali",
      "Alex Kelly"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05487"
  },
  {
    "id": "arXiv:2210.05492",
    "title": "Mastering the Game of No-Press Diplomacy via Human-Regularized  Reinforcement Learning and Planning",
    "abstract": "No-press Diplomacy is a complex strategy game involving both cooperation and\ncompetition that has served as a benchmark for multi-agent AI research. While\nself-play reinforcement learning has resulted in numerous successes in purely\nadversarial games like chess, Go, and poker, self-play alone is insufficient\nfor achieving optimal performance in domains involving cooperation with humans.\nWe address this shortcoming by first introducing a planning algorithm we call\nDiL-piKL that regularizes a reward-maximizing policy toward a human\nimitation-learned policy. We prove that this is a no-regret learning algorithm\nunder a modified utility function. We then show that DiL-piKL can be extended\ninto a self-play reinforcement learning algorithm we call RL-DiL-piKL that\nprovides a model of human play while simultaneously training an agent that\nresponds well to this human model. We used RL-DiL-piKL to train an agent we\nname Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human\nparticipants spanning skill levels from beginner to expert, two Diplodocus\nagents both achieved a higher average score than all other participants who\nplayed more than two games, and ranked first and third according to an Elo\nratings model.",
    "descriptor": "",
    "authors": [
      "Anton Bakhtin",
      "David J Wu",
      "Adam Lerer",
      "Jonathan Gray",
      "Athul Paul Jacob",
      "Gabriele Farina",
      "Alexander H Miller",
      "Noam Brown"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.05492"
  },
  {
    "id": "arXiv:2210.05494",
    "title": "Map-free Visual Relocalization: Metric Pose Relative to a Single Image",
    "abstract": "Can we relocalize in a scene represented by a single reference image?\nStandard visual relocalization requires hundreds of images and scale\ncalibration to build a scene-specific 3D map. In contrast, we propose Map-free\nRelocalization, i.e., using only one photo of a scene to enable instant, metric\nscaled relocalization. Existing datasets are not suitable to benchmark map-free\nrelocalization, due to their focus on large scenes or their limited\nvariability. Thus, we have constructed a new dataset of 655 small places of\ninterest, such as sculptures, murals and fountains, collected worldwide. Each\nplace comes with a reference image to serve as a relocalization anchor, and\ndozens of query images with known, metric camera poses. The dataset features\nchanging conditions, stark viewpoint changes, high variability across places,\nand queries with low to no visual overlap with the reference image. We identify\ntwo viable families of existing methods to provide baseline results: relative\npose regression, and feature matching combined with single-image depth\nprediction. While these methods show reasonable performance on some favorable\nscenes in our dataset, map-free relocalization proves to be a challenge that\nrequires new, innovative solutions.",
    "descriptor": "\nComments: ECCV2022 camera-ready. 14 pages + 4 reference pages\n",
    "authors": [
      "Eduardo Arnold",
      "Jamie Wynn",
      "Sara Vicente",
      "Guillermo Garcia-Hernando",
      "\u00c1ron Monszpart",
      "Victor Adrian Prisacariu",
      "Daniyar Turmukhambetov",
      "Eric Brachmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05494"
  },
  {
    "id": "arXiv:2210.05495",
    "title": "MAgNet: Mesh Agnostic Neural PDE Solver",
    "abstract": "The computational complexity of classical numerical methods for solving\nPartial Differential Equations (PDE) scales significantly as the resolution\nincreases. As an important example, climate predictions require fine\nspatio-temporal resolutions to resolve all turbulent scales in the fluid\nsimulations. This makes the task of accurately resolving these scales\ncomputationally out of reach even with modern supercomputers. As a result,\ncurrent numerical modelers solve PDEs on grids that are too coarse (3km to\n200km on each side), which hinders the accuracy and usefulness of the\npredictions. In this paper, we leverage the recent advances in Implicit Neural\nRepresentations (INR) to design a novel architecture that predicts the\nspatially continuous solution of a PDE given a spatial position query. By\naugmenting coordinate-based architectures with Graph Neural Networks (GNN), we\nenable zero-shot generalization to new non-uniform meshes and long-term\npredictions up to 250 frames ahead that are physically consistent. Our Mesh\nAgnostic Neural PDE Solver (MAgNet) is able to make accurate predictions across\na variety of PDE simulation datasets and compares favorably with existing\nbaselines. Moreover, MAgNet generalizes well to different meshes and\nresolutions up to four times those trained on.",
    "descriptor": "",
    "authors": [
      "Oussama Boussif",
      "Dan Assouline",
      "Loubna Benabbou",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2210.05495"
  },
  {
    "id": "arXiv:2210.05496",
    "title": "Experiment Design for Identification of Marine Models",
    "abstract": "In this work, experiment design for marine vessels is explored. A\ndictionary-based approach is used, i.e., a systematic way of choosing the most\ninformative combination of independent experiments out of a predefined set of\ncandidates. This idea is quite general but is here tailored to an instrumental\nvariable (IV) estimator with zero-mean instruments. This type of estimator is\nwell-suited to deal with parameter estimation for second-order modulus models,\nwhich is a class of models often used to describe motion of marine vessels. The\nmethod is evaluated using both simulated and real data, the latter from a small\nmodel ship as well as from a full-scale vessel. Further, a standard\nmotion-planning problem is modified to account for the prior-made choice of\ninformation-optimal sub-experiments, which makes it possible to obtain a plan\nfor the complete experiment in the form of a feasible trajectory.",
    "descriptor": "\nComments: 14 pages, 8 figures\n",
    "authors": [
      "Fredrik Ljungberg",
      "Jonas Linder",
      "Martin Enqvist",
      "Kalevi Tervo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05496"
  },
  {
    "id": "arXiv:2210.05497",
    "title": "Improving Sharpness-Aware Minimization with Fisher Mask for Better  Generalization on Language Models",
    "abstract": "Fine-tuning large pretrained language models on a limited training corpus\nusually suffers from poor generalization. Prior works show that the\nrecently-proposed sharpness-aware minimization (SAM) optimization method can\nimprove the model generalization. However, SAM adds a perturbation to each\nmodel parameter equally (but not all parameters contribute equally to the\noptimization of training), which we argue is sub-optimal and will lead to\nexcessive computation. In this paper, we propose a novel optimization\nprocedure, namely FSAM, which introduces a Fisher mask to improve the\nefficiency and performance of SAM. In short, instead of adding perturbation to\nall parameters, FSAM uses the Fisher information to identity the important\nparameters and formulates a Fisher mask to obtain the sparse perturbation,\ni.e., making the optimizer focus on these important parameters. Experiments on\nvarious tasks in GLUE and SuperGLUE benchmarks show that FSAM consistently\noutperforms the vanilla SAM by 0.67~1.98 average score among four different\npretrained models. We also empirically show that FSAM works well in other\ncomplex scenarios, e.g., fine-tuning on generation tasks or limited training\ndata. Encouragingly, when training data is limited, FSAM improves the SAM by a\nlarge margin, i.e., up to 15.1.",
    "descriptor": "\nComments: Accepted by EMNLP 2022 (Findings)\n",
    "authors": [
      "Qihuang Zhong",
      "Liang Ding",
      "Li Shen",
      "Peng Mi",
      "Juhua Liu",
      "Bo Du",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05497"
  },
  {
    "id": "arXiv:2210.05498",
    "title": "Adversarial Contrastive Learning for Evidence-aware Fake News Detection  with Graph Neural Networks",
    "abstract": "The prevalence and perniciousness of fake news have been a critical issue on\nthe Internet, which stimulates the development of automatic fake news detection\nin turn. In this paper, we focus on evidence-based fake news detection, where\nseveral evidences are utilized to probe the veracity of news (i.e., a claim).\nMost previous methods first employ sequential models to embed the semantic\ninformation and then capture the claim-evidence interaction based on attention\nmechanisms. Despite their effectiveness, they still suffer from three\nweaknesses. Firstly, sequential models fail to integrate the relevant\ninformation that is scattered far apart in evidences. Secondly, they\nunderestimate much redundant information in evidences may be useless or\nharmful. Thirdly, insufficient data utilization limits the separability and\nreliability of representations captured by the model. To solve these problems,\nwe propose a unified Graph-based sEmantic structure mining framework with\nConTRAstive Learning, namely GETRAL in short. Specifically, we first model\nclaims and evidences as graph-structured data to capture the long-distance\nsemantic dependency. Consequently, we reduce information redundancy by\nperforming graph structure learning. Then the fine-grained semantic\nrepresentations are fed into the claim-evidence interaction module for\npredictions. Finally, an adversarial contrastive learning module is applied to\nmake full use of data and strengthen representation learning. Comprehensive\nexperiments have demonstrated the superiority of GETRAL over the\nstate-of-the-arts and validated the efficacy of semantic mining with graph\nstructure and contrastive learning.",
    "descriptor": "\nComments: 12 pages; in submission to IEEE TKDE. arXiv admin note: substantial text overlap with arXiv:2201.06885\n",
    "authors": [
      "Junfei Wu",
      "Weizhi Xu",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05498"
  },
  {
    "id": "arXiv:2210.05499",
    "title": "Capturing Global Structural Information in Long Document Question  Answering with Compressive Graph Selector Network",
    "abstract": "Long document question answering is a challenging task due to its demands for\ncomplex reasoning over long text. Previous works usually take long documents as\nnon-structured flat texts or only consider the local structure in long\ndocuments. However, these methods usually ignore the global structure of the\nlong document, which is essential for long-range understanding. To tackle this\nproblem, we propose Compressive Graph Selector Network (CGSN) to capture the\nglobal structure in a compressive and iterative manner. Specifically, the\nproposed model consists of three modules: local graph network, global graph\nnetwork and evidence memory network. Firstly, the local graph network builds\nthe graph structure of the chunked segment in token, sentence, paragraph and\nsegment levels to capture the short-term dependency of the text. Secondly, the\nglobal graph network selectively receives the information of each level from\nthe local graph, compresses them into the global graph nodes and applies graph\nattention into the global graph nodes to build the long-range reasoning over\nthe entire text in an iterative way. Thirdly, the evidence memory network is\ndesigned to alleviate the redundancy problem in the evidence selection via\nsaving the selected result in the previous steps. Extensive experiments show\nthat the proposed model outperforms previous methods on two datasets.",
    "descriptor": "\nComments: Accepted by the main conference of EMNLP 2022\n",
    "authors": [
      "Yuxiang Nie",
      "Heyan Huang",
      "Wei Wei",
      "Xian-Ling Mao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05499"
  },
  {
    "id": "arXiv:2210.05506",
    "title": "Extracting Meaningful Attention on Source Code: An Empirical Study of  Developer and Neural Model Code Exploration",
    "abstract": "The high effectiveness of neural models of code, such as OpenAI Codex and\nAlphaCode, suggests coding capabilities of models that are at least comparable\nto those of humans. However, previous work has only used these models for their\nraw completion, ignoring how the model reasoning, in the form of attention\nweights, can be used for other downstream tasks. Disregarding the attention\nweights means discarding a considerable portion of what those models compute\nwhen queried. To profit more from the knowledge embedded in these large\npre-trained models, this work compares multiple approaches to post-process\nthese valuable attention weights for supporting code exploration. Specifically,\nwe compare to which extent the transformed attention signal of CodeGen, a large\nand publicly available pretrained neural model, agrees with how developers look\nat and explore code when each answering the same sense-making questions about\ncode. At the core of our experimental evaluation, we collect, manually\nannotate, and open-source a novel eye-tracking dataset comprising 25 developers\nanswering sense-making questions on code over 92 sessions. We empirically\nevaluate five attention-agnostic heuristics and ten attention-based post\nprocessing approaches of the attention signal against our ground truth of\ndevelopers exploring code, including the novel concept of follow-up attention\nwhich exhibits the highest agreement. Beyond the dataset contribution and the\nempirical study, we also introduce a novel practical application of the\nattention signal of pre-trained models with completely analytical solutions,\ngoing beyond how neural models' attention mechanisms have traditionally been\nused.",
    "descriptor": "",
    "authors": [
      "Matteo Paltenghi",
      "Rahul Pandita",
      "Austin Z. Henley",
      "Albert Ziegler"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05506"
  },
  {
    "id": "arXiv:2210.05508",
    "title": "Detect, Distill and Update: Learned DB Systems Facing Out of  Distribution Data",
    "abstract": "Machine Learning (ML) is changing DBs as many DB components are being\nreplaced by ML models. One open problem in this setting is how to update such\nML models in the presence of data updates. We start this investigation focusing\non data insertions (dominating updates in analytical DBs). We study how to\nupdate neural network (NN) models when new data follows a different\ndistribution (a.k.a. it is \"out-of-distribution\" -- OOD), rendering\npreviously-trained NNs inaccurate. A requirement in our problem setting is that\nlearned DB components should ensure high accuracy for tasks on old and new data\n(e.g., for approximate query processing (AQP), cardinality estimation (CE),\nsynthetic data generation (DG), etc.). This paper proposes a novel updatability\nframework (DDUp). DDUp can provide updatability for different learned DB system\ncomponents, even based on different NNs, without the high costs to retrain the\nNNs from scratch. DDUp entails two components: First, a novel, efficient, and\nprincipled statistical-testing approach to detect OOD data. Second, a novel\nmodel updating approach, grounded on the principles of transfer learning with\nknowledge distillation, to update learned models efficiently, while still\nensuring high accuracy. We develop and showcase DDUp's applicability for three\ndifferent learned DB components, AQP, CE, and DG, each employing a different\ntype of NN. Detailed experimental evaluation using real and benchmark datasets\nfor AQP, CE, and DG detail DDUp's performance advantages.",
    "descriptor": "\nComments: Accepted as a conference paper for SIGMOD 2023\n",
    "authors": [
      "Meghdad Kurmanji",
      "Peter Triantafillou"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05508"
  },
  {
    "id": "arXiv:2210.05509",
    "title": "Finding the global semantic representation in GAN through Frechet Mean",
    "abstract": "The ideally disentangled latent space in GAN involves the global\nrepresentation of latent space using semantic attribute coordinates. In other\nwords, in this disentangled space, there exists the global semantic basis as a\nvector space where each basis component describes one attribute of generated\nimages. In this paper, we propose an unsupervised method for finding this\nglobal semantic basis in the intermediate latent space in GANs. This semantic\nbasis represents sample-independent meaningful perturbations that change the\nsame semantic attribute of an image on the entire latent space. The proposed\nglobal basis, called Fr\\'echet basis, is derived by introducing Fr\\'echet mean\nto the local semantic perturbations in a latent space. Fr\\'echet basis is\ndiscovered in two stages. First, the global semantic subspace is discovered by\nthe Fr\\'echet mean in the Grassmannian manifold of the local semantic\nsubspaces. Second, Fr\\'echet basis is found by optimizing a basis of the\nsemantic subspace via the Fr\\'echet mean in the Special Orthogonal Group.\nExperimental results demonstrate that Fr\\'echet basis provides better semantic\nfactorization and robustness compared to the previous methods. Moreover, we\nsuggest the basis refinement scheme for the previous methods. The quantitative\nexperiments show that the refined basis achieves better semantic factorization\nwhile generating the same semantic subspace as the previous method.",
    "descriptor": "\nComments: 18 pages, 13 figures\n",
    "authors": [
      "Jaewoong Choi",
      "Geonho Hwang",
      "Hyunsoo Cho",
      "Myungjoo Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05509"
  },
  {
    "id": "arXiv:2210.05512",
    "title": "On the Interpolation of Contextualized Term-based Ranking with BM25 for  Query-by-Example Retrieval",
    "abstract": "Term-based ranking with pre-trained transformer-based language models has\nrecently gained attention as they bring the contextualization power of\ntransformer models into the highly efficient term-based retrieval. In this\nwork, we examine the generalizability of two of these deep contextualized\nterm-based models in the context of query-by-example (QBE) retrieval in which a\nseed document acts as the query to find relevant documents. In this setting --\nwhere queries are much longer than common keyword queries -- BERT inference at\nquery time is problematic as it involves quadratic complexity. We investigate\nTILDE and TILDEv2, both of which leverage BERT tokenizer as their query\nencoder. With this approach, there is no need for BERT inference at query time,\nand also the query can be of any length. Our extensive evaluation on the four\nQBE tasks of SciDocs benchmark shows that in a query-by-example retrieval\nsetting TILDE and TILDEv2 are still less effective than a cross-encoder BERT\nranker. However, we observe that BM25 could show a competitive ranking quality\ncompared to TILDE and TILDEv2 which is in contrast to the findings about the\nrelative performance of these three models on retrieval for short queries\nreported in prior work. This result raises the question about the use of\ncontextualized term-based ranking models being beneficial in QBE setting. We\nfollow-up on our findings by studying the score interpolation between the\nrelevance score from TILDE (TILDEv2) and BM25. We conclude that these two\ncontextualized term-based ranking models capture different relevance signals\nthan BM25 and combining the different term-based rankers results in\nstatistically significant improvements in QBE retrieval. Our work sheds light\non the challenges of retrieval settings different from the common evaluation\nbenchmarks.",
    "descriptor": "\nComments: Proceedings of the 2022 ACM SIGIR International Conference on the Theory of Information Retrieval\n",
    "authors": [
      "Amin Abolghasemi",
      "Arian Askari",
      "Suzan Verberne"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05512"
  },
  {
    "id": "arXiv:2210.05513",
    "title": "ViFiCon: Vision and Wireless Association Via Self-Supervised Contrastive  Learning",
    "abstract": "We introduce ViFiCon, a self-supervised contrastive learning scheme which\nuses synchronized information across vision and wireless modalities to perform\ncross-modal association. Specifically, the system uses pedestrian data\ncollected from RGB-D camera footage as well as WiFi Fine Time Measurements\n(FTM) from a user's smartphone device. We represent the temporal sequence by\nstacking multi-person depth data spatially within a banded image. Depth data\nfrom RGB-D (vision domain) is inherently linked with an observable pedestrian,\nbut FTM data (wireless domain) is associated only to a smartphone on the\nnetwork. To formulate the cross-modal association problem as self-supervised,\nthe network learns a scene-wide synchronization of the two modalities as a\npretext task, and then uses that learned representation for the downstream task\nof associating individual bounding boxes to specific smartphones, i.e.\nassociating vision and wireless information. We use a pre-trained region\nproposal model on the camera footage and then feed the extrapolated bounding\nbox information into a dual-branch convolutional neural network along with the\nFTM data. We show that compared to fully supervised SoTA models, ViFiCon\nachieves high performance vision-to-wireless association, finding which\nbounding box corresponds to which smartphone device, without hand-labeled\nassociation examples for training data.",
    "descriptor": "",
    "authors": [
      "Nicholas Meegan",
      "Hansi Liu",
      "Bryan Cao",
      "Abrar Alali",
      "Kristin Dana",
      "Marco Gruteser",
      "Shubham Jain",
      "Ashwin Ashok"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05513"
  },
  {
    "id": "arXiv:2210.05517",
    "title": "DeepMLE: A Robust Deep Maximum Likelihood Estimator for Two-view  Structure from Motion",
    "abstract": "Two-view structure from motion (SfM) is the cornerstone of 3D reconstruction\nand visual SLAM (vSLAM). Many existing end-to-end learning-based methods\nusually formulate it as a brute regression problem. However, the inadequate\nutilization of traditional geometry model makes the model not robust in unseen\nenvironments. To improve the generalization capability and robustness of\nend-to-end two-view SfM network, we formulate the two-view SfM problem as a\nmaximum likelihood estimation (MLE) and solve it with the proposed framework,\ndenoted as DeepMLE. First, we propose to take the deep multi-scale correlation\nmaps to depict the visual similarities of 2D image matches decided by\nego-motion. In addition, in order to increase the robustness of our framework,\nwe formulate the likelihood function of the correlations of 2D image matches as\na Gaussian and Uniform mixture distribution which takes the uncertainty caused\nby illumination changes, image noise and moving objects into account.\nMeanwhile, an uncertainty prediction module is presented to predict the\npixel-wise distribution parameters. Finally, we iteratively refine the depth\nand relative camera pose using the gradient-like information to maximize the\nlikelihood function of the correlations. Extensive experimental results on\nseveral datasets prove that our method significantly outperforms the\nstate-of-the-art end-to-end two-view SfM approaches in accuracy and\ngeneralization capability.",
    "descriptor": "\nComments: 8 pages, Accepted by IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2022)\n",
    "authors": [
      "Yuxi Xiao",
      "Li Li",
      "Xiaodi Li",
      "Jian Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05517"
  },
  {
    "id": "arXiv:2210.05519",
    "title": "Robust and Controllable Object-Centric Learning through Energy-based  Models",
    "abstract": "Humans are remarkably good at understanding and reasoning about complex\nvisual scenes. The capability to decompose low-level observations into discrete\nobjects allows us to build a grounded abstract representation and identify the\ncompositional structure of the world. Accordingly, it is a crucial step for\nmachine learning models to be capable of inferring objects and their properties\nfrom visual scenes without explicit supervision. However, existing works on\nobject-centric representation learning either rely on tailor-made neural\nnetwork modules or strong probabilistic assumptions in the underlying\ngenerative and inference processes. In this work, we present \\ours, a\nconceptually simple and general approach to learning object-centric\nrepresentations through an energy-based model. By forming a\npermutation-invariant energy function using vanilla attention blocks readily\navailable in Transformers, we can infer object-centric latent variables via\ngradient-based MCMC methods where permutation equivariance is automatically\nguaranteed. We show that \\ours can be easily integrated into existing\narchitectures and can effectively extract high-quality object-centric\nrepresentations, leading to better segmentation accuracy and competitive\ndownstream task performance. Further, empirical evaluations show that \\ours's\nlearned representations are robust against distribution shift. Finally, we\ndemonstrate the effectiveness of \\ours in systematic compositional\ngeneralization, by re-composing learned energy functions for novel scene\ngeneration and manipulation.",
    "descriptor": "",
    "authors": [
      "Ruixiang Zhang",
      "Tong Che",
      "Boris Ivanovic",
      "Renhao Wang",
      "Marco Pavone",
      "Yoshua Bengio",
      "Liam Paull"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05519"
  },
  {
    "id": "arXiv:2210.05521",
    "title": "Bi-Phase Enhanced IVFPQ for Time-Efficient Ad-hoc Retrieval",
    "abstract": "IVFPQ is a popular index paradigm for time-efficient ad-hoc retrieval.\nInstead of traversing the entire database for relevant documents, it\naccelerates the retrieval operation by 1) accessing a fraction of the database\nguided the activation of latent topics in IVF (inverted file system), and 2)\napproximating the exact relevance measurement based on PQ (product\nquantization). However, the conventional IVFPQ is limited in retrieval\nperformance due to the coarse granularity of its latent topics. On the one\nhand, it may result in severe loss of retrieval quality when visiting a small\nnumber of topics; on the other hand, it will lead to a huge retrieval cost when\nvisiting a large number of topics.\nTo mitigate the above problem, we propose a novel framework named Bi-Phase\nIVFPQ. It jointly uses two types of features: the latent topics and the\nexplicit terms, to build the inverted file system. Both types of features are\ncomplementary to each other, which helps to achieve better coverage of the\nrelevant documents. Besides, the documents' memberships to different IVF\nentries are learned by distilling knowledge from deep semantic models, which\nsubstantially improves the index quality and retrieval accuracy. We perform\ncomprehensive empirical studies on popular ad-hoc retrieval benchmarks, whose\nresults verify the effectiveness and efficiency of our proposed framework.",
    "descriptor": "",
    "authors": [
      "Peitian Zhang",
      "Zheng Liu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05521"
  },
  {
    "id": "arXiv:2210.05523",
    "title": "A hybrid neural-network and finite-difference method for solving Poisson  equation with jump discontinuities on interfaces",
    "abstract": "In this work, a new hybrid neural-network and finite-difference method is\ndeveloped for solving Poisson equation in a regular domain with jump\ndiscontinuities on an embedded irregular interface. Since the solution has low\nregularity across the interface, when applying finite difference discretization\nto this problem, an additional treatment accounting for the jump\ndiscontinuities must be employed at grid points near the interface. Here, we\naim to elevate such an extra effort to ease our implementation. The key idea is\nto decompose the solution into two parts: singular (non-smooth) and regular\n(smooth) parts. The neural network learning machinery incorporating given jump\nconditions finds the singular solution, while the standard finite difference\nmethod is used to obtain the regular solution with associated boundary\nconditions. Regardless of the interface geometry, these two tasks only require\na supervised learning task of function approximation and a fast direct solver\nof the Poisson equation, making the hybrid method easy to implement and\nefficient. The two- and three-dimensional numerical results show that the\npresent hybrid method preserves second-order accuracy for the solution and its\nderivatives, and it is comparable with the traditional immersed interface\nmethod in the literature.",
    "descriptor": "",
    "authors": [
      "Wei-Fan Hu",
      "Te-Sheng Lin",
      "Yu-Hau Tseng",
      "Ming-Chih Lai"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05523"
  },
  {
    "id": "arXiv:2210.05524",
    "title": "A Learning-Based Estimation and Control Framework for Contact-Intensive  Tight-Tolerance Tasks",
    "abstract": "We propose a novel data-driven estimation and control framework for\ncontact-rich tight tolerance tasks, which estimates the pose of the object\nprecisely using data-driven methods and compensates for the remaining error via\nreinforcement learning (RL). First, the sequential particle filter estimator\nupdates with the mixture density network (MDN), which is to represent the\ngeneral non-injective conditional probability and thus is suitable for finding\nout the pose from the measurements including relatively low-dimensional contact\nwrench sensing. We further develop the RL-based fastening controller that\nadapts to the remaining error by optimizing the admittance gain to complete the\ntask. The proposed framework is evaluated using an accurate real-time simulator\non the bolting task and successfully transferred to an experimental\nenvironment.",
    "descriptor": "",
    "authors": [
      "Bukun Son",
      "Hyelim Choi",
      "Jeamin Yoon",
      "Dongjun Lee"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05524"
  },
  {
    "id": "arXiv:2210.05528",
    "title": "Model Cascading: Towards Jointly Improving Efficiency and Accuracy of  NLP Systems",
    "abstract": "Do all instances need inference through the big models for a correct\nprediction? Perhaps not; some instances are easy and can be answered correctly\nby even small capacity models. This provides opportunities for improving the\ncomputational efficiency of systems. In this work, we present an explorative\nstudy on 'model cascading', a simple technique that utilizes a collection of\nmodels of varying capacities to accurately yet efficiently output predictions.\nThrough comprehensive experiments in multiple task settings that differ in the\nnumber of models available for cascading (K value), we show that cascading\nimproves both the computational efficiency and the prediction accuracy. For\ninstance, in K=3 setting, cascading saves up to 88.93% computation cost and\nconsistently achieves superior prediction accuracy with an improvement of up to\n2.18%. We also study the impact of introducing additional models in the cascade\nand show that it further increases the efficiency improvements. Finally, we\nhope that our work will facilitate development of efficient NLP systems making\ntheir widespread adoption in real-world applications possible.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Neeraj Varshney",
      "Chitta Baral"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05528"
  },
  {
    "id": "arXiv:2210.05529",
    "title": "An Exploration of Hierarchical Attention Transformers for Efficient Long  Document Classification",
    "abstract": "Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.",
    "descriptor": "",
    "authors": [
      "Ilias Chalkidis",
      "Xiang Dai",
      "Manos Fergadiotis",
      "Prodromos Malakasiotis",
      "Desmond Elliott"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05529"
  },
  {
    "id": "arXiv:2210.05533",
    "title": "Style-Guided Inference of Transformer for High-resolution Image  Synthesis",
    "abstract": "Transformer is eminently suitable for auto-regressive image synthesis which\npredicts discrete value from the past values recursively to make up full image.\nEspecially, combined with vector quantised latent representation, the\nstate-of-the-art auto-regressive transformer displays realistic high-resolution\nimages. However, sampling the latent code from discrete probability\ndistribution makes the output unpredictable. Therefore, it requires to generate\nlots of diverse samples to acquire desired outputs. To alleviate the process of\ngenerating lots of samples repetitively, in this article, we propose to take a\ndesired output, a style image, as an additional condition without re-training\nthe transformer. To this end, our method transfers the style to a probability\nconstraint to re-balance the prior, thereby specifying the target distribution\ninstead of the original prior. Thus, generated samples from the re-balanced\nprior have similar styles to reference style. In practice, we can choose either\nan image or a category of images as an additional condition. In our qualitative\nassessment, we show that styles of majority of outputs are similar to the input\nstyle.",
    "descriptor": "\nComments: This paper is accepted to WACV 2023 (Algorithm)\n",
    "authors": [
      "Jonghwa Yim",
      "Minjae Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05533"
  },
  {
    "id": "arXiv:2210.05534",
    "title": "Learning Inter-Superpoint Affinity for Weakly Supervised 3D Instance  Segmentation",
    "abstract": "Due to the few annotated labels of 3D point clouds, how to learn\ndiscriminative features of point clouds to segment object instances is a\nchallenging problem. In this paper, we propose a simple yet effective 3D\ninstance segmentation framework that can achieve good performance by annotating\nonly one point for each instance. Specifically, to tackle extremely few labels\nfor instance segmentation, we first oversegment the point cloud into\nsuperpoints in an unsupervised manner and extend the point-level annotations to\nthe superpoint level. Then, based on the superpoint graph, we propose an\ninter-superpoint affinity mining module that considers the semantic and spatial\nrelations to adaptively learn inter-superpoint affinity to generate\nhigh-quality pseudo labels via semantic-aware random walk. Finally, we propose\na volume-aware instance refinement module to segment high-quality instances by\napplying volume constraints of objects in clustering on the superpoint graph.\nExtensive experiments on the ScanNet-v2 and S3DIS datasets demonstrate that our\nmethod achieves state-of-the-art performance in the weakly supervised point\ncloud instance segmentation task, and even outperforms some fully supervised\nmethods.",
    "descriptor": "\nComments: accepted by ACCV 2022\n",
    "authors": [
      "Linghua Tang",
      "Le Hui",
      "Jin Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05534"
  },
  {
    "id": "arXiv:2210.05543",
    "title": "Parallel solutions for preemptive makespan scheduling on two identical  machines",
    "abstract": "We consider online preemptive scheduling of jobs arriving one by one, to be\nassigned to two identical machines, with the goal of makespan minimization. We\nstudy the effect of selecting the best solution out of two independent\nsolutions constructed in parallel in an online fashion. Two cases are analyzed,\nwhere one case is purely online, and in the other one jobs are presented sorted\nby non-increasing sizes. We show that using two solutions rather than one\nimproves the performance significantly, but that an optimal solution cannot be\nobtained for any constant number of solutions constructed in parallel. Our\nalgorithms have the best possible competitive ratios out of algorithms for each\none of the classes.",
    "descriptor": "",
    "authors": [
      "Leah Epstein"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05543"
  },
  {
    "id": "arXiv:2210.05546",
    "title": "What does a deep neural network confidently perceive? The effective  dimension of high certainty class manifolds and their low confidence  boundaries",
    "abstract": "Deep neural network classifiers partition input space into high confidence\nregions for each class. The geometry of these class manifolds (CMs) is widely\nstudied and intimately related to model performance; for example, the margin\ndepends on CM boundaries. We exploit the notions of Gaussian width and Gordon's\nescape theorem to tractably estimate the effective dimension of CMs and their\nboundaries through tomographic intersections with random affine subspaces of\nvarying dimension. We show several connections between the dimension of CMs,\ngeneralization, and robustness. In particular we investigate how CM dimension\ndepends on 1) the dataset, 2) architecture (including ResNet, WideResNet \\&\nVision Transformer), 3) initialization, 4) stage of training, 5) class, 6)\nnetwork width, 7) ensemble size, 8) label randomization, 9) training set size,\nand 10) robustness to data corruption. Together a picture emerges that higher\nperforming and more robust models have higher dimensional CMs. Moreover, we\noffer a new perspective on ensembling via intersections of CMs. Our code is at\nhttps://github.com/stanislavfort/slice-dice-optimize/",
    "descriptor": "\nComments: An extended version of /Slice, Dice, and Optimize: Measuring the Dimension of Neural Network Class Manifolds/\n",
    "authors": [
      "Stanislav Fort",
      "Ekin Dogus Cubuk",
      "Surya Ganguli",
      "Samuel S. Schoenholz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05546"
  },
  {
    "id": "arXiv:2210.05549",
    "title": "Continual Training of Language Models for Few-Shot Learning",
    "abstract": "Recent work on applying large language models (LMs) achieves impressive\nperformance in many NLP applications. Adapting or posttraining an LM using an\nunlabeled domain corpus can produce even better performance for end-tasks in\nthe domain. This paper proposes the problem of continually extending an LM by\nincrementally post-train the LM with a sequence of unlabeled domain corpora to\nexpand its knowledge without forgetting its previous skills. The goal is to\nimprove the few-shot end-task learning in these domains. The resulting system\nis called CPT (Continual PostTraining), which to our knowledge, is the first\ncontinual post-training system. Experimental results verify its effectiveness.",
    "descriptor": "",
    "authors": [
      "Zixuan Ke",
      "Haowei Lin",
      "Yijia Shao",
      "Hu Xu",
      "Lei Shu",
      "Bing Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.05549"
  },
  {
    "id": "arXiv:2210.05551",
    "title": "Intersections of linear codes and related MDS codes with new Galois  hulls",
    "abstract": "Let $\\mathrm{SLAut}(\\mathbb{F}_{q}^{n})$ denote the group of all semilinear\nisometries on $\\mathbb{F}_{q}^{n}$, where $q=p^{e}$ is a prime power. In this\npaper, we investigate general properties of linear codes associated with\n$\\sigma$ duals for $\\sigma\\in\\mathrm{SLAut}(\\mathbb{F}_{q}^{n})$. We show that\nthe dimension of the intersection of two linear codes can be determined by\ngenerator matrices of such codes and their $\\sigma$ duals. We also show that\nthe dimension of $\\sigma$ hull of a linear code can be determined by a\ngenerator matrix of it or its $\\sigma$ dual. We give a characterization on\n$\\sigma$ dual and $\\sigma$ hull of a matrix-product code. We also investigate\nthe intersection of a pair of matrix-product codes. We provide a necessary and\nsufficient condition under which any codeword of a generalized Reed-Solomon\n(GRS) code or an extended GRS code is contained in its $\\sigma$ dual. As an\napplication, we construct eleven families of $q$-ary MDS codes with new\n$\\ell$-Galois hulls satisfying $2(e-\\ell)\\mid e$, which cannot be produced by\nthe latest papers by Cao (IEEE Trans. Inf. Theory 67(12), 7964-7984, 2021) and\nby Fang et al. (Cryptogr. Commun. 14(1), 145-159, 2022) when $\\ell\\neq\n\\frac{e}{2}$.",
    "descriptor": "\nComments: 30 pages, 5 tables, comments are welcome\n",
    "authors": [
      "Meng Cao",
      "Jing Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05551"
  },
  {
    "id": "arXiv:2210.05552",
    "title": "Multi-Agent Distributed and Decentralized Geometric Task Allocation",
    "abstract": "We consider the general problem of geometric task allocation, wherein a\nlarge, decentralised swarm of simple mobile agents must detect the locations of\ntasks in the plane and position themselves nearby. The tasks are represented by\nan \\textit{a priori unknown} demand profile $\\Phi(x,y)$ that determines how\nmany agents are needed in each location. The agents are autonomous, oblivious\nand indistinguishable, and have finite sensing range. They must configure\nthemselves according to $\\Phi$ using only local information about $\\Phi$ and\nabout the positions of nearby agents. All agents act according to the same\nlocal sensing-based rule of motion, and cannot explicitly communicate nor share\ninformation.\nWe propose an optimization-based approach to the problem which results in\nattraction-repulsion dynamics. Repulsion encourages agents to spread out and\nexplore the region so as to find the tasks, and attraction causes them to\naccumulate at task locations. We derive this approach via gradient descent over\nan appropriate \"error\" functional, and test it extensively through numerical\nsimulations.\\\\ The figures in this work are snapshots of simulations that can\nbe viewed online at https://youtu.be/vB4nLRheVww.",
    "descriptor": "",
    "authors": [
      "Michael Amir",
      "Yigal Koifman",
      "Yakov Bloch",
      "Ariel Barel",
      "Alfred M. Bruckstein"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.05552"
  },
  {
    "id": "arXiv:2210.05553",
    "title": "Evaluating Unsupervised Denoising Requires Unsupervised Metrics",
    "abstract": "Unsupervised denoising is a crucial challenge in real-world imaging\napplications. Unsupervised deep-learning methods have demonstrated impressive\nperformance on benchmarks based on synthetic noise. However, no metrics are\navailable to evaluate these methods in an unsupervised fashion. This is highly\nproblematic for the many practical applications where ground-truth clean images\nare not available. In this work, we propose two novel metrics: the unsupervised\nmean squared error (MSE) and the unsupervised peak signal-to-noise ratio\n(PSNR), which are computed using only noisy data. We provide a theoretical\nanalysis of these metrics, showing that they are asymptotically consistent\nestimators of the supervised MSE and PSNR. Controlled numerical experiments\nwith synthetic noise confirm that they provide accurate approximations in\npractice. We validate our approach on real-world data from two imaging\nmodalities: videos in raw format and transmission electron microscopy. Our\nresults demonstrate that the proposed metrics enable unsupervised evaluation of\ndenoising methods based exclusively on noisy data.",
    "descriptor": "",
    "authors": [
      "Adria Marcos-Morales",
      "Matan Leibovich",
      "Sreyas Mohan",
      "Joshua Lawrence Vincent",
      "Piyush Haluai",
      "Mai Tan",
      "Peter Crozier",
      "Carlos Fernandez-Granda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05553"
  },
  {
    "id": "arXiv:2210.05556",
    "title": "ViLPAct: A Benchmark for Compositional Generalization on Multimodal  Human Activities",
    "abstract": "We introduce ViLPAct, a novel vision-language benchmark for human activity\nplanning. It is designed for a task where embodied AI agents can reason and\nforecast future actions of humans based on video clips about their initial\nactivities and intents in text. The dataset consists of 2.9k videos from\n\\charades extended with intents via crowdsourcing, a multi-choice question test\nset, and four strong baselines. One of the baselines implements a neurosymbolic\napproach based on a multi-modal knowledge base (MKB), while the other ones are\ndeep generative models adapted from recent state-of-the-art (SOTA) methods.\nAccording to our extensive experiments, the key challenges are compositional\ngeneralization and effective use of information from both modalities.",
    "descriptor": "",
    "authors": [
      "Terry Yue Zhuo",
      "Yaqing Liao",
      "Yuecheng Lei",
      "Lizhen Qu",
      "Gerard de Melo",
      "Xiaojun Chang",
      "Yazhou Ren",
      "Zenglin Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05556"
  },
  {
    "id": "arXiv:2210.05557",
    "title": "OPERA: Omni-Supervised Representation Learning with Hierarchical  Supervisions",
    "abstract": "The pretrain-finetune paradigm in modern computer vision facilitates the\nsuccess of self-supervised learning, which tends to achieve better\ntransferability than supervised learning. However, with the availability of\nmassive labeled data, a natural question emerges: how to train a better model\nwith both self and full supervision signals? In this paper, we propose\nOmni-suPErvised Representation leArning with hierarchical supervisions (OPERA)\nas a solution. We provide a unified perspective of supervisions from labeled\nand unlabeled data and propose a unified framework of fully supervised and\nself-supervised learning. We extract a set of hierarchical proxy\nrepresentations for each image and impose self and full supervisions on the\ncorresponding proxy representations. Extensive experiments on both\nconvolutional neural networks and vision transformers demonstrate the\nsuperiority of OPERA in image classification, segmentation, and object\ndetection. Code is available at: https://github.com/wangck20/OPERA.",
    "descriptor": "\nComments: Source code available at: this https URL\n",
    "authors": [
      "Chengkun Wang",
      "Wenzhao Zheng",
      "Zheng Zhu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05557"
  },
  {
    "id": "arXiv:2210.05559",
    "title": "Unifying Diffusion Models' Latent Space, with Applications to  CycleDiffusion and Guidance",
    "abstract": "Diffusion models have achieved unprecedented performance in generative\nmodeling. The commonly-adopted formulation of the latent code of diffusion\nmodels is a sequence of gradually denoised samples, as opposed to the simpler\n(e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper\nprovides an alternative, Gaussian formulation of the latent space of various\ndiffusion models, as well as an invertible DPM-Encoder that maps images into\nthe latent space. While our formulation is purely based on the definition of\ndiffusion models, we demonstrate several intriguing consequences. (1)\nEmpirically, we observe that a common latent space emerges from two diffusion\nmodels trained independently on related domains. In light of this finding, we\npropose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image\ntranslation. Furthermore, applying CycleDiffusion to text-to-image diffusion\nmodels, we show that large-scale text-to-image diffusion models can be used as\nzero-shot image-to-image editors. (2) One can guide pre-trained diffusion\nmodels and GANs by controlling the latent codes in a unified, plug-and-play\nformulation based on energy-based models. Using the CLIP model and a face\nrecognition model as guidance, we demonstrate that diffusion models have better\ncoverage of low-density sub-populations and individuals than GANs.",
    "descriptor": "",
    "authors": [
      "Chen Henry Wu",
      "Fernando De la Torre"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05559"
  },
  {
    "id": "arXiv:2210.05560",
    "title": "Comparison of encrypted control approaches and tutorial on dynamic  systems using LWE-based homomorphic encryption",
    "abstract": "Encrypted control has been introduced to protect controller data by\nencryption at the stage of computation and communication, by performing the\ncomputation directly on encrypted data. In this article, we first review and\ncategorize recent relevant studies on encrypted control. Approaches based on\nhomomorphic encryption, multi-party computation, and secret sharing are\nintroduced, compared, and then discussed with respect to computational\ncomplexity, communication load, enabled operations, security, and research\ndirections. We proceed to discuss a current challenge in the application of\nhomomorphic encryption to dynamic systems, where arithmetic operations other\nthan integer addition and multiplication are limited. We also introduce a\nhomomorphic cryptosystem called ``GSW-LWE'' and discuss its benefits that allow\nfor recursive multiplication of encrypted dynamic systems, without use of\ncomputationally expensive bootstrapping techniques.",
    "descriptor": "\nComments: 33 pages, 12 figures, submitted to Annual Reviews in Control\n",
    "authors": [
      "Junsoo Kim",
      "Dongwoo Kim",
      "Yongsoo Song",
      "Hyungbo Shim",
      "Henrik Sandberg",
      "Karl H. Johansson"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05560"
  },
  {
    "id": "arXiv:2210.05561",
    "title": "Schedule-Robust Online Continual Learning",
    "abstract": "A continual learning (CL) algorithm learns from a non-stationary data stream.\nThe non-stationarity is modeled by some schedule that determines how data is\npresented over time. Most current methods make strong assumptions on the\nschedule and have unpredictable performance when such requirements are not met.\nA key challenge in CL is thus to design methods robust against arbitrary\nschedules over the same underlying data, since in real-world scenarios\nschedules are often unknown and dynamic. In this work, we introduce the notion\nof schedule-robustness for CL and a novel approach satisfying this desirable\nproperty in the challenging online class-incremental setting. We also present a\nnew perspective on CL, as the process of learning a schedule-robust predictor,\nfollowed by adapting the predictor using only replay data. Empirically, we\ndemonstrate that our approach outperforms existing methods on CL benchmarks for\nimage classification by a large margin.",
    "descriptor": "",
    "authors": [
      "Ruohan Wang",
      "Marco Ciccone",
      "Giulia Luise",
      "Massimiliano Pontil",
      "Andrew Yapp",
      "Carlo Ciliberto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05561"
  },
  {
    "id": "arXiv:2210.05564",
    "title": "Hypergraph Convolutional Networks for Weakly-Supervised Semantic  Segmentation",
    "abstract": "Semantic segmentation is a fundamental topic in computer vision. Several deep\nlearning methods have been proposed for semantic segmentation with outstanding\nresults. However, these models require a lot of densely annotated images. To\naddress this problem, we propose a new algorithm that uses HyperGraph\nConvolutional Networks for Weakly-supervised Semantic Segmentation\n(HyperGCN-WSS). Our algorithm constructs spatial and k-Nearest Neighbor (k-NN)\ngraphs from the images in the dataset to generate the hypergraphs. Then, we\ntrain a specialized HyperGraph Convolutional Network (HyperGCN) architecture\nusing some weak signals. The outputs of the HyperGCN are denominated\npseudo-labels, which are later used to train a DeepLab model for semantic\nsegmentation. HyperGCN-WSS is evaluated on the PASCAL VOC 2012 dataset for\nsemantic segmentation, using scribbles or clicks as weak signals. Our algorithm\nshows competitive performance against previous methods.",
    "descriptor": "\nComments: Accepted in IEEE International Conference on Image Processing 2022\n",
    "authors": [
      "Jhony H. Giraldo",
      "Vincenzo Scarrica",
      "Antonino Staiano",
      "Francesco Camastra",
      "Thierry Bouwmans"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05564"
  },
  {
    "id": "arXiv:2210.05566",
    "title": "The Equalization Losses: Gradient-Driven Training for Long-tailed Object  Recognition",
    "abstract": "Long-tail distribution is widely spread in real-world applications. Due to\nthe extremely small ratio of instances, tail categories often show inferior\naccuracy. In this paper, we find such performance bottleneck is mainly caused\nby the imbalanced gradients, which can be categorized into two parts: (1)\npositive part, deriving from the samples of the same category, and (2) negative\npart, contributed by other categories. Based on comprehensive experiments, it\nis also observed that the gradient ratio of accumulated positives to negatives\nis a good indicator to measure how balanced a category is trained. Inspired by\nthis, we come up with a gradient-driven training mechanism to tackle the\nlong-tail problem: re-balancing the positive/negative gradients dynamically\naccording to current accumulative gradients, with a unified goal of achieving\nbalance gradient ratios. Taking advantage of the simple and flexible gradient\nmechanism, we introduce a new family of gradient-driven loss functions, namely\nequalization losses. We conduct extensive experiments on a wide spectrum of\nvisual tasks, including two-stage/single-stage long-tailed object detection\n(LVIS), long-tailed image classification (ImageNet-LT, Places-LT, iNaturalist),\nand long-tailed semantic segmentation (ADE20K). Our method consistently\noutperforms the baseline models, demonstrating the effectiveness and\ngeneralization ability of the proposed equalization losses. Codes will be\nreleased at https://github.com/ModelTC/United-Perception.",
    "descriptor": "",
    "authors": [
      "Jingru Tan",
      "Bo Li",
      "Xin Lu",
      "Yongqiang Yao",
      "Fengwei Yu",
      "Tong He",
      "Wanli Ouyang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05566"
  },
  {
    "id": "arXiv:2210.05567",
    "title": "Global Spectral Filter Memory Network for Video Object Segmentation",
    "abstract": "This paper studies semi-supervised video object segmentation through boosting\nintra-frame interaction. Recent memory network-based methods focus on\nexploiting inter-frame temporal reference while paying little attention to\nintra-frame spatial dependency. Specifically, these segmentation model tends to\nbe susceptible to interference from unrelated nontarget objects in a certain\nframe. To this end, we propose Global Spectral Filter Memory network (GSFM),\nwhich improves intra-frame interaction through learning long-term spatial\ndependencies in the spectral domain. The key components of GSFM is 2D (inverse)\ndiscrete Fourier transform for spatial information mixing. Besides, we\nempirically find low frequency feature should be enhanced in encoder (backbone)\nwhile high frequency for decoder (segmentation head). We attribute this to\nsemantic information extracting role for encoder and fine-grained details\nhighlighting role for decoder. Thus, Low (High) Frequency Module is proposed to\nfit this circumstance. Extensive experiments on the popular DAVIS and\nYouTube-VOS benchmarks demonstrate that GSFM noticeably outperforms the\nbaseline method and achieves state-of-the-art performance. Besides, extensive\nanalysis shows that the proposed modules are reasonable and of great\ngeneralization ability. Our source code is available at\nhttps://github.com/workforai/GSFM.",
    "descriptor": "\nComments: ECCV2022\n",
    "authors": [
      "Yong Liu",
      "Ran Yu",
      "Jiahao Wang",
      "Xinyuan Zhao",
      "Yitong Wang",
      "Yansong Tang",
      "Yujiu Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05567"
  },
  {
    "id": "arXiv:2210.05568",
    "title": "Improving Long-tailed Object Detection with Image-Level Supervision by  Multi-Task Collaborative Learning",
    "abstract": "Data in real-world object detection often exhibits the long-tailed\ndistribution. Existing solutions tackle this problem by mitigating the\ncompetition between the head and tail categories. However, due to the scarcity\nof training samples, tail categories are still unable to learn discriminative\nrepresentations. Bringing more data into the training may alleviate the\nproblem, but collecting instance-level annotations is an excruciating task. In\ncontrast, image-level annotations are easily accessible but not fully\nexploited. In this paper, we propose a novel framework CLIS (multi-task\nCollaborative Learning with Image-level Supervision), which leverage\nimage-level supervision to enhance the detection ability in a multi-task\ncollaborative way. Specifically, there are an object detection task (consisting\nof an instance-classification task and a localization task) and an\nimage-classification task in our framework, responsible for utilizing the two\ntypes of supervision. Different tasks are trained collaboratively by three key\ndesigns: (1) task-specialized sub-networks that learn specific representations\nof different tasks without feature entanglement. (2) a siamese sub-network for\nthe image-classification task that shares its knowledge with the\ninstance-classification task, resulting in feature enrichment of detectors. (3)\na contrastive learning regularization that maintains representation\nconsistency, bridging feature gaps of different supervision. Extensive\nexperiments are conducted on the challenging LVIS dataset. Without\nsophisticated loss engineering, CLIS achieves an overall AP of 31.1 with 10.1\npoint improvement on tail categories, establishing a new state-of-the-art. Code\nwill be at https://github.com/waveboo/CLIS.",
    "descriptor": "",
    "authors": [
      "Bo Li",
      "Yongqiang Yao",
      "Jingru Tan",
      "Xin Lu",
      "Fengwei Yu",
      "Ye Luo",
      "Jianwei Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05568"
  },
  {
    "id": "arXiv:2210.05572",
    "title": "Knowledge-Driven New Drug Recommendation",
    "abstract": "Drug recommendation assists doctors in prescribing personalized medications\nto patients based on their health conditions. Existing drug recommendation\nsolutions adopt the supervised multi-label classification setup and only work\nwith existing drugs with sufficient prescription data from many patients.\nHowever, newly approved drugs do not have much historical prescription data and\ncannot leverage existing drug recommendation methods. To address this, we\nformulate the new drug recommendation as a few-shot learning problem. Yet,\ndirectly applying existing few-shot learning algorithms faces two challenges:\n(1) complex relations among diseases and drugs and (2) numerous false-negative\npatients who were eligible but did not yet use the new drugs. To tackle these\nchallenges, we propose EDGE, which can quickly adapt to the recommendation for\na new drug with limited prescription data from a few support patients. EDGE\nmaintains a drug-dependent multi-phenotype few-shot learner to bridge the gap\nbetween existing and new drugs. Specifically, EDGE leverages the drug ontology\nto link new drugs to existing drugs with similar treatment effects and learns\nontology-based drug representations. Such drug representations are used to\ncustomize the metric space of the phenotype-driven patient representations,\nwhich are composed of a set of phenotypes capturing complex patient health\nstatus. Lastly, EDGE eliminates the false-negative supervision signal using an\nexternal drug-disease knowledge base. We evaluate EDGE on two real-world\ndatasets: the public EHR data (MIMIC-IV) and private industrial claims data.\nResults show that EDGE achieves 7.3% improvement on the ROC-AUC score over the\nbest baseline.",
    "descriptor": "",
    "authors": [
      "Zhenbang Wu",
      "Huaxiu Yao",
      "Zhe Su",
      "David M Liebovitz",
      "Lucas M Glass",
      "James Zou",
      "Chelsea Finn",
      "Jimeng Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05572"
  },
  {
    "id": "arXiv:2210.05573",
    "title": "Higher Order Far-Field Boundary Conditions for Crystalline Defects",
    "abstract": "Lattice defects in crystalline materials create long-range elastic fields\nthat can be modeled on the atomistic scale. The low rank structures of defect\nconfigurations are revealed by a rigorous far-field expansion of the long-range\nelastic fields, and thus the defect equilibrium can be expressed as a sum of\ncontinuum correctors and discrete multipole terms that are essentially\ncomputable. In this paper, we develop a novel family of numerical schemes that\nexploit the multipole expansions to accelerate the simulation of crystalline\ndefects. In particular, the relatively slow convergence rate of the standard\ncell approximations for defect equilibration could be significantly developed.\nTo enclose the simulation in a finite domain, a theoretically justified\napproximation of multipole tensors is therefore introduced, which leads to a\nnovel moment iteration as well as the higher order boundary conditions.\nMoreover, we consider a continuous version of multipole expansions to acquire\nefficiency in practical implementation. Several prototypical numerical examples\nof point defects are presented to test the convergence for both geometry error\nand energy error. The numerical results show that our proposed numerical scheme\ncan achieve the accelerated convergence rates in terms of computational cell\nsize with the higher order boundary conditions.",
    "descriptor": "",
    "authors": [
      "Julian Braun",
      "Christoph Ortner",
      "Yangshuai Wang",
      "Lei Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.05573"
  },
  {
    "id": "arXiv:2210.05574",
    "title": "Motion Aware Self-Supervision for Generic Event Boundary Detection",
    "abstract": "The task of Generic Event Boundary Detection (GEBD) aims to detect moments in\nvideos that are naturally perceived by humans as generic and taxonomy-free\nevent boundaries. Modeling the dynamically evolving temporal and spatial\nchanges in a video makes GEBD a difficult problem to solve. Existing approaches\ninvolve very complex and sophisticated pipelines in terms of architectural\ndesign choices, hence creating a need for more straightforward and simplified\napproaches. In this work, we address this issue by revisiting a simple and\neffective self-supervised method and augment it with a differentiable motion\nfeature learning module to tackle the spatial and temporal diversities in the\nGEBD task. We perform extensive experiments on the challenging Kinetics-GEBD\nand TAPOS datasets to demonstrate the efficacy of the proposed approach\ncompared to the other self-supervised state-of-the-art methods. We also show\nthat this simple self-supervised approach learns motion features without any\nexplicit motion-specific pretext task.",
    "descriptor": "\nComments: Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023\n",
    "authors": [
      "Ayush K. Rai",
      "Tarun Krishna",
      "Julia Dietlmeier",
      "Kevin McGuinness",
      "Alan F. Smeaton",
      "Noel E. O'Connor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05574"
  },
  {
    "id": "arXiv:2210.05577",
    "title": "What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?",
    "abstract": "The adversarial vulnerability of neural nets, and subsequent techniques to\ncreate robust models have attracted significant attention; yet we still lack a\nfull understanding of this phenomenon. Here, we study adversarial examples of\ntrained neural networks through analytical tools afforded by recent theory\nadvances connecting neural networks and kernel methods, namely the Neural\nTangent Kernel (NTK), following a growing body of work that leverages the NTK\napproximation to successfully analyze important deep learning phenomena and\ndesign algorithms for new applications. We show how NTKs allow to generate\nadversarial examples in a ``training-free'' fashion, and demonstrate that they\ntransfer to fool their finite-width neural net counterparts in the ``lazy''\nregime. We leverage this connection to provide an alternative view on robust\nand non-robust features, which have been suggested to underlie the adversarial\nbrittleness of neural nets. Specifically, we define and study features induced\nby the eigendecomposition of the kernel to better understand the role of robust\nand non-robust features, the reliance on both for standard classification and\nthe robustness-accuracy trade-off. We find that such features are surprisingly\nconsistent across architectures, and that robust features tend to correspond to\nthe largest eigenvalues of the model, and thus are learned early during\ntraining. Our framework allows us to identify and visualize non-robust yet\nuseful features. Finally, we shed light on the robustness mechanism underlying\nadversarial training of neural nets used in practice: quantifying the evolution\nof the associated empirical NTK, we demonstrate that its dynamics falls much\nearlier into the ``lazy'' regime and manifests a much stronger form of the well\nknown bias to prioritize learning features within the top eigenspaces of the\nkernel, compared to standard training.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Nikolaos Tsilivis",
      "Julia Kempe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05577"
  },
  {
    "id": "arXiv:2210.05579",
    "title": "Benefits of Permutation-Equivariance in Auction Mechanisms",
    "abstract": "Designing an incentive-compatible auction mechanism that maximizes the\nauctioneer's revenue while minimizes the bidders' ex-post regret is an\nimportant yet intricate problem in economics. Remarkable progress has been\nachieved through learning the optimal auction mechanism by neural networks. In\nthis paper, we consider the popular additive valuation and symmetric valuation\nsetting; i.e., the valuation for a set of items is defined as the sum of all\nitems' valuations in the set, and the valuation distribution is invariant when\nthe bidders and/or the items are permutated. We prove that\npermutation-equivariant neural networks have significant advantages: the\npermutation-equivariance decreases the expected ex-post regret, improves the\nmodel generalizability, while maintains the expected revenue invariant. This\nimplies that the permutation-equivariance helps approach the theoretically\noptimal dominant strategy incentive compatible condition, and reduces the\nrequired sample complexity for desired generalization. Extensive experiments\nfully support our theory. To our best knowledge, this is the first work towards\nunderstanding the benefits of permutation-equivariance in auction mechanisms.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Tian Qin",
      "Fengxiang He",
      "Dingfeng Shi",
      "Wenbing Huang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05579"
  },
  {
    "id": "arXiv:2210.05581",
    "title": "Aggregating Crowdsourced and Automatic Judgments to Scale Up a Corpus of  Anaphoric Reference for Fiction and Wikipedia Texts",
    "abstract": "Although several datasets annotated for anaphoric reference/coreference\nexist, even the largest such datasets have limitations in terms of size, range\nof domains, coverage of anaphoric phenomena, and size of documents included.\nYet, the approaches proposed to scale up anaphoric annotation haven't so far\nresulted in datasets overcoming these limitations. In this paper, we introduce\na new release of a corpus for anaphoric reference labelled via a\ngame-with-a-purpose. This new release is comparable in size to the largest\nexisting corpora for anaphoric reference due in part to substantial activity by\nthe players, in part thanks to the use of a new resolve-and-aggregate paradigm\nto 'complete' markable annotations through the combination of an anaphoric\nresolver and an aggregation method for anaphoric reference. The proposed method\ncould be adopted to greatly speed up annotation time in other projects\ninvolving games-with-a-purpose. In addition, the corpus covers genres for which\nno comparable size datasets exist (Fiction and Wikipedia); it covers singletons\nand non-referring expressions; and it includes a substantial number of long\ndocuments (> 2K in length).",
    "descriptor": "",
    "authors": [
      "Juntao Yu",
      "Silviu Paun",
      "Maris Camilleri",
      "Paloma Carretero Garcia",
      "Jon Chamberlain",
      "Udo Kruschwitz",
      "Massimo Poesio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05581"
  },
  {
    "id": "arXiv:2210.05589",
    "title": "Combining Relaying and Reflective Surfaces: Power Consumption and Energy  Efficiency Analysis",
    "abstract": "Hybrid relaying networks (HRNs) combining both a relay and an intelligent\nreflective surface (IRS) can lead to enhanced rate performance compared to\nnon-hybrid relaying schemes, where only either an IRS or a relay is utilized.\nHowever, utilizing both the relay and the IRS simultaneously results in higher\npower consumption for the HRNs compared to their counterpart. In this work, we\nstudy the required transmit power levels and the energy efficiency (EE) of HRNs\nutilizing both a half-duplex decode-and-forward (HD-DF) relay and an IRS, and\ncompare their performance with non-hybrid relaying schemes. The impact of the\nrequired channel estimation overheads is considered when the reflective\nbeamforming design (RBD) at the IRS is carried out under both instantaneous and\nstatistical channel state information models. Also, the investigation is\nperformed for both slow- and fast-changing environments. In terms of the\ntransmit power requirements, our results show that HRNs can lead to higher\npower savings if the number of reflective elements at the IRS is not very\nlarge. However, non-hybrid relaying schemes are shown to be more\nenergy-efficient, unless the targeted rate is high and the IRS is distant from\nboth transmitter and receiver but within a close proximity to the relay.",
    "descriptor": "",
    "authors": [
      "Zaid Abdullah",
      "George C. Alexandropoulos",
      "Steven Kisseleff",
      "Symeon Chatzinotas",
      "Bjorn Ottersten"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05589"
  },
  {
    "id": "arXiv:2210.05593",
    "title": "Prototypical VoteNet for Few-Shot 3D Point Cloud Object Detection",
    "abstract": "Most existing 3D point cloud object detection approaches heavily rely on\nlarge amounts of labeled training data. However, the labeling process is costly\nand time-consuming. This paper considers few-shot 3D point cloud object\ndetection, where only a few annotated samples of novel classes are needed with\nabundant samples of base classes. To this end, we propose Prototypical VoteNet\nto recognize and localize novel instances, which incorporates two new modules:\nPrototypical Vote Module (PVM) and Prototypical Head Module (PHM).\nSpecifically, as the 3D basic geometric structures can be shared among\ncategories, PVM is designed to leverage class-agnostic geometric prototypes,\nwhich are learned from base classes, to refine local features of novel\ncategories.Then PHM is proposed to utilize class prototypes to enhance the\nglobal feature of each object, facilitating subsequent object localization and\nclassification, which is trained by the episodic training strategy. To evaluate\nthe model in this new setting, we contribute two new benchmark datasets,\nFS-ScanNet and FS-SUNRGBD. We conduct extensive experiments to demonstrate the\neffectiveness of Prototypical VoteNet, and our proposed method shows\nsignificant and consistent improvements compared to baselines on two benchmark\ndatasets.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Shizhen Zhao",
      "Xiaojuan Qi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05593"
  },
  {
    "id": "arXiv:2210.05594",
    "title": "Navigating Ensemble Configurations for Algorithmic Fairness",
    "abstract": "Bias mitigators can improve algorithmic fairness in machine learning models,\nbut their effect on fairness is often not stable across data splits. A popular\napproach to train more stable models is ensemble learning, but unfortunately,\nit is unclear how to combine ensembles with mitigators to best navigate\ntrade-offs between fairness and predictive performance. To that end, we built\nan open-source library enabling the modular composition of 8 mitigators, 4\nensembles, and their corresponding hyperparameters, and we empirically explored\nthe space of configurations on 13 datasets. We distilled our insights from this\nexploration in the form of a guidance diagram for practitioners that we\ndemonstrate is robust and reproducible.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2202.00751\n",
    "authors": [
      "Michael Feffer",
      "Martin Hirzel",
      "Samuel C. Hoffman",
      "Kiran Kate",
      "Parikshit Ram",
      "Avraham Shinnar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.05594"
  },
  {
    "id": "arXiv:2210.05596",
    "title": "Geometry of Radial Basis Neural Networks for Safety Biased Approximation  of Unsafe Regions",
    "abstract": "Barrier function-based inequality constraints are a means to enforce safety\nspecifications for control systems. When used in conjunction with a convex\noptimization program, they provide a computationally efficient method to\nenforce safety for the general class of control-affine systems. One of the main\nassumptions when taking this approach is the a priori knowledge of the barrier\nfunction itself, i.e., knowledge of the safe set. In the context of navigation\nthrough unknown environments where the locally safe set evolves with time, such\nknowledge does not exist. This manuscript focuses on the synthesis of a zeroing\nbarrier function characterizing the safe set based on safe and unsafe sample\nmeasurements, e.g., from perception data in navigation applications. Prior work\nformulated a supervised machine learning algorithm whose solution guaranteed\nthe construction of a zeroing barrier function with specific level-set\nproperties. However, it did not explore the geometry of the neural network\ndesign used for the synthesis process. This manuscript describes the specific\ngeometry of the neural network used for zeroing barrier function synthesis, and\nshows how the network provides the necessary representation for splitting the\nstate space into safe and unsafe regions.",
    "descriptor": "",
    "authors": [
      "Ahmad Abuaish",
      "Mohit Srinivasan",
      "Patricio A. Vela"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05596"
  },
  {
    "id": "arXiv:2210.05598",
    "title": "Enriching Biomedical Knowledge for Low-resource Language Through  Translation",
    "abstract": "Biomedical data and benchmarks are highly valuable yet very limited in\nlow-resource languages other than English such as Vietnamese. In this paper, we\nmake use of a state-of-the-art translation model in English-Vietnamese to\ntranslate and produce both pretrained as well as supervised data in the\nbiomedical domains. Thanks to such large-scale translation, we introduce\nViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20\nmillion translated abstracts from the high-quality public PubMed corpus.\nViPubMedT5 demonstrates state-of-the-art results on two different biomedical\nbenchmarks in summarization and acronym disambiguation. Further, we release\nViMedNLI - a new NLP task in Vietnamese translated from MedNLI using the\nrecently public En-vi translation model and carefully refined by human experts,\nwith evaluations of existing methods against ViPubmedT5.",
    "descriptor": "",
    "authors": [
      "Long Phan",
      "Tai Dang",
      "Hieu Tran",
      "Vy Phan",
      "Lam D. Chau",
      "Trieu H. Trinh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05598"
  },
  {
    "id": "arXiv:2210.05599",
    "title": "Improving Sample Efficiency of Deep Learning Models in Electricity  Market",
    "abstract": "The superior performance of deep learning relies heavily on a large\ncollection of sample data, but the data insufficiency problem turns out to be\nrelatively common in global electricity markets. How to prevent overfitting in\nthis case becomes a fundamental challenge when training deep learning models in\ndifferent market applications. With this in mind, we propose a general\nframework, namely Knowledge-Augmented Training (KAT), to improve the sample\nefficiency, and the main idea is to incorporate domain knowledge into the\ntraining procedures of deep learning models. Specifically, we propose a novel\ndata augmentation technique to generate some synthetic data, which are later\nprocessed by an improved training strategy. This KAT methodology follows and\nrealizes the idea of combining analytical and deep learning models together.\nModern learning theories demonstrate the effectiveness of our method in terms\nof effective prediction error feedbacks, a reliable loss function, and rich\ngradient noises. At last, we study two popular applications in detail: user\nmodeling and probabilistic price forecasting. The proposed method outperforms\nother competitors in all numerical tests, and the underlying reasons are\nexplained by further statistical and visualization results.",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Power Systems, 12 pages, 11 figures, 6 tables\n",
    "authors": [
      "Guangchun Ruan",
      "Jianxiao Wang",
      "Haiwang Zhong",
      "Qing Xia",
      "Chongqing Kang"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05599"
  },
  {
    "id": "arXiv:2210.05600",
    "title": "Observability Analysis of Graph SLAM-Based Joint Calibration of Multiple  Microphone Arrays and Sound Source Localization",
    "abstract": "Multiple microphone arrays have many applications in robot audition,\nincluding sound source localization, audio scene perception and analysis, etc.\nHowever, accurate calibration of multiple microphone arrays remains a challenge\nbecause there are many unknown parameters to be identified, including the Euler\nangles, geometry, asynchronous factors between the microphone arrays. This\npaper is concerned with joint calibration of multiple microphone arrays and\nsound source localization using graph simultaneous localization and mapping\n(SLAM). By using a Fisher information matrix (FIM) approach, we focus on the\nobservability analysis of the graph SLAM framework for the above-mentioned\ncalibration problem. We thoroughly investigate the identifiability of the\nunknown parameters, including the Euler angles, geometry, asynchronous effects\nbetween the microphone arrays, and the sound source locations. We establish\nnecessary/sufficient conditions under which the FIM and the Jacobian matrix\nhave full column rank, which implies the identifiability of the unknown\nparameters. These conditions are closely related to the variation in the motion\nof the sound source and the configuration of microphone arrays, and have\nintuitive and physical interpretations. We also discover several scenarios\nwhere the unknown parameters are not uniquely identifiable. All theoretical\nfindings are demonstrated using simulation data.",
    "descriptor": "\nComments: This paper is accepted to and going to be presented at the 2023 IEEE/SICE International Symposium on System Integrations, Atlanta, USA\n",
    "authors": [
      "Yuanzheng He",
      "Jiang Wang",
      "Daobilige Su",
      "Kazuhiro Nakadai",
      "Junfeng Wu",
      "Shoudong Huang",
      "Youfu Li",
      "He Kong"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05600"
  },
  {
    "id": "arXiv:2210.05602",
    "title": "On Monotonicities of Interval Valued Functions",
    "abstract": "In this paper we introduce the notion of conditional monotonicity and from it\nthe concepts of conditional monotonicity given a vector of degenerated\nintervals and conditional monotonicity given a constant vector of functions to\nthe setting of intervals endowed with admissible orders. This work is a step\nafter the contribution of Sesma-Sara et al., where these monotonicities were\nintroduced in terms of the (non linear partial) \\textit{Kulisch-Miranker\norder}. Besides, whereas Sesma-Sara et. al defined weak/directional\nmonotonicities by using points in euclidean plane, we use just intevals. The\npaper also proposes the notions of conditional monotonicity with respect to a\nfunction $ G $ and a parameter $\\Lambda$ for intervals -- the interval\ncounter-part of $g$-monotonicity proposed by Santiago et. al in 2021 -- and\npre-aggregations IV-functions. The paper shows some properties, how some\ninterval implications behave with respect to such new monotonicities and the\nrelationships between abstract homogeneity and these notions.\nkeywords: Interval Valued Functions; Admissible orders; Conditional\nmonotonicity; Weak/Directional/G-weak monotonicity; Pre-aggregation functions;\nAbstract homogeneity.",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Ana Shirley Monteiro",
      "Regivan Santiago",
      "Martin Papco",
      "Radko Mesiar",
      "Humberto Bustince"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05602"
  },
  {
    "id": "arXiv:2210.05607",
    "title": "Divergence Results and Convergence of a Variance Reduced Version of ADAM",
    "abstract": "Stochastic optimization algorithms using exponential moving averages of the\npast gradients, such as ADAM, RMSProp and AdaGrad, have been having great\nsuccesses in many applications, especially in training deep neural networks.\nADAM in particular stands out as efficient and robust. Despite of its\noutstanding performance, ADAM has been proved to be divergent for some specific\nproblems. We revisit the divergent question and provide divergent examples\nunder stronger conditions such as in expectation or high probability. Under a\nvariance reduction assumption, we show that an ADAM-type algorithm converges,\nwhich means that it is the variance of gradients that causes the divergence of\noriginal ADAM. To this end, we propose a variance reduced version of ADAM and\nprovide a convergent analysis of the algorithm. Numerical experiments show that\nthe proposed algorithm has as good performance as ADAM. Our work suggests a new\ndirection for fixing the convergence issues.",
    "descriptor": "",
    "authors": [
      "Ruiqi Wang",
      "Diego Klabjan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05607"
  },
  {
    "id": "arXiv:2210.05610",
    "title": "MTet: Multi-domain Translation for English and Vietnamese",
    "abstract": "We introduce MTet, the largest publicly available parallel corpus for\nEnglish-Vietnamese translation. MTet consists of 4.2M high-quality training\nsentence pairs and a multi-domain test set refined by the Vietnamese research\ncommunity. Combining with previous works on English-Vietnamese translation, we\ngrow the existing parallel dataset to 6.2M sentence pairs. We also release the\nfirst pretrained model EnViT5 for English and Vietnamese languages. Combining\nboth resources, our model significantly outperforms previous state-of-the-art\nresults by up to 2 points in translation BLEU score, while being 1.6 times\nsmaller.",
    "descriptor": "",
    "authors": [
      "Chinh Ngo",
      "Trieu H. Trinh",
      "Long Phan",
      "Hieu Tran",
      "Tai Dang",
      "Hieu Nguyen",
      "Minh Nguyen",
      "Minh-Thang Luong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05610"
  },
  {
    "id": "arXiv:2210.05613",
    "title": "Contrastive Training Improves Zero-Shot Classification of  Semi-structured Documents",
    "abstract": "We investigate semi-structured document classification in a zero-shot\nsetting. Classification of semi-structured documents is more challenging than\nthat of standard unstructured documents, as positional, layout, and style\ninformation play a vital role in interpreting such documents. The standard\nclassification setting where categories are fixed during both training and\ntesting falls short in dynamic environments where new document categories could\npotentially emerge. We focus exclusively on the zero-shot setting where\ninference is done on new unseen classes. To address this task, we propose a\nmatching-based approach that relies on a pairwise contrastive objective for\nboth pretraining and fine-tuning. Our results show a significant boost in Macro\nF$_1$ from the proposed pretraining step in both supervised and unsupervised\nzero-shot settings.",
    "descriptor": "",
    "authors": [
      "Muhammad Khalifa",
      "Yogarshi Vyas",
      "Shuai Wang",
      "Graham Horwood",
      "Sunil Mallya",
      "Miguel Ballesteros"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05613"
  },
  {
    "id": "arXiv:2210.05614",
    "title": "An Experimental Study on Private Aggregation of Teacher Ensemble  Learning for End-to-End Speech Recognition",
    "abstract": "Differential privacy (DP) is one data protection avenue to safeguard user\ninformation used for training deep models by imposing noisy distortion on\nprivacy data. Such a noise perturbation often results in a severe performance\ndegradation in automatic speech recognition (ASR) in order to meet a privacy\nbudget $\\varepsilon$. Private aggregation of teacher ensemble (PATE) utilizes\nensemble probabilities to improve ASR accuracy when dealing with the noise\neffects controlled by small values of $\\varepsilon$. In this work, we extend\nPATE learning to work with dynamic patterns, namely speech, and perform one\nvery first experimental study on ASR to avoid acoustic data leakage. We\nevaluate three end-to-end deep models, including LAS, hybrid attention/CTC, and\nRNN transducer, on the open-source LibriSpeech and TIMIT corpora. PATE\nlearning-enhanced ASR models outperform the benchmark DP-SGD mechanisms,\nespecially under strict DP budgets, giving relative word error rate reductions\nbetween 26.2% and 27.5% for RNN transducer model evaluated with LibriSpeech. We\nalso introduce another DP-preserving ASR solution with public speech corpus\npre-training.",
    "descriptor": "\nComments: 5 pages. Accepted to IEEE SLT 2022. A first version draft was finished in Aug 2021\n",
    "authors": [
      "Chao-Han Huck Yang",
      "I-Fan Chen",
      "Andreas Stolcke",
      "Sabato Marco Siniscalchi",
      "Chin-Hui Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05614"
  },
  {
    "id": "arXiv:2210.05616",
    "title": "Neural Shape Deformation Priors",
    "abstract": "We present Neural Shape Deformation Priors, a novel method for shape\nmanipulation that predicts mesh deformations of non-rigid objects from\nuser-provided handle movements. State-of-the-art methods cast this problem as\nan optimization task, where the input source mesh is iteratively deformed to\nminimize an objective function according to hand-crafted regularizers such as\nARAP. In this work, we learn the deformation behavior based on the underlying\ngeometric properties of a shape, while leveraging a large-scale dataset\ncontaining a diverse set of non-rigid deformations. Specifically, given a\nsource mesh and desired target locations of handles that describe the partial\nsurface deformation, we predict a continuous deformation field that is defined\nin 3D space to describe the space deformation. To this end, we introduce\ntransformer-based deformation networks that represent a shape deformation as a\ncomposition of local surface deformations. It learns a set of local latent\ncodes anchored in 3D space, from which we can learn a set of continuous\ndeformation functions for local surfaces. Our method can be applied to\nchallenging deformations and generalizes well to unseen deformations. We\nvalidate our approach in experiments using the DeformingThing4D dataset, and\ncompare to both classic optimization-based and recent neural network-based\nmethods.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Jiapeng Tang",
      "Lev Markhasin",
      "Bi Wang",
      "Justus Thies",
      "Matthias Nie\u00dfner"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05616"
  },
  {
    "id": "arXiv:2210.05617",
    "title": "Scaling of Radial Basis Functions",
    "abstract": "This paper studies the influence of scaling on the behavior of Radial Basis\nFunction interpolation. It focuses on certain central aspects, but does not try\nto be exhaustive. The most important questions are: How does the error of a\nkernel-based interpolant vary with the scale of the kernel chosen? How does the\nstandard error bound vary? And since fixed functions may be in spaces that\nallow scalings, like global Sobolev spaces, is there a scale of the space that\nmatches the function best? The last question is answered in the affirmative for\nSobolev spaces, but the required scale may be hard to estimate. Scalability of\nfunctions turns out to be restricted for spaces generated by analytic kernels,\nunless the functions are band-limited. In contrast to other papers, polynomials\nand polyharmonics are included as flat limits when checking scales\nexperimentally, with an independent computation. The numerical results show\nthat the hunt for near-flat scales is questionable, if users include the flat\nlimit cases right from the start. When there are not enough data to evaluate\nerrors directly, the scale of the standard error bound can be varied, up to\nreplacing the norm of the unknown function by the norm of the interpolant. This\nfollows the behavior of the actual error qualitatively well, but is only of\nlimited value for estimating error-optimal scales. For kernels and functions\nwith unlimited smoothness, the given interpolation data are proven to be\ninsufficient for determining useful scales.",
    "descriptor": "",
    "authors": [
      "Elisabeth Larsson",
      "Robert Schaback"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05617"
  },
  {
    "id": "arXiv:2210.05619",
    "title": "Multilingual BERT has an accent: Evaluating English influences on  fluency in multilingual models",
    "abstract": "While multilingual language models can improve NLP performance on\nlow-resource languages by leveraging higher-resource languages, they also\nreduce average performance on all languages (the 'curse of multilinguality').\nHere we show another problem with multilingual models: grammatical structures\nin higher-resource languages bleed into lower-resource languages, a phenomenon\nwe call grammatical structure bias. We show this bias via a novel method for\ncomparing the fluency of multilingual models to the fluency of monolingual\nSpanish and Greek models: testing their preference for two carefully-chosen\nvariable grammatical structures (optional pronoun-drop in Spanish and optional\nSubject-Verb ordering in Greek). We find that multilingual BERT is biased\ntoward the English-like setting (explicit pronouns and Subject-Verb-Object\nordering) as compared to our monolingual control. With our case studies, we\nhope to bring to light the fine-grained ways in which dominant languages can\naffect and bias multilingual performance, and encourage more\nlinguistically-aware fluency evaluation.",
    "descriptor": "",
    "authors": [
      "Isabel Papadimitriou",
      "Kezia Lopez",
      "Dan Jurafsky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05619"
  },
  {
    "id": "arXiv:2210.05623",
    "title": "A Formal Assisted Approach for Modeling and Testing Security Attacks in  IoT Edge Devices",
    "abstract": "With the rapid growth in the number of IoT devices being added to the\nnetwork, a major concern that arises is the security of these systems. As these\ndevices are resource constrained, safety measures are difficult to implement on\nthe edge. We propose a novel approach for the detection of IoT device attacks\nbased on the use of formal modeling and mutation testing. Namely, we model the\nbehavior of small IoT devices such as motion sensors and RFID reader as state\nmachines with timeouts. We also model basic IoT attacks; namely, battery\ndraining, sleep deprivation, data falsification, replay, and man in the middle\nattacks, as special mutants of these specifications. We also consider tests for\ndetecting actual physical device manipulation. Mutation testing is then used to\nderive tests that distinguish these attacks from the original specifications.\nThe behavior of these mutants is tested in real environment by running the\ntests on them. Our experiments show that derived the number of attack mutants\nand tests is small and thus these tests can be executed many times with limited\noverhead on the physical device. Consequently, our approach is not deterred by\nrelated high costs of traditional mutation testing. In addition, we also show\nthat tests derived by our method which cover all IoT attacks do not provide\ngood coverage of mutants derived using traditional mutation code-based\noperators and this indicates the need of using our method. A framework that\nimplements our approach is presented along with some other relevant case\nstudies.",
    "descriptor": "",
    "authors": [
      "A. Bhanpurawala",
      "K. El-Fakih",
      "I. Zualkernan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05623"
  },
  {
    "id": "arXiv:2210.05625",
    "title": "Convergence of a Decoupled Splitting Scheme for the  Cahn-Hilliard-Navier-Stokes System",
    "abstract": "This paper is devoted to the analysis of an energy-stable discontinuous\nGalerkin algorithm for solving the Cahn-Hilliard-Navier-Stokes equations within\na decoupled splitting framework. We show that the proposed scheme is uniquely\nsolvable and mass conservative. The energy dissipation and the $L^\\infty$\nstability of the order parameter are obtained under a CFL condition. Optimal a\npriori error estimates in the broken gradient norm and in the $L^2$ norm are\nderived. The stability proofs and error analysis are based on induction\narguments and do not require any regularization of the potential function.",
    "descriptor": "",
    "authors": [
      "Chen Liu",
      "Rami Masri",
      "Beatrice Riviere"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05625"
  },
  {
    "id": "arXiv:2210.05626",
    "title": "Semantic Segmentation under Adverse Conditions: A Weather and  Nighttime-aware Synthetic Data-based Approach",
    "abstract": "Recent semantic segmentation models perform well under standard weather\nconditions and sufficient illumination but struggle with adverse weather\nconditions and nighttime. Collecting and annotating training data under these\nconditions is expensive, time-consuming, error-prone, and not always practical.\nUsually, synthetic data is used as a feasible data source to increase the\namount of training data. However, just directly using synthetic data may\nactually harm the model's performance under normal weather conditions while\ngetting only small gains in adverse situations. Therefore, we present a novel\narchitecture specifically designed for using synthetic training data for domain\nadaptation. We propose a simple yet powerful addition to DeepLabV3+ by using\nweather and time-of-the-day supervisors trained with multi-task learning,\nmaking it both weather and nighttime aware, which improves its mIoU accuracy by\n$14$ percentage points on the ACDC dataset while maintaining a score of $75\\%$\nmIoU on the Cityscapes dataset. Our code is available at\nhttps://github.com/lsmcolab/Semantic-Segmentation-under-Adverse-Conditions.",
    "descriptor": "\nComments: This paper is accepted by BMVC 2022\n",
    "authors": [
      "Abdulrahman Kerim",
      "Felipe Chamone",
      "Washington Ramos",
      "Leandro Soriano Marcolino",
      "Erickson R. Nascimento",
      "Richard Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05626"
  },
  {
    "id": "arXiv:2210.05631",
    "title": "On Landau's Eigenvalue Theorem for Line-of-Sight MIMO Channels",
    "abstract": "An alternative derivation is provided for the degrees of freedom (DOF)\nformula on line-of-sight (LOS) channels via Landau's eigenvalue theorem for\nbandlimited signals. Compared to other approaches, Landau's theorem provides a\ngeneral framework to compute the DOF in arbitrary environments, this framework\nis herein specialized to LOS propagation. The development shows how the\nspatially bandlimited nature of the channel relates to its geometry under the\nparaxial approximation that applies to most LOS settings of interest.",
    "descriptor": "\nComments: 5 pages, 2 figures. IEEE Wireless Communications Letters\n",
    "authors": [
      "Andrea Pizzo",
      "Angel Lozano"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.05631"
  },
  {
    "id": "arXiv:2210.05632",
    "title": "SEE-Few: Seed, Expand and Entail for Few-shot Named Entity Recognition",
    "abstract": "Few-shot named entity recognition (NER) aims at identifying named entities\nbased on only few labeled instances. Current few-shot NER methods focus on\nleveraging existing datasets in the rich-resource domains which might fail in a\ntraining-from-scratch setting where no source-domain data is used. To tackle\ntraining-from-scratch setting, it is crucial to make full use of the annotation\ninformation (the boundaries and entity types). Therefore, in this paper, we\npropose a novel multi-task (Seed, Expand and Entail) learning framework,\nSEE-Few, for Few-shot NER without using source domain data. The seeding and\nexpanding modules are responsible for providing as accurate candidate spans as\npossible for the entailing module. The entailing module reformulates span\nclassification as a textual entailment task, leveraging both the contextual\nclues and entity type information. All the three modules share the same text\nencoder and are jointly learned. Experimental results on four benchmark\ndatasets under the training-from-scratch setting show that the proposed method\noutperformed state-of-the-art few-shot NER methods with a large margin. Our\ncode is available at \\url{https://github.com/unveiled-the-red-hat/SEE-Few}.",
    "descriptor": "\nComments: Accepted by COLING 2022\n",
    "authors": [
      "Zeng Yang",
      "Linhai Zhang",
      "Deyu Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05632"
  },
  {
    "id": "arXiv:2210.05633",
    "title": "Habitat-Matterport 3D Semantics Dataset",
    "abstract": "We present the Habitat-Matterport 3D Semantics (HM3DSEM) dataset. HM3DSEM is\nthe largest dataset of 3D real-world spaces with densely annotated semantics\nthat is currently available to the academic community. It consists of 142,646\nobject instance annotations across 216 3D spaces and 3,100 rooms within those\nspaces. The scale, quality, and diversity of object annotations far exceed\nthose of datasets from prior work. A key difference setting apart HM3DSEM from\nother datasets is the use of texture information to annotate pixel-accurate\nobject boundaries. We demonstrate the effectiveness of HM3DSEM dataset for the\nObject Goal Navigation task using different methods. Policies trained using\nHM3DSEM perform comparable or better than those trained on prior datasets.",
    "descriptor": "\nComments: 14 Pages, 11 Figures, 4 Tables\n",
    "authors": [
      "Karmesh Yadav",
      "Ram Ramrakhya",
      "Santhosh Kumar Ramakrishnan",
      "Theo Gervet",
      "John Turner",
      "Aaron Gokaslan",
      "Noah Maestre",
      "Angel Xuan Chang",
      "Dhruv Batra",
      "Manolis Savva",
      "Alexander William Clegg",
      "Devendra Singh Chaplot"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05633"
  },
  {
    "id": "arXiv:2210.05634",
    "title": "The IID Prophet Inequality with Limited Flexibility",
    "abstract": "In online sales, sellers usually offer each potential buyer a posted price in\na take-it-or-leave fashion. Buyers can sometimes see posted prices faced by\nother buyers, and changing the price frequently could be considered unfair. The\nliterature on posted price mechanisms and prophet inequality problems has\nstudied the two extremes of pricing policies, the fixed price policy and fully\ndynamic pricing. The former is suboptimal in revenue but is perceived as fairer\nthan the latter. This work examines the middle situation, where there are at\nmost $k$ distinct prices over the selling horizon. Using the framework of\nprophet inequalities with independent and identically distributed random\nvariables, we propose a new prophet inequality for strategies that use at most\n$k$ thresholds. We present asymptotic results in $k$ and results for small\nvalues of $k$. For $k=2$ prices, we show an improvement of at least $11\\%$ over\nthe best fixed-price solution. Moreover, $k=5$ prices suffice to guarantee\nalmost $99\\%$ of the approximation factor obtained by a fully dynamic policy\nthat uses an arbitrary number of prices. From a technical standpoint, we use an\ninfinite-dimensional linear program in our analysis; this formulation could be\nof independent interest to other online selection problems.",
    "descriptor": "",
    "authors": [
      "Sebastian Perez-Salazar",
      "Mohit Singh",
      "Alejandro Toriello"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05634"
  },
  {
    "id": "arXiv:2210.05635",
    "title": "Oflib: Facilitating Operations with and on Optical Flow Fields in Python",
    "abstract": "We present a robust theoretical framework for the characterisation and\nmanipulation of optical flow, i.e 2D vector fields, in the context of their use\nin motion estimation algorithms and beyond. The definition of two frames of\nreference guides the mathematical derivation of flow field application,\ninversion, evaluation, and composition operations. This structured approach is\nthen used as the foundation for an implementation in Python 3, with the fully\ndifferentiable PyTorch version oflibpytorch supporting back-propagation as\nrequired for deep learning. We verify the flow composition method empirically\nand provide a working example for its application to optical flow ground truth\nin synthetic training data creation. All code is publicly available.",
    "descriptor": "\nComments: \"What is Motion for?\" - ECCV 2022 Workshop Submission\n",
    "authors": [
      "Claudio Ravasio",
      "Lyndon Da Cruz",
      "Christos Bergeles"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05635"
  },
  {
    "id": "arXiv:2210.05638",
    "title": "APSNet: Attention Based Point Cloud Sampling",
    "abstract": "Processing large point clouds is a challenging task. Therefore, the data is\noften downsampled to a smaller size such that it can be stored, transmitted and\nprocessed more efficiently without incurring significant performance\ndegradation. Traditional task-agnostic sampling methods, such as farthest point\nsampling (FPS), do not consider downstream tasks when sampling point clouds,\nand thus non-informative points to the tasks are often sampled. This paper\nexplores a task-oriented sampling for 3D point clouds, and aims to sample a\nsubset of points that are tailored specifically to a downstream task of\ninterest. Similar to FPS, we assume that point to be sampled next should depend\nheavily on the points that have already been sampled. We thus formulate point\ncloud sampling as a sequential generation process, and develop an\nattention-based point cloud sampling network (APSNet) to tackle this problem.\nAt each time step, APSNet attends to all the points in a cloud by utilizing the\nhistory of previously sampled points, and samples the most informative one.\nBoth supervised learning and knowledge distillation-based self-supervised\nlearning of APSNet are proposed. Moreover, joint training of APSNet over\nmultiple sample sizes is investigated, leading to a single APSNet that can\ngenerate arbitrary length of samples with prominent performances. Extensive\nexperiments demonstrate the superior performance of APSNet against\nstate-of-the-arts in various downstream tasks, including 3D point cloud\nclassification, reconstruction, and registration.",
    "descriptor": "\nComments: Published as a conference paper in BMVC 2022\n",
    "authors": [
      "Yang Ye",
      "Xiulong Yang",
      "Shihao Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05638"
  },
  {
    "id": "arXiv:2210.05639",
    "title": "Discovered Policy Optimisation",
    "abstract": "The last decade has been revolutionary for reinforcement learning (RL) - it\ncan now solve complex decision and control problems. Successful RL methods were\nhandcrafted using mathematical derivations, intuition, and experimentation.\nThis approach has a major shortcoming: It results in specific solutions to the\nRL problem, rather than a protocol for discovering efficient and robust\nmethods. In contrast, the emerging field of meta-learning provides a toolkit\nfor automatic machine learning method optimisation, potentially addressing this\nflaw. However, black-box approaches which attempt to discover RL algorithms\nwith minimal prior structure have thus far not been successful. Mirror\nLearning, which includes RL algorithms, such as PPO, offers a potential\nframework. In this paper we explore the Mirror Learning space by meta-learning\na \"drift\" function. We refer to the result as Learnt Policy Optimisation (LPO).\nBy analysing LPO we gain original insights into policy optimisation which we\nuse to formulate a novel, closed-form RL algorithm, Discovered Policy\nOptimisation (DPO). Our experiments in Brax environments confirm\nstate-of-the-art performance of LPO and DPO, as well as their transfer to\nunseen settings.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Chris Lu",
      "Jakub Grudzien Kuba",
      "Alistair Letcher",
      "Luke Metz",
      "Christian Schroeder de Witt",
      "Jakob Foerster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05639"
  },
  {
    "id": "arXiv:2210.05643",
    "title": "A Kernel-Based View of Language Model Fine-Tuning",
    "abstract": "It has become standard to solve NLP tasks by fine-tuning pre-trained language\nmodels (LMs), especially in low-data settings. There is minimal theoretical\nunderstanding of empirical success, e.g., why fine-tuning a model with $10^8$\nor more parameters on a couple dozen training points does not result in\noverfitting. We investigate whether the Neural Tangent Kernel (NTK) - which\noriginated as a model to study the gradient descent dynamics of infinitely wide\nnetworks with suitable random initialization - describes fine-tuning of\npre-trained LMs. This study was inspired by the decent performance of NTK for\ncomputer vision tasks (Wei et al., 2022). We also extend the NTK formalism to\nfine-tuning with Adam. We present extensive experiments that show that once the\ndownstream task is formulated as a language modeling problem through prompting,\nthe NTK lens can often reasonably describe the model updates during fine-tuning\nwith both SGD and Adam. This kernel view also suggests an explanation for\nsuccess of parameter-efficient subspace-based fine-tuning methods. Finally, we\nsuggest a path toward a formal explanation for our findings via Tensor Programs\n(Yang, 2020).",
    "descriptor": "\nComments: Code and pre-computed kernels are publicly available at this https URL\n",
    "authors": [
      "Sadhika Malladi",
      "Alexander Wettig",
      "Dingli Yu",
      "Danqi Chen",
      "Sanjeev Arora"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05643"
  },
  {
    "id": "arXiv:2210.05648",
    "title": "Entity Disambiguation with Entity Definitions",
    "abstract": "Local models have recently attained astounding performances in Entity\nDisambiguation (ED), with generative and extractive formulations being the most\npromising research directions. However, previous works limited their studies to\nusing, as the textual representation of each candidate, only its Wikipedia\ntitle. Although certainly effective, this strategy presents a few critical\nissues, especially when titles are not sufficiently informative or\ndistinguishable from one another. In this paper, we address this limitation and\ninvestigate to what extent more expressive textual representations can mitigate\nit. We thoroughly evaluate our approach against standard benchmarks in ED and\nfind extractive formulations to be particularly well-suited to these\nrepresentations: we report a new state of the art on 2 out of 6 benchmarks we\nconsider and strongly improve the generalization capability over unseen\npatterns. We release our code, data and model checkpoints at\nhttps://github.com/SapienzaNLP/extend.",
    "descriptor": "",
    "authors": [
      "Luigi Procopio",
      "Simone Conia",
      "Edoardo Barba",
      "Roberto Navigli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05648"
  },
  {
    "id": "arXiv:2210.05650",
    "title": "Regret Bounds for Risk-Sensitive Reinforcement Learning",
    "abstract": "In safety-critical applications of reinforcement learning such as healthcare\nand robotics, it is often desirable to optimize risk-sensitive objectives that\naccount for tail outcomes rather than expected reward. We prove the first\nregret bounds for reinforcement learning under a general class of\nrisk-sensitive objectives including the popular CVaR objective. Our theory is\nbased on a novel characterization of the CVaR objective as well as a novel\noptimistic MDP construction.",
    "descriptor": "",
    "authors": [
      "O. Bastani",
      "Y. J. Ma",
      "E. Shen",
      "W. Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05650"
  },
  {
    "id": "arXiv:2210.05657",
    "title": "The Unreasonable Effectiveness of Fully-Connected Layers for Low-Data  Regimes",
    "abstract": "Convolutional neural networks were the standard for solving many computer\nvision tasks until recently, when Transformers of MLP-based architectures have\nstarted to show competitive performance. These architectures typically have a\nvast number of weights and need to be trained on massive datasets; hence, they\nare not suitable for their use in low-data regimes. In this work, we propose a\nsimple yet effective framework to improve generalization from small amounts of\ndata. We augment modern CNNs with fully-connected (FC) layers and show the\nmassive impact this architectural change has in low-data regimes. We further\npresent an online joint knowledge-distillation method to utilize the extra FC\nlayers at train time but avoid them during test time. This allows us to improve\nthe generalization of a CNN-based model without any increase in the number of\nweights at test time. We perform classification experiments for a large range\nof network backbones and several standard datasets on supervised learning and\nactive learning. Our experiments significantly outperform the networks without\nfully-connected layers, reaching a relative improvement of up to $16\\%$\nvalidation accuracy in the supervised setting without adding any extra\nparameters during inference.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022, Homepage: this https URL 24 pages, 14 figures, 12 tables\n",
    "authors": [
      "Peter Kocsis",
      "Peter S\u00faken\u00edk",
      "Guillem Bras\u00f3",
      "Matthias Nie\u00dfner",
      "Laura Leal-Taix\u00e9",
      "Ismail Elezi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05657"
  },
  {
    "id": "arXiv:2210.05660",
    "title": "The Typical Behavior of Bandit Algorithms",
    "abstract": "We establish strong laws of large numbers and central limit theorems for the\nregret of two of the most popular bandit algorithms: Thompson sampling and UCB.\nHere, our characterizations of the regret distribution complement the\ncharacterizations of the tail of the regret distribution recently developed by\nFan and Glynn (2021) (arXiv:2109.13595). The tail characterizations there are\nassociated with atypical bandit behavior on trajectories where the optimal arm\nmean is under-estimated, leading to mis-identification of the optimal arm and\nlarge regret. In contrast, our SLLN's and CLT's here describe the typical\nbehavior and fluctuation of regret on trajectories where the optimal arm mean\nis properly estimated. We find that Thompson sampling and UCB satisfy the same\nSLLN and CLT, with the asymptotics of both the SLLN and the (mean) centering\nsequence in the CLT matching the asymptotics of expected regret. Both the mean\nand variance in the CLT grow at $\\log(T)$ rates with the time horizon $T$.\nAsymptotically as $T \\to \\infty$, the variability in the number of plays of\neach sub-optimal arm depends only on the rewards received for that arm, which\nindicates that each sub-optimal arm contributes independently to the overall\nCLT variance.",
    "descriptor": "",
    "authors": [
      "Lin Fan",
      "Peter W. Glynn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.05660"
  },
  {
    "id": "arXiv:2210.05662",
    "title": "Understanding or Manipulation: Rethinking Online Performance Gains of  Modern Recommender Systems",
    "abstract": "Recommender systems are expected to be assistants that help human users find\nrelevant information in an automatic manner without explicit queries. As\nrecommender systems evolve, increasingly sophisticated learning techniques are\napplied and have achieved better performance in terms of user engagement\nmetrics such as clicks and browsing time. The increase of the measured\nperformance, however, can have two possible attributions: a better\nunderstanding of user preferences, and a more proactive ability to utilize\nhuman bounded rationality to seduce user over-consumption. A natural following\nquestion is whether current recommendation algorithms are manipulating user\npreferences. If so, can we measure the manipulation level? In this paper, we\npresent a general framework for benchmarking the degree of manipulations of\nrecommendation algorithms, in both slate recommendation and sequential\nrecommendation scenarios. The framework consists of three stages, initial\npreference calculation, algorithm training and interaction, and metrics\ncalculation that involves two proposed metrics, Manipulation Score and\nPreference Shift. We benchmark some representative recommendation algorithms in\nboth synthetic and real-world datasets under the proposed framework. We have\nobserved that a high online click-through rate does not mean a better\nunderstanding of user initial preference, but ends in prompting users to choose\nmore documents they initially did not favor. Moreover, we find that the\nproperties of training data have notable impacts on the manipulation degrees,\nand algorithms with more powerful modeling abilities are more sensitive to such\nimpacts. The experiments also verified the usefulness of the proposed metrics\nfor measuring the degree of manipulations. We advocate that future\nrecommendation algorithm studies should be treated as an optimization problem\nwith constrained user preference manipulations.",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Zhengbang Zhu",
      "Rongjun Qin",
      "Junjie Huang",
      "Xinyi Dai",
      "Yang Yu",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05662"
  },
  {
    "id": "arXiv:2210.05663",
    "title": "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory",
    "abstract": "We propose CLIP-Fields, an implicit scene model that can be trained with no\ndirect human supervision. This model learns a mapping from spatial locations to\nsemantic embedding vectors. The mapping can then be used for a variety of\ntasks, such as segmentation, instance identification, semantic search over\nspace, and view localization. Most importantly, the mapping can be trained with\nsupervision coming only from web-image and web-text trained models such as\nCLIP, Detic, and Sentence-BERT. When compared to baselines like Mask-RCNN, our\nmethod outperforms on few-shot instance identification or semantic segmentation\non the HM3D dataset with only a fraction of the examples. Finally, we show that\nusing CLIP-Fields as a scene memory, robots can perform semantic navigation in\nreal-world environments. Our code and demonstrations are available here:\nhttps://mahis.life/clip-fields/",
    "descriptor": "\nComments: Code, video, and interactive demonstrations available at this https URL\n",
    "authors": [
      "Nur Muhammad Mahi Shafiullah",
      "Chris Paxton",
      "Lerrel Pinto",
      "Soumith Chintala",
      "Arthur Szlam"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05663"
  },
  {
    "id": "arXiv:2210.05664",
    "title": "Social Influence Dialogue Systems: A Scoping Survey of the Efforts  Towards Influence Capabilities of Dialogue Systems",
    "abstract": "Dialogue systems capable of social influence such as persuasion, negotiation,\nand therapy, are essential for extending the use of technology to numerous\nrealistic scenarios. However, existing research primarily focuses on either\ntask-oriented or open-domain scenarios, a categorization that has been\ninadequate for capturing influence skills systematically. There exists no\nformal definition or category for dialogue systems with these skills and\ndata-driven efforts in this direction are highly limited. In this work, we\nformally define and introduce the category of \\emph{social influence dialogue\nsystems} that influence users' cognitive and emotional responses, leading to\nchanges in thoughts, opinions, and behaviors through natural conversations. We\npresent a survey of various tasks, datasets, and methods, compiling the\nprogress across seven diverse domains. We discuss the commonalities and\ndifferences between the examined systems, identify limitations, and recommend\nfuture directions. This study serves as a comprehensive reference for social\ninfluence dialogue systems to inspire more dedicated research and discussion in\nthis emerging area.",
    "descriptor": "",
    "authors": [
      "Kushal Chawla",
      "Weiyan Shi",
      "Jingwen Zhang",
      "Gale Lucas",
      "Zhou Yu",
      "Jonathan Gratch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05664"
  },
  {
    "id": "arXiv:2210.05665",
    "title": "HiFECap: Monocular High-Fidelity and Expressive Capture of Human  Performances",
    "abstract": "Monocular 3D human performance capture is indispensable for many applications\nin computer graphics and vision for enabling immersive experiences. However,\ndetailed capture of humans requires tracking of multiple aspects, including the\nskeletal pose, the dynamic surface, which includes clothing, hand gestures as\nwell as facial expressions. No existing monocular method allows joint tracking\nof all these components. To this end, we propose HiFECap, a new neural human\nperformance capture approach, which simultaneously captures human pose,\nclothing, facial expression, and hands just from a single RGB video. We\ndemonstrate that our proposed network architecture, the carefully designed\ntraining strategy, and the tight integration of parametric face and hand models\nto a template mesh enable the capture of all these individual aspects.\nImportantly, our method also captures high-frequency details, such as deforming\nwrinkles on the clothes, better than the previous works. Furthermore, we show\nthat HiFECap outperforms the state-of-the-art human performance capture\napproaches qualitatively and quantitatively while for the first time capturing\nall aspects of the human.",
    "descriptor": "\nComments: Got accepted by BMVC2022\n",
    "authors": [
      "Yue Jiang",
      "Marc Habermann",
      "Vladislav Golyanik",
      "Christian Theobalt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.05665"
  },
  {
    "id": "arXiv:2210.05666",
    "title": "Point Transformer V2: Grouped Vector Attention and Partition-based  Pooling",
    "abstract": "As a pioneering work exploring transformer architecture for 3D point cloud\nunderstanding, Point Transformer achieves impressive results on multiple highly\ncompetitive benchmarks. In this work, we analyze the limitations of the Point\nTransformer and propose our powerful and efficient Point Transformer V2 model\nwith novel designs that overcome the limitations of previous work. In\nparticular, we first propose group vector attention, which is more effective\nthan the previous version of vector attention. Inheriting the advantages of\nboth learnable weight encoding and multi-head attention, we present a highly\neffective implementation of grouped vector attention with a novel grouped\nweight encoding layer. We also strengthen the position information for\nattention by an additional position encoding multiplier. Furthermore, we design\nnovel and lightweight partition-based pooling methods which enable better\nspatial alignment and more efficient sampling. Extensive experiments show that\nour model achieves better performance than its predecessor and achieves\nstate-of-the-art on several challenging 3D point cloud understanding\nbenchmarks, including 3D point cloud segmentation on ScanNet v2 and S3DIS and\n3D point cloud classification on ModelNet40. Our code will be available at\nhttps://github.com/Gofinge/PointTransformerV2.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Xiaoyang Wu",
      "Yixing Lao",
      "Li Jiang",
      "Xihui Liu",
      "Hengshuang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05666"
  },
  {
    "id": "arXiv:2210.05667",
    "title": "Human Body Measurement Estimation with Adversarial Augmentation",
    "abstract": "We present a Body Measurement network (BMnet) for estimating 3D\nanthropomorphic measurements of the human body shape from silhouette images.\nTraining of BMnet is performed on data from real human subjects, and augmented\nwith a novel adversarial body simulator (ABS) that finds and synthesizes\nchallenging body shapes. ABS is based on the skinned multiperson linear (SMPL)\nbody model, and aims to maximize BMnet measurement prediction error with\nrespect to latent SMPL shape parameters. ABS is fully differentiable with\nrespect to these parameters, and trained end-to-end via backpropagation with\nBMnet in the loop. Experiments show that ABS effectively discovers adversarial\nexamples, such as bodies with extreme body mass indices (BMI), consistent with\nthe rarity of extreme-BMI bodies in BMnet's training set. Thus ABS is able to\nreveal gaps in training data and potential failures in predicting\nunder-represented body shapes. Results show that training BMnet with ABS\nimproves measurement prediction accuracy on real bodies by up to 10%, when\ncompared to no augmentation or random body shape sampling. Furthermore, our\nmethod significantly outperforms SOTA measurement estimation methods by as much\nas 3x. Finally, we release BodyM, the first challenging, large-scale dataset of\nphoto silhouettes and body measurements of real human subjects, to further\npromote research in this area. Project website:\nhttps://adversarialbodysim.github.io",
    "descriptor": "\nComments: Published at the International Conference on 3D Vision (3DV) 2022\n",
    "authors": [
      "Nataniel Ruiz",
      "Miriam Bellver",
      "Timo Bolkart",
      "Ambuj Arora",
      "Ming C. Lin",
      "Javier Romero",
      "Raja Bala"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05667"
  },
  {
    "id": "arXiv:2210.05668",
    "title": "Understanding Embodied Reference with Touch-Line Transformer",
    "abstract": "We study embodied reference understanding, the task of locating referents\nusing embodied gestural signals and language references. Human studies have\nrevealed that objects referred to or pointed to do not lie on the elbow-wrist\nline, a common misconception; instead, they lie on the so-called virtual touch\nline. However, existing human pose representations fail to incorporate the\nvirtual touch line. To tackle this problem, we devise the touch-line\ntransformer: It takes as input tokenized visual and textual features and\nsimultaneously predicts the referent's bounding box and a touch-line vector.\nLeveraging this touch-line prior, we further devise a geometric consistency\nloss that encourages the co-linearity between referents and touch lines. Using\nthe touch-line as gestural information improves model performances\nsignificantly. Experiments on the YouRefIt dataset show our method achieves a\n+25.0% accuracy improvement under the 0.75 IoU criterion, closing 63.6% of the\ngap between model and human performances. Furthermore, we computationally\nverify prior human studies by showing that computational models more accurately\nlocate referents when using the virtual touch line than when using the\nelbow-wrist line.",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Yang Li",
      "Xiaoxue Chen",
      "Hao Zhao",
      "Jiangtao Gong",
      "Guyue Zhou",
      "Federico Rossano",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05668"
  },
  {
    "id": "arXiv:2210.05669",
    "title": "A generic diffusion-based approach for 3D human pose prediction in the  wild",
    "abstract": "3D human pose forecasting, i.e., predicting a sequence of future human 3D\nposes given a sequence of past observed ones, is a challenging spatio-temporal\ntask. It can be more challenging in real-world applications where occlusions\nwill inevitably happen, and estimated 3D coordinates of joints would contain\nsome noise. We provide a unified formulation in which incomplete elements (no\nmatter in the prediction or observation) are treated as noise and propose a\nconditional diffusion model that denoises them and forecasts plausible poses.\nInstead of naively predicting all future frames at once, our model consists of\ntwo cascaded sub-models, each specialized for modeling short and long horizon\ndistributions. We also propose a generic framework to improve any 3D pose\nforecasting model by leveraging our diffusion model in two additional steps: a\npre-processing step to repair the inputs and a post-processing step to refine\nthe outputs. We investigate our findings on four standard datasets (Human3.6M,\nHumanEva-I, AMASS, and 3DPW) and obtain significant improvements over the\nstate-of-the-art. The code will be made available online.",
    "descriptor": "",
    "authors": [
      "Saeed Saadatnejad",
      "Ali Rasekh",
      "Mohammadreza Mofayezi",
      "Yasamin Medghalchi",
      "Sara Rajabzadeh",
      "Taylor Mordan",
      "Alexandre Alahi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05669"
  },
  {
    "id": "arXiv:2210.04893",
    "title": "Equivariant Shape-Conditioned Generation of 3D Molecules for  Ligand-Based Drug Design",
    "abstract": "Shape-based virtual screening is widely employed in ligand-based drug design\nto search chemical libraries for molecules with similar 3D shapes yet novel 2D\nchemical structures compared to known ligands. 3D deep generative models have\nthe potential to automate this exploration of shape-conditioned 3D chemical\nspace; however, no existing models can reliably generate valid drug-like\nmolecules in conformations that adopt a specific shape such as a known binding\npose. We introduce a new multimodal 3D generative model that enables\nshape-conditioned 3D molecular design by equivariantly encoding molecular shape\nand variationally encoding chemical identity. We ensure local geometric and\nchemical validity of generated molecules by using autoregressive fragment-based\ngeneration with heuristic bonding geometries, allowing the model to prioritize\nthe scoring of rotatable bonds to best align the growing conformational\nstructure to the target shape. We evaluate our 3D generative model in tasks\nrelevant to drug design including shape-conditioned generation of chemically\ndiverse molecular structures and shape-constrained molecular property\noptimization, demonstrating its utility over virtual screening of enumerated\nlibraries.",
    "descriptor": "",
    "authors": [
      "Keir Adams",
      "Connor W. Coley"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2210.04893"
  },
  {
    "id": "arXiv:2210.04898",
    "title": "Improving The Reconstruction Quality by Overfitted Decoder Bias in  Neural Image Compression",
    "abstract": "End-to-end trainable models have reached the performance of traditional\nhandcrafted compression techniques on videos and images. Since the parameters\nof these models are learned over large training sets, they are not optimal for\nany given image to be compressed. In this paper, we propose an instance-based\nfine-tuning of a subset of decoder's bias to improve the reconstruction quality\nin exchange for extra encoding time and minor additional signaling cost. The\nproposed method is applicable to any end-to-end compression methods, improving\nthe state-of-the-art neural image compression BD-rate by $3-5\\%$.",
    "descriptor": "\nComments: PCS2022\n",
    "authors": [
      "Oussama Jourairi",
      "Muhammet Balcilar",
      "Anne Lambert",
      "Fran\u00e7ois Schnitzler"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04898"
  },
  {
    "id": "arXiv:2210.04939",
    "title": "Solving polynomial equations and applications",
    "abstract": "These notes accompany an introductory lecture given by the author at the\nworkshop on solving polynomial equations & applications at CWI Amsterdam in the\ncontext of the 2022 fall semester programme on polynomial optimization &\napplications. We introduce systems of polynomial equations and the main\napproaches for solving them. We also discuss applications and solution counts.\nThe theory is illustrated by many examples.",
    "descriptor": "\nComments: 22 pages, 8 figures\n",
    "authors": [
      "Simon Telen"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.04939"
  },
  {
    "id": "arXiv:2210.04974",
    "title": "Function-space regularized R\u00e9nyi divergences",
    "abstract": "We propose a new family of regularized R\\'enyi divergences parametrized not\nonly by the order $\\alpha$ but also by a variational function space. These new\nobjects are defined by taking the infimal convolution of the standard R\\'enyi\ndivergence with the integral probability metric (IPM) associated with the\nchosen function space. We derive a novel dual variational representation that\ncan be used to construct numerically tractable divergence estimators. This\nrepresentation avoids risk-sensitive terms and therefore exhibits lower\nvariance, making it well-behaved when $\\alpha>1$; this addresses a notable\nweakness of prior approaches. We prove several properties of these new\ndivergences, showing that they interpolate between the classical R\\'enyi\ndivergences and IPMs. We also study the $\\alpha\\to\\infty$ limit, which leads to\na regularized worst-case-regret and a new variational representation in the\nclassical case. Moreover, we show that the proposed regularized R\\'enyi\ndivergences inherit features from IPMs such as the ability to compare\ndistributions that are not absolutely continuous, e.g., empirical measures and\ndistributions with low-dimensional support. We present numerical results on\nboth synthetic and real datasets, showing the utility of these new divergences\nin both estimation and GAN training applications; in particular, we demonstrate\nsignificantly reduced variance and improved training performance.",
    "descriptor": "\nComments: 22 pages, 4 figures\n",
    "authors": [
      "Jeremiah Birrell",
      "Yannis Pantazis",
      "Paul Dupuis",
      "Markos A. Katsoulakis",
      "Luc Rey-Bellet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04974"
  },
  {
    "id": "arXiv:2210.04987",
    "title": "Loop Unrolled Shallow Equilibrium Regularizer (LUSER) -- A  Memory-Efficient Inverse Problem Solver",
    "abstract": "In inverse problems we aim to reconstruct some underlying signal of interest\nfrom potentially corrupted and often ill-posed measurements. Classical\noptimization-based techniques proceed by optimizing a data consistency metric\ntogether with a regularizer. Current state-of-the-art machine learning\napproaches draw inspiration from such techniques by unrolling the iterative\nupdates for an optimization-based solver and then learning a regularizer from\ndata. This loop unrolling (LU) method has shown tremendous success, but often\nrequires a deep model for the best performance leading to high memory costs\nduring training. Thus, to address the balance between computation cost and\nnetwork expressiveness, we propose an LU algorithm with shallow equilibrium\nregularizers (LUSER). These implicit models are as expressive as deeper\nconvolutional networks, but far more memory efficient during training. The\nproposed method is evaluated on image deblurring, computed tomography (CT), as\nwell as single-coil Magnetic Resonance Imaging (MRI) tasks and shows similar,\nor even better, performance while requiring up to 8 times less computational\nresources during training when compared against a more typical LU architecture\nwith feedforward convolutional regularizers.",
    "descriptor": "",
    "authors": [
      "Peimeng Guan",
      "Jihui Jin",
      "Justin Romberg",
      "Mark A. Davenport"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.04987"
  },
  {
    "id": "arXiv:2210.04994",
    "title": "Sampling-based inference for large linear models, with application to  linearised Laplace",
    "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with\ncontemporary application as surrogate models for neural network uncertainty\nquantification; that is, the linearised Laplace method. Alas, the computational\ncost associated with Bayesian linear models constrains this method's\napplication to small networks, small output spaces and small datasets. We\naddress this limitation by introducing a scalable sample-based Bayesian\ninference method for conjugate Gaussian multi-output linear models, together\nwith a matching method for hyperparameter (regularisation) selection.\nFurthermore, we use a classic feature normalisation method (the g-prior) to\nresolve a previously highlighted pathology of the linearised Laplace method.\nTogether, these contributions allow us to perform linearised neural network\ninference with ResNet-18 on CIFAR100 (11M parameters, 100 output dimensions x\n50k datapoints) and with a U-Net on a high-resolution tomographic\nreconstruction task (2M parameters, 251k output dimensions).",
    "descriptor": "",
    "authors": [
      "Javier Antor\u00e1n",
      "Shreyas Padhy",
      "Riccardo Barbano",
      "Eric Nalisnick",
      "David Janz",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04994"
  },
  {
    "id": "arXiv:2210.05009",
    "title": "Multi-term fractional linear equations modeling oxygen subdiffusion  through capillaries",
    "abstract": "For $0<\\nu_2<\\nu_1\\leq 1$, we analyze a linear integro-differential equation\non the space-time cylinder $\\Omega\\times(0,T)$ in the unknown $u=u(x,t)$\n$$\\mathbf{D}_{t}^{\\nu_1}(\\varrho_{1}u)-\\mathbf{D}_{t}^{\\nu_2}(\\varrho_2\nu)-\\mathcal{L}_{1}u-\\mathcal{K}*\\mathcal{L}_{2}u =f$$ where\n$\\mathbf{D}_{t}^{\\nu_i}$ are the Caputo fractional derivatives,\n$\\varrho_i=\\varrho_i(x,t)$ with $\\varrho_1\\geq \\mu_0>0$, $\\mathcal{L}_{i}$ are\nuniform elliptic operators with time-dependent smooth coefficients,\n$\\mathcal{K}$ is a summable convolution kernel, and $f$ is an external force.\nParticular cases of this equation are the recently proposed advanced models of\noxygen transport through capillaries. Under suitable conditions on the given\ndata, the global classical solvability of the associated initial-boundary value\nproblems is addressed. To this end, a special technique is needed, adapting the\nconcept of a regularizer from the theory of parabolic equations. This allows us\nto remove the usual assumption about the nonnegativity of the kernel\nrepresenting fractional derivatives. The problem is also investigated from the\nnumerical point of view.",
    "descriptor": "",
    "authors": [
      "Vittorino Pata",
      "Sergii Siryk",
      "Nataliya Vasylyeva"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05009"
  },
  {
    "id": "arXiv:2210.05013",
    "title": "Supervisory Coordination of Robotic Fiber Positioners in Multi-Object  Spectrographs",
    "abstract": "In this paper, we solve the complete coordination problem of robotic fiber\npositioners using supervisory control theory. In particular, we model\npositioners and their behavioral specifications as discrete-event systems by\nthe discretization of their motion spaces. We synthesize a coordination\nsupervisor associated with a specific set of positioners. In particular, the\ncoordination supervisor includes the solutions to the complete coordination\nproblem of its corresponding positioners. Then, we use the backtracking\nforcibility technique of supervisory control theory to present an algorithm\nbased on a completeness condition to solve the coordination problem similar to\na reconfiguration problem. We illustrate the functionality of our method using\nan example.",
    "descriptor": "",
    "authors": [
      "Matin Macktoobian",
      "Denis Gillet",
      "Jean-Paul Kneib"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05013"
  },
  {
    "id": "arXiv:2210.05023",
    "title": "Using Deep Learning to Improve Early Diagnosis of Pneumonia in  Underdeveloped Countries",
    "abstract": "As advancements in technology and medicine are being made, many countries are\nstill unable to access quality medical care due to cost and lack of qualified\nmedical personnel. This discrepancy in healthcare has caused many preventable\ndeaths, either due to lack of detection or lack of care. One of the most\nprevalent diseases in the world is pneumonia, an infection of the lungs that\nkilled 2.56 million people worldwide in 2017. In this same year, the United\nStates recorded a pneumonia death rate of 15.88 people per 100000 in\npopulation, while much of Sub-Saharan Africa, such as Chad and Guinea,\nexperienced death rates of over 150 people per 100000. In sub-Saharan Africa,\nthere is an extreme shortage of doctors and nurses, estimated to be around 2.4\nmillion. The hypothesis being tested is that a deep learning model can receive\ninput in the form of an x-ray and produce a diagnosis with the equivalent\naccuracy of a physician, compared to a prediagnosed image. The model used in\nthis project is a modified convolutional neural network. The model was trained\non a set of 2000 x-ray images that have predetermined normal and abnormal lung\nfindings, and then tested on a set of 400 images that contains evenly split\nimages of pneumonia and healthy lungs. For each computer-run test, data was\ncollected on a base measurement of accuracy, as well as more specific metrics\nsuch as specificity and sensitivity. Results show that the algorithm tested was\nable to accurately identify abnormal lung findings an average of 82.5% of the\ntime. The model achieved a maximum specificity of 98.5% and a maximum\nsensitivity of 90% separately, and the highest simultaneous values of these two\nmetrics was a sensitivity of 90% and a specificity of 78.5%. This research can\nbe further improved by testing other deep learning models as well as machine\nlearning models to improve the metric scores and chance of correct diagnoses.",
    "descriptor": "\nComments: 11 pages, 8 figures\n",
    "authors": [
      "Kyler Larsen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05023"
  },
  {
    "id": "arXiv:2210.05046",
    "title": "Data-Driven Feedback Linearization using the Koopman Generator",
    "abstract": "This paper contributes a theoretical framework for data-driven feedback\nlinearization of nonlinear control-affine systems. We unify the traditional\ngeometric perspective on feedback linearization with an operator-theoretic\nperspective involving the Koopman operator. We first show that if the\ndistribution of the control vector field and its repeated Lie brackets with the\ndrift vector field is involutive, then there exists an output and a feedback\ncontrol law for which the Koopman generator is finite-dimensional and locally\nnilpotent. We use this connection to propose a data-driven algorithm for\nfeedback linearization. Particularly, we use experimental data to identify the\nstate transformation and control feedback from a dictionary of functions for\nwhich feedback linearization is achieved in a least-squares sense. Finally, we\nprovide numerical examples for the data-driven algorithm and compare it with\nmodel-based feedback linearization. We also numerically study the effect of the\nrichness of the dictionary and the size of the data set on the effectiveness of\nfeedback linearization.",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Darshan Gadginmath",
      "Vishaal Krishnan",
      "Fabio Pasqualetti"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05046"
  },
  {
    "id": "arXiv:2210.05051",
    "title": "Spreading Processes with Mutations over Multi-layer Networks",
    "abstract": "A key scientific challenge during the outbreak of novel infectious diseases\nis to predict how changes to the patterns of interaction in the host population\n(arising from different countermeasures) impact the spread of infection. Most\nepidemiological models do not consider the role of mutations in the pathogens\nor the heterogeneity in the type of contacts over which the infection\npropagates. However, pathogens often mutate in response to changing\nenvironments and medical interventions. Moreover, the spread of infectious\ndiseases depends intimately on the structural properties of the contact network\nof the host population, e.g., different congregate settings such as schools and\noffices pose varying risks of transmission. In this work, we propose and\nanalyze a multi-layer multi-strain model that more closely resembles real-world\npandemics by simultaneously taking into account the multi-layer structure\ntypical to human contact networks and mutations in the contagion. We derive the\nprobability of emergence of an epidemic, the mean fraction of individuals\ninfected with each strain, and the phase transition point beyond which an\nepidemic emerges. Our results highlight that existing models fail to fully\ncharacterize the epidemic outbreak caused by mutating pathogens on multi-layer\ncontact networks. We demonstrate that the impact of imposing/lifting mitigation\nmeasures concerning different contact network layers (e.g., school closures or\nwork-from-home policies) should be evaluated in connection with their impact on\nthe likelihood of the emergence of new pathogen strains. Our work further\nreinforces the need to develop network-based epidemiological models that\nsimultaneously account for the heterogeneity in the pathogen strains and\nnetwork structure to better predict the course of the disease outbreak.",
    "descriptor": "",
    "authors": [
      "Mansi Sood",
      "Anirudh Sridhar",
      "Rashad Eletreby",
      "Chai Wah Wu",
      "Simon A. Levin",
      "H. Vincent Poor",
      "Osman Yagan"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05051"
  },
  {
    "id": "arXiv:2210.05073",
    "title": "Self-supervised Model Based on Masked Autoencoders Advance CT Scans  Classification",
    "abstract": "The coronavirus pandemic has been going on since the year 2019, and the trend\nis still not abating. Therefore, it is particularly important to classify\nmedical CT scans to assist in medical diagnosis. At present, Supervised Deep\nLearning algorithms have made a great success in the classification task of\nmedical CT scans, but medical image datasets often require professional image\nannotation, and many research datasets are not publicly available. To solve\nthis problem, this paper is inspired by the self-supervised learning algorithm\nMAE and uses the MAE model pre-trained on ImageNet to perform transfer learning\non CT Scans dataset. This method improves the generalization performance of the\nmodel and avoids the risk of overfitting on small datasets. Through extensive\nexperiments on the COVID-CT dataset and the SARS-CoV-2 dataset, we compare the\nSSL-based method in this paper with other state-of-the-art supervised\nlearning-based pretraining methods. Experimental results show that our method\nimproves the generalization performance of the model more effectively and\navoids the risk of overfitting on small datasets. The model achieved almost the\nsame accuracy as supervised learning on both test datasets. Finally, ablation\nexperiments aim to fully demonstrate the effectiveness of our method and how it\nworks.",
    "descriptor": "",
    "authors": [
      "Jiashu Xu",
      "Sergii Stirenko"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05073"
  },
  {
    "id": "arXiv:2210.05080",
    "title": "How to construct the symmetric cycle of length 5 using Haj\u00f3s  construction with an adapted Rank Genetic Algorithm",
    "abstract": "In 2020 Bang-Jensen et. al. generalized the Haj\\'os join of two graphs to the\nclass of digraphs and generalized several results for vertex colorings in\ndigraphs. Although, as a consequence of these results, a digraph can be\nobtained by Haj\\'os constructions (directed Haj\\'os join and identifying\nnon-adjacent vertices), determining the Haj\\'os constructions to obtain the\ndigraph is a complex problem. In particular, Bang-Jensen et. al. posed the\nproblem of determining the Haj\\'os operations to construct the symmetric\n5-cycle from the complete symmetric digraph of order 3 using only Haj\\'os\nconstructions. We successfully adapted a rank-based genetic algorithm to solve\nthis problem by the introduction of innovative recombination and mutation\noperators from Graph Theory. The Haj\\'os Join became the recombination operator\nand the identification of independent vertices became the mutation operator. In\nthis way, we were able to obtain a sequence of only 16 Haj\\'os operations to\nconstruct the symmetric cycle of order 5.",
    "descriptor": "\nComments: 11 pages, 5 figures, 6 algoritms\n",
    "authors": [
      "Juan Carlos Garc\u00eda-Altamirano",
      "Mika Olsen",
      "Jorge Cervantes-Ojeda"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05080"
  },
  {
    "id": "arXiv:2210.05104",
    "title": "3D Matting: A Benchmark Study on Soft Segmentation Method for Pulmonary  Nodules Applied in Computed Tomography",
    "abstract": "Usually, lesions are not isolated but are associated with the surrounding\ntissues. For example, the growth of a tumour can depend on or infiltrate into\nthe surrounding tissues. Due to the pathological nature of the lesions, it is\nchallenging to distinguish their boundaries in medical imaging. However, these\nuncertain regions may contain diagnostic information. Therefore, the simple\nbinarization of lesions by traditional binary segmentation can result in the\nloss of diagnostic information. In this work, we introduce the image matting\ninto the 3D scenes and use the alpha matte, i.e., a soft mask, to describe\nlesions in a 3D medical image. The traditional soft mask acted as a training\ntrick to compensate for the easily mislabelled or under-labelled ambiguous\nregions. In contrast, 3D matting uses soft segmentation to characterize the\nuncertain regions more finely, which means that it retains more structural\ninformation for subsequent diagnosis and treatment. The current study of image\nmatting methods in 3D is limited. To address this issue, we conduct a\ncomprehensive study of 3D matting, including both traditional and\ndeep-learning-based methods. We adapt four state-of-the-art 2D image matting\nalgorithms to 3D scenes and further customize the methods for CT images to\ncalibrate the alpha matte with the radiodensity. Moreover, we propose the first\nend-to-end deep 3D matting network and implement a solid 3D medical image\nmatting benchmark. Its efficient counterparts are also proposed to achieve a\ngood performance-computation balance. Furthermore, there is no high-quality\nannotated dataset related to 3D matting, slowing down the development of\ndata-driven deep-learning-based methods. To address this issue, we construct\nthe first 3D medical matting dataset. The validity of the dataset was verified\nthrough clinicians' assessments and downstream experiments.",
    "descriptor": "\nComments: Accepted by Computers in Biology and Medicine. arXiv admin note: substantial text overlap with arXiv:2209.07843\n",
    "authors": [
      "Lin Wang",
      "Xiufen Ye",
      "Donghao Zhang",
      "Wanji He",
      "Lie Ju",
      "Yi Luo",
      "Huan Luo",
      "Xin Wang",
      "Wei Feng",
      "Kaimin Song",
      "Xin Zhao",
      "Zongyuan Ge"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05104"
  },
  {
    "id": "arXiv:2210.05108",
    "title": "Functional Constrained Optimization for Risk Aversion and Sparsity  Control",
    "abstract": "Risk and sparsity requirements often need to be enforced simultaneously in\nmany applications, e.g., in portfolio optimization, assortment planning, and\ntreatment planning. Properly balancing these potentially conflicting\nrequirements entails the formulation of functional constrained optimization\nwith either convex or nonconvex objectives. In this paper, we focus on\nprojection-free methods that can generate a sparse trajectory for solving these\nchallenging functional constrained optimization problems. Specifically, for the\nconvex setting, we propose a Level Conditional Gradient (LCG) method, which\nleverages a level-set framework to update the approximation of the optimal\nvalue and an inner conditional gradient oracle (CGO) for solving mini-max\nsubproblems. We show that the method achieves\n$\\mathcal{O}\\big(\\frac{1}{\\epsilon^2}\\log\\frac{1}{\\epsilon}\\big)$ iteration\ncomplexity for solving both smooth and nonsmooth cases without dependency on a\npossibly large size of optimal dual Lagrange multiplier. For the nonconvex\nsetting, we introduce the Level Inexact Proximal Point (IPP-LCG) method and the\nDirect Nonconvex Conditional Gradient (DNCG) method. The first approach taps\ninto the advantage of LCG by transforming the problem into a series of convex\nsubproblems and exhibits an\n$\\mathcal{O}\\big(\\frac{1}{\\epsilon^3}\\log\\frac{1}{\\epsilon}\\big)$ iteration\ncomplexity for finding an ($\\epsilon,\\epsilon$)-KKT point. The DNCG is the\nfirst single-loop projection-free method, with iteration complexity bounded by\n$\\mathcal{O}\\big(1/\\epsilon^4\\big)$ for computing a so-called $\\epsilon$-Wolfe\npoint. We demonstrate the effectiveness of LCG, IPP-LCG and DNCG by devising\nformulations and conducting numerical experiments on two risk averse sparse\noptimization applications: a portfolio selection problem with and without\ncardinality requirement, and a radiation therapy planning problem in\nhealthcare.",
    "descriptor": "",
    "authors": [
      "Yi Cheng",
      "Guanghui Lan",
      "H. Edwin Romeijn"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05108"
  },
  {
    "id": "arXiv:2210.05117",
    "title": "DA-VSR: Domain Adaptable Volumetric Super-Resolution For Medical Images",
    "abstract": "Medical image super-resolution (SR) is an active research area that has many\npotential applications, including reducing scan time, bettering visual\nunderstanding, increasing robustness in downstream tasks, etc. However,\napplying deep-learning-based SR approaches for clinical applications often\nencounters issues of domain inconsistency, as the test data may be acquired by\ndifferent machines or on different organs. In this work, we present a novel\nalgorithm called domain adaptable volumetric super-resolution (DA-VSR) to\nbetter bridge the domain inconsistency gap. DA-VSR uses a unified feature\nextraction backbone and a series of network heads to improve image quality over\ndifferent planes. Furthermore, DA-VSR leverages the in-plane and through-plane\nresolution differences on the test data to achieve a self-learned domain\nadaptation. As such, DA-VSR combines the advantages of a strong feature\ngenerator learned through supervised training and the ability to tune to the\nidiosyncrasies of the test volumes through unsupervised learning. Through\nexperiments, we demonstrate that DA-VSR significantly improves super-resolution\nquality across numerous datasets of different domains, thereby taking a further\nstep toward real clinical applications.",
    "descriptor": "\nComments: MICCAI2021\n",
    "authors": [
      "Cheng Peng",
      "S. Kevin Zhou",
      "Rama Chellappa"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05117"
  },
  {
    "id": "arXiv:2210.05124",
    "title": "Persistence Diagram Bundles: A Multidimensional Generalization of  Vineyards",
    "abstract": "A persistence diagram (PD) summarizes the persistent homology of a\nfiltration. I introduce the concept of a persistence diagram bundle, which is\nthe space of PDs associated with a fibered filtration function (a set $\\{f_t:\n\\mathcal{K}^t \\to \\mathbb{R}\\}_{t \\in \\mathcal{T}}$ of filtrations\nparameterized by a topological space $\\mathcal{T}$). Special cases include\nvineyards, the persistent homology transform, and fibered barcodes of\nmultiparameter persistence modules. I prove that if $\\mathcal{T}$ is a compact\n$n$-dimensional manifold, then for generic fibered filtration functions,\n$\\mathcal{T}$ is stratified such that within each $n$-dimensional stratum $S$,\nthere is a single PD \"template\" (a list of birth and death simplices) that can\nbe used to obtain $PD(f_t)$ for any $t \\in S$. I also show that not every local\nsection can be extended to a global section. Consequently, the points in the\nPDs do not typically trace out separate manifolds as $t \\in \\mathcal{T}$\nvaries; this is unlike a vineyard, in which the points in the PDs trace out\ncurves (\"vines\").",
    "descriptor": "\nComments: working paper\n",
    "authors": [
      "Abigail Hickok"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.05124"
  },
  {
    "id": "arXiv:2210.05138",
    "title": "Commitments to Quantum States",
    "abstract": "What does it mean to commit to a quantum state? In this work, we propose a\nsimple answer: a commitment to quantum messages is binding if, after the commit\nphase, the committed state is hidden from the sender's view. We accompany this\nnew definition with several instantiations. We build the first non-interactive\nsuccinct quantum state commitments, which can be seen as an analogue of\ncollision-resistant hashing for quantum messages. We also show that hiding\nquantum state commitments (QSCs) are implied by any commitment scheme for\nclassical messages. All of our constructions can be based on\nquantum-cryptographic assumptions that are implied by but are potentially\nweaker than one-way functions.\nCommitments to quantum states open the door to many new cryptographic\npossibilities. Our flagship application of a succinct QSC is a\nquantum-communication version of Kilian's succinct arguments for any language\nthat has quantum PCPs with constant error and polylogarithmic locality.\nPlugging in the PCP theorem, this yields succinct arguments for NP under\nsignificantly weaker assumptions than required classically; moreover, if the\nquantum PCP conjecture holds, this extends to QMA. At the heart of our security\nproof is a new rewinding technique for extracting quantum information.",
    "descriptor": "",
    "authors": [
      "Sam Gunn",
      "Nathan Ju",
      "Fermi Ma",
      "Mark Zhandry"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05138"
  },
  {
    "id": "arXiv:2210.05164",
    "title": "Tight Error Bounds for Nonnegative Orthogonality Constraints and Exact  Penalties",
    "abstract": "For the intersection of the Stiefel manifold and the set of nonnegative\nmatrices in $\\mathbb{R}^{n\\times r}$, we present global and local error bounds\nwith easily computable residual functions and explicit coefficients. Moreover,\nwe show that the error bounds cannot be improved except for the coefficients,\nwhich explains why two square-root terms are necessary in the bounds when $1 <\nr < n$ for the nonnegativity and orthogonality, respectively. The error bounds\nare applied to penalty methods for minimizing a Lipschitz continuous function\nwith nonnegative orthogonality constraints. Under only the Lipschitz continuity\nof the objective function, we prove the exactness of penalty problems that\npenalize the nonnegativity constraint, or the orthogonality constraint, or both\nconstraints. Our results cover both global and local minimizers.",
    "descriptor": "",
    "authors": [
      "Xiaojun Chen",
      "Yifan He",
      "Zaikun Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05164"
  },
  {
    "id": "arXiv:2210.05165",
    "title": "Combining datasets to increase the number of samples and improve model  fitting",
    "abstract": "For many use cases, combining information from different datasets can be of\ninterest to improve a machine learning model's performance, especially when the\nnumber of samples from at least one of the datasets is small. However, a\npotential challenge in such cases is that the features from these datasets are\nnot identical, even though there are some commonly shared features among the\ndatasets. To tackle this challenge, we propose a novel framework called Combine\ndatasets based on Imputation (ComImp). In addition, we propose a variant of\nComImp that uses Principle Component Analysis (PCA), PCA-ComImp in order to\nreduce dimension before combining datasets. This is useful when the datasets\nhave a large number of features that are not shared between them. Furthermore,\nour framework can also be utilized for data preprocessing by imputing missing\ndata, i.e., filling in the missing entries while combining different datasets.\nTo illustrate the power of the proposed methods and their potential usages, we\nconduct experiments for various tasks: regression, classification, and for\ndifferent data types: tabular data, time series data, when the datasets to be\ncombined have missing data. We also investigate how the devised methods can be\nused with transfer learning to provide even further model training improvement.\nOur results indicate that the proposed methods are somewhat similar to transfer\nlearning in that the merge can significantly improve the accuracy of a\nprediction model on smaller datasets. In addition, the methods can boost\nperformance by a significant margin when combining small datasets together and\ncan provide extra improvement when being used with transfer learning.",
    "descriptor": "",
    "authors": [
      "Thu Nguyen",
      "Rabindra Khadka",
      "Nhan Phan",
      "Anis Yazidi",
      "P\u00e5l Halvorsen",
      "Michael A. Riegler"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05165"
  },
  {
    "id": "arXiv:2210.05167",
    "title": "The Fast and Accurate Approach to Detection and Segmentation of Melanoma  Skin Cancer using Fine-tuned Yolov3 and SegNet Based on Deep Transfer  Learning",
    "abstract": "Melanoma is one of the most serious skin cancers that can occur in any part\nof the human skin. Early diagnosing melanoma lesions will significantly\nincrease their chances of being cured. Improving melanoma segmentation will\nhelp doctors or surgical robots remove the lesion more accurately from body\nparts. Recently, the learning-based segmentation methods achieved desired\nresults in image segmentation compared to traditional algorithms. This study\nproposes a new method to improve melanoma skin lesions detection and\nsegmentation by defining a two-step pipeline based on deep learning models. Our\nmethods were evaluated on ISIC 2018 (Skin Lesion Analysis Towards Melanoma\nDetection Challenge Dataset) well-known dataset. The proposed methods consist\nof two main parts for real-time detection of lesion location and segmentation.\nIn the detection section, the location of the skin lesion is precisely detected\nby the fine-tuned You Only Look Once version 3 (F-YOLOv3) and then fed into the\nfine-tuned Segmentation Network (F-SegNet). Skin lesion localization helps to\nreduce the unnecessary calculation of whole images for segmentation. The\nresults show that our proposed F-YOLOv3 achieves better performance as 96% in\nmAP. Compared to state-of-the-art segmentation approaches, our F-SegNet\nachieves higher performance for accuracy, dice coefficient, and Jaccard index\nat 95.16%, 92.81%, and 86.2%, respectively.",
    "descriptor": "",
    "authors": [
      "Mohamad Taghizadeh",
      "Karim Mohammadi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05167"
  },
  {
    "id": "arXiv:2210.05258",
    "title": "EOCSA: Predicting Prognosis of Epithelial Ovarian Cancer with Whole  Slide Histopathological Images",
    "abstract": "Ovarian cancer is one of the most serious cancers that threaten women around\nthe world. Epithelial ovarian cancer (EOC), as the most commonly seen subtype\nof ovarian cancer, has rather high mortality rate and poor prognosis among\nvarious gynecological cancers. Survival analysis outcome is able to provide\ntreatment advices to doctors. In recent years, with the development of medical\nimaging technology, survival prediction approaches based on pathological images\nhave been proposed. In this study, we designed a deep framework named EOCSA\nwhich analyzes the prognosis of EOC patients based on pathological whole slide\nimages (WSIs). Specifically, we first randomly extracted patches from WSIs and\ngrouped them into multiple clusters. Next, we developed a survival prediction\nmodel, named DeepConvAttentionSurv (DCAS), which was able to extract\npatch-level features, removed less discriminative clusters and predicted the\nEOC survival precisely. Particularly, channel attention, spatial attention, and\nneuron attention mechanisms were used to improve the performance of feature\nextraction. Then patient-level features were generated from our weight\ncalculation method and the survival time was finally estimated using LASSO-Cox\nmodel. The proposed EOCSA is efficient and effective in predicting prognosis of\nEOC and the DCAS ensures more informative and discriminative features can be\nextracted. As far as we know, our work is the first to analyze the survival of\nEOC based on WSIs and deep neural network technologies. The experimental\nresults demonstrate that our proposed framework has achieved state-of-the-art\nperformance of 0.980 C-index. The implementation of the approach can be found\nat https://github.com/RanSuLab/EOCprognosis.",
    "descriptor": "\nComments: Published in Expert Systems with Applications 2022\n",
    "authors": [
      "Tianling Liu",
      "Ran Su",
      "Changming Sun",
      "Xiuting Li",
      "Leyi Wei"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05258"
  },
  {
    "id": "arXiv:2210.05262",
    "title": "Factors of Influence of the Overestimation Bias of Q-Learning",
    "abstract": "We study whether the learning rate $\\alpha$, the discount factor $\\gamma$ and\nthe reward signal $r$ have an influence on the overestimation bias of the\nQ-Learning algorithm. Our preliminary results in environments which are\nstochastic and that require the use of neural networks as function\napproximators, show that all three parameters influence overestimation\nsignificantly. By carefully tuning $\\alpha$ and $\\gamma$, and by using an\nexponential moving average of $r$ in Q-Learning's temporal difference target,\nwe show that the algorithm can learn value estimates that are more accurate\nthan the ones of several other popular model-free methods that have addressed\nits overestimation bias in the past.",
    "descriptor": "",
    "authors": [
      "Julius Wagenbach",
      "Matthia Sabatelli"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05262"
  },
  {
    "id": "arXiv:2210.05354",
    "title": "Constructing Prediction Intervals with Neural Networks: An Empirical  Evaluation of Bootstrapping and Conformal Inference Methods",
    "abstract": "Artificial neural networks (ANNs) are popular tools for accomplishing many\nmachine learning tasks, including predicting continuous outcomes. However, the\ngeneral lack of confidence measures provided with ANN predictions limit their\napplicability. Supplementing point predictions with prediction intervals (PIs)\nis common for other learning algorithms, but the complex structure and training\nof ANNs renders constructing PIs difficult. This work provides the network\ndesign choices and inferential methods for creating better performing PIs with\nANNs. A two-step experiment is executed across 11 data sets, including an\nimaged-based data set. Two distribution-free methods for constructing PIs,\nbootstrapping and conformal inference, are considered. The results of the first\nexperimental step reveal that the choices inherent to building an ANN affect PI\nperformance. Guidance is provided for optimizing PI performance with respect to\neach network feature and PI method. In the second step, 20 algorithms for\nconstructing PIs, each using the principles of bootstrapping or conformal\ninference, are implemented to determine which provides the best performance\nwhile maintaining reasonable computational burden. In general, this trade-off\nis optimized when implementing the cross-conformal method, which maintained\ninterval coverage and efficiency with decreased computational burden.",
    "descriptor": "\nComments: 59 pages, 35 figures, 12 tables\n",
    "authors": [
      "Alex Contarino",
      "Christine Schubert Kabban",
      "Chancellor Johnstone",
      "Fairul Mohd-Zaid"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05354"
  },
  {
    "id": "arXiv:2210.05360",
    "title": "The Development of a Multi-Physics Approach for Modelling the Response  of Aerospace Fastener Assemblies to Lightning Attachment",
    "abstract": "This work is concerned with the development of a numerical modelling approach\nfor studying the time-accurate response of aerospace fasteners subjected to\nhigh electrical current loading from a simulated lightning strike. The\nelectromagnetic, thermal and elastoplastic response of individual fastener\ncomponents is captured by this method allowing a critical analysis of fastener\ndesign and material layering. Under high electrical current loading, ionisation\nof gas filled cavities in the fastener assembly can lead to viable current\npaths across internal voids. This ionisation can lead to localised pockets of\nhigh pressure plasma through the Joule heating effect. The multi-physics\napproach developed in this paper extends an existing methodology that allows a\ntwo-way dynamic non-linear coupling of the plasma arc, the titanium aerospace\nfastener components, the surrounding aircraft panels, the internal supporting\nstructure and internal plasma-filled cavities. Results from this model are\ncompared with experimental measurements of a titanium fastener holding together\ncarbon composite panels separated by thin dielectric layers. The current\ndistribution measurements are shown to be accurately reproduced. A parameter\nstudy is used to assess the internal cavity modelling strategy and to quantify\nthe relation between the internal cavity plasma pressure, the electrical\ncurrent distribution and changes in the internal cavity geometry.",
    "descriptor": "\nComments: 16 pages, 17 figures\n",
    "authors": [
      "William Bennett",
      "Stephen Millmore",
      "Nikolaos Nikiforakis"
    ],
    "subjectives": [
      "Computational Physics (physics.comp-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Applied Physics (physics.app-ph)",
      "Plasma Physics (physics.plasm-ph)",
      "Space Physics (physics.space-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.05360"
  },
  {
    "id": "arXiv:2210.05431",
    "title": "Non-Asymptotic Analysis of a UCB-based Top Two Algorithm",
    "abstract": "A Top Two sampling rule for bandit identification is a method which selects\nthe next arm to sample from among two candidate arms, a leader and a\nchallenger. Due to their simplicity and good empirical performance, they have\nreceived increased attention in recent years. For fixed-confidence best arm\nidentification, theoretical guarantees for Top Two methods have only been\nobtained in the asymptotic regime, when the error level vanishes. We derive the\nfirst non-asymptotic upper bound on the expected sample complexity of a Top Two\nalgorithm holding for any error level. Our analysis highlights sufficient\nproperties for a regret minimization algorithm to be used as leader. They are\nsatisfied by the UCB algorithm and our proposed UCB-based Top Two algorithm\nenjoys simultaneously non-asymptotic guarantees and competitive empirical\nperformance.",
    "descriptor": "\nComments: 32 pages, 5 figures, 3 tables\n",
    "authors": [
      "Marc Jourdan",
      "R\u00e9my Degenne"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05431"
  },
  {
    "id": "arXiv:2210.05436",
    "title": "Retinex Image Enhancement Based on Sequential Decomposition With a  Plug-and-Play Framework",
    "abstract": "The Retinex model is one of the most representative and effective methods for\nlow-light image enhancement. However, the Retinex model does not explicitly\ntackle the noise problem, and shows unsatisfactory enhancing results. In recent\nyears, due to the excellent performance, deep learning models have been widely\nused in low-light image enhancement. However, these methods have two\nlimitations: i) The desirable performance can only be achieved by deep learning\nwhen a large number of labeled data are available. However, it is not easy to\ncurate massive low/normal-light paired data; ii) Deep learning is notoriously a\nblack-box model [1]. It is difficult to explain their inner-working mechanism\nand understand their behaviors. In this paper, using a sequential Retinex\ndecomposition strategy, we design a plug-and-play framework based on the\nRetinex theory for simultaneously image enhancement and noise removal.\nMeanwhile, we develop a convolutional neural network-based (CNN-based) denoiser\ninto our proposed plug-and-play framework to generate a reflectance component.\nThe final enhanced image is produced by integrating the illumination and\nreflectance with gamma correction. The proposed plug-and-play framework can\nfacilitate both post hoc and ad hoc interpretability. Extensive experiments on\ndifferent datasets demonstrate that our framework outcompetes the\nstate-of-the-art methods in both image enhancement and denoising.",
    "descriptor": "",
    "authors": [
      "Tingting Wu",
      "Wenna Wu",
      "Ying Yang",
      "Feng-Lei Fan",
      "Tieyong Zeng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05436"
  },
  {
    "id": "arXiv:2210.05439",
    "title": "Network Topology Inference based on Timing Meta-Data",
    "abstract": "Consider a processor having access only to meta-data consisting of the\ntimings of data packets and acknowledgment (ACK) packets from all nodes in a\nnetwork. The meta-data report the source node of each packet, but not the\ndestination nodes or the contents of the packets. The goal of the processor is\nto infer the network topology based solely on such information. Prior work\nleveraged causality metrics to identify which links are active. If the data\ntimings and ACK timings of two nodes -- say node 1 and node 2, respectively --\nare causally related, this may be taken as evidence that node 1 is\ncommunicating to node 2 (which sends back ACK packets to node 1). This paper\nstarts with the observation that packet losses can weaken the causality\nrelationship between data and ACK timing streams. To obviate this problem, a\nnew Expectation Maximization (EM)-based algorithm is introduced -- EM-causality\ndiscovery algorithm (EM-CDA) -- which treats packet losses as latent variables.\nEM-CDA iterates between the estimation of packet losses and the evaluation of\ncausality metrics. The method is validated through extensive experiments in\nwireless sensor networks on the NS-3 simulation platform.",
    "descriptor": "\nComments: submitted\n",
    "authors": [
      "Wenbo Du",
      "Tao Tan",
      "Haijun Zhang",
      "Xianbin Cao",
      "Gang Yan",
      "Osvaldo Simeone"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.05439"
  },
  {
    "id": "arXiv:2210.05440",
    "title": "CIRCA: comprehensible online system in support of chest X-rays-based  COVID-19 diagnosis",
    "abstract": "Due to the large accumulation of patients requiring hospitalization, the\nCOVID-19 pandemic disease caused a high overload of health systems, even in\ndeveloped countries. Deep learning techniques based on medical imaging data can\nhelp in the faster detection of COVID-19 cases and monitoring of disease\nprogression. Regardless of the numerous proposed solutions for lung X-rays,\nnone of them is a product that can be used in the clinic. Five different\ndatasets (POLCOVID, AIforCOVID, COVIDx, NIH, and artificially generated data)\nwere used to construct a representative dataset of 23 799 CXRs for model\ntraining; 1 050 images were used as a hold-out test set, and 44 247 as\nindependent test set (BIMCV database). A U-Net-based model was developed to\nidentify a clinically relevant region of the CXR. Each image class (normal,\npneumonia, and COVID-19) was divided into 3 subtypes using a 2D Gaussian\nmixture model. A decision tree was used to aggregate predictions from the\nInceptionV3 network based on processed CXRs and a dense neural network on\nradiomic features. The lung segmentation model gave the Sorensen-Dice\ncoefficient of 94.86% in the validation dataset, and 93.36% in the testing\ndataset. In 5-fold cross-validation, the accuracy for all classes ranged from\n91% to 93%, keeping slightly higher specificity than sensitivity and NPV than\nPPV. In the hold-out test set, the balanced accuracy ranged between 68% and\n100%. The highest performance was obtained for the subtypes N1, P1, and C1. A\nsimilar performance was obtained on the independent dataset for normal and\nCOVID-19 class subtypes. Seventy-six percent of COVID-19 patients wrongly\nclassified as normal cases were annotated by radiologists as with no signs of\ndisease. Finally, we developed the online service (https://circa.aei.polsl.pl)\nto provide access to fast diagnosis support tools.",
    "descriptor": "",
    "authors": [
      "Wojciech Prazuch",
      "Aleksandra Suwalska",
      "Marek Socha",
      "Joanna Tobiasz",
      "Pawel Foszner",
      "Jerzy Jaroszewicz",
      "Katarzyna Gruszczynska",
      "Magdalena Sliwinska",
      "Jerzy Walecki",
      "Tadeusz Popiela",
      "Grzegorz Przybylski",
      "Andrzej Cieszanowski",
      "Mateusz Nowak",
      "Malgorzata Pawlowska",
      "Robert Flisiak",
      "Krzysztof Simon",
      "Gabriela Zapolska",
      "Barbara Gizycka",
      "Edyta Szurowska",
      "POLCOVID Study Group",
      "Michal Marczyk",
      "Joanna Polanska"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05440"
  },
  {
    "id": "arXiv:2210.05443",
    "title": "QuCNN : A Quantum Convolutional Neural Network with Entanglement Based  Backpropagation",
    "abstract": "Quantum Machine Learning continues to be a highly active area of interest\nwithin Quantum Computing. Many of these approaches have adapted classical\napproaches to the quantum settings, such as QuantumFlow, etc. We push forward\nthis trend and demonstrate an adaption of the Classical Convolutional Neural\nNetworks to quantum systems - namely QuCNN. QuCNN is a parameterised\nmulti-quantum-state based neural network layer computing similarities between\neach quantum filter state and each quantum data state. With QuCNN, back\npropagation can be achieved through a single-ancilla qubit quantum routine.\nQuCNN is validated by applying a convolutional layer with a data state and a\nfilter state over a small subset of MNIST images, comparing the back propagated\ngradients, and training a filter state against an ideal target state.",
    "descriptor": "",
    "authors": [
      "Samuel A. Stein",
      "Ying Mao",
      "James Ang",
      "Ang Li"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05443"
  },
  {
    "id": "arXiv:2210.05446",
    "title": "Disentangling Causal Effects from Sets of Interventions in the Presence  of Unobserved Confounders",
    "abstract": "The ability to answer causal questions is crucial in many domains, as causal\ninference allows one to understand the impact of interventions. In many\napplications, only a single intervention is possible at a given time. However,\nin some important areas, multiple interventions are concurrently applied.\nDisentangling the effects of single interventions from jointly applied\ninterventions is a challenging task -- especially as simultaneously applied\ninterventions can interact. This problem is made harder still by unobserved\nconfounders, which influence both treatments and outcome. We address this\nchallenge by aiming to learn the effect of a single-intervention from both\nobservational data and sets of interventions. We prove that this is not\ngenerally possible, but provide identification proofs demonstrating that it can\nbe achieved under non-linear continuous structural causal models with additive,\nmultivariate Gaussian noise -- even when unobserved confounders are present.\nImportantly, we show how to incorporate observed covariates and learn\nheterogeneous treatment effects. Based on the identifiability proofs, we\nprovide an algorithm that learns the causal model parameters by pooling data\nfrom different regimes and jointly maximizing the combined likelihood. The\neffectiveness of our method is empirically demonstrated on both synthetic and\nreal-world data.",
    "descriptor": "\nComments: Accepted at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Olivier Jeunen",
      "Ciar\u00e1n M. Gilligan-Lee",
      "Rishabh Mehrotra",
      "Mounia Lalmas"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05446"
  },
  {
    "id": "arXiv:2210.05454",
    "title": "Low Complexity Convolutional Neural Networks for Equalization in Optical  Fiber Transmission",
    "abstract": "A convolutional neural network is proposed to mitigate fiber transmission\neffects, achieving a five-fold reduction in trainable parameters compared to\nalternative equalizers, and 3.5 dB improvement in MSE compared to DBP with\ncomparable complexity.",
    "descriptor": "\nComments: 2 pages, 3 figures. Submitted to the OSA Advanced Photonics Congress 2021. Presented in Signal Processing in Photonic Communications (SPPCom) 2021. From the session: Neural Networks Applications for Photonic Systems (SpM5C)\n",
    "authors": [
      "Mohannad Abu-romoh",
      "Nelson Costa",
      "Antonio Napoli",
      "Jo\u00e3o Pedro",
      "Yves Jaou\u00ebn",
      "Mansoor Yousefi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05454"
  },
  {
    "id": "arXiv:2210.05468",
    "title": "High-precision Density Mapping of Marine Debris and Floating Plastics  via Satellite Imagery",
    "abstract": "Combining multi-spectral satellite data and machine learning has been\nsuggested as a method for monitoring plastic pollutants in the ocean\nenvironment. Recent studies have made theoretical progress regarding the\nidentification of marine plastic via machine learning. However, no study has\nassessed the application of these methods for mapping and monitoring\nmarine-plastic density. As such, this paper comprised of three main components:\n(1) the development of a machine learning model, (2) the construction of the\nMAP-Mapper, an automated tool for mapping marine-plastic density, and finally\n(3) an evaluation of the whole system for out-of-distribution test locations.\nThe findings from this paper leverage the fact that machine learning models\nneed to be high-precision to reduce the impact of false positives on results.\nThe developed MAP-Mapper architectures provide users choices to reach\nhigh-precision ($\\textit{abbv.}$ -HP) or optimum precision-recall\n($\\textit{abbv.}$ -Opt) values in terms of the training/test data set. Our\nMAP-Mapper-HP model greatly increased the precision of plastic detection to\n95\\%, whilst MAP-Mapper-Opt reaches precision-recall pair of 87\\%-88\\%. The\nMAP-Mapper contributes to the literature with the first tool to exploit\nadvanced deep/machine learning and multi-spectral imagery to map marine-plastic\ndensity in automated software. The proposed data pipeline has taken a novel\napproach to map plastic density in ocean regions. As such, this enables an\ninitial assessment of the challenges and opportunities of this method to help\nguide future work and scientific study.",
    "descriptor": "\nComments: 10 pages, 1 table, 4 figures\n",
    "authors": [
      "Henry Booth",
      "Wanli Ma",
      "Oktay Karakus"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05468"
  },
  {
    "id": "arXiv:2210.05469",
    "title": "On the Partial Differential L\u00fcroth's Theorem",
    "abstract": "We study the L\\\"{u}roth problem for partial differential fields. The main\nresult is the following partial differential analog of generalized L\\\"{u}roth's\ntheorem: Let $\\mathcal{F}$ be a differential field of characteristic 0 with $m$\nderivation operators, $\\textbf{u}=u_1,\\ldots,u_n$ a set of differential\nindeterminates over $\\mathcal{F}$. We prove that an intermediate differential\nfield $\\mathcal{G}$ between $\\mathcal{F}$ and $\\mathcal{F}\\langle\n\\textbf{u}\\rangle$ is a simple differential extension of $\\mathcal{F}$ if and\nonly if the differential dimension polynomial of $\\textbf{u}$ over\n$\\mathcal{G}$ is of the form $\\omega_{\\textbf{u}/\\mathcal{G}}(t)=n{t+m\\choose\nm}-{t+m-s\\choose m}$ for some $s\\in\\mathbb N$. This result generalizes the\nclassical differential L\\\"uroth's theorem proved by Ritt and Kolchin in the\ncase $m=n=1$. We then present an algorithm to decide whether a given finitely\ngenerated differential extension field of $\\mathcal{F}$ contained in\n$\\mathcal{F}\\langle \\textbf{u}\\rangle$ is a simple extension, and in the\naffirmative case, to compute a L\\\"{u}roth generator. As an application, we\nsolve the proper re-parameterization problem for unirational differential\ncurves.",
    "descriptor": "\nComments: 19 pages\n",
    "authors": [
      "Wei Li",
      "Chen-Rui Wei"
    ],
    "subjectives": [
      "Algebraic Geometry (math.AG)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2210.05469"
  },
  {
    "id": "arXiv:2210.05475",
    "title": "GENIE: Higher-Order Denoising Diffusion Solvers",
    "abstract": "Denoising diffusion models (DDMs) have emerged as a powerful class of\ngenerative models. A forward diffusion process slowly perturbs the data, while\na deep model learns to gradually denoise. Synthesis amounts to solving a\ndifferential equation (DE) defined by the learnt model. Solving the DE requires\nslow iterative solvers for high-quality generation. In this work, we propose\nHigher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor\nmethods, we derive a novel higher-order solver that significantly accelerates\nsynthesis. Our solver relies on higher-order gradients of the perturbed data\ndistribution, that is, higher-order score functions. In practice, only\nJacobian-vector products (JVPs) are required and we propose to extract them\nfrom the first-order score network via automatic differentiation. We then\ndistill the JVPs into a separate neural network that allows us to efficiently\ncompute the necessary higher-order terms for our novel sampler during\nsynthesis. We only need to train a small additional head on top of the\nfirst-order score network. We validate GENIE on multiple image generation\nbenchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike\nrecent methods that fundamentally alter the generation process in DDMs, our\nGENIE solves the true generative DE and still enables applications such as\nencoding and guided sampling. Project page and code:\nhttps://nv-tlabs.github.io/GENIE.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Tim Dockhorn",
      "Arash Vahdat",
      "Karsten Kreis"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05475"
  },
  {
    "id": "arXiv:2210.05490",
    "title": "Pooling Strategies for Simplicial Convolutional Networks",
    "abstract": "The goal of this paper is to introduce pooling strategies for simplicial\nconvolutional neural networks. Inspired by graph pooling methods, we introduce\na general formulation for a simplicial pooling layer that performs: i) local\naggregation of simplicial signals; ii) principled selection of sampling sets;\niii) downsampling and simplicial topology adaptation. The general layer is then\ncustomized to design four different pooling strategies (i.e., max, top-k,\nself-attention, and separated top-k) grounded in the theory of topological\nsignal processing. Also, we leverage the proposed layers in a hierarchical\narchitecture that reduce complexity while representing data at different\nresolutions. Numerical results on real data benchmarks (i.e., flow and graph\nclassification) illustrate the advantage of the proposed methods with respect\nto the state of the art.",
    "descriptor": "",
    "authors": [
      "Domenico Mattia Cinque",
      "Claudio Battiloro",
      "Paolo Di Lorenzo"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05490"
  },
  {
    "id": "arXiv:2210.05491",
    "title": "Strong negation in the theory of computable functionals TCF",
    "abstract": "We incorporate strong negation in the theory of computable functionals TCF, a\ncommon extension of Plotkin's PCF and G\\\"{o}del's system $\\mathbf{T}$, by\ndefining simultaneously the strong negation $A^{\\mathbf{N}}$ of a formula $A$\nand the strong negation $P^{\\mathbf{N}}$ of a predicate $P$ in TCF. As a\nspecial case of the latter, we get the strong negation of an inductive and a\ncoinductive predicate of TCF. We prove appropriate versions of the Ex falso\nquodlibet and of the double negation elimination for strong negation in TCF,\nand we study the so-called tight formulas of TCF i.e., formulas implied from\nthe weak negation of their strong negation. We present various case-studies and\nexamples, which reveal the naturality of our definition of strong negation in\nTCF and justify the use of TCF as a formal system for a large part of\nBishop-style constructive mathematics.",
    "descriptor": "",
    "authors": [
      "Nils K\u00f6pp",
      "Iosif Petrakis"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05491"
  },
  {
    "id": "arXiv:2210.05518",
    "title": "Autonomous Asteroid Characterization Through Nanosatellite Swarming",
    "abstract": "This paper first defines a class of estimation problem called simultaneous\nnavigation and characterization (SNAC), which is a superset of simultaneous\nlocalization and mapping (SLAM). A SNAC framework is then developed for the\nAutonomous Nanosatellite Swarming (ANS) mission concept to autonomously\nnavigate about and characterize an asteroid including the asteroid gravity\nfield, rotational motion, and 3D shape. The ANS SNAC framework consists of\nthree modules: 1) multi-agent optical landmark tracking and 3D point\nreconstruction using stereovision, 2) state estimation through a\ncomputationally efficient and robust unscented Kalman filter, and 3)\nreconstruction of an asteroid spherical harmonic shape model by leveraging a\npriori knowledge of the shape properties of celestial bodies. Despite\nsignificant interest in asteroids, there are several limitations to current\nasteroid rendezvous mission concepts. First, completed missions heavily rely on\nhuman oversight and Earth-based resources. Second, proposed solutions to\nincrease autonomy make oversimplifying assumptions about state knowledge and\ninformation processing. Third, asteroid mission concepts often opt for high\nsize, weight, power, and cost (SWaP-C) avionics for environmental measurements.\nFinally, such missions often utilize a single spacecraft, neglecting the\nbenefits of distributed space systems. In contrast, ANS is composed of multiple\nautonomous nanosatellites equipped with low SWaP-C avionics. The ANS SNAC\nframework is validated through a numerical simulation of three spacecraft\norbiting asteroid 433 Eros. The simulation results demonstrate that the\nproposed architecture provides autonomous and accurate SNAC in a safe manner\nwithout an a priori shape model and using only low SWaP-C avionics.",
    "descriptor": "\nComments: Submitted for publication to the IEEE Transactions on Aerospace and Electronic Systems\n",
    "authors": [
      "Kaitlin Dennison",
      "Nathan Stacey",
      "Simone D'Amico"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05518"
  },
  {
    "id": "arXiv:2210.05558",
    "title": "Causal and counterfactual views of missing data models",
    "abstract": "It is often said that the fundamental problem of causal inference is a\nmissing data problem -- the comparison of responses to two hypothetical\ntreatment assignments is made difficult because for every experimental unit\nonly one potential response is observed. In this paper, we consider the\nimplications of the converse view: that missing data problems are a form of\ncausal inference. We make explicit how the missing data problem of recovering\nthe complete data law from the observed law can be viewed as identification of\na joint distribution over counterfactual variables corresponding to values had\nwe (possibly contrary to fact) been able to observe them. Drawing analogies\nwith causal inference, we show how identification assumptions in missing data\ncan be encoded in terms of graphical models defined over counterfactual and\nobserved variables. We review recent results in missing data identification\nfrom this viewpoint. In doing so, we note interesting similarities and\ndifferences between missing data and causal identification theories.",
    "descriptor": "",
    "authors": [
      "Razieh Nabi",
      "Rohit Bhattacharya",
      "Ilya Shpitser",
      "James Robins"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05558"
  },
  {
    "id": "arXiv:2210.05571",
    "title": "Misspecified Phase Retrieval with Generative Priors",
    "abstract": "In this paper, we study phase retrieval under model misspecification and\ngenerative priors. In particular, we aim to estimate an $n$-dimensional signal\n$\\mathbf{x}$ from $m$ i.i.d.~realizations of the single index model $y =\nf(\\mathbf{a}^T\\mathbf{x})$, where $f$ is an unknown and possibly random\nnonlinear link function and $\\mathbf{a} \\in \\mathbb{R}^n$ is a standard\nGaussian vector. We make the assumption\n$\\mathrm{Cov}[y,(\\mathbf{a}^T\\mathbf{x})^2] \\ne 0$, which corresponds to the\nmisspecified phase retrieval problem. In addition, the underlying signal\n$\\mathbf{x}$ is assumed to lie in the range of an $L$-Lipschitz continuous\ngenerative model with bounded $k$-dimensional inputs. We propose a two-step\napproach, for which the first step plays the role of spectral initialization\nand the second step refines the estimated vector produced by the first step\niteratively. We show that both steps enjoy a statistical rate of order\n$\\sqrt{(k\\log L)\\cdot (\\log m)/m}$ under suitable conditions. Experiments on\nimage datasets are performed to demonstrate that our approach performs on par\nwith or even significantly outperforms several competing methods.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Zhaoqiang Liu",
      "Xinshao Wang",
      "Jiulong Liu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05571"
  },
  {
    "id": "arXiv:2210.05582",
    "title": "Digital Twin-Based Multiple Access Optimization and Monitoring via  Model-Driven Bayesian Learning",
    "abstract": "Commonly adopted in the manufacturing and aerospace sectors, digital twin\n(DT) platforms are increasingly seen as a promising paradigm to control and\nmonitor software-based, \"open\", communication systems, which play the role of\nthe physical twin (PT). In the general framework presented in this work, the DT\nbuilds a Bayesian model of the communication system, which is leveraged to\nenable core DT functionalities such as control via multi-agent reinforcement\nlearning (MARL) and monitoring of the PT for anomaly detection. We specifically\ninvestigate the application of the proposed framework to a simple case-study\nsystem encompassing multiple sensing devices that report to a common receiver.\nThe Bayesian model trained at the DT has the key advantage of capturing\nepistemic uncertainty regarding the communication system, e.g., regarding\ncurrent traffic conditions, which arise from limited PT-to-DT data transfer.\nExperimental results validate the effectiveness of the proposed Bayesian\nframework as compared to standard frequentist model-based solutions.",
    "descriptor": "\nComments: submitted for conference publication\n",
    "authors": [
      "Clement Ruah",
      "Osvaldo Simeone",
      "Bashir Al-Hashimi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.05582"
  },
  {
    "id": "arXiv:2210.05584",
    "title": "On Adaptivity in Non-stationary Stochastic Optimization With Bandit  Feedback",
    "abstract": "In this paper we study the non-stationary stochastic optimization question\nwith bandit feedback and dynamic regret measures. The seminal work of Besbes et\nal. (2015) shows that, when aggregated function changes is known a priori, a\nsimple re-starting algorithm attains the optimal dynamic regret. In this work,\nwe designed a stochastic optimization algorithm with fixed step sizes, which\ncombined together with the multi-scale sampling framework of Wei and Luo (2021)\nachieves the optimal dynamic regret in non-stationary stochastic optimization\nwithout requiring prior knowledge of function change budget, thereby closes a\nquestion that has been open for a while. We also establish an additional result\nshowing that any algorithm achieving good regret against stationary benchmarks\nwith high probability could be automatically converted to an algorithm that\nachieves good regret against dynamic benchmarks, which is applicable to a wide\nclass of bandit convex optimization algorithms.",
    "descriptor": "",
    "authors": [
      "Yining Wang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05584"
  },
  {
    "id": "arXiv:2210.05597",
    "title": "Digitization of Raster Logs: A Deep Learning Approach",
    "abstract": "Raster well-log images are digital representations of well-logs data\ngenerated over the years. Raster digital well logs represent bitmaps of the log\nimage in a rectangular array of black (zeros) and white dots (ones) called\npixels. Experts study the raster logs manually or with software applications\nthat still require a tremendous amount of manual input. Besides the loss of\nthousands of person-hours, this process is erroneous and tedious. To digitize\nthese raster logs, one must buy a costly digitizer that is not only manual and\ntime-consuming but also a hidden technical debt since enterprises stand to lose\nmore money in additional servicing and consulting charges. We propose a deep\nneural network architecture called VeerNet to semantically segment the raster\nimages from the background grid and classify and digitize the well-log curves.\nRaster logs have a substantially greater resolution than images traditionally\nconsumed by image segmentation pipelines. Since the input has a low\nsignal-to-resolution ratio, we require rapid downsampling to alleviate\nunnecessary computation. We thus employ a modified UNet-inspired architecture\nthat balances retaining key signals and reducing result dimensionality. We use\nattention augmented read-process-write architecture. This architecture\nefficiently classifies and digitizes the curves with an overall F1 score of 35%\nand IoU of 30%. When compared to the actual las values for Gamma-ray and\nderived value of Gamma-ray from VeerNet, a high Pearson coefficient score of\n0.62 was achieved.",
    "descriptor": "",
    "authors": [
      "M Quamer Nasim",
      "Narendra Patwardhan",
      "Tannistha Maiti",
      "Tarry Singh"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05597"
  },
  {
    "id": "arXiv:2210.05618",
    "title": "Zero-Order One-Point Estimate with Distributed Stochastic  Gradient-Tracking Technique",
    "abstract": "In this work, we consider a distributed multi-agent stochastic optimization\nproblem, where each agent holds a local objective function that is smooth and\nconvex, and that is subject to a stochastic process. The goal is for all agents\nto collaborate to find a common solution that optimizes the sum of these local\nfunctions. With the practical assumption that agents can only obtain noisy\nnumerical function queries at exactly one point at a time, we extend the\ndistributed stochastic gradient-tracking method to the bandit setting where we\ndon't have an estimate of the gradient, and we introduce a zero-order (ZO)\none-point estimate (1P-DSGT). We analyze the convergence of this novel\ntechnique for smooth and convex objectives using stochastic approximation\ntools, and we prove that it converges almost surely to the optimum. We then\nstudy the convergence rate for when the objectives are additionally strongly\nconvex. We obtain a rate of $O(\\frac{1}{\\sqrt{k}})$ after a sufficient number\nof iterations $k > K_2$ which is usually optimal for techniques utilizing\none-point estimators. We also provide a regret bound of $O(\\sqrt{k})$, which is\nexceptionally good compared to the aforementioned techniques. We further\nillustrate the usefulness of the proposed technique using numerical\nexperiments.",
    "descriptor": "\nComments: 36 pages, 8 figures\n",
    "authors": [
      "Elissa Mhanna",
      "Mohamad Assaad"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05618"
  },
  {
    "id": "arXiv:2210.05630",
    "title": "EllipsoNet: Deep-learning-enabled optical ellipsometry for complex thin  films",
    "abstract": "Optical spectroscopy is indispensable for research and development in\nnanoscience and nanotechnology, microelectronics, energy, and advanced\nmanufacturing. Advanced optical spectroscopy tools often require both\nspecifically designed high-end instrumentation and intricate data analysis\ntechniques. Beyond the common analytical tools, deep learning methods are well\nsuited for interpreting high-dimensional and complicated spectroscopy data.\nThey offer great opportunities to extract subtle and deep information about\noptical properties of materials with simpler optical setups, which would\notherwise require sophisticated instrumentation. In this work, we propose a\ncomputational ellipsometry approach based on a conventional tabletop optical\nmicroscope and a deep learning model called EllipsoNet. Without any prior\nknowledge about the multilayer substrates, EllipsoNet can predict the complex\nrefractive indices of thin films on top of these nontrivial substrates from\nexperimentally measured optical reflectance spectra with high accuracies. This\ntask was not feasible previously with traditional reflectometry or ellipsometry\nmethods. Fundamental physical principles, such as the Kramers-Kronig relations,\nare spontaneously learned by the model without any further training. This\napproach enables in-operando optical characterization of functional materials\nwithin complex photonic structures or optoelectronic devices.",
    "descriptor": "",
    "authors": [
      "Ziyang Wang",
      "Yuxuan Cosmi Lin",
      "Kunyan Zhang",
      "Wenjing Wu",
      "Shengxi Huang"
    ],
    "subjectives": [
      "Optics (physics.optics)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05630"
  },
  {
    "id": "arXiv:2210.05644",
    "title": "Simulating single-photon detector array sensors for depth imaging",
    "abstract": "Single-Photon Avalanche Detector (SPAD) arrays are a rapidly emerging\ntechnology. These multi-pixel sensors have single-photon sensitivities and\npico-second temporal resolutions thus they can rapidly generate depth images\nwith millimeter precision. Such sensors are a key enabling technology for\nfuture autonomous systems as they provide guidance and situational awareness.\nHowever, to fully exploit the capabilities of SPAD array sensors, it is crucial\nto establish the quality of depth images they are able to generate in a wide\nrange of scenarios. Given a particular optical system and a finite image\nacquisition time, what is the best-case depth resolution and what are realistic\nimages generated by SPAD arrays? In this work, we establish a robust yet simple\nnumerical procedure that rapidly establishes the fundamental limits to depth\nimaging with SPAD arrays under real world conditions. Our approach accurately\ngenerates realistic depth images in a wide range of scenarios, allowing the\nperformance of an optical depth imaging system to be established without the\nneed for costly and laborious field testing. This procedure has applications in\nobject detection and tracking for autonomous systems and could be easily\nextended to systems for underwater imaging or for imaging around corners.",
    "descriptor": "",
    "authors": [
      "Stirling Scholes",
      "Germ\u00e1n Mora-Mart\u00edn",
      "Feng Zhu",
      "Istvan Gyongy",
      "Phil Soan",
      "Jonathan Leach"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2210.05644"
  },
  {
    "id": "arXiv:1211.6940",
    "title": "Choice Disjunctive Queries in Logic Programming",
    "abstract": "Comments: IEICE transaction on Information and Systems (to appear)",
    "descriptor": "\nComments: IEICE transaction on Information and Systems (to appear)\n",
    "authors": [
      "Keehang Kwon",
      "Daeseong Kang"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/1211.6940"
  },
  {
    "id": "arXiv:1803.09941",
    "title": "Iteration-complexity of first-order augmented Lagrangian methods for  convex conic programming",
    "abstract": "Comments: accepted by SIAM Journal on Optimization",
    "descriptor": "\nComments: accepted by SIAM Journal on Optimization\n",
    "authors": [
      "Zhaosong Lu",
      "Zirui Zhou"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1803.09941"
  },
  {
    "id": "arXiv:1902.10664",
    "title": "Local Function Complexity for Active Learning via Mixture of Gaussian  Processes",
    "abstract": "Comments: 16 pages (+16 pages of references and appendices), 16 figures",
    "descriptor": "\nComments: 16 pages (+16 pages of references and appendices), 16 figures\n",
    "authors": [
      "Danny Panknin",
      "Stefan Chmiela",
      "Klaus-Robert M\u00fcller",
      "Shinichi Nakajima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1902.10664"
  },
  {
    "id": "arXiv:1911.12446",
    "title": "QubitHD: A Stochastic Acceleration Method for HD Computing-Based Machine  Learning",
    "abstract": "Comments: 8 pages, 5 figures, 3 tables",
    "descriptor": "\nComments: 8 pages, 5 figures, 3 tables\n",
    "authors": [
      "Samuel Bosch",
      "Alexander Sanchez de la Cerda",
      "Mohsen Imani",
      "Tajana Simunic Rosing",
      "Giovanni De Micheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1911.12446"
  },
  {
    "id": "arXiv:2002.00107",
    "title": "Generative Modeling with Denoising Auto-Encoders and Langevin Sampling",
    "abstract": "Generative Modeling with Denoising Auto-Encoders and Langevin Sampling",
    "descriptor": "",
    "authors": [
      "Adam Block",
      "Youssef Mroueh",
      "Alexander Rakhlin"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2002.00107"
  },
  {
    "id": "arXiv:2003.03007",
    "title": "Unifying Graph Embedding Features with Graph Convolutional Networks for  Skeleton-based Action Recognition",
    "abstract": "Unifying Graph Embedding Features with Graph Convolutional Networks for  Skeleton-based Action Recognition",
    "descriptor": "",
    "authors": [
      "Dong Yang",
      "Monica Mengqi Li",
      "Hong Fu",
      "Jicong Fan",
      "Zhao Zhang",
      "Howard Leung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2003.03007"
  },
  {
    "id": "arXiv:2003.05699",
    "title": "On Hop-Constrained Steiner Trees in Tree-Like Metrics",
    "abstract": "On Hop-Constrained Steiner Trees in Tree-Like Metrics",
    "descriptor": "",
    "authors": [
      "Martin B\u00f6hm",
      "Ruben Hoeksma",
      "Nicole Megow",
      "Lukas N\u00f6lke",
      "Bertrand Simon"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2003.05699"
  },
  {
    "id": "arXiv:2006.12683",
    "title": "Improving Workflow Integration with xPath: Design and Evaluation of a  Human-AI Diagnosis System in Pathology",
    "abstract": "Comments: 31 pages, 13 figures",
    "descriptor": "\nComments: 31 pages, 13 figures\n",
    "authors": [
      "Hongyan Gu",
      "Yuan Liang",
      "Yifan Xu",
      "Christopher Kazu Williams",
      "Shino Magaki",
      "Negar Khanlou",
      "Harry Vinters",
      "Zesheng Chen",
      "Shuo Ni",
      "Chunxu Yang",
      "Xinhai Robert Zhang",
      "Yang Li",
      "Mohammad Haeri",
      "Xiang 'Anthony' Chen"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2006.12683"
  },
  {
    "id": "arXiv:2007.10785",
    "title": "Automated Detection and Forecasting of COVID-19 using Deep Learning  Techniques: A Review",
    "abstract": "Automated Detection and Forecasting of COVID-19 using Deep Learning  Techniques: A Review",
    "descriptor": "",
    "authors": [
      "Afshin Shoeibi",
      "Marjane Khodatars",
      "Roohallah Alizadehsani",
      "Navid Ghassemi",
      "Mahboobeh Jafari",
      "Parisa Moridian",
      "Ali Khadem",
      "Delaram Sadeghi",
      "Sadiq Hussain",
      "Assef Zare",
      "Zahra Alizadeh Sani",
      "Javad Bazeli",
      "Fahime Khozeimeh",
      "Abbas Khosravi",
      "Saeid Nahavandi",
      "U. Rajendra Acharya",
      "Juan M. Gorriz",
      "Peng Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2007.10785"
  },
  {
    "id": "arXiv:2008.05348",
    "title": "Approaching Neural Chinese Word Segmentation as a Low-Resource Machine  Translation Task",
    "abstract": "Comments: PACLIC 2022",
    "descriptor": "\nComments: PACLIC 2022\n",
    "authors": [
      "Pinzhen Chen",
      "Kenneth Heafield"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2008.05348"
  },
  {
    "id": "arXiv:2009.10939",
    "title": "Scene Graph to Image Generation with Contextualized Object Layout  Refinement",
    "abstract": "Comments: Appeared at IEEE International Conference in Image Processing (ICIP) 2021",
    "descriptor": "\nComments: Appeared at IEEE International Conference in Image Processing (ICIP) 2021\n",
    "authors": [
      "Maor Ivgi",
      "Yaniv Benny",
      "Avichai Ben-David",
      "Jonathan Berant",
      "Lior Wolf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2009.10939"
  },
  {
    "id": "arXiv:2010.00261",
    "title": "NodeSig: Binary Node Embeddings via Random Walk Diffusion",
    "abstract": "Comments: The manuscript was accepted to the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)",
    "descriptor": "\nComments: The manuscript was accepted to the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)\n",
    "authors": [
      "Abdulkadir \u00c7elikkanat",
      "Fragkiskos D. Malliaros",
      "Apostolos N. Papadopoulos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2010.00261"
  },
  {
    "id": "arXiv:2010.07602",
    "title": "Vehicular Networks for Combating a Worldwide Pandemic: Preventing the  Spread of COVID-19",
    "abstract": "Comments: Accepted Paper in Smart Health",
    "descriptor": "\nComments: Accepted Paper in Smart Health\n",
    "authors": [
      "Ahmet M. Elbir",
      "Gokhan Gurbilek",
      "Burak Soner",
      "Anastasios K. Papazafeiropoulos",
      "Pandelis Kourtessis",
      "Sinem Coleri"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2010.07602"
  },
  {
    "id": "arXiv:2010.08202",
    "title": "Manipulation-Oriented Object Perception in Clutter through Affordance  Coordinate Frames",
    "abstract": "Comments: Humanoids 2022 paper. video link: this https URL; this https URL, github link: this https URL",
    "descriptor": "\nComments: Humanoids 2022 paper. video link: this https URL; this https URL, github link: this https URL\n",
    "authors": [
      "Xiaotong Chen",
      "Kaizhi Zheng",
      "Zhen Zeng",
      "Cameron Kisailus",
      "Shreshtha Basu",
      "James Cooney",
      "Jana Pavlasek",
      "Odest Chadwicke Jenkins"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.08202"
  },
  {
    "id": "arXiv:2010.13735",
    "title": "Personalised Meta-path Generation for Heterogeneous GNNs",
    "abstract": "Personalised Meta-path Generation for Heterogeneous GNNs",
    "descriptor": "",
    "authors": [
      "Zhiqiang Zhong",
      "Cheng-Te Li",
      "Jun Pang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2010.13735"
  },
  {
    "id": "arXiv:2010.15277",
    "title": "Class-incremental learning: survey and performance evaluation on image  classification",
    "abstract": "Comments: Paper accepted for publication at TPAMI 2022. Code publicly available at this https URL",
    "descriptor": "\nComments: Paper accepted for publication at TPAMI 2022. Code publicly available at this https URL\n",
    "authors": [
      "Marc Masana",
      "Xialei Liu",
      "Bartlomiej Twardowski",
      "Mikel Menta",
      "Andrew D. Bagdanov",
      "Joost van de Weijer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2010.15277"
  },
  {
    "id": "arXiv:2011.04590",
    "title": "From Eye-blinks to State Construction: Diagnostic Benchmarks for Online  Representation Learning",
    "abstract": "From Eye-blinks to State Construction: Diagnostic Benchmarks for Online  Representation Learning",
    "descriptor": "",
    "authors": [
      "Banafsheh Rafiee",
      "Zaheer Abbas",
      "Sina Ghiassian",
      "Raksha Kumaraswamy",
      "Richard Sutton",
      "Elliot Ludvig",
      "Adam White"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2011.04590"
  },
  {
    "id": "arXiv:2011.11558",
    "title": "Beta-CoRM: A Bayesian Approach for $n$-gram Profiles Analysis",
    "abstract": "Beta-CoRM: A Bayesian Approach for $n$-gram Profiles Analysis",
    "descriptor": "",
    "authors": [
      "Jos\u00e9 A. Perusqu\u00eda",
      "Jim E. Griffin",
      "Cristiano Villa"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Cryptography and Security (cs.CR)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2011.11558"
  },
  {
    "id": "arXiv:2012.01699",
    "title": "Content-Adaptive Pixel Discretization to Improve Model Robustness",
    "abstract": "Content-Adaptive Pixel Discretization to Improve Model Robustness",
    "descriptor": "",
    "authors": [
      "Ryan Feng",
      "Wu-chi Feng",
      "Atul Prakash"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2012.01699"
  },
  {
    "id": "arXiv:2012.13214",
    "title": "The Age of Incorrect Information: an Enabler of Semantics-Empowered  Communication",
    "abstract": "The Age of Incorrect Information: an Enabler of Semantics-Empowered  Communication",
    "descriptor": "",
    "authors": [
      "Ali Maatouk",
      "Mohamad Assaad",
      "Anthony Ephremides"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2012.13214"
  },
  {
    "id": "arXiv:2102.06028",
    "title": "Synthesis of Winning Attacks on Communication Protocols using  Supervisory Control Theory: Two Case Studies",
    "abstract": "Comments: 31 pages, 16 figures",
    "descriptor": "\nComments: 31 pages, 16 figures\n",
    "authors": [
      "Shoma Matsui",
      "St\u00e9phane Lafortune"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2102.06028"
  },
  {
    "id": "arXiv:2102.07148",
    "title": "A New Look and Convergence Rate of Federated Multi-Task Learning with  Laplacian Regularization",
    "abstract": "A New Look and Convergence Rate of Federated Multi-Task Learning with  Laplacian Regularization",
    "descriptor": "",
    "authors": [
      "Canh T. Dinh",
      "Tung T. Vu",
      "Nguyen H. Tran",
      "Minh N. Dao",
      "Hongyu Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2102.07148"
  },
  {
    "id": "arXiv:2102.10185",
    "title": "Cornus: Atomic Commit for a Cloud DBMS with Storage Disaggregation  (Extended Version)",
    "abstract": "Cornus: Atomic Commit for a Cloud DBMS with Storage Disaggregation  (Extended Version)",
    "descriptor": "",
    "authors": [
      "Zhihan Guo",
      "Xinyu Zeng",
      "Kan Wu",
      "Wuh-Chwen Hwang",
      "Ziwei Ren",
      "Xiangyao Yu",
      "Mahesh Balakrishnan",
      "Philip A. Bernstein"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2102.10185"
  },
  {
    "id": "arXiv:2102.10423",
    "title": "An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks",
    "abstract": "Comments: 13 pages, 15 figures, 8 tables, published in IISWC 2022",
    "descriptor": "\nComments: 13 pages, 15 figures, 8 tables, published in IISWC 2022\n",
    "authors": [
      "Kiran Seshadri",
      "Berkin Akin",
      "James Laudon",
      "Ravi Narayanaswami",
      "Amir Yazdanbakhsh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2102.10423"
  },
  {
    "id": "arXiv:2103.02343",
    "title": "Provability in BI's Sequent Calculus is Decidable",
    "abstract": "Comments: The paper is incorrect",
    "descriptor": "\nComments: The paper is incorrect\n",
    "authors": [
      "Alexander Gheorghiu",
      "Simon Docherty",
      "David Pym"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Symbolic Computation (cs.SC)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2103.02343"
  },
  {
    "id": "arXiv:2103.02850",
    "title": "MPED: Quantifying Point Cloud Distortion based on Multiscale Potential  Energy Discrepancy",
    "abstract": "MPED: Quantifying Point Cloud Distortion based on Multiscale Potential  Energy Discrepancy",
    "descriptor": "",
    "authors": [
      "Qi Yang",
      "Yujie Zhang",
      "Siheng Chen",
      "Yiling Xu",
      "Jun Sun",
      "Zhan Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2103.02850"
  },
  {
    "id": "arXiv:2104.05892",
    "title": "CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge  Distillation",
    "abstract": "Comments: Accepted to ECCV 2022",
    "descriptor": "\nComments: Accepted to ECCV 2022\n",
    "authors": [
      "Yujin Oh",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.05892"
  },
  {
    "id": "arXiv:2104.08721",
    "title": "Embedding-Enhanced Giza++: Improving Alignment in Low- and High-  Resource Scenarios Using Embedding Space Geometry",
    "abstract": "Comments: AMTA2022 Camera Ready",
    "descriptor": "\nComments: AMTA2022 Camera Ready\n",
    "authors": [
      "Kelly Marchisio",
      "Conghao Xiong",
      "Philipp Koehn"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2104.08721"
  },
  {
    "id": "arXiv:2104.10219",
    "title": "Scalable Synthesis of Verified Controllers in Deep Reinforcement  Learning",
    "abstract": "Scalable Synthesis of Verified Controllers in Deep Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Zikang Xiong",
      "Suresh Jagannathan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.10219"
  },
  {
    "id": "arXiv:2105.01535",
    "title": "Fourier Plane-Wave Series Expansion for Holographic MIMO Communications",
    "abstract": "Comments: IEEE Transactions on Wireless Communications",
    "descriptor": "\nComments: IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Andrea Pizzo",
      "Luca Sanguinetti",
      "Thomas L. Marzetta"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2105.01535"
  },
  {
    "id": "arXiv:2105.03109",
    "title": "Laplace Matching for fast Approximate Inference in Latent Gaussian  Models",
    "abstract": "Comments: Added experiments and clarifications; Currently under review at JMLR",
    "descriptor": "\nComments: Added experiments and clarifications; Currently under review at JMLR\n",
    "authors": [
      "Marius Hobbhahn",
      "Philipp Hennig"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.03109"
  },
  {
    "id": "arXiv:2105.06194",
    "title": "Geometric Model Checking of Continuous Space",
    "abstract": "Geometric Model Checking of Continuous Space",
    "descriptor": "",
    "authors": [
      "Nick Bezhanishvili",
      "Vincenzo Ciancia",
      "David Gabelaia",
      "Gianluca Grilletti",
      "Diego Latella",
      "Mieke Massink"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2105.06194"
  },
  {
    "id": "arXiv:2105.06715",
    "title": "Maximizing Mutual Information Across Feature and Topology Views for  Learning Graph Representations",
    "abstract": "Maximizing Mutual Information Across Feature and Topology Views for  Learning Graph Representations",
    "descriptor": "",
    "authors": [
      "Xiaolong Fan",
      "Maoguo Gong",
      "Yue Wu",
      "Hao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.06715"
  },
  {
    "id": "arXiv:2106.01425",
    "title": "GAL: Gradient Assisted Learning for Decentralized Multi-Organization  Collaborations",
    "abstract": "GAL: Gradient Assisted Learning for Decentralized Multi-Organization  Collaborations",
    "descriptor": "",
    "authors": [
      "Enmao Diao",
      "Jie Ding",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01425"
  },
  {
    "id": "arXiv:2106.01432",
    "title": "SemiFL: Semi-Supervised Federated Learning for Unlabeled Clients with  Alternate Training",
    "abstract": "SemiFL: Semi-Supervised Federated Learning for Unlabeled Clients with  Alternate Training",
    "descriptor": "",
    "authors": [
      "Enmao Diao",
      "Jie Ding",
      "Vahid Tarokh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.01432"
  },
  {
    "id": "arXiv:2106.09947",
    "title": "Indicators of Attack Failure: Debugging and Improving Optimization of  Adversarial Examples",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Maura Pintor",
      "Luca Demetrio",
      "Angelo Sotgiu",
      "Ambra Demontis",
      "Nicholas Carlini",
      "Battista Biggio",
      "Fabio Roli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.09947"
  },
  {
    "id": "arXiv:2106.13045",
    "title": "Where are we in semantic concept extraction for Spoken Language  Understanding?",
    "abstract": "Comments: Accepted in the SPECOM 2021 conference",
    "descriptor": "\nComments: Accepted in the SPECOM 2021 conference\n",
    "authors": [
      "Sahar Ghannay",
      "Antoine Caubri\u00e8re",
      "Salima Mdhaffar",
      "Ga\u00eblle Laperri\u00e8re",
      "Bassam Jabaian",
      "Yannick Est\u00e8ve"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2106.13045"
  },
  {
    "id": "arXiv:2106.14075",
    "title": "Decentralized Composite Optimization in Stochastic Networks: A Dual  Averaging Approach with Linear Convergence",
    "abstract": "Comments: 17 pages, 2 figures",
    "descriptor": "\nComments: 17 pages, 2 figures\n",
    "authors": [
      "Changxin Liu",
      "Zirui Zhou",
      "Jian Pei",
      "Yong Zhang",
      "Yang Shi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2106.14075"
  },
  {
    "id": "arXiv:2106.14313",
    "title": "AniVis: Generating Animated Transitions Between Statistical Charts with  a Tree Model",
    "abstract": "Comments: Changed to 2-column format",
    "descriptor": "\nComments: Changed to 2-column format\n",
    "authors": [
      "Wenchao Li",
      "Yun Wang",
      "He Huang",
      "Weiwei Cui",
      "Haidong Zhang",
      "Huamin Qu",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2106.14313"
  },
  {
    "id": "arXiv:2106.15788",
    "title": "Exploring Localization for Self-supervised Fine-grained Contrastive  Learning",
    "abstract": "Comments: BMVC 2022 camera-ready. 15 pages (main) with 5 pages appendix",
    "descriptor": "\nComments: BMVC 2022 camera-ready. 15 pages (main) with 5 pages appendix\n",
    "authors": [
      "Di Wu",
      "Siyuan Li",
      "Zelin Zang",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2106.15788"
  },
  {
    "id": "arXiv:2107.09597",
    "title": "Positively Weighted Kernel Quadrature via Subsampling",
    "abstract": "Comments: 29 pages, NeurIPS 2022 camera-ready version",
    "descriptor": "\nComments: 29 pages, NeurIPS 2022 camera-ready version\n",
    "authors": [
      "Satoshi Hayakawa",
      "Harald Oberhauser",
      "Terry Lyons"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2107.09597"
  },
  {
    "id": "arXiv:2107.10443",
    "title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors",
    "abstract": "Comments: Outdated. Superseded by arXiv:2112.05224 and published at IEEE S&P'22 with title: \"Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures\"",
    "descriptor": "\nComments: Outdated. Superseded by arXiv:2112.05224 and published at IEEE S&P'22 with title: \"Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures\"\n",
    "authors": [
      "Eugene Bagdasaryan",
      "Vitaly Shmatikov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.10443"
  },
  {
    "id": "arXiv:2108.00046",
    "title": "On the finite element approximation of a semicoercive Stokes variational  inequality arising in glaciology",
    "abstract": "On the finite element approximation of a semicoercive Stokes variational  inequality arising in glaciology",
    "descriptor": "",
    "authors": [
      "Gonzalo G. de Diego",
      "Patrick E. Farrell",
      "Ian J. Hewitt"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2108.00046"
  },
  {
    "id": "arXiv:2108.02180",
    "title": "Ordered Attention for Coherent Visual Storytelling",
    "abstract": "Comments: 9 pages, 7 figures",
    "descriptor": "\nComments: 9 pages, 7 figures\n",
    "authors": [
      "Tom Braude",
      "Idan Schwartz",
      "Alexander Schwing",
      "Ariel Shamir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.02180"
  },
  {
    "id": "arXiv:2108.03712",
    "title": "Generalizing Dynamic Mode Decomposition: Balancing Accuracy and  Expressiveness in Koopman Approximations",
    "abstract": "Generalizing Dynamic Mode Decomposition: Balancing Accuracy and  Expressiveness in Koopman Approximations",
    "descriptor": "",
    "authors": [
      "Masih Haseli",
      "Jorge Cort\u00e9s"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2108.03712"
  },
  {
    "id": "arXiv:2108.10718",
    "title": "Convexity via Weak Distributive Laws",
    "abstract": "Comments: An extended abstract for this article is available at this https URL . This updated version takes into account the referees' comments and will appear in Logical Methods in Computer Science. arXiv admin note: substantial text overlap with arXiv:2012.14778",
    "descriptor": "\nComments: An extended abstract for this article is available at this https URL . This updated version takes into account the referees' comments and will appear in Logical Methods in Computer Science. arXiv admin note: substantial text overlap with arXiv:2012.14778\n",
    "authors": [
      "Filippo Bonchi",
      "Alessio Santamaria"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2108.10718"
  },
  {
    "id": "arXiv:2108.13767",
    "title": "Stochastic Discontinuous Galerkin Methods for Robust Deterministic  Control of Convection Diffusion Equations with Uncertain Coefficients",
    "abstract": "Comments: 41 pages, 8 figures, 7 tables",
    "descriptor": "\nComments: 41 pages, 8 figures, 7 tables\n",
    "authors": [
      "Pelin \u00c7ilo\u011flu",
      "Hamdullah Y\u00fccel"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2108.13767"
  },
  {
    "id": "arXiv:2109.08548",
    "title": "Load Balancing in Compute Clusters with Delayed Feedback",
    "abstract": "Comments: Accepted at IEEE Transactions on Computers 2022",
    "descriptor": "\nComments: Accepted at IEEE Transactions on Computers 2022\n",
    "authors": [
      "Anam Tahir",
      "Bastian Alt",
      "Amr Rizk",
      "Heinz Koeppl"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2109.08548"
  },
  {
    "id": "arXiv:2109.10040",
    "title": "Nyquist-Sampling and Degrees of Freedom of Electromagnetic Fields",
    "abstract": "Comments: IEEE Transactions on Signal Processing",
    "descriptor": "\nComments: IEEE Transactions on Signal Processing\n",
    "authors": [
      "Andrea Pizzo",
      "Andrea de Jesus Torres",
      "Luca Sanguinetti",
      "Thomas L. Marzetta"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2109.10040"
  },
  {
    "id": "arXiv:2109.12255",
    "title": "Constrained Attack-Resilient Estimation of Stochastic Cyber-Physical  Systems",
    "abstract": "Constrained Attack-Resilient Estimation of Stochastic Cyber-Physical  Systems",
    "descriptor": "",
    "authors": [
      "Wenbin Wan",
      "Hunmin Kim",
      "Naira Hovakimyan",
      "Petros Voulgaris"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2109.12255"
  },
  {
    "id": "arXiv:2109.13392",
    "title": "The Tensor Brain: A Unified Theory of Perception, Memory and Semantic  Decoding",
    "abstract": "Comments: Accepted for publication at Neural Computation",
    "descriptor": "\nComments: Accepted for publication at Neural Computation\n",
    "authors": [
      "Volker Tresp",
      "Sahand Sharifzadeh",
      "Hang Li",
      "Dario Konopatzki",
      "Yunpu Ma"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.13392"
  },
  {
    "id": "arXiv:2110.04845",
    "title": "What Makes Sentences Semantically Related: A Textual Relatedness Dataset  and Empirical Study",
    "abstract": "What Makes Sentences Semantically Related: A Textual Relatedness Dataset  and Empirical Study",
    "descriptor": "",
    "authors": [
      "Mohamed Abdalla",
      "Krishnapriya Vishnubhotla",
      "Saif M. Mohammad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.04845"
  },
  {
    "id": "arXiv:2110.05151",
    "title": "WeTS: A Benchmark for Translation Suggestion",
    "abstract": "Comments: Translation suggestion, Transformer, EMNLP2022 main conference",
    "descriptor": "\nComments: Translation suggestion, Transformer, EMNLP2022 main conference\n",
    "authors": [
      "Zhen Yang",
      "Fandong Meng",
      "Yingxue Zhang",
      "Ernan Li",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.05151"
  },
  {
    "id": "arXiv:2110.07792",
    "title": "A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text  Classification",
    "abstract": "Comments: Accepted to CoNLL 2022",
    "descriptor": "\nComments: Accepted to CoNLL 2022\n",
    "authors": [
      "Sosuke Nishikawa",
      "Ikuya Yamada",
      "Yoshimasa Tsuruoka",
      "Isao Echizen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.07792"
  },
  {
    "id": "arXiv:2110.09506",
    "title": "MEMO: Test Time Robustness via Adaptation and Augmentation",
    "abstract": "Comments: NeurIPS '22 camera ready. Code: this https URL",
    "descriptor": "\nComments: NeurIPS '22 camera ready. Code: this https URL\n",
    "authors": [
      "Marvin Zhang",
      "Sergey Levine",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.09506"
  },
  {
    "id": "arXiv:2110.10832",
    "title": "Ensemble of Averages: Improving Model Selection and Boosting Performance  in Domain Generalization",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Devansh Arpit",
      "Huan Wang",
      "Yingbo Zhou",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.10832"
  },
  {
    "id": "arXiv:2110.11929",
    "title": "Double Trouble: How to not explain a text classifier's decisions using  counterfactuals synthesized by masked language models?",
    "abstract": "Comments: 9 pages. Long paper to appear at AACL-IJCNLP 2022",
    "descriptor": "\nComments: 9 pages. Long paper to appear at AACL-IJCNLP 2022\n",
    "authors": [
      "Thang M. Pham",
      "Trung Bui",
      "Long Mai",
      "Anh Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.11929"
  },
  {
    "id": "arXiv:2110.12427",
    "title": "Image-Based CLIP-Guided Essence Transfer",
    "abstract": "Comments: To appear in ECCV'22",
    "descriptor": "\nComments: To appear in ECCV'22\n",
    "authors": [
      "Hila Chefer",
      "Sagie Benaim",
      "Roni Paiss",
      "Lior Wolf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.12427"
  },
  {
    "id": "arXiv:2110.14553",
    "title": "GenURL: A General Framework for Unsupervised Representation Learning",
    "abstract": "Comments: Tech report (revision) with 14 pages and 7 figures",
    "descriptor": "\nComments: Tech report (revision) with 14 pages and 7 figures\n",
    "authors": [
      "Siyuan Li",
      "Zelin Zang",
      "Di Wu",
      "Zhiyuan Chen",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14553"
  },
  {
    "id": "arXiv:2110.15843",
    "title": "Adaptive Discretization in Online Reinforcement Learning",
    "abstract": "Comments: 77 pages, 7 figures. arXiv admin note: text overlap with arXiv:2007.00717",
    "descriptor": "\nComments: 77 pages, 7 figures. arXiv admin note: text overlap with arXiv:2007.00717\n",
    "authors": [
      "Sean R. Sinclair",
      "Siddhartha Banerjee",
      "Christina Lee Yu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.15843"
  },
  {
    "id": "arXiv:2111.04063",
    "title": "LiMuSE: Lightweight Multi-modal Speaker Extraction",
    "abstract": "Comments: Accepted to IEEE SLT 2022",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Qinghua Liu",
      "Yating Huang",
      "Yunzhe Hao",
      "Jiaming Xu",
      "Bo Xu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2111.04063"
  },
  {
    "id": "arXiv:2111.07495",
    "title": "Distribution-Free Model for Community Detection",
    "abstract": "Comments: 21 pages, 9 figures, 1 table, comments are welcome",
    "descriptor": "\nComments: 21 pages, 9 figures, 1 table, comments are welcome\n",
    "authors": [
      "Huan Qing"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Machine Learning (cs.LG)",
      "Physics and Society (physics.soc-ph)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.07495"
  },
  {
    "id": "arXiv:2111.07671",
    "title": "NeuralPDE: Modelling Dynamical Systems from Data",
    "abstract": "NeuralPDE: Modelling Dynamical Systems from Data",
    "descriptor": "",
    "authors": [
      "Andrzej Dulny",
      "Andreas Hotho",
      "Anna Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.07671"
  },
  {
    "id": "arXiv:2111.10557",
    "title": "HybNet: A Hybrid Deep Learning -- Matched Filter Approach for IoT Signal  Detection",
    "abstract": "HybNet: A Hybrid Deep Learning -- Matched Filter Approach for IoT Signal  Detection",
    "descriptor": "",
    "authors": [
      "Kosta Dakic",
      "Bassel Al Homssi",
      "Margaret Lech",
      "Akram Al-Hourani"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2111.10557"
  },
  {
    "id": "arXiv:2111.11546",
    "title": "Lightweight Transformer Backbone for Medical Object Detection",
    "abstract": "Lightweight Transformer Backbone for Medical Object Detection",
    "descriptor": "",
    "authors": [
      "Yifan Zhang",
      "Haoyu Dong",
      "Nicholas Konz",
      "Hanxue Gu",
      "Maciej A. Mazurowski"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.11546"
  },
  {
    "id": "arXiv:2111.14552",
    "title": "Robust On-Policy Sampling for Data-Efficient Policy Evaluation in  Reinforcement Learning",
    "abstract": "Comments: Published in 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Published in 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Rujie Zhong",
      "Duohan Zhang",
      "Lukas Sch\u00e4fer",
      "Stefano V. Albrecht",
      "Josiah P. Hanna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.14552"
  },
  {
    "id": "arXiv:2111.15323",
    "title": "The signature and cusp geometry of hyperbolic knots",
    "abstract": "Comments: 28 pages, 13 figures. v3: revised final version. Accepted by Geometry & Topology",
    "descriptor": "\nComments: 28 pages, 13 figures. v3: revised final version. Accepted by Geometry & Topology\n",
    "authors": [
      "Alex Davies",
      "Andr\u00e1s Juh\u00e1sz",
      "Marc Lackenby",
      "Nenad Tomasev"
    ],
    "subjectives": [
      "Geometric Topology (math.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.15323"
  },
  {
    "id": "arXiv:2112.02721",
    "title": "NL-Augmenter: A Framework for Task-Sensitive Natural Language  Augmentation",
    "abstract": "Comments: 39 pages, repository at this https URL",
    "descriptor": "\nComments: 39 pages, repository at this https URL\n",
    "authors": [
      "Kaustubh D. Dhole",
      "Varun Gangal",
      "Sebastian Gehrmann",
      "Aadesh Gupta",
      "Zhenhao Li",
      "Saad Mahamood",
      "Abinaya Mahendiran",
      "Simon Mille",
      "Ashish Shrivastava",
      "Samson Tan",
      "Tongshuang Wu",
      "Jascha Sohl-Dickstein",
      "Jinho D. Choi",
      "Eduard Hovy",
      "Ondrej Dusek",
      "Sebastian Ruder",
      "Sajant Anand",
      "Nagender Aneja",
      "Rabin Banjade",
      "Lisa Barthe",
      "Hanna Behnke",
      "Ian Berlot-Attwell",
      "Connor Boyle",
      "Caroline Brun",
      "Marco Antonio Sobrevilla Cabezudo",
      "Samuel Cahyawijaya",
      "Emile Chapuis",
      "Wanxiang Che",
      "Mukund Choudhary",
      "Christian Clauss",
      "Pierre Colombo",
      "Filip Cornell",
      "Gautier Dagan",
      "Mayukh Das",
      "Tanay Dixit",
      "Thomas Dopierre",
      "Paul-Alexis Dray",
      "Suchitra Dubey",
      "Tatiana Ekeinhor",
      "Marco Di Giovanni",
      "Tanya Goyal",
      "Rishabh Gupta",
      "Rishabh Gupta",
      "Louanes Hamla",
      "Sang Han",
      "Fabrice Harel-Canada",
      "Antoine Honore",
      "Ishan Jindal",
      "Przemyslaw K. Joniak",
      "Denis Kleyko"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.02721"
  },
  {
    "id": "arXiv:2112.03377",
    "title": "RafterNet: Probabilistic predictions in multi-response regression",
    "abstract": "RafterNet: Probabilistic predictions in multi-response regression",
    "descriptor": "",
    "authors": [
      "Marius Hofert",
      "Avinash Prasad",
      "Mu Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2112.03377"
  },
  {
    "id": "arXiv:2112.03605",
    "title": "Some Basic Techniques allowing Petri Net Synthesis: Complexity and  Algorithmic Issues",
    "abstract": "Some Basic Techniques allowing Petri Net Synthesis: Complexity and  Algorithmic Issues",
    "descriptor": "",
    "authors": [
      "Raymond Devillers",
      "Ronny Tredup"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.03605"
  },
  {
    "id": "arXiv:2112.06864",
    "title": "Frontiers in Collective Intelligence: A Workshop Report",
    "abstract": "Comments: acknowledgments added",
    "descriptor": "\nComments: acknowledgments added\n",
    "authors": [
      "Tyler Millhouse",
      "Melanie Moses",
      "Melanie Mitchell"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2112.06864"
  },
  {
    "id": "arXiv:2112.07337",
    "title": "Multi-Instance Training for Question Answering Across Table and Linked  Text",
    "abstract": "Multi-Instance Training for Question Answering Across Table and Linked  Text",
    "descriptor": "",
    "authors": [
      "Vishwajeet Kumar",
      "Saneem Chemmengath",
      "Yash Gupta",
      "Jaydeep Sen",
      "Samarth Bharadwaj",
      "Soumen Chakrabarti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.07337"
  },
  {
    "id": "arXiv:2112.12175",
    "title": "Recur, Attend or Convolve? On Whether Temporal Modeling Matters for  Cross-Domain Robustness in Action Recognition",
    "abstract": "Recur, Attend or Convolve? On Whether Temporal Modeling Matters for  Cross-Domain Robustness in Action Recognition",
    "descriptor": "",
    "authors": [
      "Sofia Broom\u00e9",
      "Ernest Pokropek",
      "Boyu Li",
      "Hedvig Kjellstr\u00f6m"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.12175"
  },
  {
    "id": "arXiv:2112.13339",
    "title": "Quasi-Taylor Samplers for Diffusion Generative Models based on Ideal  Derivatives",
    "abstract": "Comments: Major update from 2112.13339v1. 47 pages, 24 figures",
    "descriptor": "\nComments: Major update from 2112.13339v1. 47 pages, 24 figures\n",
    "authors": [
      "Hideyuki Tachibana",
      "Mocho Go",
      "Muneyoshi Inahara",
      "Yotaro Katayama",
      "Yotaro Watanabe"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2112.13339"
  },
  {
    "id": "arXiv:2201.03182",
    "title": "Non-Asymptotic Guarantees for Robust Statistical Learning under Infinite  Variance Assumption",
    "abstract": "Comments: 44 pages",
    "descriptor": "\nComments: 44 pages\n",
    "authors": [
      "Lihu Xu",
      "Fang Yao",
      "Qiuran Yao",
      "Huiming Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2201.03182"
  },
  {
    "id": "arXiv:2201.03335",
    "title": "DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge  Base Population",
    "abstract": "Comments: Accepted by EMNLP 2022 System Demonstrations and the project website is this http URL",
    "descriptor": "\nComments: Accepted by EMNLP 2022 System Demonstrations and the project website is this http URL\n",
    "authors": [
      "Ningyu Zhang",
      "Xin Xu",
      "Liankuan Tao",
      "Haiyang Yu",
      "Hongbin Ye",
      "Shuofei Qiao",
      "Xin Xie",
      "Xiang Chen",
      "Zhoubo Li",
      "Lei Li",
      "Xiaozhuan Liang",
      "Yunzhi Yao",
      "Shumin Deng",
      "Peng Wang",
      "Wen Zhang",
      "Zhenru Zhang",
      "Chuanqi Tan",
      "Qiang Chen",
      "Feiyu Xiong",
      "Fei Huang",
      "Guozhou Zheng",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.03335"
  },
  {
    "id": "arXiv:2201.03664",
    "title": "Structural measures of similarity and complementarity in complex  networks",
    "abstract": "Comments: Authors' copy of the published version",
    "descriptor": "\nComments: Authors' copy of the published version\n",
    "authors": [
      "Szymon Talaga",
      "Andrzej Nowak"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2201.03664"
  },
  {
    "id": "arXiv:2201.07637",
    "title": "On off-diagonal ordered Ramsey numbers of nested matchings",
    "abstract": "Comments: 17 pages, 7 figures, minor revisions",
    "descriptor": "\nComments: 17 pages, 7 figures, minor revisions\n",
    "authors": [
      "Martin Balko",
      "Marian Poljak"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2201.07637"
  },
  {
    "id": "arXiv:2201.07984",
    "title": "AstBERT: Enabling Language Model for Financial Code Understanding with  Abstract Syntax Trees",
    "abstract": "AstBERT: Enabling Language Model for Financial Code Understanding with  Abstract Syntax Trees",
    "descriptor": "",
    "authors": [
      "Rong Liang",
      "Tiehua Zhang",
      "Yujie Lu",
      "Yuze Liu",
      "Zhen Huang",
      "Xin Chen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.07984"
  },
  {
    "id": "arXiv:2201.09104",
    "title": "Understanding the Effects of Second-Order Approximations in Natural  Policy Gradient Reinforcement Learning",
    "abstract": "Understanding the Effects of Second-Order Approximations in Natural  Policy Gradient Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Brennan Gebotys",
      "Alexander Wong",
      "David A. Clausi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.09104"
  },
  {
    "id": "arXiv:2201.11903",
    "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
    "abstract": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
    "descriptor": "",
    "authors": [
      "Jason Wei",
      "Xuezhi Wang",
      "Dale Schuurmans",
      "Maarten Bosma",
      "Brian Ichter",
      "Fei Xia",
      "Ed Chi",
      "Quoc Le",
      "Denny Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.11903"
  },
  {
    "id": "arXiv:2201.12427",
    "title": "Towards Safe Reinforcement Learning with a Safety Editor Policy",
    "abstract": "Comments: NeurIPS 2022 camera-ready version",
    "descriptor": "\nComments: NeurIPS 2022 camera-ready version\n",
    "authors": [
      "Haonan Yu",
      "Wei Xu",
      "Haichao Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2201.12427"
  },
  {
    "id": "arXiv:2202.00063",
    "title": "Efficient Reinforcement Learning in Block MDPs: A Model-free  Representation Learning Approach",
    "abstract": "Efficient Reinforcement Learning in Block MDPs: A Model-free  Representation Learning Approach",
    "descriptor": "",
    "authors": [
      "Xuezhou Zhang",
      "Yuda Song",
      "Masatoshi Uehara",
      "Mengdi Wang",
      "Alekh Agarwal",
      "Wen Sun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.00063"
  },
  {
    "id": "arXiv:2202.01003",
    "title": "Thermal and Visual Tracking of Photovoltaic Plants for Autonomous UAV  inspection",
    "abstract": "Comments: 17 pages, 34 figures",
    "descriptor": "\nComments: 17 pages, 34 figures\n",
    "authors": [
      "Luca Morando",
      "Carmine Tommaso Recchiuto",
      "Jacopo Call\u00e0",
      "Paolo Scuteri",
      "Antonio Sgorbissa"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2202.01003"
  },
  {
    "id": "arXiv:2202.01116",
    "title": "An Optimal Transport Perspective on Unpaired Image Super-Resolution",
    "abstract": "An Optimal Transport Perspective on Unpaired Image Super-Resolution",
    "descriptor": "",
    "authors": [
      "Milena Gazdieva",
      "Litu Rout",
      "Alexander Korotin",
      "Andrey Kravchenko",
      "Alexander Filippov",
      "Evgeny Burnaev"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.01116"
  },
  {
    "id": "arXiv:2202.01727",
    "title": "Skeleton-Based Action Segmentation with Multi-Stage Spatial-Temporal  Graph Convolutional Neural Networks",
    "abstract": "Skeleton-Based Action Segmentation with Multi-Stage Spatial-Temporal  Graph Convolutional Neural Networks",
    "descriptor": "",
    "authors": [
      "Benjamin Filtjens",
      "Bart Vanrumste",
      "Peter Slaets"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.01727"
  },
  {
    "id": "arXiv:2202.05441",
    "title": "Learning Causally Invariant Representations for Out-of-Distribution  Generalization on Graphs",
    "abstract": "Comments: NeurIPS2022, 46 pages, 72 figures",
    "descriptor": "\nComments: NeurIPS2022, 46 pages, 72 figures\n",
    "authors": [
      "Yongqiang Chen",
      "Yonggang Zhang",
      "Yatao Bian",
      "Han Yang",
      "Kaili Ma",
      "Binghui Xie",
      "Tongliang Liu",
      "Bo Han",
      "James Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05441"
  },
  {
    "id": "arXiv:2202.08465",
    "title": "End-to-End Training of Both Translation Models in the Back-Translation  Framework",
    "abstract": "End-to-End Training of Both Translation Models in the Back-Translation  Framework",
    "descriptor": "",
    "authors": [
      "DongNyeong Heo",
      "Heeyoul Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.08465"
  },
  {
    "id": "arXiv:2202.10201",
    "title": "OG-SGG: Ontology-Guided Scene Graph Generation. A Case Study in Transfer  Learning for Telepresence Robotics",
    "abstract": "Comments: 19 pages; added additional experimental sections, corrected minor issues, improved explanations",
    "descriptor": "\nComments: 19 pages; added additional experimental sections, corrected minor issues, improved explanations\n",
    "authors": [
      "Fernando Amodeo",
      "Fernando Caballero",
      "Natalia D\u00edaz-Rodr\u00edguez",
      "Luis Merino"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.10201"
  },
  {
    "id": "arXiv:2202.11860",
    "title": "Robust Transmission Design for RIS-assisted Secure Multiuser  Communication Systems in the Presence of Hardware Impairments",
    "abstract": "Comments: Revised version in IEEE TWC. Keywords: Reconfigurable intelligent surface (RIS), intelligent reflecting surface (IRS)",
    "descriptor": "\nComments: Revised version in IEEE TWC. Keywords: Reconfigurable intelligent surface (RIS), intelligent reflecting surface (IRS)\n",
    "authors": [
      "Zhangjie Peng",
      "Ruisong Weng",
      "Cunhua Pan",
      "Gui Zhou",
      "Marco Di Renzo",
      "A. Lee Swindlehurst"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2202.11860"
  },
  {
    "id": "arXiv:2202.13924",
    "title": "Bounds on quantum evolution complexity via lattice cryptography",
    "abstract": "Comments: v3: minor changes, figure and references added; The MATLAB code and data to reproduce numerical results are available at this https URL",
    "descriptor": "\nComments: v3: minor changes, figure and references added; The MATLAB code and data to reproduce numerical results are available at this https URL\n",
    "authors": [
      "Ben Craps",
      "Marine De Clerck",
      "Oleg Evnin",
      "Philip Hacker",
      "Maxim Pavlov"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)",
      "High Energy Physics - Theory (hep-th)",
      "Optimization and Control (math.OC)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2202.13924"
  },
  {
    "id": "arXiv:2203.00328",
    "title": "BERT-LID: Leveraging BERT to Improve Spoken Language Identification",
    "abstract": "Comments: accepted by ISCSLP 2022",
    "descriptor": "\nComments: accepted by ISCSLP 2022\n",
    "authors": [
      "Yuting Nie",
      "Junhong Zhao",
      "Wei-Qiang Zhang",
      "Jinfeng Bai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.00328"
  },
  {
    "id": "arXiv:2203.01212",
    "title": "A Quantitative Geometric Approach to Neural-Network Smoothness",
    "abstract": "Comments: Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Thirty-Sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Zi Wang",
      "Gautam Prakriya",
      "Somesh Jha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.01212"
  },
  {
    "id": "arXiv:2203.03265",
    "title": "Efficient Policy Generation in Multi-Agent Systems via Hypergraph Neural  Network",
    "abstract": "Comments: 12 pages, 6 figures",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Bin Zhang",
      "Yunpeng Bai",
      "Zhiwei Xu",
      "Dapeng Li",
      "Guoliang Fan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2203.03265"
  },
  {
    "id": "arXiv:2203.05818",
    "title": "ZIN: When and How to Learn Invariance Without Environment Partition?",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Yong Lin",
      "Shengyu Zhu",
      "Lu Tan",
      "Peng Cui"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.05818"
  },
  {
    "id": "arXiv:2203.08383",
    "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An  Iterative Prompting Approach",
    "abstract": "Comments: Accepted to EMNLP-22. Code will be available at this https URL",
    "descriptor": "\nComments: Accepted to EMNLP-22. Code will be available at this https URL\n",
    "authors": [
      "Boshi Wang",
      "Xiang Deng",
      "Huan Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08383"
  },
  {
    "id": "arXiv:2203.09095",
    "title": "Automating Code Review Activities by Large-Scale Pre-training",
    "abstract": "Comments: ESEC/FSE 2022, camera-ready version",
    "descriptor": "\nComments: ESEC/FSE 2022, camera-ready version\n",
    "authors": [
      "Zhiyu Li",
      "Shuai Lu",
      "Daya Guo",
      "Nan Duan",
      "Shailesh Jannu",
      "Grant Jenks",
      "Deep Majumder",
      "Jared Green",
      "Alexey Svyatkovskiy",
      "Shengyu Fu",
      "Neel Sundaresan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.09095"
  },
  {
    "id": "arXiv:2203.09268",
    "title": "Progressive Subsampling for Oversampled Data - Application to  Quantitative MRI",
    "abstract": "Comments: Accepted In: Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022",
    "descriptor": "\nComments: Accepted In: Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022\n",
    "authors": [
      "Stefano B. Blumberg",
      "Hongxiang Lin",
      "Francesco Grussu",
      "Yukun Zhou",
      "Matteo Figini",
      "Daniel C. Alexander"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2203.09268"
  },
  {
    "id": "arXiv:2203.12513",
    "title": "Sensing Theorems for Unsupervised Learning in Linear Inverse Problems",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2201.12151",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2201.12151\n",
    "authors": [
      "Juli\u00e1n Tachella",
      "Dongdong Chen",
      "Mike Davies"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2203.12513"
  },
  {
    "id": "arXiv:2203.12830",
    "title": "TIGRIS: An Informed Sampling-based Algorithm for Informative Path  Planning",
    "abstract": "Comments: 7 pages, 5 figures, IROS 2022",
    "descriptor": "\nComments: 7 pages, 5 figures, IROS 2022\n",
    "authors": [
      "Brady Moon",
      "Satrajit Chatterjee",
      "Sebastian Scherer"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.12830"
  },
  {
    "id": "arXiv:2203.13789",
    "title": "FLUTE: A Scalable, Extensible Framework for High-Performance Federated  Learning Simulations",
    "abstract": "Comments: 14 Pages, 3 Figures, 11 Tables",
    "descriptor": "\nComments: 14 Pages, 3 Figures, 11 Tables\n",
    "authors": [
      "Dimitrios Dimitriadis",
      "Mirian Hipolito Garcia",
      "Daniel Madrigal Diaz",
      "Andre Manoel",
      "Robert Sim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.13789"
  },
  {
    "id": "arXiv:2203.15544",
    "title": "Graph Neural Networks are Dynamic Programmers",
    "abstract": "Comments: To appear at NeurIPS 2022. 18 pages, 4 figures",
    "descriptor": "\nComments: To appear at NeurIPS 2022. 18 pages, 4 figures\n",
    "authors": [
      "Andrew Dudzik",
      "Petar Veli\u010dkovi\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Data Structures and Algorithms (cs.DS)",
      "Category Theory (math.CT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.15544"
  },
  {
    "id": "arXiv:2203.16437",
    "title": "Weakly supervised causal representation learning",
    "abstract": "Comments: Published at NeurIPS 2022. v3: Experiments with higher-dimensional data and larger graphs, improved writing, and added references; matches camera-ready version",
    "descriptor": "\nComments: Published at NeurIPS 2022. v3: Experiments with higher-dimensional data and larger graphs, improved writing, and added references; matches camera-ready version\n",
    "authors": [
      "Johann Brehmer",
      "Pim de Haan",
      "Phillip Lippe",
      "Taco Cohen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.16437"
  },
  {
    "id": "arXiv:2204.02227",
    "title": "SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution",
    "abstract": "Comments: WACV 2023",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Shwai He",
      "Chenbo Jiang",
      "Daize Dong",
      "Liang Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.02227"
  },
  {
    "id": "arXiv:2204.03416",
    "title": "Inverse Cauchy problems: revisit and a new approach",
    "abstract": "Inverse Cauchy problems: revisit and a new approach",
    "descriptor": "",
    "authors": [
      "Rongfang Gong",
      "Min Wang",
      "Qin Huang",
      "Ye Zhang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.03416"
  },
  {
    "id": "arXiv:2204.04301",
    "title": "Learning Generalized Policy Automata for Relational Stochastic Shortest  Path Problems",
    "abstract": "Learning Generalized Policy Automata for Relational Stochastic Shortest  Path Problems",
    "descriptor": "",
    "authors": [
      "Rushang Karia",
      "Rashmeet Kaur Nayyar",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.04301"
  },
  {
    "id": "arXiv:2204.05002",
    "title": "Fast evaluation of B-spline functions and rendering of multiple B-spline  curves using linear-time algorithm for computing the Bernstein-B\u00e9zier  coefficients of B-spline functions",
    "abstract": "Fast evaluation of B-spline functions and rendering of multiple B-spline  curves using linear-time algorithm for computing the Bernstein-B\u00e9zier  coefficients of B-spline functions",
    "descriptor": "",
    "authors": [
      "Filip Chudy",
      "Pawe\u0142 Wo\u017any"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2204.05002"
  },
  {
    "id": "arXiv:2204.05232",
    "title": "Survey of Aspect-based Sentiment Analysis Datasets",
    "abstract": "Comments: Updated with new datasets and some introduction",
    "descriptor": "\nComments: Updated with new datasets and some introduction\n",
    "authors": [
      "Siva Uday Sampreeth Chebolu",
      "Franck Dernoncourt",
      "Nedim Lipka",
      "Thamar Solorio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.05232"
  },
  {
    "id": "arXiv:2204.05573",
    "title": "Assessment of convolutional recurrent autoencoder network for learning  wave propagation",
    "abstract": "Assessment of convolutional recurrent autoencoder network for learning  wave propagation",
    "descriptor": "",
    "authors": [
      "Wrik Mallik",
      "Rajeev K. Jaiman",
      "Jasmin Jelovica"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.05573"
  },
  {
    "id": "arXiv:2204.09672",
    "title": "TropeTwist: Trope-based Narrative Structure Generation",
    "abstract": "Comments: 8 pages, Accepted and to appear in Proceedings of the 13th Workshop on Procedural Content Generation, at the Foundations of Digital Games (FDG), 2022",
    "descriptor": "\nComments: 8 pages, Accepted and to appear in Proceedings of the 13th Workshop on Procedural Content Generation, at the Foundations of Digital Games (FDG), 2022\n",
    "authors": [
      "Alberto Alvarez",
      "Jose Font"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2204.09672"
  },
  {
    "id": "arXiv:2204.12446",
    "title": "Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for  Full-Batch GD",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Konstantinos E. Nikolakakis",
      "Farzin Haddadpour",
      "Amin Karbasi",
      "Dionysios S. Kalogerias"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.12446"
  },
  {
    "id": "arXiv:2204.12581",
    "title": "RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Marc Rigter",
      "Bruno Lacerda",
      "Nick Hawes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.12581"
  },
  {
    "id": "arXiv:2205.01733",
    "title": "Application of belief functions to medical image segmentation: A review",
    "abstract": "Comments: Preprint submitted to Information fusion",
    "descriptor": "\nComments: Preprint submitted to Information fusion\n",
    "authors": [
      "Ling Huang",
      "Su Ruan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.01733"
  },
  {
    "id": "arXiv:2205.03635",
    "title": "Ultrafast image categorization in vivo and in silico",
    "abstract": "Ultrafast image categorization in vivo and in silico",
    "descriptor": "",
    "authors": [
      "Jean-Nicolas J\u00e9r\u00e9mie",
      "Laurent U Perrinet"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.03635"
  },
  {
    "id": "arXiv:2205.04602",
    "title": "A Unified Model for Reverse Dictionary and Definition Modelling",
    "abstract": "Comments: AACL-IJCNLP 2022",
    "descriptor": "\nComments: AACL-IJCNLP 2022\n",
    "authors": [
      "Pinzhen Chen",
      "Zheng Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.04602"
  },
  {
    "id": "arXiv:2205.04934",
    "title": "The spatial computer: A model for energy-efficient parallel computation",
    "abstract": "The spatial computer: A model for energy-efficient parallel computation",
    "descriptor": "",
    "authors": [
      "Lukas Gianinazzi",
      "Tal Ben-Nun",
      "Maciej Besta",
      "Saleh Ashkboos",
      "Yves Baumann",
      "Piotr Luczynski",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.04934"
  },
  {
    "id": "arXiv:2205.05055",
    "title": "Data Distributional Properties Drive Emergent In-Context Learning in  Transformers",
    "abstract": "Comments: Code is available at: this https URL",
    "descriptor": "\nComments: Code is available at: this https URL\n",
    "authors": [
      "Stephanie C.Y. Chan",
      "Adam Santoro",
      "Andrew K. Lampinen",
      "Jane X. Wang",
      "Aaditya Singh",
      "Pierre H. Richemond",
      "Jay McClelland",
      "Felix Hill"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.05055"
  },
  {
    "id": "arXiv:2205.05429",
    "title": "Learning a Better Control Barrier Function",
    "abstract": "Comments: Accepted at 61st IEEE Conference on Decision and Control (CDC) 2022",
    "descriptor": "\nComments: Accepted at 61st IEEE Conference on Decision and Control (CDC) 2022\n",
    "authors": [
      "Bolun Dai",
      "Prashanth Krishnamurthy",
      "Farshad Khorrami"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2205.05429"
  },
  {
    "id": "arXiv:2205.08316",
    "title": "Self-Supervised Learning of Multi-Object Keypoints for Robotic  Manipulation",
    "abstract": "Comments: Presented at IEEE ICRA 2022 Workshop 'Reinforcement Learning for Contact-Rich Manipulation'",
    "descriptor": "\nComments: Presented at IEEE ICRA 2022 Workshop 'Reinforcement Learning for Contact-Rich Manipulation'\n",
    "authors": [
      "Jan Ole von Hartz",
      "Eugenio Chisari",
      "Tim Welschehold",
      "Abhinav Valada"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.08316"
  },
  {
    "id": "arXiv:2205.09438",
    "title": "Gold-standard solutions to the Schr\u00f6dinger equation using deep  learning: How much physics do we need?",
    "abstract": "Comments: 10 pages + apppendix, 7 figures; V2: minor corrections to citations and reference energies for F, Ne, H2O; V3: final version for NEURIPS",
    "descriptor": "\nComments: 10 pages + apppendix, 7 figures; V2: minor corrections to citations and reference energies for F, Ne, H2O; V3: final version for NEURIPS\n",
    "authors": [
      "Leon Gerard",
      "Michael Scherbela",
      "Philipp Marquetand",
      "Philipp Grohs"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2205.09438"
  },
  {
    "id": "arXiv:2205.09821",
    "title": "Unsupervised Learning of Depth, Camera Pose and Optical Flow from  Monocular Video",
    "abstract": "Comments: 8 pages, 2 figures. arXiv admin note: text overlap with arXiv:1803.02276 by other authors",
    "descriptor": "\nComments: 8 pages, 2 figures. arXiv admin note: text overlap with arXiv:1803.02276 by other authors\n",
    "authors": [
      "Dipan Mandal",
      "Abhilash Jain"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09821"
  },
  {
    "id": "arXiv:2205.09949",
    "title": "Clustering as Attention: Unified Image Segmentation with Hierarchical  Clustering",
    "abstract": "Comments: Code: this https URL",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Teppei Suzuki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09949"
  },
  {
    "id": "arXiv:2205.09990",
    "title": "Set-based Meta-Interpolation for Few-Task Meta-Learning",
    "abstract": "Comments: First two authors contributed equally. Name order decided by a coin toss",
    "descriptor": "\nComments: First two authors contributed equally. Name order decided by a coin toss\n",
    "authors": [
      "Seanie Lee",
      "Bruno Andreis",
      "Kenji Kawaguchi",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.09990"
  },
  {
    "id": "arXiv:2205.10186",
    "title": "Bayesian Active Learning with Fully Bayesian Gaussian Processes",
    "abstract": "Bayesian Active Learning with Fully Bayesian Gaussian Processes",
    "descriptor": "",
    "authors": [
      "Christoffer Riis",
      "Francisco Antunes",
      "Frederik Boe H\u00fcttel",
      "Carlos Lima Azevedo",
      "Francisco C\u00e2mara Pereira"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10186"
  },
  {
    "id": "arXiv:2205.11366",
    "title": "Statistical inference as Green's functions",
    "abstract": "Comments: 21 pages, 1 figure",
    "descriptor": "\nComments: 21 pages, 1 figure\n",
    "authors": [
      "Hyun Keun Lee",
      "Chulan Kwon",
      "Yong Woon Kim"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11366"
  },
  {
    "id": "arXiv:2205.11412",
    "title": "Instance-Based Uncertainty Estimation for Gradient-Boosted Regression  Trees",
    "abstract": "Comments: 26 pages, 7 figures, 3 tables, and 3 algorithms. Accepted at NeurIPS 2022",
    "descriptor": "\nComments: 26 pages, 7 figures, 3 tables, and 3 algorithms. Accepted at NeurIPS 2022\n",
    "authors": [
      "Jonathan Brophy",
      "Daniel Lowd"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11412"
  },
  {
    "id": "arXiv:2205.11440",
    "title": "Federated Distillation based Indoor Localization for IoT Networks",
    "abstract": "Federated Distillation based Indoor Localization for IoT Networks",
    "descriptor": "",
    "authors": [
      "Yaya Etiabi",
      "Marwa Chafii",
      "El Mehdi Amhoud"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.11440"
  },
  {
    "id": "arXiv:2205.11658",
    "title": "Penguins Don't Fly: Reasoning about Generics through Instantiations and  Exceptions",
    "abstract": "Penguins Don't Fly: Reasoning about Generics through Instantiations and  Exceptions",
    "descriptor": "",
    "authors": [
      "Emily Allaway",
      "Jena D. Hwang",
      "Chandra Bhagavatula",
      "Kathleen McKeown",
      "Doug Downey",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11658"
  },
  {
    "id": "arXiv:2205.11894",
    "title": "Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs",
    "abstract": "Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs",
    "descriptor": "",
    "authors": [
      "\u00c7a\u011fatay Y\u0131ld\u0131z",
      "Melih Kandemir",
      "Barbara Rakitsch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11894"
  },
  {
    "id": "arXiv:2205.11966",
    "title": "Benchmark Data and Evaluation Framework for Intent Discovery Around  COVID-19 Vaccine Hesitancy",
    "abstract": "Benchmark Data and Evaluation Framework for Intent Discovery Around  COVID-19 Vaccine Hesitancy",
    "descriptor": "",
    "authors": [
      "Shai Gretz",
      "Assaf Toledo",
      "Roni Friedman",
      "Dan Lahav",
      "Rose Weeks",
      "Naor Bar-Zeev",
      "Jo\u00e3o Sedoc",
      "Pooja Sangha",
      "Yoav Katz",
      "Noam Slonim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11966"
  },
  {
    "id": "arXiv:2205.12268",
    "title": "Wavelet Feature Maps Compression for Image-to-Image CNNs",
    "abstract": "Wavelet Feature Maps Compression for Image-to-Image CNNs",
    "descriptor": "",
    "authors": [
      "Shahaf E. Finder",
      "Yair Zohav",
      "Maor Ashkenazi",
      "Eran Treister"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2205.12268"
  },
  {
    "id": "arXiv:2205.12393",
    "title": "Fine-tuned Language Models are Continual Learners",
    "abstract": "Fine-tuned Language Models are Continual Learners",
    "descriptor": "",
    "authors": [
      "Thomas Scialom",
      "Tuhin Chakrabarty",
      "Smaranda Muresan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12393"
  },
  {
    "id": "arXiv:2205.13331",
    "title": "TransBoost: Improving the Best ImageNet Performance using Deep  Transduction",
    "abstract": "TransBoost: Improving the Best ImageNet Performance using Deep  Transduction",
    "descriptor": "",
    "authors": [
      "Omer Belhasin",
      "Guy Bar-Shalom",
      "Ran El-Yaniv"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13331"
  },
  {
    "id": "arXiv:2205.13799",
    "title": "Generalization Bounds for Gradient Methods via Discrete and Continuous  Prior",
    "abstract": "Comments: Published in NeurIPS 2022",
    "descriptor": "\nComments: Published in NeurIPS 2022\n",
    "authors": [
      "Xuanyuan Luo",
      "Luo Bei",
      "Jian Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13799"
  },
  {
    "id": "arXiv:2205.14704",
    "title": "Decoupling Knowledge from Memorization: Retrieval-augmented Prompt  Learning",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Xiang Chen",
      "Lei Li",
      "Ningyu Zhang",
      "Xiaozhuan Liang",
      "Shumin Deng",
      "Chuanqi Tan",
      "Fei Huang",
      "Luo Si",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14704"
  },
  {
    "id": "arXiv:2205.14870",
    "title": "Compressible-composable NeRF via Rank-residual Decomposition",
    "abstract": "Comments: NeurIPS 2022 camera-ready version",
    "descriptor": "\nComments: NeurIPS 2022 camera-ready version\n",
    "authors": [
      "Jiaxiang Tang",
      "Xiaokang Chen",
      "Jingbo Wang",
      "Gang Zeng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.14870"
  },
  {
    "id": "arXiv:2205.15209",
    "title": "Flowification: Everything is a Normalizing Flow",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "B\u00e1lint M\u00e1t\u00e9",
      "Samuel Klein",
      "Tobias Golling",
      "Fran\u00e7ois Fleuret"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15209"
  },
  {
    "id": "arXiv:2206.00364",
    "title": "Elucidating the Design Space of Diffusion-Based Generative Models",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Tero Karras",
      "Miika Aittala",
      "Timo Aila",
      "Samuli Laine"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00364"
  },
  {
    "id": "arXiv:2206.00707",
    "title": "Collaborative Learning of Discrete Distributions under Heterogeneity and  Communication Constraints",
    "abstract": "Collaborative Learning of Discrete Distributions under Heterogeneity and  Communication Constraints",
    "descriptor": "",
    "authors": [
      "Xinmeng Huang",
      "Donghwan Lee",
      "Edgar Dobriban",
      "Hamed Hassani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00707"
  },
  {
    "id": "arXiv:2206.01079",
    "title": "When does return-conditioned supervised learning work for offline  reinforcement learning?",
    "abstract": "When does return-conditioned supervised learning work for offline  reinforcement learning?",
    "descriptor": "",
    "authors": [
      "David Brandfonbrener",
      "Alberto Bietti",
      "Jacob Buckman",
      "Romain Laroche",
      "Joan Bruna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01079"
  },
  {
    "id": "arXiv:2206.01191",
    "title": "EfficientFormer: Vision Transformers at MobileNet Speed",
    "abstract": "EfficientFormer: Vision Transformers at MobileNet Speed",
    "descriptor": "",
    "authors": [
      "Yanyu Li",
      "Geng Yuan",
      "Yang Wen",
      "Ju Hu",
      "Georgios Evangelidis",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Jian Ren"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01191"
  },
  {
    "id": "arXiv:2206.01606",
    "title": "Excess risk analysis for epistemic uncertainty with application to  variational inference",
    "abstract": "Excess risk analysis for epistemic uncertainty with application to  variational inference",
    "descriptor": "",
    "authors": [
      "Futoshi Futami",
      "Tomoharu Iwata",
      "Naonori Ueda",
      "Issei Sato",
      "Masashi Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.01606"
  },
  {
    "id": "arXiv:2206.02016",
    "title": "Is $L^2$ Physics-Informed Loss Always Suitable for Training  Physics-Informed Neural Network?",
    "abstract": "Is $L^2$ Physics-Informed Loss Always Suitable for Training  Physics-Informed Neural Network?",
    "descriptor": "",
    "authors": [
      "Chuwei Wang",
      "Shanda Li",
      "Di He",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.02016"
  },
  {
    "id": "arXiv:2206.02626",
    "title": "Infinite Recommendation Networks: A Data-Centric Approach",
    "abstract": "Comments: Published at NeurIPS '22, 24 pages, 15 figures. $\\infty$-AE code available at this https URL and Distill-CF code available at this https URL",
    "descriptor": "\nComments: Published at NeurIPS '22, 24 pages, 15 figures. $\\infty$-AE code available at this https URL and Distill-CF code available at this https URL\n",
    "authors": [
      "Noveen Sachdeva",
      "Mehak Preet Dhaliwal",
      "Carole-Jean Wu",
      "Julian McAuley"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02626"
  },
  {
    "id": "arXiv:2206.03466",
    "title": "Adversarial Reprogramming Revisited",
    "abstract": "Adversarial Reprogramming Revisited",
    "descriptor": "",
    "authors": [
      "Matthias Englert",
      "Ranko Lazic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03466"
  },
  {
    "id": "arXiv:2206.03491",
    "title": "EiX-GNN : Concept-level eigencentrality explainer for graph neural  networks",
    "abstract": "EiX-GNN : Concept-level eigencentrality explainer for graph neural  networks",
    "descriptor": "",
    "authors": [
      "Adrien Raison",
      "Pascal Bourdon",
      "David Helbert"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03491"
  },
  {
    "id": "arXiv:2206.03665",
    "title": "Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with  Communication Compression",
    "abstract": "Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with  Communication Compression",
    "descriptor": "",
    "authors": [
      "Xinmeng Huang",
      "Yiming Chen",
      "Wotao Yin",
      "Kun Yuan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2206.03665"
  },
  {
    "id": "arXiv:2206.04033",
    "title": "High-order approximation to generalized Caputo derivatives and  generalized fractional advection-diffusion equations",
    "abstract": "Comments: 30 pages, 3 figures",
    "descriptor": "\nComments: 30 pages, 3 figures\n",
    "authors": [
      "Sarita Kumari",
      "Rajesh K. Pandey",
      "R. P. Agarwal"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.04033"
  },
  {
    "id": "arXiv:2206.04122",
    "title": "ESCHER: Eschewing Importance Sampling in Games by Computing a History  Value Function to Estimate Regret",
    "abstract": "ESCHER: Eschewing Importance Sampling in Games by Computing a History  Value Function to Estimate Regret",
    "descriptor": "",
    "authors": [
      "Stephen McAleer",
      "Gabriele Farina",
      "Marc Lanctot",
      "Tuomas Sandholm"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04122"
  },
  {
    "id": "arXiv:2206.04564",
    "title": "TwiBot-22: Towards Graph-Based Twitter Bot Detection",
    "abstract": "Comments: NeurIPS 2022, Datasets and Benchmarks Track",
    "descriptor": "\nComments: NeurIPS 2022, Datasets and Benchmarks Track\n",
    "authors": [
      "Shangbin Feng",
      "Zhaoxuan Tan",
      "Herun Wan",
      "Ningnan Wang",
      "Zilong Chen",
      "Binchi Zhang",
      "Qinghua Zheng",
      "Wenqian Zhang",
      "Zhenyu Lei",
      "Shujie Yang",
      "Xinshun Feng",
      "Qingyue Zhang",
      "Hongrui Wang",
      "Yuhan Liu",
      "Yuyang Bai",
      "Heng Wang",
      "Zijian Cai",
      "Yanbo Wang",
      "Lijing Zheng",
      "Zihan Ma",
      "Jundong Li",
      "Minnan Luo"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04564"
  },
  {
    "id": "arXiv:2206.05669",
    "title": "Universality and approximation bounds for echo state networks with  random weights",
    "abstract": "Universality and approximation bounds for echo state networks with  random weights",
    "descriptor": "",
    "authors": [
      "Zhen Li",
      "Yunfei Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05669"
  },
  {
    "id": "arXiv:2206.06164",
    "title": "Metric Program Synthesis",
    "abstract": "Metric Program Synthesis",
    "descriptor": "",
    "authors": [
      "John Feser",
      "Isil Dillig",
      "Armando Solar-Lezama"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2206.06164"
  },
  {
    "id": "arXiv:2206.07698",
    "title": "Neural Deformable Voxel Grid for Fast Optimization of Dynamic View  Synthesis",
    "abstract": "Comments: Technical Report: 29 pages; project page: this https URL",
    "descriptor": "\nComments: Technical Report: 29 pages; project page: this https URL\n",
    "authors": [
      "Xiang Guo",
      "Guanying Chen",
      "Yuchao Dai",
      "Xiaoqing Ye",
      "Jiadai Sun",
      "Xiao Tan",
      "Errui Ding"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07698"
  },
  {
    "id": "arXiv:2206.07729",
    "title": "Taxonomy of Benchmarks in Graph Representation Learning",
    "abstract": "Taxonomy of Benchmarks in Graph Representation Learning",
    "descriptor": "",
    "authors": [
      "Renming Liu",
      "Semih Cant\u00fcrk",
      "Frederik Wenkel",
      "Sarah McGuire",
      "Xinyi Wang",
      "Anna Little",
      "Leslie O'Bray",
      "Michael Perlmutter",
      "Bastian Rieck",
      "Matthew Hirn",
      "Guy Wolf",
      "Ladislav Ramp\u00e1\u0161ek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07729"
  },
  {
    "id": "arXiv:2206.07870",
    "title": "How to talk so AI will learn: Instructions, descriptions, and autonomy",
    "abstract": "Comments: 10 pages, 5 figures. Published as a conference paper at NeurIPS 2022",
    "descriptor": "\nComments: 10 pages, 5 figures. Published as a conference paper at NeurIPS 2022\n",
    "authors": [
      "Theodore R Sumers",
      "Robert D Hawkins",
      "Mark K Ho",
      "Thomas L Griffiths",
      "Dylan Hadfield-Menell"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07870"
  },
  {
    "id": "arXiv:2206.08686",
    "title": "Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement  Learning",
    "abstract": "Comments: 38 pages, 8 figures",
    "descriptor": "\nComments: 38 pages, 8 figures\n",
    "authors": [
      "Yuanpei Chen",
      "Tianhao Wu",
      "Shengjie Wang",
      "Xidong Feng",
      "Jiechuang Jiang",
      "Stephen Marcus McAleer",
      "Yiran Geng",
      "Hao Dong",
      "Zongqing Lu",
      "Song-Chun Zhu",
      "Yaodong Yang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2206.08686"
  },
  {
    "id": "arXiv:2206.08873",
    "title": "Mirror Descent with Relative Smoothness in Measure Spaces, with  application to Sinkhorn and EM",
    "abstract": "Mirror Descent with Relative Smoothness in Measure Spaces, with  application to Sinkhorn and EM",
    "descriptor": "",
    "authors": [
      "Pierre-Cyril Aubin-Frankowski",
      "Anna Korba",
      "Flavien L\u00e9ger"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08873"
  },
  {
    "id": "arXiv:2206.09674",
    "title": "EAGER: Asking and Answering Questions for Automatic Reward Shaping in  Language-guided RL",
    "abstract": "Comments: 24 pages, 16 figures, 5 tables",
    "descriptor": "\nComments: 24 pages, 16 figures, 5 tables\n",
    "authors": [
      "Thomas Carta",
      "Sylvain Lamprier",
      "Pierre-Yves Oudeyer",
      "Olivier Sigaud"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.09674"
  },
  {
    "id": "arXiv:2206.09677",
    "title": "GraphFramEx: Towards Systematic Evaluation of Explainability Methods for  Graph Neural Networks",
    "abstract": "Comments: Submitted to Learning on Graphs 2022 and New Frontiers in Graph Learning Workshop (Neurips 2022)",
    "descriptor": "\nComments: Submitted to Learning on Graphs 2022 and New Frontiers in Graph Learning Workshop (Neurips 2022)\n",
    "authors": [
      "Kenza Amara",
      "Rex Ying",
      "Zitao Zhang",
      "Zhihao Han",
      "Yinan Shan",
      "Ulrik Brandes",
      "Sebastian Schemm",
      "Ce Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.09677"
  },
  {
    "id": "arXiv:2206.10540",
    "title": "Rethinking Symbolic Regression Datasets and Benchmarks for Scientific  Discovery",
    "abstract": "Comments: Preprint. Code and datasets are available at this https URL this https URL this https URL this https URL",
    "descriptor": "\nComments: Preprint. Code and datasets are available at this https URL this https URL this https URL this https URL\n",
    "authors": [
      "Yoshitomo Matsubara",
      "Naoya Chiba",
      "Ryo Igarashi",
      "Tatsunori Taniai",
      "Yoshitaka Ushiku"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2206.10540"
  },
  {
    "id": "arXiv:2206.11460",
    "title": "pyKT: A Python Library to Benchmark Deep Learning based Knowledge  Tracing Models",
    "abstract": "Comments: Accepted in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks",
    "descriptor": "\nComments: Accepted in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks\n",
    "authors": [
      "Zitao Liu",
      "Qiongqiong Liu",
      "Jiahao Chen",
      "Shuyan Huang",
      "Jiliang Tang",
      "Weiqi Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.11460"
  },
  {
    "id": "arXiv:2206.12034",
    "title": "DialogID: A Dialogic Instruction Dataset for Improving Teaching  Effectiveness in Online Environments",
    "abstract": "Comments: Accepted in 31st ACM International Conference on Information and Knowledge Management (CIKM 2022)",
    "descriptor": "\nComments: Accepted in 31st ACM International Conference on Information and Knowledge Management (CIKM 2022)\n",
    "authors": [
      "Jiahao Chen",
      "Shuyan Huang",
      "Zitao Liu",
      "Weiqi Luo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.12034"
  },
  {
    "id": "arXiv:2206.12227",
    "title": "Adversarial Robustness of Deep Neural Networks: A Survey from a Formal  Verification Perspective",
    "abstract": "Adversarial Robustness of Deep Neural Networks: A Survey from a Formal  Verification Perspective",
    "descriptor": "",
    "authors": [
      "Mark Huasong Meng",
      "Guangdong Bai",
      "Sin Gee Teo",
      "Zhe Hou",
      "Yan Xiao",
      "Yun Lin",
      "Jin Song Dong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2206.12227"
  },
  {
    "id": "arXiv:2206.13631",
    "title": "Learning Semantics-Aware Locomotion Skills from Human Demonstration",
    "abstract": "Learning Semantics-Aware Locomotion Skills from Human Demonstration",
    "descriptor": "",
    "authors": [
      "Yuxiang Yang",
      "Xiangyun Meng",
      "Wenhao Yu",
      "Tingnan Zhang",
      "Jie Tan",
      "Byron Boots"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.13631"
  },
  {
    "id": "arXiv:2206.15476",
    "title": "AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly  Detection",
    "abstract": "AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly  Detection",
    "descriptor": "",
    "authors": [
      "Marius Dragoi",
      "Elena Burceanu",
      "Emanuela Haller",
      "Andrei Manolache",
      "Florin Brad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.15476"
  },
  {
    "id": "arXiv:2207.00768",
    "title": "Sum-of-Max Partition under a Knapsack Constraint",
    "abstract": "Sum-of-Max Partition under a Knapsack Constraint",
    "descriptor": "",
    "authors": [
      "Kai Jin",
      "Danna Zhang",
      "Canhui Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2207.00768"
  },
  {
    "id": "arXiv:2207.00787",
    "title": "Object Representations as Fixed Points: Training Iterative Refinement  Algorithms with Implicit Differentiation",
    "abstract": "Comments: 19 pages, 13 figures, Accepted to the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: 19 pages, 13 figures, Accepted to the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Michael Chang",
      "Thomas L. Griffiths",
      "Sergey Levine"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2207.00787"
  },
  {
    "id": "arXiv:2207.04122",
    "title": "Sudowoodo: Contrastive Self-supervised Learning for Multi-purpose Data  Integration and Preparation",
    "abstract": "Sudowoodo: Contrastive Self-supervised Learning for Multi-purpose Data  Integration and Preparation",
    "descriptor": "",
    "authors": [
      "Runhui Wang",
      "Yuliang Li",
      "Jin Wang"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2207.04122"
  },
  {
    "id": "arXiv:2207.04327",
    "title": "Error Analysis of Tensor-Train Cross Approximation",
    "abstract": "Error Analysis of Tensor-Train Cross Approximation",
    "descriptor": "",
    "authors": [
      "Zhen Qin",
      "Alexander Lidiak",
      "Zhexuan Gong",
      "Gongguo Tang",
      "Michael B. Wakin",
      "Zhihui Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2207.04327"
  },
  {
    "id": "arXiv:2207.04487",
    "title": "Automatic differentiation and the optimization of differential equation  models in biology",
    "abstract": "Automatic differentiation and the optimization of differential equation  models in biology",
    "descriptor": "",
    "authors": [
      "Steven A. Frank"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.04487"
  },
  {
    "id": "arXiv:2207.04649",
    "title": "Fast Density-Peaks Clustering: Multicore-based Parallelization Approach",
    "abstract": "Fast Density-Peaks Clustering: Multicore-based Parallelization Approach",
    "descriptor": "",
    "authors": [
      "Daichi Amagata",
      "Takahiro Hara"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2207.04649"
  },
  {
    "id": "arXiv:2207.05697",
    "title": "A Newton-CG based barrier method for finding a second-order stationary  point of nonconvex conic optimization with complexity guarantees",
    "abstract": "Comments: accepted by SIAM Journal on Optimization",
    "descriptor": "\nComments: accepted by SIAM Journal on Optimization\n",
    "authors": [
      "Chuan He",
      "Zhaosong Lu"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.05697"
  },
  {
    "id": "arXiv:2207.06194",
    "title": "Bond-based peridynamics, a survey prospecting nonlocal theories of  fluid-dynamics",
    "abstract": "Comments: Accepted for pubblication in Advances in Continuous and Discrete Models: Theory and Applications",
    "descriptor": "\nComments: Accepted for pubblication in Advances in Continuous and Discrete Models: Theory and Applications\n",
    "authors": [
      "Nunzio Dimola",
      "Alessandro Coclite",
      "Giuseppe Fanizza",
      "Tiziano Politi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2207.06194"
  },
  {
    "id": "arXiv:2207.09081",
    "title": "Generalizing Goal-Conditioned Reinforcement Learning with Variational  Causal Reasoning",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Wenhao Ding",
      "Haohong Lin",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2207.09081"
  },
  {
    "id": "arXiv:2207.09670",
    "title": "Optimal Path Planning for Connected and Automated Vehicles in Lane-free  Traffic with Vehicle Nudging",
    "abstract": "Optimal Path Planning for Connected and Automated Vehicles in Lane-free  Traffic with Vehicle Nudging",
    "descriptor": "",
    "authors": [
      "Venkata Karteek Yanumula",
      "Panagiotis Typaldos",
      "Dimitrios Troullinos",
      "Milad Malekzadeh",
      "Ioannis Papamichail",
      "Markos Papageorgiou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2207.09670"
  },
  {
    "id": "arXiv:2207.11839",
    "title": "A Deep Dive into Deep Cluster",
    "abstract": "A Deep Dive into Deep Cluster",
    "descriptor": "",
    "authors": [
      "Ahmad Mustapha",
      "Wael Khreich",
      "Wasim Masr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.11839"
  },
  {
    "id": "arXiv:2207.11957",
    "title": "Uniqueness of a solution to a general class of discrete system defined  on connected graphs",
    "abstract": "Comments: 14 pages, 1 figure",
    "descriptor": "\nComments: 14 pages, 1 figure\n",
    "authors": [
      "Avetik Arakelyan",
      "Farid Bozorgnia"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Combinatorics (math.CO)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2207.11957"
  },
  {
    "id": "arXiv:2207.12576",
    "title": "WinoGAViL: Gamified Association Benchmark to Challenge  Vision-and-Language Models",
    "abstract": "Comments: Accepted to NeurIPS 2022, Datasets and Benchmarks. Website: this https URL",
    "descriptor": "\nComments: Accepted to NeurIPS 2022, Datasets and Benchmarks. Website: this https URL\n",
    "authors": [
      "Yonatan Bitton",
      "Nitzan Bitton Guetta",
      "Ron Yosef",
      "Yuval Elovici",
      "Mohit Bansal",
      "Gabriel Stanovsky",
      "Roy Schwartz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2207.12576"
  },
  {
    "id": "arXiv:2207.12878",
    "title": "Safe Model Predictive Control Approach for Non-holonomic Mobile Robots",
    "abstract": "Safe Model Predictive Control Approach for Non-holonomic Mobile Robots",
    "descriptor": "",
    "authors": [
      "Xinjie Liu",
      "Vassil Atanassov"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2207.12878"
  },
  {
    "id": "arXiv:2207.13352",
    "title": "COVID-19 and social media: Beyond polarization",
    "abstract": "COVID-19 and social media: Beyond polarization",
    "descriptor": "",
    "authors": [
      "Giacomo De Nicola",
      "Victor H. Tuekam Mambou",
      "G\u00f6ran Kauermann"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2207.13352"
  },
  {
    "id": "arXiv:2207.13799",
    "title": "Network polarization, filter bubbles, and echo chambers: An annotated  review of measures and reduction methods",
    "abstract": "Network polarization, filter bubbles, and echo chambers: An annotated  review of measures and reduction methods",
    "descriptor": "",
    "authors": [
      "Ruben Interian",
      "Ruslan G. Marzo",
      "Isela Mendoza",
      "Celso C. Ribeiro"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2207.13799"
  },
  {
    "id": "arXiv:2207.14284",
    "title": "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated  Convolutions",
    "abstract": "Comments: project page: this https URL",
    "descriptor": "\nComments: project page: this https URL\n",
    "authors": [
      "Yongming Rao",
      "Wenliang Zhao",
      "Yansong Tang",
      "Jie Zhou",
      "Ser-Nam Lim",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.14284"
  },
  {
    "id": "arXiv:2208.00361",
    "title": "One for All: One-stage Referring Expression Comprehension with Dynamic  Reasoning",
    "abstract": "Comments: 16 pages, 2 figures",
    "descriptor": "\nComments: 16 pages, 2 figures\n",
    "authors": [
      "Zhipeng Zhang",
      "Zhimin Wei",
      "Zhongzhen Huang",
      "Rui Niu",
      "Peng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.00361"
  },
  {
    "id": "arXiv:2208.01909",
    "title": "Rethinking the Evaluation of Unbiased Scene Graph Generation",
    "abstract": "Rethinking the Evaluation of Unbiased Scene Graph Generation",
    "descriptor": "",
    "authors": [
      "Xingchen Li",
      "Long Chen",
      "Jian Shao",
      "Shaoning Xiao",
      "Songyang Zhang",
      "Jun Xiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.01909"
  },
  {
    "id": "arXiv:2208.02668",
    "title": "SoftIGA: soft isogeometric analysis",
    "abstract": "SoftIGA: soft isogeometric analysis",
    "descriptor": "",
    "authors": [
      "Quanling Deng",
      "Pouria Behnoudfar",
      "Victor M. Calo"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2208.02668"
  },
  {
    "id": "arXiv:2208.02998",
    "title": "Localized Sparse Incomplete Multi-view Clustering",
    "abstract": "Comments: Published in IEEE Transactions on Multimedia (TMM). The code is available in Github this https URL",
    "descriptor": "\nComments: Published in IEEE Transactions on Multimedia (TMM). The code is available in Github this https URL\n",
    "authors": [
      "Chengliang Liu",
      "Zhihao Wu",
      "Jie Wen",
      "Chao Huang",
      "Yong Xu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.02998"
  },
  {
    "id": "arXiv:2208.05013",
    "title": "Computing Brascamp-Lieb Constants through the lens of Thompson Geometry",
    "abstract": "Comments: Under Review",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Melanie Weber",
      "Suvrit Sra"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Functional Analysis (math.FA)"
    ],
    "url": "https://arxiv.org/abs/2208.05013"
  },
  {
    "id": "arXiv:2208.05863",
    "title": "GEM-2: Next Generation Molecular Property Prediction Network by Modeling  Full-range Many-body Interactions",
    "abstract": "GEM-2: Next Generation Molecular Property Prediction Network by Modeling  Full-range Many-body Interactions",
    "descriptor": "",
    "authors": [
      "Lihang Liu",
      "Donglong He",
      "Xiaomin Fang",
      "Shanzhuo Zhang",
      "Fan Wang",
      "Jingzhou He",
      "Hua Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Molecular Networks (q-bio.MN)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2208.05863"
  },
  {
    "id": "arXiv:2208.07833",
    "title": "What Your Firmware Tells You Is Not How You Should Emulate It: A  Specification-Guided Approach for Firmware Emulation (Extended Version)",
    "abstract": "Comments: Wei Zhou and Lan Zhang contributed equally to this work",
    "descriptor": "\nComments: Wei Zhou and Lan Zhang contributed equally to this work\n",
    "authors": [
      "Wei Zhou",
      "Lan Zhang",
      "Le Guan",
      "Peng Liu",
      "Yuqing Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2208.07833"
  },
  {
    "id": "arXiv:2208.08749",
    "title": "Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim  Verification with Pattern Exploiting Training",
    "abstract": "Active PETs: Active Data Annotation Prioritisation for Few-Shot Claim  Verification with Pattern Exploiting Training",
    "descriptor": "",
    "authors": [
      "Xia Zeng",
      "Arkaitz Zubiaga"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.08749"
  },
  {
    "id": "arXiv:2208.10073",
    "title": "Local Geometry of Nonconvex Spike Deconvolution from Low-Pass  Measurements",
    "abstract": "Local Geometry of Nonconvex Spike Deconvolution from Low-Pass  Measurements",
    "descriptor": "",
    "authors": [
      "Maxime Ferreira Da Costa",
      "Yuejie Chi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2208.10073"
  },
  {
    "id": "arXiv:2208.12104",
    "title": "Algorithmic Differentiation for Automatized Modelling of Machine Learned  Force Fields",
    "abstract": "Comments: 16 pages, 4 figures",
    "descriptor": "\nComments: 16 pages, 4 figures\n",
    "authors": [
      "Niklas Frederik Schmitz",
      "Klaus-Robert M\u00fcller",
      "Stefan Chmiela"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.12104"
  },
  {
    "id": "arXiv:2209.00638",
    "title": "Unified Fully and Timestamp Supervised Temporal Action Segmentation via  Sequence to Sequence Translation",
    "abstract": "Comments: ECCV 2022 (Main Conference)",
    "descriptor": "\nComments: ECCV 2022 (Main Conference)\n",
    "authors": [
      "Nadine Behrmann",
      "S. Alireza Golestaneh",
      "Zico Kolter",
      "Juergen Gall",
      "Mehdi Noroozi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.00638"
  },
  {
    "id": "arXiv:2209.02577",
    "title": "Avgust: Automating Usage-Based Test Generation from Videos of App  Executions",
    "abstract": "Avgust: Automating Usage-Based Test Generation from Videos of App  Executions",
    "descriptor": "",
    "authors": [
      "Yixue Zhao",
      "Saghar Talebipour",
      "Kesina Baral",
      "Hyojae Park",
      "Leon Yee",
      "Safwat Ali Khan",
      "Yuriy Brun",
      "Nenad Medvidovic",
      "Kevin Moran"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.02577"
  },
  {
    "id": "arXiv:2209.03042",
    "title": "Graph Neural Networks for Low-Energy Event Classification &  Reconstruction in IceCube",
    "abstract": "Comments: Prepared for submission to JINST",
    "descriptor": "\nComments: Prepared for submission to JINST\n",
    "authors": [
      "R. Abbasi",
      "M. Ackermann",
      "J. Adams",
      "N. Aggarwal",
      "J. A. Aguilar",
      "M. Ahlers",
      "M. Ahrens",
      "J.M. Alameddine",
      "A. A. Alves Jr.",
      "N. M. Amin",
      "K. Andeen",
      "T. Anderson",
      "G. Anton",
      "C. Arg\u00fcelles",
      "Y. Ashida",
      "S. Athanasiadou",
      "S. Axani",
      "X. Bai",
      "A. Balagopal V.",
      "M. Baricevic",
      "S. W. Barwick",
      "V. Basu",
      "R. Bay",
      "J. J. Beatty",
      "K.-H. Becker",
      "J. Becker Tjus",
      "J. Beise",
      "C. Bellenghi",
      "S. Benda",
      "S. BenZvi",
      "D. Berley",
      "E. Bernardini",
      "D. Z. Besson",
      "G. Binder",
      "D. Bindig",
      "E. Blaufuss",
      "S. Blot",
      "F. Bontempo",
      "J. Y. Book",
      "J. Borowka",
      "C. Boscolo Meneguolo",
      "S. B\u00f6ser",
      "O. Botner",
      "J. B\u00f6ttcher",
      "E. Bourbeau",
      "J. Braun",
      "B. Brinson",
      "J. Brostean-Kaiser",
      "R. T. Burley",
      "R. S. Busse",
      "M. A. Campana",
      "E. G. Carnie-Bronca",
      "C. Chen",
      "Z. Chen",
      "D. Chirkin",
      "K. Choi",
      "B. A. Clark",
      "L. Classen",
      "A. Coleman",
      "G. H. Collin",
      "A. Connolly",
      "J. M. Conrad"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Instrumentation and Detectors (physics.ins-det)"
    ],
    "url": "https://arxiv.org/abs/2209.03042"
  },
  {
    "id": "arXiv:2209.03534",
    "title": "CWP: Instance complexity weighted channel-wise soft masks for network  pruning",
    "abstract": "CWP: Instance complexity weighted channel-wise soft masks for network  pruning",
    "descriptor": "",
    "authors": [
      "Jiapeng Wang",
      "Ming Ma",
      "Zhenhua Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.03534"
  },
  {
    "id": "arXiv:2209.03919",
    "title": "Multiobjective Ranking and Selection Using Stochastic Kriging",
    "abstract": "Comments: 29 pages, 12 figures",
    "descriptor": "\nComments: 29 pages, 12 figures\n",
    "authors": [
      "Sebastian Rojas Gonzalez",
      "Juergen Branke",
      "Inneke van Nieuwenhuyse"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.03919"
  },
  {
    "id": "arXiv:2209.06652",
    "title": "CoHS-CQG: Context and History Selection for Conversational Question  Generation",
    "abstract": "Comments: Accepted by 29th International Conference on Computational Linguistics (COLING 2022)",
    "descriptor": "\nComments: Accepted by 29th International Conference on Computational Linguistics (COLING 2022)\n",
    "authors": [
      "Xuan Long Do",
      "Bowei Zou",
      "Liangming Pan",
      "Nancy F. Chen",
      "Shafiq Joty",
      "Ai Ti Aw"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.06652"
  },
  {
    "id": "arXiv:2209.07036",
    "title": "Langevin Autoencoders for Learning Deep Latent Variable Models",
    "abstract": "Comments: accepted at Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: accepted at Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Shohei Taniguchi",
      "Yusuke Iwasawa",
      "Wataru Kumagai",
      "Yutaka Matsuo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.07036"
  },
  {
    "id": "arXiv:2209.07618",
    "title": "Differentiable Bilevel Programming for Stackelberg Congestion Games",
    "abstract": "Differentiable Bilevel Programming for Stackelberg Congestion Games",
    "descriptor": "",
    "authors": [
      "Jiayang Li",
      "Jing Yu",
      "Qianni Wang",
      "Boyi Liu",
      "Zhaoran Wang",
      "Yu Marco Nie"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.07618"
  },
  {
    "id": "arXiv:2209.07695",
    "title": "Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation",
    "abstract": "Comments: Accepted at NeurIPS2022",
    "descriptor": "\nComments: Accepted at NeurIPS2022\n",
    "authors": [
      "Lin Chen",
      "Zhixiang Wei",
      "Xin Jin",
      "Huaian Chen",
      "Miao Zheng",
      "Kai Chen",
      "Yi Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.07695"
  },
  {
    "id": "arXiv:2209.08284",
    "title": "Flexible and Structured Knowledge Grounded Question Answering",
    "abstract": "Comments: The extended abstract is not ready",
    "descriptor": "\nComments: The extended abstract is not ready\n",
    "authors": [
      "Yujie Lu",
      "Siqi Ouyang",
      "Kairui Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.08284"
  },
  {
    "id": "arXiv:2209.08514",
    "title": "Imbalanced Node Processing Method in Graph Neural Network Classification  Task",
    "abstract": "Comments: 6 pages,3 figures",
    "descriptor": "\nComments: 6 pages,3 figures\n",
    "authors": [
      "Min Liu",
      "Siwen Jin",
      "Luo Jin",
      "Shuohan Wang",
      "Yu Fang",
      "Yuliang Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.08514"
  },
  {
    "id": "arXiv:2209.09482",
    "title": "Incorporating Causal Analysis into Diversified and Logical Response  Generation",
    "abstract": "Comments: Accepted at COLING 2022",
    "descriptor": "\nComments: Accepted at COLING 2022\n",
    "authors": [
      "Jiayi Liu",
      "Wei Wei",
      "Zhixuan Chu",
      "Xing Gao",
      "Ji Zhang",
      "Tan Yan",
      "Yulin Kang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.09482"
  },
  {
    "id": "arXiv:2209.10003",
    "title": "Macro-Action-Based Multi-Agent/Robot Deep Reinforcement Learning under  Partial Observability",
    "abstract": "Comments: PhD thesis",
    "descriptor": "\nComments: PhD thesis\n",
    "authors": [
      "Yuchen Xiao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.10003"
  },
  {
    "id": "arXiv:2209.10113",
    "title": "Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning",
    "abstract": "Comments: arXiv admin note: substantial text overlap with arXiv:2209.10003",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2209.10003\n",
    "authors": [
      "Yuchen Xiao",
      "Weihao Tan",
      "Christopher Amato"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.10113"
  },
  {
    "id": "arXiv:2209.10587",
    "title": "DeepVARwT: Deep Learning for a VAR Model with Trend",
    "abstract": "DeepVARwT: Deep Learning for a VAR Model with Trend",
    "descriptor": "",
    "authors": [
      "Xixi Li",
      "Jingsong Yuan"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.10587"
  },
  {
    "id": "arXiv:2209.11919",
    "title": "Concordance based Survival Cobra with regression type weak learners",
    "abstract": "Concordance based Survival Cobra with regression type weak learners",
    "descriptor": "",
    "authors": [
      "Rahul Goswami",
      "Arabin Kumar Dey"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2209.11919"
  },
  {
    "id": "arXiv:2209.13017",
    "title": "Public Wisdom Matters! Discourse-Aware Hyperbolic Fourier Co-Attention  for Social-Text Classification",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Karish Grover",
      "S.M. Phaneendra Angara",
      "Md. Shad Akhtar",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.13017"
  },
  {
    "id": "arXiv:2209.13205",
    "title": "Adaptive approximation of nonlinear eigenproblems by minimal rational  interpolation",
    "abstract": "Adaptive approximation of nonlinear eigenproblems by minimal rational  interpolation",
    "descriptor": "",
    "authors": [
      "Davide Pradovera"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.13205"
  },
  {
    "id": "arXiv:2209.14065",
    "title": "LL-GNN: Low Latency Graph Neural Networks on FPGAs for Particle  Detectors",
    "abstract": "Comments: 13 pages and 12 figures",
    "descriptor": "\nComments: 13 pages and 12 figures\n",
    "authors": [
      "Zhiqiang Que",
      "Hongxiang Fan",
      "Marcus Loo",
      "Michaela Blott",
      "Maurizio Pierini",
      "Alexander D Tapper",
      "Wayne Luk"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)",
      "Instrumentation and Detectors (physics.ins-det)"
    ],
    "url": "https://arxiv.org/abs/2209.14065"
  },
  {
    "id": "arXiv:2209.14826",
    "title": "Towards Lightweight Black-Box Attacks against Deep Neural Networks",
    "abstract": "Towards Lightweight Black-Box Attacks against Deep Neural Networks",
    "descriptor": "",
    "authors": [
      "Chenghao Sun",
      "Yonggang Zhang",
      "Wan Chaoqun",
      "Qizhou Wang",
      "Ya Li",
      "Tongliang Liu",
      "Bo Han",
      "Xinmei Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.14826"
  },
  {
    "id": "arXiv:2209.15220",
    "title": "Assortment Optimization Under the Multivariate MNL Model",
    "abstract": "Assortment Optimization Under the Multivariate MNL Model",
    "descriptor": "",
    "authors": [
      "Xin Chen",
      "Jiachun Li",
      "Menglong Li",
      "Tiancheng Zhao",
      "Yuan Zhou"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.15220"
  },
  {
    "id": "arXiv:2210.00590",
    "title": "Community Learning: Understanding A Community Through NLP for Positive  Impact",
    "abstract": "Comments: The article has been withdrawn as the work is incomplete at this point in time. There are significant evaluations required before this work is ready for pre-print. Furthermore, the dataset of NextDoor used in this paper is also not complete. As of this time this work is not applicable",
    "descriptor": "\nComments: The article has been withdrawn as the work is incomplete at this point in time. There are significant evaluations required before this work is ready for pre-print. Furthermore, the dataset of NextDoor used in this paper is also not complete. As of this time this work is not applicable\n",
    "authors": [
      "Md Towhidul Absar Chowdhury",
      "Naveen Sharma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.00590"
  },
  {
    "id": "arXiv:2210.01301",
    "title": "GIDN: A Lightweight Graph Inception Diffusion Network for High-efficient  Link Prediction",
    "abstract": "GIDN: A Lightweight Graph Inception Diffusion Network for High-efficient  Link Prediction",
    "descriptor": "",
    "authors": [
      "Zixiao Wang",
      "Yuluo Guo",
      "Jin Zhao",
      "Yu Zhang",
      "Hui Yu",
      "Xiaofei Liao",
      "Hai Jin",
      "Biao Wang",
      "Ting Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.01301"
  },
  {
    "id": "arXiv:2210.01849",
    "title": "Link Partitioning on Simplicial Complexes Using Higher-Order Laplacians",
    "abstract": "Comments: Accepted to 22nd IEEE International Conference on Data Mining (ICDM 2022). Fixed some typos in v1",
    "descriptor": "\nComments: Accepted to 22nd IEEE International Conference on Data Mining (ICDM 2022). Fixed some typos in v1\n",
    "authors": [
      "Xinyi Wu",
      "Arnab Sarker",
      "Ali Jadbabaie"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2210.01849"
  },
  {
    "id": "arXiv:2210.01863",
    "title": "Group Personalized Federated Learning",
    "abstract": "Group Personalized Federated Learning",
    "descriptor": "",
    "authors": [
      "Zhe Liu",
      "Yue Hui",
      "Fuchun Peng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01863"
  },
  {
    "id": "arXiv:2210.02040",
    "title": "GT-GAN: General Purpose Time Series Synthesis with Generative  Adversarial Networks",
    "abstract": "Comments: NeurIPs 2022",
    "descriptor": "\nComments: NeurIPs 2022\n",
    "authors": [
      "Jinsung Jeon",
      "Jeonghak Kim",
      "Haryong Song",
      "Seunghyeon Cho",
      "Noseong Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02040"
  },
  {
    "id": "arXiv:2210.02174",
    "title": "CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted  Empirical Risk Minimization",
    "abstract": "Comments: v2: minor update in dataset and results (no changes in improvements or conclusions)",
    "descriptor": "\nComments: v2: minor update in dataset and results (no changes in improvements or conclusions)\n",
    "authors": [
      "Eesha Kumar",
      "Yiming Zhang",
      "Stefano Pini",
      "Simon Stent",
      "Ana Ferreira",
      "Sergey Zagoruyko",
      "Christian S. Perone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02174"
  },
  {
    "id": "arXiv:2210.02226",
    "title": "Null Hypothesis Test for Anomaly Detection",
    "abstract": "Comments: 8 pages, 3 figures. Updated version prepared for submission. All code is now available at this https URL Comments welcome!",
    "descriptor": "\nComments: 8 pages, 3 figures. Updated version prepared for submission. All code is now available at this https URL Comments welcome!\n",
    "authors": [
      "Jernej F. Kamenik",
      "Manuel Szewc"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)"
    ],
    "url": "https://arxiv.org/abs/2210.02226"
  },
  {
    "id": "arXiv:2210.02432",
    "title": "Coercive second-kind boundary integral equations for the Laplace  Dirichlet problem on Lipschitz domains",
    "abstract": "Coercive second-kind boundary integral equations for the Laplace  Dirichlet problem on Lipschitz domains",
    "descriptor": "",
    "authors": [
      "Simon N. Chandler-Wilde",
      "Euan A. Spence"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.02432"
  },
  {
    "id": "arXiv:2210.02621",
    "title": "U3E: Unsupervised and Erasure-based Evidence Extraction for Machine  Reading Comprehension",
    "abstract": "U3E: Unsupervised and Erasure-based Evidence Extraction for Machine  Reading Comprehension",
    "descriptor": "",
    "authors": [
      "Suzhe He",
      "Shumin Shi",
      "Chenghao Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02621"
  },
  {
    "id": "arXiv:2210.02656",
    "title": "Trust in Motion: Capturing Trust Ascendancy in Open-Source Projects  using Hybrid AI",
    "abstract": "Comments: 6 pages, 4 figures, 2 tables",
    "descriptor": "\nComments: 6 pages, 4 figures, 2 tables\n",
    "authors": [
      "Huascar Sanchez",
      "Briland Hitaj"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02656"
  },
  {
    "id": "arXiv:2210.02969",
    "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger  Zero-Shot Learners",
    "abstract": "Guess the Instruction! Flipped Learning Makes Language Models Stronger  Zero-Shot Learners",
    "descriptor": "",
    "authors": [
      "Seonghyeon Ye",
      "Doyoung Kim",
      "Joel Jang",
      "Joongbo Shin",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02969"
  },
  {
    "id": "arXiv:2210.02974",
    "title": "Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach  for Rotating Machinery exploiting Augmented Synthetic Data",
    "abstract": "Comments: 25 pages",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Lucas Costa Brito",
      "Gian Antonio Susto",
      "Jorge Nei Brito",
      "Marcus Antonio Viana Duarte"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02974"
  },
  {
    "id": "arXiv:2210.03029",
    "title": "Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization",
    "abstract": "Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization",
    "descriptor": "",
    "authors": [
      "Seonghyeon Ye",
      "Joel Jang",
      "Doyoung Kim",
      "Yongrae Jo",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.03029"
  },
  {
    "id": "arXiv:2210.03300",
    "title": "Multi-Robot Localization and Target Tracking with Connectivity  Maintenance and Collision Avoidance",
    "abstract": "Multi-Robot Localization and Target Tracking with Connectivity  Maintenance and Collision Avoidance",
    "descriptor": "",
    "authors": [
      "Rahul Zahroof",
      "Jiazhen Liu",
      "Lifeng Zhou",
      "Vijay Kumar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.03300"
  },
  {
    "id": "arXiv:2210.03418",
    "title": "Data-driven probability density forecast for stochastic dynamical  systems",
    "abstract": "Comments: typos corrected",
    "descriptor": "\nComments: typos corrected\n",
    "authors": [
      "Meng Zhao",
      "Lijian Jiang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.03418"
  },
  {
    "id": "arXiv:2210.03588",
    "title": "Understanding Transformer Memorization Recall Through Idioms",
    "abstract": "Understanding Transformer Memorization Recall Through Idioms",
    "descriptor": "",
    "authors": [
      "Adi Haviv",
      "Ido Cohen",
      "Jacob Gidron",
      "Roei Schuster",
      "Yoav Goldberg",
      "Mor Geva"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.03588"
  },
  {
    "id": "arXiv:2210.03709",
    "title": "Understanding Practices, Challenges, and Opportunities for User-Driven  Algorithm Auditing in Industry Practice",
    "abstract": "Comments: Pre-print in double-column format",
    "descriptor": "\nComments: Pre-print in double-column format\n",
    "authors": [
      "Wesley Hanwen Deng",
      "Bill Boyuan Guo",
      "Alicia DeVrio",
      "Hong Shen",
      "Motahhare Eslami",
      "Kenneth Holstein"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.03709"
  },
  {
    "id": "arXiv:2210.03850",
    "title": "Toward an Over-parameterized Direct-Fit Model of Visual Perception",
    "abstract": "Toward an Over-parameterized Direct-Fit Model of Visual Perception",
    "descriptor": "",
    "authors": [
      "Xin Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.03850"
  },
  {
    "id": "arXiv:2210.03894",
    "title": "GRANITE: A Graph Neural Network Model for Basic Block Throughput  Estimation",
    "abstract": "Comments: 13 pages; 5 figures; published at IISWC 2022; Included IEEE copyright;",
    "descriptor": "\nComments: 13 pages; 5 figures; published at IISWC 2022; Included IEEE copyright;\n",
    "authors": [
      "Ondrej Sykora",
      "Phitchaya Mangpo Phothilimthana",
      "Charith Mendis",
      "Amir Yazdanbakhsh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2210.03894"
  },
  {
    "id": "arXiv:2210.03906",
    "title": "5G NR-LTE Coexistence: Opportunities, Challenges, and Solutions",
    "abstract": "5G NR-LTE Coexistence: Opportunities, Challenges, and Solutions",
    "descriptor": "",
    "authors": [
      "Sneihil Gopal",
      "David Griffith",
      "Richard A. Rouil",
      "Chunmei Liu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.03906"
  },
  {
    "id": "arXiv:2210.03967",
    "title": "Asymptotically Unbiased Instance-wise Regularized Partial AUC  Optimization: Theory and Algorithm",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Huiyang Shao",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Shilong Bao",
      "Qingming Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.03967"
  },
  {
    "id": "arXiv:2210.04045",
    "title": "The FBHHRBNRSSSHK-Algorithm for Multiplication in  $\\mathbb{Z}_2^{5\\times5}$ is still not the end of the story",
    "abstract": "The FBHHRBNRSSSHK-Algorithm for Multiplication in  $\\mathbb{Z}_2^{5\\times5}$ is still not the end of the story",
    "descriptor": "",
    "authors": [
      "Manuel Kauers",
      "Jakob Moosbauer"
    ],
    "subjectives": [
      "Symbolic Computation (cs.SC)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.04045"
  },
  {
    "id": "arXiv:2210.04080",
    "title": "Delivery to Safety with Two Cooperating Robots",
    "abstract": "Delivery to Safety with Two Cooperating Robots",
    "descriptor": "",
    "authors": [
      "Jared Coleman",
      "Evangelos Kranakis",
      "Danny Krizanc",
      "Oscar Morales-Ponce"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.04080"
  },
  {
    "id": "arXiv:2210.04186",
    "title": "Analogy Generation by Prompting Large Language Models: A Case Study of  InstructGPT",
    "abstract": "Comments: Accepted to 15th International Conference on Natural Language Generation (INLG 2022)",
    "descriptor": "\nComments: Accepted to 15th International Conference on Natural Language Generation (INLG 2022)\n",
    "authors": [
      "Bhavya Bhavya",
      "Jinjun Xiong",
      "Chengxiang Zhai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04186"
  },
  {
    "id": "arXiv:2210.04216",
    "title": "AMPose: Alternatively Mixed Global-Local Attention Model for 3D Human  Pose Estimation",
    "abstract": "Comments: 7 pages, 4 figures",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Hongxin Lin",
      "Yunwei Chiu",
      "Peiyuan Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.04216"
  },
  {
    "id": "arXiv:2210.04277",
    "title": "Event-Driven Tactile Learning with Various Location Spiking Neurons",
    "abstract": "Comments: This paper is under review in IEEE TNNLS. Please note that this paper is a journal extension of our previous conference paper: arXiv:2209.01080. This paper includes more novel models, data, experiments, and demonstration",
    "descriptor": "\nComments: This paper is under review in IEEE TNNLS. Please note that this paper is a journal extension of our previous conference paper: arXiv:2209.01080. This paper includes more novel models, data, experiments, and demonstration\n",
    "authors": [
      "Peng Kang",
      "Srutarshi Banerjee",
      "Henry Chopp",
      "Aggelos Katsaggelos",
      "Oliver Cossairt"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.04277"
  },
  {
    "id": "arXiv:2210.04284",
    "title": "SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency  of Adapters",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Shwai He",
      "Liang Ding",
      "Daize Dong",
      "Miao Zhang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04284"
  },
  {
    "id": "arXiv:2210.04325",
    "title": "ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Jiannan Xiang",
      "Zhengzhong Liu",
      "Yucheng Zhou",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04325"
  },
  {
    "id": "arXiv:2210.04427",
    "title": "Asymmetric Temperature Scaling Makes Larger Networks Teach Well Again",
    "abstract": "Comments: Accepted By NeurIPS 2022",
    "descriptor": "\nComments: Accepted By NeurIPS 2022\n",
    "authors": [
      "Xin-Chun Li",
      "Wen-Shu Fan",
      "Shaoming Song",
      "Yinchuan Li",
      "Bingshuai Li",
      "Yunfeng Shao",
      "De-Chuan Zhan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04427"
  },
  {
    "id": "arXiv:2210.04525",
    "title": "SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup  Training",
    "abstract": "Comments: COLING-2022, oral presentation",
    "descriptor": "\nComments: COLING-2022, oral presentation\n",
    "authors": [
      "Dan Qiao",
      "Chenchen Dai",
      "Yuyang Ding",
      "Juntao Li",
      "Qiang Chen",
      "Wenliang Chen",
      "Min Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04525"
  },
  {
    "id": "arXiv:2210.04555",
    "title": "Everything is Varied: The Surprising Impact of Individual Variation on  ML Robustness in Medicine",
    "abstract": "Everything is Varied: The Surprising Impact of Individual Variation on  ML Robustness in Medicine",
    "descriptor": "",
    "authors": [
      "Andrea Campagner",
      "Lorenzo Famiglini",
      "Anna Carobene",
      "Federico Cabitza"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04555"
  },
  {
    "id": "arXiv:2210.04590",
    "title": "The Small Solution Hypothesis for MAPF on Directed Graphs Is True",
    "abstract": "The Small Solution Hypothesis for MAPF on Directed Graphs Is True",
    "descriptor": "",
    "authors": [
      "Bernhard Nebel"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.04590"
  },
  {
    "id": "arXiv:2210.04610",
    "title": "Red-Teaming the Stable Diffusion Safety Filter",
    "abstract": "Red-Teaming the Stable Diffusion Safety Filter",
    "descriptor": "",
    "authors": [
      "Javier Rando",
      "Daniel Paleka",
      "David Lindner",
      "Lennard Heim",
      "Florian Tram\u00e8r"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04610"
  },
  {
    "id": "arXiv:2210.04624",
    "title": "WebCrowds: An Authoring Tool for Crowd Simulation",
    "abstract": "Comments: Accepted on the 2022 Brazilian Symposium on Games and Digital Entertainment (SBGames 2022 - this https URL). 7 pages, 4 figures, conference",
    "descriptor": "\nComments: Accepted on the 2022 Brazilian Symposium on Games and Digital Entertainment (SBGames 2022 - this https URL). 7 pages, 4 figures, conference\n",
    "authors": [
      "Gabriel Silva",
      "Paulo Knob",
      "Rubens Montanha",
      "Soraia Musse"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.04624"
  },
  {
    "id": "arXiv:2210.04709",
    "title": "Error analysis of a backward Euler positive preserving stabilized scheme  for a Chemotaxis system",
    "abstract": "Error analysis of a backward Euler positive preserving stabilized scheme  for a Chemotaxis system",
    "descriptor": "",
    "authors": [
      "Panagiotis Chatzipantelidis",
      "Christos Pervolianakis"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.04709"
  },
  {
    "id": "arXiv:2210.04710",
    "title": "Empowering the Fact-checkers! Automatic Identification of Claim Spans on  Twitter",
    "abstract": "Comments: Accepted at EMNLP22. 16 pages including Appendix",
    "descriptor": "\nComments: Accepted at EMNLP22. 16 pages including Appendix\n",
    "authors": [
      "Megha Sundriyal",
      "Atharva Kulkarni",
      "Vaibhav Pulastya",
      "Md Shad Akhtar",
      "Tanmoy Chakraborty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04710"
  },
  {
    "id": "arXiv:2210.04787",
    "title": "LMQFormer: A Laplace-Prior-Guided Mask Query Transformer for Lightweight  Snow Removal",
    "abstract": "Comments: 9 pages, 11 figures",
    "descriptor": "\nComments: 9 pages, 11 figures\n",
    "authors": [
      "Junhong Lin",
      "Nanfeng Jiang",
      "Zhentao Zhang",
      "Weiling Chen",
      "Tiesong Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.04787"
  },
  {
    "id": "arXiv:2210.04834",
    "title": "Knowledge Distillation Transfer Sets and their Impact on Downstream NLU  Tasks",
    "abstract": "Comments: 7 pages, 2 figures, 2 tables (+ 4 tables in Appendix), Accepted to EMNLP 2022 (industry track)",
    "descriptor": "\nComments: 7 pages, 2 figures, 2 tables (+ 4 tables in Appendix), Accepted to EMNLP 2022 (industry track)\n",
    "authors": [
      "Charith Peris",
      "Lizhen Tan",
      "Thomas Gueudre",
      "Turan Gojayev",
      "Pan Wei",
      "Gokmen Oz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04834"
  },
  {
    "id": "arXiv:2210.04885",
    "title": "What the DAAM: Interpreting Stable Diffusion Using Cross Attention",
    "abstract": "Comments: 5 pages, 5 figures",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Raphael Tang",
      "Akshat Pandey",
      "Zhiying Jiang",
      "Gefei Yang",
      "Karun Kumar",
      "Jimmy Lin",
      "Ferhan Ture"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04885"
  }
]