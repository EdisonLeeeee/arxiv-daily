[
  {
    "id": "arXiv:2210.01795",
    "title": "BayesFT: Bayesian Optimization for Fault Tolerant Neural Network  Architecture",
    "abstract": "To deploy deep learning algorithms on resource-limited scenarios, an emerging\ndevice-resistive random access memory (ReRAM) has been regarded as promising\nvia analog computing. However, the practicability of ReRAM is primarily limited\ndue to the weight drifting of ReRAM neural networks due to multi-factor\nreasons, including manufacturing, thermal noises, and etc. In this paper, we\npropose a novel Bayesian optimization method for fault tolerant neural network\narchitecture (BayesFT). For neural architecture search space design, instead of\nconducting neural architecture search on the whole feasible neural architecture\nsearch space, we first systematically explore the weight drifting tolerance of\ndifferent neural network components, such as dropout, normalization, number of\nlayers, and activation functions in which dropout is found to be able to\nimprove the neural network robustness to weight drifting. Based on our\nanalysis, we propose an efficient search space by only searching for dropout\nrates for each layer. Then, we use Bayesian optimization to search for the\noptimal neural architecture robust to weight drifting. Empirical experiments\ndemonstrate that our algorithmic framework has outperformed the\nstate-of-the-art methods by up to 10 times on various tasks, such as image\nclassification and object detection.",
    "descriptor": "",
    "authors": [
      "Nanyang Ye",
      "Jingbiao Mei",
      "Zhicheng Fang",
      "Yuwen Zhang",
      "Ziqing Zhang",
      "Huaying Wu",
      "Xiaoyao Liang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01795"
  },
  {
    "id": "arXiv:2210.01796",
    "title": "Multi-objective Deep Data Generation with Correlated Property Control",
    "abstract": "Developing deep generative models has been an emerging field due to the\nability to model and generate complex data for various purposes, such as image\nsynthesis and molecular design. However, the advancement of deep generative\nmodels is limited by challenges to generate objects that possess multiple\ndesired properties: 1) the existence of complex correlation among real-world\nproperties is common but hard to identify; 2) controlling individual property\nenforces an implicit partially control of its correlated properties, which is\ndifficult to model; 3) controlling multiple properties under various manners\nsimultaneously is hard and under-explored. We address these challenges by\nproposing a novel deep generative framework that recovers semantics and the\ncorrelation of properties through disentangled latent vectors. The correlation\nis handled via an explainable mask pooling layer, and properties are precisely\nretained by generated objects via the mutual dependence between latent vectors\nand properties. Our generative model preserves properties of interest while\nhandling correlation and conflicts of properties under a multi-objective\noptimization framework. The experiments demonstrate our model's superior\nperformance in generating data with desired properties.",
    "descriptor": "",
    "authors": [
      "Shiyu Wang",
      "Xiaojie Guo",
      "Xuanyang Lin",
      "Bo Pan",
      "Yuanqi Du",
      "Yinkai Wang",
      "Yanfang Ye",
      "Ashley Ann Petersen",
      "Austin Leitgeb",
      "Saleh AlKhalifa",
      "Kevin Minbiole",
      "Bill Wuest",
      "Amarda Shehu",
      "Liang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01796"
  },
  {
    "id": "arXiv:2210.01797",
    "title": "Ten Years after ImageNet: A 360\u00b0 Perspective on AI",
    "abstract": "It is ten years since neural networks made their spectacular comeback.\nPrompted by this anniversary, we take a holistic perspective on Artificial\nIntelligence (AI). Supervised Learning for cognitive tasks is effectively\nsolved - provided we have enough high-quality labeled data. However, deep\nneural network models are not easily interpretable, and thus the debate between\nblackbox and whitebox modeling has come to the fore. The rise of attention\nnetworks, self-supervised learning, generative modeling, and graph neural\nnetworks has widened the application space of AI. Deep Learning has also\npropelled the return of reinforcement learning as a core building block of\nautonomous decision making systems. The possible harms made possible by new AI\ntechnologies have raised socio-technical issues such as transparency, fairness,\nand accountability. The dominance of AI by Big-Tech who control talent,\ncomputing resources, and most importantly, data may lead to an extreme AI\ndivide. Failure to meet high expectations in high profile, and much heralded\nflagship projects like self-driving vehicles could trigger another AI winter.",
    "descriptor": "",
    "authors": [
      "Sanjay Chawla",
      "Preslav Nakov",
      "Ahmed Ali",
      "Wendy Hall",
      "Issa Khalil",
      "Xiaosong Ma",
      "Husrev Taha Sencar",
      "Ingmar Weber",
      "Michael Wooldridge",
      "Ting Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.01797"
  },
  {
    "id": "arXiv:2210.01798",
    "title": "Latent Hierarchical Causal Structure Discovery with Rank Constraints",
    "abstract": "Most causal discovery procedures assume that there are no latent confounders\nin the system, which is often violated in real-world problems. In this paper,\nwe consider a challenging scenario for causal structure identification, where\nsome variables are latent and they form a hierarchical graph structure to\ngenerate the measured variables; the children of latent variables may still be\nlatent and only leaf nodes are measured, and moreover, there can be multiple\npaths between every pair of variables (i.e., it is beyond tree structure). We\npropose an estimation procedure that can efficiently locate latent variables,\ndetermine their cardinalities, and identify the latent hierarchical structure,\nby leveraging rank deficiency constraints over the measured variables. We show\nthat the proposed algorithm can find the correct Markov equivalence class of\nthe whole graph asymptotically under proper restrictions on the graph\nstructure.",
    "descriptor": "",
    "authors": [
      "Biwei Huang",
      "Charles Jia Han Low",
      "Feng Xie",
      "Clark Glymour",
      "Kun Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01798"
  },
  {
    "id": "arXiv:2210.01799",
    "title": "STGIN: A Spatial Temporal Graph-Informer Network for Long Sequence  Traffic Speed Forecasting",
    "abstract": "Accurate long series forecasting of traffic information is critical for the\ndevelopment of intelligent traffic systems. We may benefit from the rapid\ngrowth of neural network analysis technology to better understand the\nunderlying functioning patterns of traffic networks as a result of this\nprogress. Due to the fact that traffic data and facility utilization\ncircumstances are sequentially dependent on past and present situations,\nseveral related neural network techniques based on temporal dependency\nextraction models have been developed to solve the problem. The complicated\ntopological road structure, on the other hand, amplifies the effect of spatial\ninterdependence, which cannot be captured by pure temporal extraction\napproaches. Additionally, the typical Deep Recurrent Neural Network (RNN)\ntopology has a constraint on global information extraction, which is required\nfor comprehensive long-term prediction. This study proposes a new\nspatial-temporal neural network architecture, called Spatial-Temporal\nGraph-Informer (STGIN), to handle the long-term traffic parameters forecasting\nissue by merging the Informer and Graph Attention Network (GAT) layers for\nspatial and temporal relationships extraction. The attention mechanism\npotentially guarantees long-term prediction performance without significant\ninformation loss from distant inputs. On two real-world traffic datasets with\nvarying horizons, experimental findings validate the long sequence prediction\nabilities, and further interpretation is provided.",
    "descriptor": "\nComments: 12 pages, 18 figures and 2 tables\n",
    "authors": [
      "Ruikang Luo",
      "Yaofeng Song",
      "Liping Huang",
      "Yicheng Zhang",
      "Rong Su"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01799"
  },
  {
    "id": "arXiv:2210.01800",
    "title": "Bayesian Q-learning With Imperfect Expert Demonstrations",
    "abstract": "Guided exploration with expert demonstrations improves data efficiency for\nreinforcement learning, but current algorithms often overuse expert\ninformation. We propose a novel algorithm to speed up Q-learning with the help\nof a limited amount of imperfect expert demonstrations. The algorithm avoids\nexcessive reliance on expert data by relaxing the optimal expert assumption and\ngradually reducing the usage of uninformative expert data. Experimentally, we\nevaluate our approach on a sparse-reward chain environment and six more\ncomplicated Atari games with delayed rewards. With the proposed methods, we can\nachieve better results than Deep Q-learning from Demonstrations (Hester et al.,\n2017) in most environments.",
    "descriptor": "",
    "authors": [
      "Fengdi Che",
      "Xiru Zhu",
      "Doina Precup",
      "David Meger",
      "Gregory Dudek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01800"
  },
  {
    "id": "arXiv:2210.01801",
    "title": "Safe Reinforcement Learning From Pixels Using a Stochastic Latent  Representation",
    "abstract": "We address the problem of safe reinforcement learning from pixel\nobservations. Inherent challenges in such settings are (1) a trade-off between\nreward optimization and adhering to safety constraints, (2) partial\nobservability, and (3) high-dimensional observations. We formalize the problem\nin a constrained, partially observable Markov decision process framework, where\nan agent obtains distinct reward and safety signals. To address the curse of\ndimensionality, we employ a novel safety critic using the stochastic latent\nactor-critic (SLAC) approach. The latent variable model predicts rewards and\nsafety violations, and we use the safety critic to train safe policies. Using\nwell-known benchmark environments, we demonstrate competitive performance over\nexisting approaches with respects to computational requirements, final reward\nreturn, and satisfying the safety constraints.",
    "descriptor": "",
    "authors": [
      "Yannick Hogewind",
      "Thiago D. Simao",
      "Tal Kachman",
      "Nils Jansen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01801"
  },
  {
    "id": "arXiv:2210.01802",
    "title": "Alternating Differentiation for Optimization Layers",
    "abstract": "The idea of embedding optimization problems into deep neural networks as\noptimization layers to encode constraints and inductive priors has taken hold\nin recent years. Most existing methods focus on implicitly differentiating\nKarush-Kuhn-Tucker (KKT) conditions in a way that requires expensive\ncomputations on the Jacobian matrix, which can be slow and memory-intensive. In\nthis paper, we developed a new framework, named Alternating Differentiation\n(Alt-Diff), that differentiates optimization problems (here, specifically in\nthe form of convex optimization problems with polyhedral constraints) in a fast\nand recursive way. Alt-Diff decouples the differentiation procedure into a\nprimal update and a dual update in an alternating way. Accordingly, Alt-Diff\nsubstantially decreases the dimensions of the Jacobian matrix and thus\nsignificantly increases the computational speed of implicit differentiation.\nFurther, we present the computational complexity of the forward and backward\npass of Alt-Diff and show that Alt-Diff enjoys quadratic computational\ncomplexity in the backward pass. Another notable difference between Alt-Diff\nand state-of-the-arts is that Alt-Diff can be truncated for the optimization\nlayer. We theoretically show that: 1) Alt-Diff can converge to consistent\ngradients obtained by differentiating KKT conditions; 2) the error between the\ngradient obtained by the truncated Alt-Diff and by differentiating KKT\nconditions is upper bounded by the same order of variables' truncation error.\nTherefore, Alt-Diff can be truncated to further increases computational speed\nwithout sacrificing much accuracy. A series of comprehensive experiments\ndemonstrate that Alt-Diff yields results comparable to the state-of-the-arts in\nfar less time.",
    "descriptor": "",
    "authors": [
      "Haixiang Sun",
      "Ye Shi",
      "Jingya Wang",
      "Hoang Duong Tuan",
      "H. Vincent Poor",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.01802"
  },
  {
    "id": "arXiv:2210.01803",
    "title": "Federated Graph-based Networks with Shared Embedding",
    "abstract": "Nowadays, user privacy is becoming an issue that cannot be bypassed for\nsystem developers, especially for that of web applications where data can be\neasily transferred through internet. Thankfully, federated learning proposes an\ninnovative method to train models with distributed devices while data are kept\nin local storage. However, unlike general neural networks, although graph-based\nnetworks have achieved great success in classification tasks and advanced\nrecommendation system, its high performance relies on the rich context provided\nby a graph structure, which is vulnerable when data attributes are incomplete.\nTherefore, the latter becomes a realistic problem when implementing federated\nlearning for graph-based networks. Knowing that data embedding is a\nrepresentation in a different space, we propose our Federated Graph-based\nNetworks with Shared Embedding (Feras), which uses shared embedding data to\ntrain the network and avoids the direct sharing of original data. A solid\ntheoretical proof of the convergence of Feras is given in this work.\nExperiments on different datasets (PPI, Flickr, Reddit) are conducted to show\nthe efficiency of Feras for centralized learning. Finally, Feras enables the\ntraining of current graph-based models in the federated learning framework for\nprivacy concern.",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Tianyi Yu",
      "Pei Lai",
      "Fei Teng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01803"
  },
  {
    "id": "arXiv:2210.01805",
    "title": "CostNet: An End-to-End Framework for Goal-Directed Reinforcement  Learning",
    "abstract": "Reinforcement Learning (RL) is a general framework concerned with an agent\nthat seeks to maximize rewards in an environment. The learning typically\nhappens through trial and error using explorative methods, such as\nepsilon-greedy. There are two approaches, model-based and model-free\nreinforcement learning, that show concrete results in several disciplines.\nModel-based RL learns a model of the environment for learning the policy while\nmodel-free approaches are fully explorative and exploitative without\nconsidering the underlying environment dynamics. Model-free RL works\nconceptually well in simulated environments, and empirical evidence suggests\nthat trial and error lead to a near-optimal behavior with enough training. On\nthe other hand, model-based RL aims to be sample efficient, and studies show\nthat it requires far less training in the real environment for learning a good\npolicy.\nA significant challenge with RL is that it relies on a well-defined reward\nfunction to work well for complex environments and such a reward function is\nchallenging to define. Goal-Directed RL is an alternative method that learns an\nintrinsic reward function with emphasis on a few explored trajectories that\nreveals the path to the goal state.\nThis paper introduces a novel reinforcement learning algorithm for predicting\nthe distance between two states in a Markov Decision Process. The learned\ndistance function works as an intrinsic reward that fuels the agent's learning.\nUsing the distance-metric as a reward, we show that the algorithm performs\ncomparably to model-free RL while having significantly better\nsample-efficiently in several test environments.",
    "descriptor": "\nComments: 14 pages, 5 figures, In Proceedings of the International Conference on Innovative Techniques and Applications of Artificial Intelligence, SGAI2020\n",
    "authors": [
      "Per-Arne Andersen",
      "Morten Goodwin",
      "Ole-Christoffer Granmo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01805"
  },
  {
    "id": "arXiv:2210.01807",
    "title": "TripleE: Easy Domain Generalization via Episodic Replay",
    "abstract": "Learning how to generalize the model to unseen domains is an important area\nof research. In this paper, we propose TripleE, and the main idea is to\nencourage the network to focus on training on subsets (learning with replay)\nand enlarge the data space in learning on subsets. Learning with replay\ncontains two core designs, EReplayB and EReplayD, which conduct the replay\nschema on batch and dataset, respectively. Through this, the network can focus\non learning with subsets instead of visiting the global set at a glance,\nenlarging the model diversity in ensembling. To enlarge the data space in\nlearning on subsets, we verify that an exhaustive and singular augmentation\n(ESAug) performs surprisingly well on expanding the data space in subsets\nduring replays. Our model dubbed TripleE is frustratingly easy, based on simple\naugmentation and ensembling. Without bells and whistles, our TripleE method\nsurpasses prior arts on six domain generalization benchmarks, showing that this\napproach could serve as a stepping stone for future research in domain\ngeneralization.",
    "descriptor": "",
    "authors": [
      "Xiaomeng Li",
      "Hongyu Ren",
      "Huifeng Yao",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01807"
  },
  {
    "id": "arXiv:2210.01808",
    "title": "Maximum-Likelihood Inverse Reinforcement Learning with Finite-Time  Guarantees",
    "abstract": "Inverse reinforcement learning (IRL) aims to recover the reward function and\nthe associated optimal policy that best fits observed sequences of states and\nactions implemented by an expert. Many algorithms for IRL have an inherently\nnested structure: the inner loop finds the optimal policy given parametrized\nrewards while the outer loop updates the estimates towards optimizing a measure\nof fit. For high dimensional environments such nested-loop structure entails a\nsignificant computational burden. To reduce the computational burden of a\nnested loop, novel methods such as SQIL [1] and IQ-Learn [2] emphasize policy\nestimation at the expense of reward estimation accuracy. However, without\naccurate estimated rewards, it is not possible to do counterfactual analysis\nsuch as predicting the optimal policy under different environment dynamics\nand/or learning new tasks. In this paper we develop a novel single-loop\nalgorithm for IRL that does not compromise reward estimation accuracy. In the\nproposed algorithm, each policy improvement step is followed by a stochastic\ngradient step for likelihood maximization. We show that the proposed algorithm\nprovably converges to a stationary solution with a finite-time guarantee. If\nthe reward is parameterized linearly, we show the identified solution\ncorresponds to the solution of the maximum entropy IRL problem. Finally, by\nusing robotics control problems in MuJoCo and their transfer settings, we show\nthat the proposed algorithm achieves superior performance compared with other\nIRL and imitation learning benchmarks.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2210.01282\n",
    "authors": [
      "Siliang Zeng",
      "Chenliang Li",
      "Alfredo Garcia",
      "Mingyi Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01808"
  },
  {
    "id": "arXiv:2210.01820",
    "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision  Models",
    "abstract": "This paper presents MOAT, a family of neural networks that build on top of\nMObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the\ncurrent works that stack separate mobile convolution and transformer blocks, we\neffectively merge them into a MOAT block. Starting with a standard Transformer\nblock, we replace its multi-layer perceptron with a mobile convolution block,\nand further reorder it before the self-attention operation. The mobile\nconvolution block not only enhances the network representation capacity, but\nalso produces better downsampled features. Our conceptually simple MOAT\nnetworks are surprisingly effective, achieving 89.1% top-1 accuracy on\nImageNet-1K with ImageNet-22K pretraining. Additionally, MOAT can be seamlessly\napplied to downstream tasks that require large resolution inputs by simply\nconverting the global attention to window attention. Thanks to the mobile\nconvolution that effectively exchanges local information between pixels (and\nthus cross-windows), MOAT does not need the extra window-shifting mechanism. As\na result, on COCO object detection, MOAT achieves 59.2% box AP with 227M model\nparameters (single-scale inference, and hard NMS), and on ADE20K semantic\nsegmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale\ninference). Finally, the tiny-MOAT family, obtained by simply reducing the\nchannel sizes, also surprisingly outperforms several mobile-specific\ntransformer-based models on ImageNet. We hope our simple yet effective MOAT\nwill inspire more seamless integration of convolution and self-attention. Code\nis made publicly available.",
    "descriptor": "",
    "authors": [
      "Chenglin Yang",
      "Siyuan Qiao",
      "Qihang Yu",
      "Xiaoding Yuan",
      "Yukun Zhu",
      "Alan Yuille",
      "Hartwig Adam",
      "Liang-Chieh Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.01820"
  },
  {
    "id": "arXiv:2210.01834",
    "title": "Invariant Aggregator for Defending Federated Backdoor Attacks",
    "abstract": "Federated learning is gaining popularity as it enables training of\nhigh-utility models across several clients without directly sharing their\nprivate data. As a downside, the federated setting makes the model vulnerable\nto various adversarial attacks in the presence of malicious clients.\nSpecifically, an adversary can perform backdoor attacks to control model\npredictions via poisoning the training dataset with a trigger. In this work, we\npropose a mitigation for backdoor attacks in a federated learning setup. Our\nsolution forces the model optimization trajectory to focus on the invariant\ndirections that are generally useful for utility and avoid selecting directions\nthat favor few and possibly malicious clients. Concretely, we consider the sign\nconsistency of the pseudo-gradient (the client update) as an estimation of the\ninvariance. Following this, our approach performs dimension-wise filtering to\nremove pseudo-gradient elements with low sign consistency. Then, a robust mean\nestimator eliminates outliers among the remaining dimensions. Our theoretical\nanalysis further shows the necessity of the defense combination and illustrates\nhow our proposed solution defends the federated learning model. Empirical\nresults on three datasets with different modalities and varying number of\nclients show that our approach mitigates backdoor attacks with a negligible\ncost on the model utility.",
    "descriptor": "",
    "authors": [
      "Xiaoyang Wang",
      "Dimitrios Dimitriadis",
      "Sanmi Koyejo",
      "Shruti Tople"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.01834"
  },
  {
    "id": "arXiv:2210.01839",
    "title": "Reactive fungal insoles",
    "abstract": "Mycelium bound composites are promising materials for a diverse range of\napplications including wearables and building elements. Their functionality\nsurpasses some of the capabilities of traditionally passive materials, such as\nsynthetic fibres, reconstituted cellulose fibres and natural fibres. Thereby,\ncreating novel propositions including augmented functionality (sensory) and\naesthetic (personal fashion). Biomaterials can offer multiple modal sensing\ncapability such as mechanical loading (compressive and tensile) and moisture\ncontent. To assess the sensing potential of fungal insoles we undertook\nlaboratory experiments on electrical response of bespoke insoles made from\ncapillary matting colonised with oyster fungi Pleurotus ostreatus to\ncompressive stress which mimics human loading when standing and walking. We\nhave shown changes in electrical activity with compressive loading. The results\nadvance the development of intelligent sensing insoles which are a building\nblock towards more generic reactive fungal wearables. Using FitzhHugh-Nagumo\nmodel we numerically illustrated how excitation wave-fronts behave in a\nmycelium network colonising an insole and shown that it may be possible to\ndiscern pressure points from the mycelium electrical activity.",
    "descriptor": "",
    "authors": [
      "Anna Nikolaidou",
      "Neil Phillips",
      "Michail-Antisthenis Tsompanas",
      "Andrew Adamatzky"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.01839"
  },
  {
    "id": "arXiv:2210.01840",
    "title": "Detecting Anomalies within Smart Buildings using Do-It-Yourself Internet  of Things",
    "abstract": "Detecting anomalies at the time of happening is vital in environments like\nbuildings and homes to identify potential cyber-attacks. This paper discussed\nthe various mechanisms to detect anomalies as soon as they occur. We shed light\non crucial considerations when building machine learning models. We constructed\nand gathered data from multiple self-build (DIY) IoT devices with different\nin-situ sensors and found effective ways to find the point, contextual and\ncombine anomalies. We also discussed several challenges and potential solutions\nwhen dealing with sensing devices that produce data at different sampling rates\nand how we need to pre-process them in machine learning models. This paper also\nlooks at the pros and cons of extracting sub-datasets based on environmental\nconditions.",
    "descriptor": "\nComments: Journal of Ambient Intelligence and Humanized Computing (2022)\n",
    "authors": [
      "Yasar Majib",
      "Mahmoud Barhamgi",
      "Behzad Momahed Heravi",
      "Sharadha Kariyawasam",
      "Charith Perera"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01840"
  },
  {
    "id": "arXiv:2210.01841",
    "title": "Learning Perception-Aware Agile Flight in Cluttered Environments",
    "abstract": "Recently, neural control policies have outperformed existing model-based\nplanning-and-control methods for autonomously navigating quadrotors through\ncluttered environments in minimum time. However, they are not perception aware,\na crucial requirement in vision-based navigation due to the camera's limited\nfield of view and the underactuated nature of a quadrotor. We propose a method\nto learn neural network policies that achieve perception-aware, minimum-time\nflight in cluttered environments. Our method combines imitation learning and\nreinforcement learning (RL) by leveraging a privileged learning-by-cheating\nframework. Using RL, we first train a perception-aware teacher policy with\nfull-state information to fly in minimum time through cluttered environments.\nThen, we use imitation learning to distill its knowledge into a vision-based\nstudent policy that only perceives the environment via a camera. Our approach\ntightly couples perception and control, showing a significant advantage in\ncomputation speed (10x faster) and success rate. We demonstrate the closed-loop\ncontrol performance using a physical quadrotor and hardware-in-the-loop\nsimulation at speeds up to 50km/h.",
    "descriptor": "",
    "authors": [
      "Yunlong Song",
      "Kexin Shi",
      "Robert Penicka",
      "Davide Scaramuzza"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01841"
  },
  {
    "id": "arXiv:2210.01848",
    "title": "Explaining Patterns in Data with Language Models via Interpretable  Autoprompting",
    "abstract": "Large language models (LLMs) have displayed an impressive ability to harness\nnatural language to perform complex tasks. In this work, we explore whether we\ncan leverage this learned ability to find and explain patterns in data.\nSpecifically, given a pre-trained LLM and data examples, we introduce\ninterpretable autoprompting (iPrompt), an algorithm that generates a\nnatural-language string explaining the data. iPrompt iteratively alternates\nbetween generating explanations with an LLM and reranking them based on their\nperformance when used as a prompt. Experiments on a wide range of datasets,\nfrom synthetic mathematics to natural-language understanding, show that iPrompt\ncan yield meaningful insights by accurately finding groundtruth dataset\ndescriptions. Moreover, the prompts produced by iPrompt are simultaneously\nhuman-interpretable and highly effective for generalization: on real-world\nsentiment classification datasets, iPrompt produces prompts that match or even\nimprove upon human-written prompts for GPT-3. Finally, experiments with an fMRI\ndataset show the potential for iPrompt to aid in scientific discovery. All code\nfor using the methods and data here is made available on Github.",
    "descriptor": "\nComments: The two first authors contributed equally\n",
    "authors": [
      "Chandan Singh",
      "John X. Morris",
      "Jyoti Aneja",
      "Alexander M. Rush",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01848"
  },
  {
    "id": "arXiv:2210.01849",
    "title": "Link Partitioning on Simplicial Complexes Using Higher-Order Laplacians",
    "abstract": "Link partitioning is a popular approach in network science used for\ndiscovering overlapping communities by identifying clusters of strongly\nconnected links. Current link partitioning methods are specifically designed\nfor networks modelled by graphs representing pairwise relationships. Therefore,\nthese methods are incapable of utilizing higher-order information about group\ninteractions in network data which is increasingly available. Simplicial\ncomplexes extend the dyadic model of graphs and can model polyadic\nrelationships which are ubiquitous and crucial in many complex social and\ntechnological systems. In this paper, we introduce a link partitioning method\nthat leverages higher-order (i.e. triadic and higher) information in simplicial\ncomplexes for better community detection. Our method utilizes a novel random\nwalk on links of simplicial complexes defined by the higher-order Laplacian--a\ngeneralization of the graph Laplacian that incorporates polyadic relationships\nof the network. We transform this random walk into a graph-based random walk on\na lifted line graph--a dual graph in which links are nodes while nodes and\nhigher-order connections are links--and optimize for the standard notion of\nmodularity. We show that our method is guaranteed to provide interpretable link\npartitioning results under mild assumptions. We also offer new theoretical\nresults on the spectral properties of simplicial complexes by studying the\nspectrum of the link random walk. Experiment results on real-world community\ndetection tasks show that our higher-order approach significantly outperforms\nexisting graph-based link partitioning methods.",
    "descriptor": "\nComments: Accepted to 22nd IEEE International Conference on Data Mining (ICDM 2022)\n",
    "authors": [
      "Xinyi Wu",
      "Arnab Sarker",
      "Ali Jadbabaie"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Data Structures and Algorithms (cs.DS)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2210.01849"
  },
  {
    "id": "arXiv:2210.01853",
    "title": "Privacy-Patterns for IoT Application Developers",
    "abstract": "Designing Internet of things (IoT) applications (apps) is challenging due to\nthe heterogeneous nature of the systems on which these apps are deployed.\nPersonal data, often classified as sensitive, may be collected and analysed by\nIoT apps, where data privacy laws are expected to protect such information.\nVarious approaches already exist to support privacy-by-design (PbD) schemes,\nenabling developers to take data privacy into account at the design phase of\napplication development. However, developers are not widely adopting these\napproaches because of understandability and interpretation challenges. A\nlimited number of tools currently exist to assist developers in this context --\nleading to our proposal for \"PARROT\" (PrivAcy by design tool foR inteRnet Of\nThings). PARROT supports a number of techniques to enable PbD techniques to be\nmore widely used. We present the findings of a controlled study and discuss how\nthis privacy-preserving tool increases the ability of IoT developers to apply\nprivacy laws (such as GDPR) and privacy patterns. Our students demonstrate that\nthe PARROT prototype tool increases the awareness of privacy requirements in\ndesign and increases the likelihood of the subsequent design to be more\ncognisant of data privacy requirements.",
    "descriptor": "\nComments: In Adjunct Proceedings of the 2022 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp/ISWC '22)\n",
    "authors": [
      "Nada Alhirabi",
      "Stephanie Beaumont",
      "Omer Rana",
      "Charith Perera"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.01853"
  },
  {
    "id": "arXiv:2210.01855",
    "title": "Multifaceted Hierarchical Report Identification for Non-Functional Bugs  in Deep Learning Frameworks",
    "abstract": "Non-functional bugs (e.g., performance- or accuracy-related bugs) in Deep\nLearning (DL) frameworks can lead to some of the most devastating consequences.\nReporting those bugs on a repository such as GitHub is a standard route to fix\nthem. Yet, given the growing number of new GitHub reports for DL frameworks, it\nis intrinsically difficult for developers to distinguish those that reveal\nnon-functional bugs among the others, and assign them to the right contributor\nfor investigation in a timely manner. In this paper, we propose MHNurf - an\nend-to-end tool for automatically identifying non-functional bug related\nreports in DL frameworks. The core of MHNurf is a Multifaceted Hierarchical\nAttention Network (MHAN) that tackles three unaddressed challenges: (1)\nlearning the semantic knowledge, but doing so by (2) considering the hierarchy\n(e.g., words/tokens in sentences/statements) and focusing on the important\nparts (i.e., words, tokens, sentences, and statements) of a GitHub report,\nwhile (3) independently extracting information from different types of\nfeatures, i.e., content, comment, code, command, and label.\nTo evaluate MHNurf, we leverage 3,721 GitHub reports from five DL frameworks\nfor conducting experiments. The results show that MHNurf works the best with a\ncombination of content, comment, and code, which considerably outperforms the\nclassic HAN where only the content is used. MHNurf also produces significantly\nmore accurate results than nine other state-of-the-art classifiers with strong\nstatistical significance, i.e., up to 71% AUC improvement and has the best\nScott-Knott rank on four frameworks while 2nd on the remaining one. To\nfacilitate reproduction and promote future research, we have made our dataset,\ncode, and detailed supplementary results publicly available at:\nhttps://github.com/ideas-labo/APSEC2022-MHNurf.",
    "descriptor": "\nComments: Accepted at APSEC 2022\n",
    "authors": [
      "Guoming Long",
      "Tao Chen",
      "Georgina Cosma"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01855"
  },
  {
    "id": "arXiv:2210.01857",
    "title": "Centerpoints Are All You Need in Overhead Imagery",
    "abstract": "Labeling data to use for training object detectors is expensive and time\nconsuming. Publicly available overhead datasets for object detection are\nlabeled with image-aligned bounding boxes, object-aligned bounding boxes, or\nobject masks, but it is not clear whether such detailed labeling is necessary.\nTo test the idea, we developed novel single- and two-stage network\narchitectures that use centerpoints for labeling. In this paper we show that\nthese architectures achieve nearly equivalent performance to approaches using\nmore detailed labeling on three overhead object detection datasets.",
    "descriptor": "",
    "authors": [
      "James Mason Inder",
      "Mark Lowell",
      "Andrew J. Maltenfort"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01857"
  },
  {
    "id": "arXiv:2210.01858",
    "title": "Structural Balance Considerations for Networks with Preference Orders as  Node Attributes",
    "abstract": "We discuss possible definitions of structural balance conditions in a network\nwith preference orderings as node attributes. The main result is that for the\ncase with three alternatives ($A,B,C$) we reduce the $(3!)^3 = 216$ possible\nconfigurations of triangles to $10$ equivalence classes, and use these as\nmeasures of balance of a triangle towards possible extensions of structural\nbalance theory. Moreover, we derive a general formula for the number of\nequivalent classes for preferences on $n$ alternatives. Finally, we analyze a\nreal-world data set and compare its empirical distribution of triangle\nequivalence classes to a null hypothesis in which preferences are randomly\nassigned to the nodes.",
    "descriptor": "\nComments: Accepted for publication in the Asilomar Conference on Signals, Systems and Computers, 2022\n",
    "authors": [
      "Olle Abrahamsson",
      "Danyo Danev",
      "Erik G. Larsson"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.01858"
  },
  {
    "id": "arXiv:2210.01860",
    "title": "Efficient Prototype Selection via Multi-Armed Bandits",
    "abstract": "In this work, we propose a multi-armed bandit based framework for identifying\na compact set of informative data instances (i.e., the prototypes) that best\nrepresents a given target set. Prototypical examples of a given dataset offer\ninterpretable insights into the underlying data distribution and assist in\nexample-based reasoning, thereby influencing every sphere of human decision\nmaking. A key challenge is the large-scale setting, in which similarity\ncomparison between pairs of data points needs to be done for almost all\npossible pairs. We propose to overcome this limitation by employing stochastic\ngreedy search on the space of prototypical examples and multi-armed bandit\napproach for reducing the number of similarity comparisons. We analyze the\ntotal number of similarity comparisons needed by approach and provide an upper\nbound independent of the size of the target set.",
    "descriptor": "\nComments: Accepted to ACML 2022\n",
    "authors": [
      "Arghya Roy Chaudhuri",
      "Pratik Jawanpuria",
      "Bamdev Mishra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01860"
  },
  {
    "id": "arXiv:2210.01864",
    "title": "Recycling Scraps: Improving Private Learning by Leveraging Intermediate  Checkpoints",
    "abstract": "All state-of-the-art (SOTA) differentially private machine learning (DP ML)\nmethods are iterative in nature, and their privacy analyses allow publicly\nreleasing the intermediate training checkpoints. However, DP ML benchmarks, and\neven practical deployments, typically use only the final training checkpoint to\nmake predictions. In this work, for the first time, we comprehensively explore\nvarious methods that aggregate intermediate checkpoints to improve the utility\nof DP training. Empirically, we demonstrate that checkpoint aggregations\nprovide significant gains in the prediction accuracy over the existing SOTA for\nCIFAR10 and StackOverflow datasets, and that these gains get magnified in\nsettings with periodically varying training data distributions. For instance,\nwe improve SOTA StackOverflow accuracies to 22.7% (+0.43% absolute) for\n$\\epsilon=8.2$, and 23.84% (+0.43%) for $\\epsilon=18.9$. Theoretically, we show\nthat uniform tail averaging of checkpoints improves the empirical risk\nminimization bound compared to the last checkpoint of DP-SGD. Lastly, we\ninitiate an exploration into estimating the uncertainty that DP noise adds in\nthe predictions of DP ML models. We prove that, under standard assumptions on\nthe loss function, the sample variance from last few checkpoints provides a\ngood approximation of the variance of the final model of a DP run. Empirically,\nwe show that the last few checkpoints can provide a reasonable lower bound for\nthe variance of a converged DP model.",
    "descriptor": "",
    "authors": [
      "Virat Shejwalkar",
      "Arun Ganesh",
      "Rajiv Mathews",
      "Om Thakkar",
      "Abhradeep Thakurta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.01864"
  },
  {
    "id": "arXiv:2210.01868",
    "title": "Capturing and Animation of Body and Clothing from Monocular Video",
    "abstract": "While recent work has shown progress on extracting clothed 3D human avatars\nfrom a single image, video, or a set of 3D scans, several limitations remain.\nMost methods use a holistic representation to jointly model the body and\nclothing, which means that the clothing and body cannot be separated for\napplications like virtual try-on. Other methods separately model the body and\nclothing, but they require training from a large set of 3D clothed human meshes\nobtained from 3D/4D scanners or physics simulations. Our insight is that the\nbody and clothing have different modeling requirements. While the body is well\nrepresented by a mesh-based parametric 3D model, implicit representations and\nneural radiance fields are better suited to capturing the large variety in\nshape and appearance present in clothing. Building on this insight, we propose\nSCARF (Segmented Clothed Avatar Radiance Field), a hybrid model combining a\nmesh-based body with a neural radiance field. Integrating the mesh into the\nvolumetric rendering in combination with a differentiable rasterizer enables us\nto optimize SCARF directly from monocular videos, without any 3D supervision.\nThe hybrid modeling enables SCARF to (i) animate the clothed body avatar by\nchanging body poses (including hand articulation and facial expressions), (ii)\nsynthesize novel views of the avatar, and (iii) transfer clothing between\navatars in virtual try-on applications. We demonstrate that SCARF reconstructs\nclothing with higher visual quality than existing methods, that the clothing\ndeforms with changing body pose and body shape, and that clothing can be\nsuccessfully transferred between avatars of different subjects. The code and\nmodels are available at https://github.com/YadiraF/SCARF.",
    "descriptor": "\nComments: 7 pages main paper, 2 pages supp. mat\n",
    "authors": [
      "Yao Feng",
      "Jinlong Yang",
      "Marc Pollefeys",
      "Michael J. Black",
      "Timo Bolkart"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.01868"
  },
  {
    "id": "arXiv:2210.01869",
    "title": "Memory in humans and deep language models: Linking hypotheses for model  augmentation",
    "abstract": "The computational complexity of the self-attention mechanism in Transformer\nmodels significantly limits their ability to generalize over long temporal\ndurations. Memory-augmentation, or the explicit storing of past information in\nexternal memory for subsequent predictions, has become a constructive avenue\nfor mitigating this limitation. We argue that memory-augmented Transformers can\nbenefit substantially from considering insights from the memory literature in\nhumans. We detail an approach to integrating evidence from the human memory\nsystem through the specification of cross-domain linking hypotheses. We then\nprovide an empirical demonstration to evaluate the use of surprisal as a\nlinking hypothesis, and further identify the limitations of this approach to\ninform future research.",
    "descriptor": "\nComments: 5 figures\n",
    "authors": [
      "Omri Raccah",
      "Pheobe Chen",
      "Ted L. Willke",
      "David Poeppel",
      "Vy A. Vo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01869"
  },
  {
    "id": "arXiv:2210.01877",
    "title": "Towards Improving Faithfulness in Abstractive Summarization",
    "abstract": "Despite the success achieved in neural abstractive summarization based on\npre-trained language models, one unresolved issue is that the generated\nsummaries are not always faithful to the input document. There are two possible\ncauses of the unfaithfulness problem: (1) the summarization model fails to\nunderstand or capture the gist of the input text, and (2) the model over-relies\non the language model to generate fluent but inadequate words. In this work, we\npropose a Faithfulness Enhanced Summarization model (FES), which is designed\nfor addressing these two problems and improving faithfulness in abstractive\nsummarization. For the first problem, we propose to use question-answering (QA)\nto examine whether the encoder fully grasps the input document and can answer\nthe questions on the key information in the input. The QA attention on the\nproper input words can also be used to stipulate how the decoder should attend\nto the source. For the second problem, we introduce a max-margin loss defined\non the difference between the language and the summarization model, aiming to\nprevent the overconfidence of the language model. Extensive experiments on two\nbenchmark summarization datasets, CNN/DM and XSum, demonstrate that our model\nsignificantly outperforms strong baselines. The evaluation of factual\nconsistency also shows that our model generates more faithful summaries than\nbaselines.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Xiuying Chen",
      "Mingzhe Li",
      "Xin Gao",
      "Xiangliang Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.01877"
  },
  {
    "id": "arXiv:2210.01878",
    "title": "Opportunistic Qualitative Planning in Stochastic Systems with Incomplete  Preferences over Reachability Objectives",
    "abstract": "Preferences play a key role in determining what goals/constraints to satisfy\nwhen not all constraints can be satisfied simultaneously. In this paper, we\nstudy how to synthesize preference satisfying plans in stochastic systems,\nmodeled as an MDP, given a (possibly incomplete) combinative preference model\nover temporally extended goals. We start by introducing new semantics to\ninterpret preferences over infinite plays of the stochastic system. Then, we\nintroduce a new notion of improvement to enable comparison between two prefixes\nof an infinite play. Based on this, we define two solution concepts called safe\nand positively improving (SPI) and safe and almost-surely improving (SASI) that\nenforce improvements with a positive probability and with probability one,\nrespectively. We construct a model called an improvement MDP, in which the\nsynthesis of SPI and SASI strategies that guarantee at least one improvement\nreduces to computing positive and almost-sure winning strategies in an MDP. We\npresent an algorithm to synthesize the SPI and SASI strategies that induce\nmultiple sequential improvements. We demonstrate the proposed approach using a\nrobot motion planning problem.",
    "descriptor": "\nComments: 7 pages, 3 figures, under review for IEEE ACC 2023\n",
    "authors": [
      "Abhishek N. Kulkarni",
      "Jie Fu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.01878"
  },
  {
    "id": "arXiv:2210.01879",
    "title": "A Perceptual Quality Metric for Video Frame Interpolation",
    "abstract": "Research on video frame interpolation has made significant progress in recent\nyears. However, existing methods mostly use off-the-shelf metrics to measure\nthe quality of interpolation results with the exception of a few methods that\nemploy user studies, which is time-consuming. As video frame interpolation\nresults often exhibit unique artifacts, existing quality metrics sometimes are\nnot consistent with human perception when measuring the interpolation results.\nSome recent deep learning-based perceptual quality metrics are shown more\nconsistent with human judgments, but their performance on videos is compromised\nsince they do not consider temporal information. In this paper, we present a\ndedicated perceptual quality metric for measuring video frame interpolation\nresults. Our method learns perceptual features directly from videos instead of\nindividual frames. It compares pyramid features extracted from video frames and\nemploys Swin Transformer blocks-based spatio-temporal modules to extract\nspatio-temporal information. To train our metric, we collected a new video\nframe interpolation quality assessment dataset. Our experiments show that our\ndedicated quality metric outperforms state-of-the-art methods when measuring\nvideo frame interpolation results. Our code and model are made publicly\navailable at \\url{https://github.com/hqqxyy/VFIPS}.",
    "descriptor": "\nComments: ECCV 2022\n",
    "authors": [
      "Qiqi Hou",
      "Abhijay Ghildyal",
      "Feng Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.01879"
  },
  {
    "id": "arXiv:2210.01881",
    "title": "Uncertainty-Aware Meta-Learning for Multimodal Task Distributions",
    "abstract": "Meta-learning or learning to learn is a popular approach for learning new\ntasks with limited data (i.e., few-shot learning) by leveraging the\ncommonalities among different tasks. However, meta-learned models can perform\npoorly when context data is limited, or when data is drawn from an\nout-of-distribution (OoD) task. Especially in safety-critical settings, this\nnecessitates an uncertainty-aware approach to meta-learning. In addition, the\noften multimodal nature of task distributions can pose unique challenges to\nmeta-learning methods. In this work, we present UnLiMiTD (uncertainty-aware\nmeta-learning for multimodal task distributions), a novel method for\nmeta-learning that (1) makes probabilistic predictions on in-distribution tasks\nefficiently, (2) is capable of detecting OoD context data at test time, and (3)\nperforms on heterogeneous, multimodal task distributions. To achieve this goal,\nwe take a probabilistic perspective and train a parametric, tuneable\ndistribution over tasks on the meta-dataset. We construct this distribution by\nperforming Bayesian inference on a linearized neural network, leveraging\nGaussian process theory. We demonstrate that UnLiMiTD's predictions compare\nfavorably to, and outperform in most cases, the standard baselines, especially\nin the low-data regime. Furthermore, we show that UnLiMiTD is effective in\ndetecting data from OoD tasks. Finally, we confirm that both of these findings\ncontinue to hold in the multimodal task-distribution setting.",
    "descriptor": "\nComments: 21 pages, 10 figures\n",
    "authors": [
      "Cesar Almecija",
      "Apoorva Sharma",
      "Navid Azizan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01881"
  },
  {
    "id": "arXiv:2210.01882",
    "title": "A Collaborative Approach to the Analysis of the COVID-19 Response in  Africa",
    "abstract": "The COVID-19 crisis has emphasized the need for scientific methods such as\nmachine learning to speed up the discovery of solutions to the pandemic.\nHarnessing machine learning techniques requires quality data, skilled personnel\nand advanced compute infrastructure. In Africa, however, machine learning\ncompetencies and compute infrastructures are limited. This paper demonstrates a\ncross-border collaborative capacity building approach to the application of\nmachine learning techniques in discovering answers to COVID-19 questions.",
    "descriptor": "\nComments: Presented at the NeurIPS 2021 Workshop on Machine Learning for the Developing World\n",
    "authors": [
      "Sharon Okwako",
      "Irene Wanyana",
      "Alice Namale",
      "Betty Kivumbi Nannyonga",
      "Sekou L. Remy",
      "William Ogallo",
      "Susan Kizito",
      "Aisha Walcott-Bryant",
      "Rhoda Wanyenze"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.01882"
  },
  {
    "id": "arXiv:2210.01883",
    "title": "Contrastive Learning Can Find An Optimal Basis For Approximately  View-Invariant Functions",
    "abstract": "Contrastive learning is a powerful framework for learning self-supervised\nrepresentations that generalize well to downstream supervised tasks. We show\nthat multiple existing contrastive learning methods can be reinterpreted as\nlearning kernel functions that approximate a fixed positive-pair kernel. We\nthen prove that a simple representation obtained by combining this kernel with\nPCA provably minimizes the worst-case approximation error of linear predictors,\nunder a straightforward assumption that positive pairs have similar labels. Our\nanalysis is based on a decomposition of the target function in terms of the\neigenfunctions of a positive-pair Markov chain, and a surprising equivalence\nbetween these eigenfunctions and the output of Kernel PCA. We give\ngeneralization bounds for downstream linear prediction using our Kernel PCA\nrepresentation, and show empirically on a set of synthetic tasks that applying\nKernel PCA to contrastive learning models can indeed approximately recover the\nMarkov chain eigenfunctions, although the accuracy depends on the kernel\nparameterization as well as on the augmentation strength.",
    "descriptor": "",
    "authors": [
      "Daniel D. Johnson",
      "Ayoub El Hanchi",
      "Chris J. Maddison"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01883"
  },
  {
    "id": "arXiv:2210.01884",
    "title": "Self-supervised Pre-training for Semantic Segmentation in an Indoor  Scene",
    "abstract": "The ability to endow maps of indoor scenes with semantic information is an\nintegral part of robotic agents which perform different tasks such as target\ndriven navigation, object search or object rearrangement. The state-of-the-art\nmethods use Deep Convolutional Neural Networks (DCNNs) for predicting semantic\nsegmentation of an image as useful representation for these tasks. The accuracy\nof semantic segmentation depends on the availability and the amount of labeled\ndata from the target environment or the ability to bridge the domain gap\nbetween test and training environment. We propose RegConsist, a method for\nself-supervised pre-training of a semantic segmentation model, exploiting the\nability of the agent to move and register multiple views in the novel\nenvironment. Given the spatial and temporal consistency cues used for pixel\nlevel data association, we use a variant of contrastive learning to train a\nDCNN model for predicting semantic segmentation from RGB views in the target\nenvironment. The proposed method outperforms models pre-trained on ImageNet and\nachieves competitive performance when using models that are trained for exactly\nthe same task but on a different dataset. We also perform various ablation\nstudies to analyze and demonstrate the efficacy of our proposed method.",
    "descriptor": "",
    "authors": [
      "Sulabh Shrestha",
      "Yimeng Li",
      "Jana Kosecka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.01884"
  },
  {
    "id": "arXiv:2210.01886",
    "title": "Multi-view Human Body Mesh Translator",
    "abstract": "Existing methods for human mesh recovery mainly focus on single-view\nframeworks, but they often fail to produce accurate results due to the\nill-posed setup. Considering the maturity of the multi-view motion capture\nsystem, in this paper, we propose to solve the prior ill-posed problem by\nleveraging multiple images from different views, thus significantly enhancing\nthe quality of recovered meshes. In particular, we present a novel\n\\textbf{M}ulti-view human body \\textbf{M}esh \\textbf{T}ranslator (MMT) model\nfor estimating human body mesh with the help of vision transformer.\nSpecifically, MMT takes multi-view images as input and translates them to\ntargeted meshes in a single-forward manner. MMT fuses features of different\nviews in both encoding and decoding phases, leading to representations embedded\nwith global information. Additionally, to ensure the tokens are intensively\nfocused on the human pose and shape, MMT conducts cross-view alignment at the\nfeature level by projecting 3D keypoint positions to each view and enforcing\ntheir consistency in geometry constraints. Comprehensive experiments\ndemonstrate that MMT outperforms existing single or multi-view models by a\nlarge margin for human mesh recovery task, notably, 28.8\\% improvement in MPVE\nover the current state-of-the-art method on the challenging HUMBI dataset.\nQualitative evaluation also verifies the effectiveness of MMT in reconstructing\nhigh-quality human mesh. Codes will be made available upon acceptance.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Xiangjian Jiang",
      "Xuecheng Nie",
      "Zitian Wang",
      "Luoqi Liu",
      "Si Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01886"
  },
  {
    "id": "arXiv:2210.01887",
    "title": "Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose  Transfer by Permuting Textures",
    "abstract": "Human pose transfer aims to synthesize a new view of a person under a given\npose. Recent works achieve this via self-reconstruction, which disentangles\npose and texture features from the person image, then combines the two features\nto reconstruct the person. Such feature-level disentanglement is a difficult\nand ill-defined problem that could lead to loss of details and unwanted\nartifacts. In this paper, we propose a self-driven human pose transfer method\nthat permutes the textures at random, then reconstructs the image with a dual\nbranch attention to achieve image-level disentanglement and detail-preserving\ntexture transfer. We find that compared with feature-level disentanglement,\nimage-level disentanglement is more controllable and reliable. Furthermore, we\nintroduce a dual kernel encoder that gives different sizes of receptive fields\nin order to reduce the noise caused by permutation and thus recover clothing\ndetails while aligning pose and textures. Extensive experiments on DeepFashion\nand Market-1501 shows that our model improves the quality of generated images\nin terms of FID, LPIPS and SSIM over other self-driven methods, and even\noutperforming some fully-supervised methods. A user study also shows that among\nself-driven approaches, images generated by our method are preferred in 72% of\ncases over prior work.",
    "descriptor": "",
    "authors": [
      "Nannan Li",
      "Kevin J. Shih",
      "Bryan A. Plummer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.01887"
  },
  {
    "id": "arXiv:2210.01888",
    "title": "Bicriteria Approximation Algorithms for Priority Matroid Median",
    "abstract": "Fairness considerations have motivated new clustering problems and algorithms\nin recent years. In this paper we consider the Priority Matroid Median problem\nwhich generalizes the Priority $k$-Median problem that has recently been\nstudied. The input consists of a set of facilities $\\mathcal{F}$ and a set of\nclients $\\mathcal{C}$ that lie in a metric space $(\\mathcal{F} \\cup\n\\mathcal{C},d)$, and a matroid $\\mathcal{M}=(\\mathcal{F},\\mathcal{I})$ over the\nfacilities. In addition each client $j$ has a specified radius $r_j \\ge 0$ and\neach facility $i \\in \\mathcal{F}$ has an opening cost $f_i$. The goal is to\nchoose a subset $S \\subseteq \\mathcal{F}$ of facilities to minimize the\n$\\sum_{i \\in \\mathcal{F}} f_i + \\sum_{j \\in \\mathcal{C}} d(j,S)$ subject to two\nconstraints: (i) $S$ is an independent set in $\\mathcal{M}$ (that is $S \\in\n\\mathcal{I}$) and (ii) for each client $j$, its distance to an open facility is\nat most $r_j$ (that is, $d(j,S) \\le r_j$). For this problem we describe the\nfirst bicriteria $(c_1,c_2)$ approximations for fixed constants $c_1,c_2$: the\nradius constraints of the clients are violated by at most a factor of $c_1$ and\nthe objective cost is at most $c_2$ times the optimum cost. We also improve the\npreviously known bicriteria approximation for the uniform radius setting ($r_j\n:= L$ $\\forall j \\in \\mathcal{C}$).",
    "descriptor": "\nComments: 22 pages, 2 figures\n",
    "authors": [
      "Tanvi Bajpai",
      "Chandra Chekuri"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01888"
  },
  {
    "id": "arXiv:2210.01889",
    "title": "Optimizing Two-Truck Platooning with Deadlines",
    "abstract": "We study a transportation problem where two heavy-duty trucks travel across\nthe national highway from separate origins to destinations, subject to\nindividual deadline constraints. Our objective is to minimize their total fuel\nconsumption by jointly optimizing path planning, speed planning, and platooning\nconfiguration. Such a two-truck platooning problem is pervasive in practice yet\nchallenging to solve due to hard deadline constraints and enormous platooning\nconfigurations to consider. We first leverage a unique problem structure to\nsignificantly simplify platooning optimization and present a novel formulation.\nWe prove that the two-truck platooning problem is weakly NP-hard and admits a\nFully Polynomial Time Approximation Scheme (FPTAS). The FPTAS can achieve a\nfuel consumption within a ratio of $(1+\\epsilon)$ to the optimal (for any\n$\\epsilon>0$) with a time complexity polynomial in the size of the\ntransportation network and $1/\\epsilon$. These results are in sharp contrast to\nthe general multi-truck platooning problem, which is known to be APX-hard and\nrepels any FPTAS. As the FPTAS still incurs excessive running time for\nlarge-scale cases, we design an efficient dual-subgradient algorithm for\nsolving large-/national- scale instances. It is an iterative algorithm that\nalways converges. We prove that each iteration only incurs polynomial-time\ncomplexity, albeit it requires solving an integer linear programming problem\noptimally. We characterize a condition under which the algorithm generates an\noptimal solution and derive a posterior performance bound when the condition is\nnot met. Extensive simulations based on real-world traces show that our joint\nsolution of path planning, speed planning, and platooning saves up to $24\\%$\nfuel as compared to baseline alternatives.",
    "descriptor": "",
    "authors": [
      "Wenjie Xu",
      "Titing Cui",
      "Minghua Chen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.01889"
  },
  {
    "id": "arXiv:2210.01891",
    "title": "AdaWAC: Adaptively Weighted Augmentation Consistency Regularization for  Volumetric Medical Image Segmentation",
    "abstract": "Sample reweighting is an effective strategy for learning from training data\ncoming from a mixture of subpopulations. In volumetric medical image\nsegmentation, the data inputs are similarly distributed, but the associated\ndata labels fall into two subpopulations -- \"label-sparse\" and \"label-dense\" --\ndepending on whether the data image occurs near the beginning/end of the\nvolumetric scan or the middle. Existing reweighting algorithms have focused on\nhard- and soft- thresholding of the label-sparse data, which results in loss of\ninformation and reduced sample efficiency by discarding valuable data input.\nFor this setting, we propose AdaWAC as an adaptive weighting algorithm that\nintroduces a set of trainable weights which, at the saddle point of the\nunderlying objective, assigns label-dense samples to supervised cross-entropy\nloss and label-sparse samples to unsupervised consistency regularization. We\nprovide a convergence guarantee for AdaWAC by recasting the optimization as\nonline mirror descent on a saddle point problem. Moreover, we empirically\ndemonstrate that AdaWAC not only enhances segmentation performance and sample\nefficiency but also improves robustness to the subpopulation shift in labels.",
    "descriptor": "",
    "authors": [
      "Yijun Dong",
      "Yuege Xie",
      "Rachel Ward"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01891"
  },
  {
    "id": "arXiv:2210.01892",
    "title": "Polysemanticity and Capacity in Neural Networks",
    "abstract": "Individual neurons in neural networks often represent a mixture of unrelated\nfeatures. This phenomenon, called polysemanticity, can make interpreting neural\nnetworks more difficult and so we aim to understand its causes. We propose\ndoing so through the lens of feature \\emph{capacity}, which is the fractional\ndimension each feature consumes in the embedding space. We show that in a toy\nmodel the optimal capacity allocation tends to monosemantically represent the\nmost important features, polysemantically represent less important features (in\nproportion to their impact on the loss), and entirely ignore the least\nimportant features. Polysemanticity is more prevalent when the inputs have\nhigher kurtosis or sparsity and more prevalent in some architectures than\nothers. Given an optimal allocation of capacity, we go on to study the geometry\nof the embedding space. We find a block-semi-orthogonal structure, with\ndiffering block sizes in different models, highlighting the impact of model\narchitecture on the interpretability of its neurons.",
    "descriptor": "\nComments: 22 pages, 7 figures\n",
    "authors": [
      "Adam Scherlis",
      "Kshitij Sachan",
      "Adam S. Jermyn",
      "Joe Benton",
      "Buck Shlegeris"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01892"
  },
  {
    "id": "arXiv:2210.01897",
    "title": "The DAG Visit approach for Pebbling and I/O Lower Bounds",
    "abstract": "We introduce the notion of an $r$-visit of a Directed Acyclic Graph DAG\n$G=(V,E)$, a sequence of the vertices of the DAG complying with a given rule\n$r$. A rule $r$ specifies for each vertex $v\\in V$ a family of $r$-enabling\nsets of (immediate) predecessors: before visiting $v$, at least one of its\nenabling sets must have been visited. Special cases are the $r^{(top)}$-rule\n(or, topological rule), for which the only enabling set is the set of all\npredecessors and the $r^{(sin)}$-rule (or, singleton rule), for which the\nenabling sets are the singletons containing exactly one predecessor. The\n$r$-boundary complexity of a DAG $G$, $b_{r}\\left(G\\right)$, is the minimum\ninteger $b$ such that there is an $r$-visit where, at each stage, for at most\n$b$ of the vertices yet to be visited an enabling set has already been visited.\nBy a reformulation of known results, it is shown that the boundary complexity\nof a DAG $G$ is a lower bound to the pebbling number of the reverse DAG, $G^R$.\nSeveral known pebbling lower bounds can be cast in terms of the\n$r^{(sin)}$-boundary complexity.\nA visit partition technique for I/O lower bounds, which generalizes the\n$S$-partition I/O technique introduced by Hong and Kung in their classic paper\n\"I/O complexity: The Red-Blue pebble game\". The visit partition approach yields\ntight I/O bounds for some DAGs for which the $S$-partition technique can only\nyield an $\\Omega(1)$ lower bound.",
    "descriptor": "\nComments: Extended version of manuscript published in the Proceedings of FSTTCS22\n",
    "authors": [
      "Gianfranco Bilardi",
      "Lorenzo De Stefani"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.01897"
  },
  {
    "id": "arXiv:2210.01898",
    "title": "Reproducible Bandits",
    "abstract": "In this paper, we introduce the notion of reproducible policies in the\ncontext of stochastic bandits, one of the canonical problems in interactive\nlearning. A policy in the bandit environment is called reproducible if it\npulls, with high probability, the \\emph{exact} same sequence of arms in two\ndifferent and independent executions (i.e., under independent reward\nrealizations). We show that not only do reproducible policies exist, but also\nthey achieve almost the same optimal (non-reproducible) regret bounds in terms\nof the time horizon. More specifically, in the stochastic multi-armed bandits\nsetting, we develop a policy with an optimal problem-dependent regret bound\nwhose dependence on the reproducibility parameter is also optimal. Similarly,\nfor stochastic linear bandits (with finitely and infinitely many arms) we\ndevelop reproducible policies that achieve the best-known problem-independent\nregret bounds with an optimal dependency on the reproducibility parameter. Our\nresults show that even though randomization is crucial for the\nexploration-exploitation trade-off, an optimal balance can still be achieved\nwhile pulling the exact same arms in two different rounds of executions.",
    "descriptor": "",
    "authors": [
      "Hossein Esfandiari",
      "Alkis Kalavasis",
      "Amin Karbasi",
      "Andreas Krause",
      "Vahab Mirrokni",
      "Grigoris Velegkas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01898"
  },
  {
    "id": "arXiv:2210.01900",
    "title": "When would online platforms pay data dividends",
    "abstract": "Online platforms, including social media and search platforms, have routinely\nused their users' data for targeted ads, to improve their services, and to sell\nto third-party buyers. But an increasing awareness of the importance of users'\ndata privacy has led to new laws that regulate data-sharing by platforms.\nFurther, there have been political discussions on introducing data dividends,\nthat is paying users for their data. Three interesting questions are then: When\nwould these online platforms be incentivized to pay data dividends? How does\ntheir decision depend on whether users value their privacy more than the\nplatform's free services? And should platforms invest in protecting users'\ndata? This paper considers various factors affecting the users' and platform's\ndecisions through utility functions. We construct a principal-agent model using\na Stackelberg game to calculate their optimal decisions and qualitatively\ndiscuss the implications. Our results could inform a policymaker trying to\nunderstand the consequences of mandating data dividends.",
    "descriptor": "",
    "authors": [
      "Sukanya Kudva",
      "Anil Aswani"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.01900"
  },
  {
    "id": "arXiv:2210.01902",
    "title": "Increasing Data Equity Through Accessibility",
    "abstract": "This position statement is a response to the Office of Science and Technology\nPolicy's Request for Information on \"Equitable Data Engagement and\nAccountability.\" This response considers data equity specifically for people\nwith disabilities. The RFI asks \"how Federal agencies can better support\ncollaboration with other levels of government, civil society, and the research\ncommunity around the production and use of equitable data.\" We argue that one\ncritically underserved community in the context of data equity is people with\ndisabilities. Today's tools make it extremely difficult for disabled people to\n(1) interact with data and data visualizations and (2) take jobs that involve\nworking with and visualizing data. Yet access to such data is increasingly\ncritical, and integral, to engaging with government and civil society. We must\nchange the standards and expectations around data practices to include disabled\npeople and support the research necessary to achieve those goals.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Frank Elavsky",
      "Jennifer Mankoff",
      "Arvind Satyanarayan"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.01902"
  },
  {
    "id": "arXiv:2210.01905",
    "title": "Representing missing values through polar encoding",
    "abstract": "We propose polar encoding, a representation of categorical and numerical\n$[0,1]$-valued attributes with missing values that preserves the information\nencoded in the distribution of the missing values. Unlike the existing\nmissing-indicator approach, this does not require imputation. We support our\nproposal with three different arguments. Firstly, polar encoding ensures that\nmissing values become equidistant from all non-missing values by mapping the\nlatter onto the unit circle. Secondly, polar encoding lets decision trees\nchoose how missing values should be split, providing a practical realisation of\nthe missingness incorporated in attributes (MIA) proposal. And lastly, polar\nencoding corresponds to the normalised representation of categorical and\n$[0,1]$-valued attributes when viewed as barycentric attributes, a new concept\nbased on traditional barycentric coordinates. In particular, we show that\nbarycentric attributes are fuzzified categorical attributes, that their\nnormalised representation generalises one-hot encoding, and that the polar\nencoding of $[0, 1]$-valued attributes is analogous to the one-hot encoding of\nbinary attributes. With an experiment based on twenty real-life datasets with\nmissing values, we show that polar encoding performs about as well or better\nthan the missing-indicator approach in terms of the resulting classification\nperformance.",
    "descriptor": "",
    "authors": [
      "Oliver Urs Lenz",
      "Daniel Peralta",
      "Chris Cornelis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01905"
  },
  {
    "id": "arXiv:2210.01906",
    "title": "Tree Mover's Distance: Bridging Graph Metrics and Stability of Graph  Neural Networks",
    "abstract": "Understanding generalization and robustness of machine learning models\nfundamentally relies on assuming an appropriate metric on the data space.\nIdentifying such a metric is particularly challenging for non-Euclidean data\nsuch as graphs. Here, we propose a pseudometric for attributed graphs, the Tree\nMover's Distance (TMD), and study its relation to generalization. Via a\nhierarchical optimal transport problem, TMD reflects the local distribution of\nnode attributes as well as the distribution of local computation trees, which\nare known to be decisive for the learning behavior of graph neural networks\n(GNNs). First, we show that TMD captures properties relevant to graph\nclassification: a simple TMD-SVM performs competitively with standard GNNs.\nSecond, we relate TMD to generalization of GNNs under distribution shifts, and\nshow that it correlates well with performance drop under such shifts.",
    "descriptor": "",
    "authors": [
      "Ching-Yao Chuang",
      "Stefanie Jegelka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01906"
  },
  {
    "id": "arXiv:2210.01907",
    "title": "A Self-Play Posterior Sampling Algorithm for Zero-Sum Markov Games",
    "abstract": "Existing studies on provably efficient algorithms for Markov games (MGs)\nalmost exclusively build on the \"optimism in the face of uncertainty\" (OFU)\nprinciple. This work focuses on a different approach of posterior sampling,\nwhich is celebrated in many bandits and reinforcement learning settings but\nremains under-explored for MGs. Specifically, for episodic two-player zero-sum\nMGs, a novel posterior sampling algorithm is developed with general function\napproximation. Theoretical analysis demonstrates that the posterior sampling\nalgorithm admits a $\\sqrt{T}$-regret bound for problems with a low multi-agent\ndecoupling coefficient, which is a new complexity measure for MGs, where $T$\ndenotes the number of episodes. When specialized to linear MGs, the obtained\nregret bound matches the state-of-the-art results. To the best of our\nknowledge, this is the first provably efficient posterior sampling algorithm\nfor MGs with frequentist regret guarantees, which enriches the toolbox for MGs\nand promotes the broad applicability of posterior sampling.",
    "descriptor": "\nComments: Accepted to ICML 2022\n",
    "authors": [
      "Wei Xiong",
      "Han Zhong",
      "Chengshuai Shi",
      "Cong Shen",
      "Tong Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.01907"
  },
  {
    "id": "arXiv:2210.01908",
    "title": "Supervised Metric Learning for Retrieval via Contextual Similarity  Optimization",
    "abstract": "Existing deep metric learning approaches fall into three general categories:\ncontrastive learning, average precision (AP) maximization, and classification.\nWe propose a novel alternative approach, \\emph{contextual similarity\noptimization}, inspired by work in unsupervised metric learning. Contextual\nsimilarity is a discrete similarity measure based on relationships between\nneighborhood sets, and is widely used in the unsupervised setting as\npseudo-supervision. Inspired by this success, we propose a framework which\noptimizes \\emph{a combination of contextual and cosine similarities}.\nContextual similarity calculation involves several non-differentiable\noperations, including the heaviside function and intersection of sets. We show\nhow to circumvent non-differentiability to explicitly optimize contextual\nsimilarity, and we further incorporate appropriate similarity regularization to\nyield our novel metric learning loss. The resulting loss function achieves\nstate-of-the-art Recall @ 1 accuracy on standard supervised image retrieval\nbenchmarks when combined with the standard contrastive loss. Code is released\nhere:\n\\url{https://github.com/Chris210634/metric-learning-using-contextual-similarity}",
    "descriptor": "",
    "authors": [
      "Christopher Liao",
      "Theodoros Tsiligkaridis",
      "Brian Kulis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.01908"
  },
  {
    "id": "arXiv:2210.01910",
    "title": "Learning Signal Temporal Logic through Neural Network for Interpretable  Classification",
    "abstract": "Machine learning techniques using neural networks have achieved promising\nsuccess for time-series data classification. However, the models that they\nproduce are challenging to verify and interpret. In this paper, we propose an\nexplainable neural-symbolic framework for the classification of time-series\nbehaviors. In particular, we use an expressive formal language, namely Signal\nTemporal Logic (STL), to constrain the search of the computation graph for a\nneural network. We design a novel time function and sparse softmax function to\nimprove the soundness and precision of the neural-STL framework. As a result,\nwe can efficiently learn a compact STL formula for the classification of\ntime-series data through off-the-shelf gradient-based tools. We demonstrate the\ncomputational efficiency, compactness, and interpretability of the proposed\nmethod through driving scenarios and naval surveillance case studies, compared\nwith state-of-the-art baselines.",
    "descriptor": "",
    "authors": [
      "Danyang Li",
      "Mingyu Cai",
      "Cristian-Ioan Vasile",
      "Roberto Tron"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01910"
  },
  {
    "id": "arXiv:2210.01911",
    "title": "Grounding Language with Visual Affordances over Unstructured Data",
    "abstract": "Recent works have shown that Large Language Models (LLMs) can be applied to\nground natural language to a wide variety of robot skills. However, in\npractice, learning multi-task, language-conditioned robotic skills typically\nrequires large-scale data collection and frequent human intervention to reset\nthe environment or help correcting the current policies. In this work, we\npropose a novel approach to efficiently learn general-purpose\nlanguage-conditioned robot skills from unstructured, offline and reset-free\ndata in the real world by exploiting a self-supervised visuo-lingual affordance\nmodel, which requires annotating as little as 1% of the total data with\nlanguage. We evaluate our method in extensive experiments both in simulated and\nreal-world robotic tasks, achieving state-of-the-art performance on the\nchallenging CALVIN benchmark and learning over 25 distinct visuomotor\nmanipulation tasks with a single policy in the real world. We find that when\npaired with LLMs to break down abstract natural language instructions into\nsubgoals via few-shot prompting, our method is capable of completing\nlong-horizon, multi-tier tasks in the real world, while requiring an order of\nmagnitude less data than previous approaches. Code and videos are available at\nthis http URL",
    "descriptor": "\nComments: Project website: this http URL\n",
    "authors": [
      "Oier Mees",
      "Jessica Borja-Diaz",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01911"
  },
  {
    "id": "arXiv:2210.01913",
    "title": "Comparative Study of Blockchain Development Platforms: Features and  Applications",
    "abstract": "Many developers have ideas to create blockchain applications but do not know\nwhere to begin. Often these developers default to using the first blockchain\ndevelopment platform they discover, which may not be the best platform for\ntheir project. Over 8000 new blockchain-related projects are added to GitHub a\nyear. The near-constant influx of new projects can make it difficult for\ndevelopers to search through existing projects and platforms to find the best\nplatform for their projects. We considered 65 blockchain development platforms\nfor this work and provided a brief yet comprehensive summary of the 23 most\npopular platforms. Our aim for this work is to assist developers in selecting\nthe most appropriate platform for their blockchain projects.",
    "descriptor": "\nComments: 25 pages, 8 tables, 52 references, preprint\n",
    "authors": [
      "Collin Connors",
      "Dilip Sarkar"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.01913"
  },
  {
    "id": "arXiv:2210.01917",
    "title": "Dfferentiable Raycasting for Self-supervised Occupancy Forecasting",
    "abstract": "Motion planning for safe autonomous driving requires learning how the\nenvironment around an ego-vehicle evolves with time. Ego-centric perception of\ndriveable regions in a scene not only changes with the motion of actors in the\nenvironment, but also with the movement of the ego-vehicle itself.\nSelf-supervised representations proposed for large-scale planning, such as\nego-centric freespace, confound these two motions, making the representation\ndifficult to use for downstream motion planners. In this paper, we use\ngeometric occupancy as a natural alternative to view-dependent representations\nsuch as freespace. Occupancy maps naturally disentangle the motion of the\nenvironment from the motion of the ego-vehicle. However, one cannot directly\nobserve the full 3D occupancy of a scene (due to occlusion), making it\ndifficult to use as a signal for learning. Our key insight is to use\ndifferentiable raycasting to \"render\" future occupancy predictions into future\nLiDAR sweep predictions, which can be compared with ground-truth sweeps for\nself-supervised learning. The use of differentiable raycasting allows occupancy\nto emerge as an internal representation within the forecasting network. In the\nabsence of groundtruth occupancy, we quantitatively evaluate the forecasting of\nraycasted LiDAR sweeps and show improvements of upto 15 F1 points. For\ndownstream motion planners, where emergent occupancy can be directly used to\nguide non-driveable regions, this representation relatively reduces the number\nof collisions with objects by up to 17% as compared to freespace-centric motion\nplanners.",
    "descriptor": "\nComments: ECCV 2022. Code available at this https URL\n",
    "authors": [
      "Tarasha Khurana",
      "Peiyun Hu",
      "Achal Dave",
      "Jason ZIglar",
      "David Held",
      "Deva Ramanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.01917"
  },
  {
    "id": "arXiv:2210.01918",
    "title": "Non-Parametric and Regularized Dynamical Wasserstein Barycenters for  Time-Series Analysis",
    "abstract": "We consider probabilistic time-series models for systems that gradually\ntransition among a finite number of states, in contrast to the more commonly\nconsidered case where such transitions are abrupt or instantaneous. We are\nparticularly motivated by applications such as human activity analysis where\nthe observed time-series contains segments representing distinct activities\nsuch as running or walking as well as segments characterized by continuous\ntransition among these states. Accordingly, the dynamical Wasserstein\nbarycenter (DWB) model introduced in Cheng et al. in 2021 [1] associates with\neach state, which we call a pure state, its own probability distribution, and\nmodels these continuous transitions with the dynamics of the barycentric\nweights that combine the pure state distributions via the Wasserstein\nbarycenter. This is in contrast to methods that model these transitions with a\nmixture of the pure state distributions. Here, focusing on the univariate case\nwhere Wasserstein distances and barycenters can be computed in closed form, we\nextend [1] by discussing two challenges associated with learning a DWB model\nand two improvements. First, we highlight the issue of uniqueness in\nidentifying the model parameters. Secondly, we discuss the challenge of\nestimating a dynamically evolving distribution given a limited number of\nsamples. The uncertainty associated with this estimation may cause a model's\nlearned dynamics to not reflect the gradual transitions characteristic of the\nsystem. The first improvement introduces a regularization framework that\naddresses this uncertainty by imposing temporal smoothness on the dynamics of\nthe barycentric weights while leveraging the understanding of the\nnon-uniqueness of the problem. Our second improvement lifts the Gaussian\nassumption on the pure states distributions in [1] by proposing a\nquantile-based non-parametric representation.",
    "descriptor": "",
    "authors": [
      "Kevin C. Cheng",
      "Shuchin Aeron",
      "Michael C. Hughes",
      "Eric L. Miller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.01918"
  },
  {
    "id": "arXiv:2210.01919",
    "title": "Convex and Nonconvex Sublinear Regression with Application to  Data-driven Learning of Reach Sets",
    "abstract": "We consider estimating a compact set from finite data by approximating the\nsupport function of that set via sublinear regression. Support functions\nuniquely characterize a compact set up to closure of convexification, and are\nsublinear (convex as well as positive homogeneous of degree one). Conversely,\nany sublinear function is the support function of a compact set. We leverage\nthis property to transcribe the task of learning a compact set to that of\nlearning its support function. We propose two algorithms to perform the\nsublinear regression, one via convex and another via nonconvex programming. The\nconvex programming approach involves solving a quadratic program (QP) followed\nby a linear program (LP), and is referred to as QP-LP. The nonconvex\nprogramming approach involves training a input sublinear neural network. We\nillustrate the proposed methods via numerical examples on learning the reach\nsets of controlled dynamics subject to set-valued input uncertainties from\ntrajectory data.",
    "descriptor": "",
    "authors": [
      "Shadi Haddad",
      "Abhishek Halder"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.01919"
  },
  {
    "id": "arXiv:2210.01922",
    "title": "Semantics-aware Dataset Discovery from Data Lakes with Contextualized  Column-based Representation Learning",
    "abstract": "Dataset discovery from data lakes is essential in many real application\nscenarios. In this paper, we propose Starmie, an end-to-end framework for\ndataset discovery from data lakes (with table union search as the main use\ncase). Our proposed framework features a contrastive learning method to train\ncolumn encoders from pre-trained language models in a fully unsupervised\nmanner. The column encoder of Starmie captures the rich contextual semantic\ninformation within tables by leveraging a contrastive multi-column pre-training\nstrategy. We utilize the cosine similarity between column embedding vectors as\nthe column unionability score and propose a filter-and-verification framework\nthat allows exploring a variety of design choices to compute the unionability\nscore between two tables accordingly. Empirical evaluation results on real\ntable benchmark datasets show that Starmie outperforms the best-known solutions\nin the effectiveness of table union search by 6.8 in MAP and recall. Moreover,\nStarmie is the first to employ the HNSW (Hierarchical Navigable Small World)\nindex for accelerate query processing of table union search which provides a\n3,000X performance gain over the linear scan baseline and a 400X performance\ngain over an LSH index (the state-of-the-art solution for data lake indexing).",
    "descriptor": "",
    "authors": [
      "Grace Fan",
      "Jin Wang",
      "Yuliang Li",
      "Dan Zhang",
      "Ren\u00e9e Miller"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.01922"
  },
  {
    "id": "arXiv:2210.01927",
    "title": "Building a healthier feed: Private location trace intersection driven  feed recommendations",
    "abstract": "The physical environment you navigate strongly determines which communities\nand people matter most to individuals. These effects drive both personal access\nto opportunities and the social capital of communities, and can often be\nobserved in the personal mobility traces of individuals. Traditional social\nmedia feeds underutilize these mobility-based features, or do so in a privacy\nexploitative manner. Here we propose a consent-first private information\nsharing paradigm for driving social feeds from users' personal private data,\nspecifically using mobility traces. This approach designs the feed to\nexplicitly optimize for integrating the user into the local community and for\nsocial capital building through leveraging mobility trace overlaps as a proxy\nfor existing or potential real-world social connections, creating\nproportionality between whom a user sees in their feed, and whom the user is\nlikely to see in person. These claims are validated against existing\nsocial-mobility data, and a reference implementation of the proposed algorithm\nis built for demonstration. In total, this work presents a novel technique for\ndesigning feeds that represent real offline social connections through private\nset intersections requiring no third party, or public data exposure.",
    "descriptor": "",
    "authors": [
      "Tobin South",
      "Nick Lothian",
      "Alex \"Sandy\" Pentland"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.01927"
  },
  {
    "id": "arXiv:2210.01930",
    "title": "Benchmarking Learnt Radio Localisation under Distribution Shift",
    "abstract": "Deploying radio frequency (RF) localisation systems invariably entails\nnon-trivial effort, particularly for the latest learning-based breeds. There\nhas been little prior work on characterising and comparing how learnt localiser\nnetworks can be deployed in the field under real-world RF distribution shifts.\nIn this paper, we present RadioBench: a suite of 8 learnt localiser nets from\nthe state-of-the-art to study and benchmark their real-world deployability,\nutilising five novel industry-grade datasets. We train 10k models to analyse\nthe inner workings of these learnt localiser nets and uncover their differing\nbehaviours across three performance axes: (i) learning, (ii) proneness to\ndistribution shift, and (iii) localisation. We use insights gained from this\nanalysis to recommend best practices for the deployability of learning-based RF\nlocalisation under practical constraints.",
    "descriptor": "",
    "authors": [
      "Maximilian Arnold",
      "Mohammed Alloulah"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.01930"
  },
  {
    "id": "arXiv:2210.01932",
    "title": "Regression-Based Elastic Metric Learning on Shape Spaces of Elastic  Curves",
    "abstract": "We propose a new metric learning paradigm, Regression-based Elastic Metric\nLearning (REML), which optimizes the elastic metric for manifold regression on\nthe manifold of discrete curves. Our method recognizes that the \"ideal\" metric\nis trajectory-dependent and thus creates an opportunity for improved regression\nfit on trajectories of curves. When tested on cell shape trajectories, REML's\nlearned metric generates a better regression fit than the conventionally used\nsquare-root-velocity SRV metric.",
    "descriptor": "\nComments: 5 pages, 2 figures, derivations in appendix\n",
    "authors": [
      "Adele Myers",
      "Nina Miolane"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01932"
  },
  {
    "id": "arXiv:2210.01933",
    "title": "PreprintMatch: a tool for preprint publication detection applied to  analyze global inequities in scientific publishing",
    "abstract": "Preprints, versions of scientific manuscripts that precede peer review, are\ngrowing in popularity. They offer an opportunity to democratize and accelerate\nresearch, as they have no publication costs or a lengthy peer review process.\nPreprints are often later published in peer-reviewed venues, but these\npublications and the original preprints are frequently not linked in any way.\nTo this end, we developed a tool, PreprintMatch, to find matches between\npreprints and their corresponding published papers, if they exist. This tool\noutperforms existing techniques to match preprints and papers, both on matching\nperformance and speed. PreprintMatch was applied to search for matches between\npreprints (from bioRxiv and medRxiv), and PubMed. The preliminary nature of\npreprints offers a unique perspective into scientific projects at a relatively\nearly stage, and with better matching between preprint and paper, we explored\nquestions related to research inequity. We found that preprints from low income\ncountries are published as peer-reviewed papers at a lower rate than high\nincome countries (39.6\\% and 61.1\\%, respectively), and our data is consistent\nwith previous work that cite a lack of resources, lack of stability, and policy\nchoices to explain this discrepancy. Preprints from low income countries were\nalso found to be published quicker (178 vs 203 days) and with less title,\nabstract, and author similarity to the published version compared to high\nincome countries. Low income countries add more authors from the preprint to\nthe published version than high income countries (0.42 authors vs 0.32,\nrespectively), a practice that is significantly more frequent in China compared\nto similar countries. Finally, we find that some publishers publish work with\nauthors from lower income countries more frequently than others. PreprintMatch\nis available at \\url{https://github.com/PeterEckmann1/preprint-match}.",
    "descriptor": "\nComments: 16 pages, 6 figures\n",
    "authors": [
      "Peter Eckmann",
      "Anita Bandrowski"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.01933"
  },
  {
    "id": "arXiv:2210.01936",
    "title": "When and why vision-language models behave like bag-of-words models, and  what to do about it?",
    "abstract": "Despite the success of large vision and language models (VLMs) in many\ndownstream applications, it is unclear how well they encode compositional\ninformation. Here, we create the Attribution, Relation, and Order (ARO)\nbenchmark to systematically evaluate the ability of VLMs to understand\ndifferent types of relationships, attributes, and order. ARO consists of Visual\nGenome Attribution, to test the understanding of objects' properties; Visual\nGenome Relation, to test for relational understanding; and COCO &\nFlickr30k-Order, to test for order sensitivity. ARO is orders of magnitude\nlarger than previous benchmarks of compositionality, with more than 50,000 test\ncases. We show where state-of-the-art VLMs have poor relational understanding,\ncan blunder when linking objects to their attributes, and demonstrate a severe\nlack of order sensitivity. VLMs are predominantly trained and evaluated on\nlarge datasets with rich compositional structure in the images and captions.\nYet, training on these datasets has not been enough to address the lack of\ncompositional understanding, and evaluating on these datasets has failed to\nsurface this deficiency. To understand why these limitations emerge and are not\nrepresented in the standard tests, we zoom into the evaluation and training\nprocedures. We demonstrate that it is possible to perform well on retrieval\nover existing datasets without using the composition and order information.\nGiven that contrastive pretraining optimizes for retrieval on datasets with\nsimilar shortcuts, we hypothesize that this can explain why the models do not\nneed to learn to represent compositional information. This finding suggests a\nnatural solution: composition-aware hard negative mining. We show that a\nsimple-to-implement modification of contrastive learning significantly improves\nthe performance on tasks requiring understanding of order and compositionality.",
    "descriptor": "",
    "authors": [
      "Mert Yuksekgonul",
      "Federico Bianchi",
      "Pratyusha Kalluri",
      "Dan Jurafsky",
      "James Zou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01936"
  },
  {
    "id": "arXiv:2210.01940",
    "title": "On the Robustness of Deep Clustering Models: Adversarial Attacks and  Defenses",
    "abstract": "Clustering models constitute a class of unsupervised machine learning methods\nwhich are used in a number of application pipelines, and play a vital role in\nmodern data science. With recent advancements in deep learning -- deep\nclustering models have emerged as the current state-of-the-art over traditional\nclustering approaches, especially for high-dimensional image datasets. While\ntraditional clustering approaches have been analyzed from a robustness\nperspective, no prior work has investigated adversarial attacks and robustness\nfor deep clustering models in a principled manner. To bridge this gap, we\npropose a blackbox attack using Generative Adversarial Networks (GANs) where\nthe adversary does not know which deep clustering model is being used, but can\nquery it for outputs. We analyze our attack against multiple state-of-the-art\ndeep clustering models and real-world datasets, and find that it is highly\nsuccessful. We then employ some natural unsupervised defense approaches, but\nfind that these are unable to mitigate our attack. Finally, we attack Face++, a\nproduction-level face clustering API service, and find that we can\nsignificantly reduce its performance as well. Through this work, we thus aim to\nmotivate the need for truly robust deep clustering models.",
    "descriptor": "\nComments: Accepted to the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Anshuman Chhabra",
      "Ashwin Sekhari",
      "Prasant Mohapatra"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.01940"
  },
  {
    "id": "arXiv:2210.01941",
    "title": "SIMPLE: A Gradient Estimator for $k$-Subset Sampling",
    "abstract": "$k$-subset sampling is ubiquitous in machine learning, enabling\nregularization and interpretability through sparsity. The challenge lies in\nrendering $k$-subset sampling amenable to end-to-end learning. This has\ntypically involved relaxing the reparameterized samples to allow for\nbackpropagation, with the risk of introducing high bias and high variance. In\nthis work, we fall back to discrete $k$-subset sampling on the forward pass.\nThis is coupled with using the gradient with respect to the exact marginals,\ncomputed efficiently, as a proxy for the true gradient. We show that our\ngradient estimator, SIMPLE, exhibits lower bias and variance compared to\nstate-of-the-art estimators, including the straight-through Gumbel estimator\nwhen $k = 1$. Empirical results show improved performance on learning to\nexplain and sparse linear regression. We provide an algorithm for computing the\nexact ELBO for the $k$-subset distribution, obtaining significantly lower loss\ncompared to SOTA.",
    "descriptor": "",
    "authors": [
      "Kareem Ahmed",
      "Zhe Zeng",
      "Mathias Niepert",
      "Guy Van den Broeck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01941"
  },
  {
    "id": "arXiv:2210.01942",
    "title": "IGNiteR: News Recommendation in Microblogging Applications (Extended  Version)",
    "abstract": "News recommendation is one of the most challenging tasks in recommender\nsystems, mainly due to the ephemeral relevance of news to users. As social\nmedia, and particularly microblogging applications like Twitter or Weibo, gains\npopularity as platforms for news dissemination, personalized news\nrecommendation in this context becomes a significant challenge. We revisit news\nrecommendation in the microblogging scenario, by taking into consideration\nsocial interactions and observations tracing how the information that is up for\nrecommendation spreads in an underlying network. We propose a deep-learning\nbased approach that is diffusion and influence-aware, called Influence-Graph\nNews Recommender (IGNiteR). It is a content-based deep recommendation model\nthat jointly exploits all the data facets that may impact adoption decisions,\nnamely semantics, diffusion-related features pertaining to local and global\ninfluence among users, temporal attractiveness, and timeliness, as well as\ndynamic user preferences. To represent the news, a multi-level attention-based\nencoder is used to reveal the different interests of users. This news encoder\nrelies on a CNN for the news content and on an attentive LSTM for the diffusion\ntraces. For the latter, by exploiting previously observed news diffusions\n(cascades) in the microblogging medium, users are mapped to a latent space that\ncaptures potential influence on others or susceptibility of being influenced\nfor news adoptions. Similarly, a time-sensitive user encoder enables us to\ncapture the dynamic preferences of users with an attention-based bidirectional\nLSTM. We perform extensive experiments on two real-world datasets, showing that\nIGNiteR outperforms the state-of-the-art deep-learning based news\nrecommendation methods.",
    "descriptor": "\nComments: News recommendation, deep learning, diffusion\n",
    "authors": [
      "Yuting Feng",
      "Bogdan Cautis"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.01942"
  },
  {
    "id": "arXiv:2210.01944",
    "title": "A Framework for Large Scale Synthetic Graph Dataset Generation",
    "abstract": "Recently there has been increasing interest in developing and deploying deep\ngraph learning algorithms for many graph analysis tasks such as node and edge\nclassification, link prediction, and clustering with numerous practical\napplications such as fraud detection, drug discovery, or recommender systems.\nAllbeit there is a limited number of publicly available graph-structured\ndatasets, most of which are tiny compared to production-sized applications with\ntrillions of edges and billions of nodes. Further, new algorithms and models\nare benchmarked across similar datasets with similar properties. In this work,\nwe tackle this shortcoming by proposing a scalable synthetic graph generation\ntool that can mimic the original data distribution of real-world graphs and\nscale them to arbitrary sizes. This tool can be used then to learn a set of\nparametric models from proprietary datasets that can subsequently be released\nto researchers to study various graph methods on the synthetic data increasing\nprototype development and novel applications. Finally, the performance of the\ngraph learning algorithms depends not only on the size but also on the\ndataset's structure. We show how our framework generalizes across a set of\ndatasets, mimicking both structural and feature distributions as well as its\nscalability across varying dataset sizes.",
    "descriptor": "",
    "authors": [
      "Sajad Darabi",
      "Piotr Bigaj",
      "Dawid Majchrowski",
      "Pawel Morkisz",
      "Alex Fit-Florea"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.01944"
  },
  {
    "id": "arXiv:2210.01946",
    "title": "Affection: Learning Affective Explanations for Real-World Visual Data",
    "abstract": "In this work, we explore the emotional reactions that real-world images tend\nto induce by using natural language as the medium to express the rationale\nbehind an affective response to a given visual stimulus. To embark on this\njourney, we introduce and share with the research community a large-scale\ndataset that contains emotional reactions and free-form textual explanations\nfor 85,007 publicly available images, analyzed by 6,283 annotators who were\nasked to indicate and explain how and why they felt in a particular way when\nobserving a specific image, producing a total of 526,749 responses. Even though\nemotional reactions are subjective and sensitive to context (personal mood,\nsocial status, past experiences) - we show that there is significant common\nground to capture potentially plausible emotional responses with a large\nsupport in the subject population. In light of this crucial observation, we ask\nthe following questions: i) Can we develop multi-modal neural networks that\nprovide reasonable affective responses to real-world visual data, explained\nwith language? ii) Can we steer such methods towards producing explanations\nwith varying degrees of pragmatic language or justifying different emotional\nreactions while adapting to the underlying visual stimulus? Finally, iii) How\ncan we evaluate the performance of such methods for this novel task? With this\nwork, we take the first steps in addressing all of these questions, thus paving\nthe way for richer, more human-centric, and emotionally-aware image analysis\nsystems. Our introduced dataset and all developed methods are available on\nhttps://affective-explanations.org",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Panos Achlioptas",
      "Maks Ovsjanikov",
      "Leonidas Guibas",
      "Sergey Tulyakov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.01946"
  },
  {
    "id": "arXiv:2210.01951",
    "title": "Finite-Blocklength Results for the A-channel: Applications to Unsourced  Random Access and Group Testing",
    "abstract": "We present finite-blocklength achievability bounds for the unsourced\nA-channel. In this multiple-access channel, users noiselessly transmit\ncodewords picked from a common codebook with entries generated from a $q$-ary\nalphabet. At each channel use, the receiver observes the set of different\ntransmitted symbols but not their multiplicity. We show that the A-channel\nfinds applications in unsourced random-access (URA) and group testing.\nLeveraging the insights provided by the finite-blocklength bounds and the\nconnection between URA and non-adaptive group testing through the A-channel, we\npropose improved decoding methods for state-of-the-art A-channel codes and we\nshowcase how A-channel codes provide a new class of structured group testing\nmatrices. The developed bounds allow to evaluate the achievable error\nprobabilities of group testing matrices based on random A-channel codes for\narbitrary numbers of tests, items and defectives. We show that such a\nconstruction asymptotically achieves the optimal number of tests. In addition,\nevery efficiently decodable A-channel code can be used to construct a group\ntesting matrix with sub-linear recovery time.",
    "descriptor": "\nComments: 11 pages, 4 figures, extended version of the paper presented at the 58th Annual Allerton Conference on Communication, Control, and Computing (2022)\n",
    "authors": [
      "Alejandro Lancho",
      "Alexander Fengler",
      "Yury Polyanskiy"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.01951"
  },
  {
    "id": "arXiv:2210.01953",
    "title": "Robust Fair Clustering: A Novel Fairness Attack and Defense Framework",
    "abstract": "Clustering algorithms are widely used in many societal resource allocation\napplications, such as loan approvals and candidate recruitment, among others,\nand hence, biased or unfair model outputs can adversely impact individuals that\nrely on these applications. To this end, many fair clustering approaches have\nbeen recently proposed to counteract this issue. Due to the potential for\nsignificant harm, it is essential to ensure that fair clustering algorithms\nprovide consistently fair outputs even under adversarial influence. However,\nfair clustering algorithms have not been studied from an adversarial attack\nperspective. In contrast to previous research, we seek to bridge this gap and\nconduct a robustness analysis against fair clustering by proposing a novel\nblack-box fairness attack. Through comprehensive experiments, we find that\nstate-of-the-art models are highly susceptible to our attack as it can reduce\ntheir fairness performance significantly. Finally, we propose Consensus Fair\nClustering (CFC), the first robust fair clustering approach that transforms\nconsensus clustering into a fair graph partitioning problem, and iteratively\nlearns to generate fair cluster outputs. Experimentally, we observe that CFC is\nhighly robust to the proposed attack and is thus a truly robust fair clustering\nalternative.",
    "descriptor": "",
    "authors": [
      "Anshuman Chhabra",
      "Peizhao Li",
      "Prasant Mohapatra",
      "Hongfu Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.01953"
  },
  {
    "id": "arXiv:2210.01954",
    "title": "Rectangular Ruler Wrapping",
    "abstract": "In 1985 Hopcroft, Joseph and Whitesides introduced the problem of folding a\ncarpenter's ruler with $n$ hinged segments of lengths $\\ell_1, \\ldots, \\ell_n$\ninto an interval of a given length $k$. By {\\em folding} we mean that each\nhinge must be either straight or folded 180 degrees, with folded hinges\nalternating between being folded clockwise and counterclockwise. They showed\nthis problem is NP-hard in the weak sense by a reduction from {\\sc Partition},\ngave a pseudo-polynomial-time algorithm for it, and gave an $O (n)$-time\n2-approximation algorithm. Gagie, Saeidi and Sapucaia recently considered the\nrelated problem of wrapping the ruler into the interval instead of folding it.\nBy {\\em wrapping} we mean that all the folded hinges must be folded 180 degrees\nclockwise. They gave an $O (n)$-time algorithm for this problem, based on the\n$O (n \\log n)$-time algorithm for {\\sc Longest Increasing Subsequence} that\nFredman analyzed and attributed to Knuth, thus answering a question posed by\nO'Rourke during the open-problems session of CCCG '21.\nHopcroft et al.\\ and Gagie et al.\\ assume the ruler's segments are lines and\nits hinges are points, so a folded ruler has length but zero width. As anyone\nwho has ever folded something knows, however, making perfect 180-degree folds\nis not always possible. In this paper we initiate the study of {\\em\nrectangular} ruler wrapping, for which all folded hinges are to be folded 90\ndegrees clockwise instead of 180 degrees and we want the wrapped ruler to fit\nin a rectangle of height $h$ and width $w$. We give a quartic algorithm and\nthen an $O (n^2\\,\\mathrm{polylog} (n))$-time algorithm.",
    "descriptor": "",
    "authors": [
      "Xing Lyu",
      "Travis Gagie"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.01954"
  },
  {
    "id": "arXiv:2210.01955",
    "title": "Learning Dynamic Abstract Representations for Sample-Efficient  Reinforcement Learning",
    "abstract": "In many real-world problems, the learning agent needs to learn a problem's\nabstractions and solution simultaneously. However, most such abstractions need\nto be designed and refined by hand for different problems and domains of\napplication. This paper presents a novel top-down approach for constructing\nstate abstractions while carrying out reinforcement learning. Starting with\nstate variables and a simulator, it presents a novel domain-independent\napproach for dynamically computing an abstraction based on the dispersion of\nQ-values in abstract states as the agent continues acting and learning.\nExtensive empirical evaluation on multiple domains and problems shows that this\napproach automatically learns abstractions that are finely-tuned to the\nproblem, yield powerful sample efficiency, and result in the RL agent\nsignificantly outperforming existing approaches.",
    "descriptor": "",
    "authors": [
      "Mehdi Dadvar",
      "Rashmeet Kaur Nayyar",
      "Siddharth Srivastava"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01955"
  },
  {
    "id": "arXiv:2210.01959",
    "title": "Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot  Document-Level Question Answering",
    "abstract": "Businesses generate thousands of documents that communicate their strategic\nvision and provide details of key products, services, entities, and processes.\nKnowledge workers then face the laborious task of reading these documents to\nidentify, extract, and synthesize information relevant to their organizational\ngoals. To automate information gathering, question answering (QA) offers a\nflexible framework where human-posed questions can be adapted to extract\ndiverse knowledge. Finetuning QA systems requires access to labeled data\n(tuples of context, question, and answer). However, data curation for document\nQA is uniquely challenging because the context (i.e., answer evidence passage)\nneeds to be retrieved from potentially long, ill-formatted documents. Existing\nQA datasets sidestep this challenge by providing short, well-defined contexts\nthat are unrealistic in real-world applications. We present a three-stage\ndocument QA approach: (1) text extraction from PDF; (2) evidence retrieval from\nextracted texts to form well-posed contexts; (3) QA to extract knowledge from\ncontexts to return high-quality answers - extractive, abstractive, or Boolean.\nUsing QASPER as a surrogate to our proprietary data, our\ndetect-retrieve-comprehend (DRC) system achieves a +6.25 improvement in\nAnswer-F1 over existing baselines while delivering superior context selection.\nOur results demonstrate that DRC holds tremendous promise as a flexible\nframework for practical document QA.",
    "descriptor": "",
    "authors": [
      "Tavish McDonald",
      "Brian Tsan",
      "Amar Saini",
      "Juanita Ordonez",
      "Luis Gutierrez",
      "Phan Nguyen",
      "Blake Mason",
      "Brenda Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01959"
  },
  {
    "id": "arXiv:2210.01961",
    "title": "Split Federated Learning on Micro-controllers: A Keyword Spotting  Showcase",
    "abstract": "Nowadays, AI companies improve service quality by aggressively collecting\nusers' data generated by edge devices, which jeopardizes data privacy. To\nprevent this, Federated Learning is proposed as a private learning scheme,\nusing which users can locally train the model without collecting users' raw\ndata to servers. However, for machine-learning applications on edge devices\nthat have hard memory constraints, implementing a large model using FL is\ninfeasible. To meet the memory requirement, a recent collaborative learning\nscheme named split federal learning is a potential solution since it keeps a\nsmall model on the device and keeps the rest of the model on the server. In\nthis work, we implement a simply SFL framework on the Arduino board and verify\nits correctness on the Chinese digits audio dataset for keyword spotting\napplication with over 90% accuracy. Furthermore, on the English digits audio\ndataset, our SFL implementation achieves 13.89% higher accuracy compared to a\nstate-of-the-art FL implementation.",
    "descriptor": "\nComments: A demo of SFL for MCU (preliminary work)\n",
    "authors": [
      "Jingtao Li",
      "Runcong Kuang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.01961"
  },
  {
    "id": "arXiv:2210.01963",
    "title": "COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge  and Inheritance in Pre-trained Language Models",
    "abstract": "A characteristic feature of human semantic memory is its ability to not only\nstore and retrieve the properties of concepts observed through experience, but\nto also facilitate the inheritance of properties (can breathe) from\nsuperordinate concepts (animal) to their subordinates (dog) -- i.e. demonstrate\nproperty inheritance. In this paper, we present COMPS, a collection of minimal\npair sentences that jointly tests pre-trained language models (PLMs) on their\nability to attribute properties to concepts and their ability to demonstrate\nproperty inheritance behavior. Analyses of 22 different PLMs on COMPS reveal\nthat they can easily distinguish between concepts on the basis of a property\nwhen they are trivially different, but find it relatively difficult when\nconcepts are related on the basis of nuanced knowledge representations.\nFurthermore, we find that PLMs can demonstrate behavior consistent with\nproperty inheritance to a great extent, but fail in the presence of distracting\ninformation, which decreases the performance of many models, sometimes even\nbelow chance. This lack of robustness in demonstrating simple reasoning raises\nimportant questions about PLMs' capacity to make correct inferences even when\nthey appear to possess the prerequisite knowledge.",
    "descriptor": "\nComments: WIP; to be submitted to [REDACTED]\n",
    "authors": [
      "Kanishka Misra",
      "Julia Taylor Rayz",
      "Allyson Ettinger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01963"
  },
  {
    "id": "arXiv:2210.01964",
    "title": "The Calibration Generalization Gap",
    "abstract": "Calibration is a fundamental property of a good predictive model: it requires\nthat the model predicts correctly in proportion to its confidence. Modern\nneural networks, however, provide no strong guarantees on their calibration --\nand can be either poorly calibrated or well-calibrated depending on the\nsetting. It is currently unclear which factors contribute to good calibration\n(architecture, data augmentation, overparameterization, etc), though various\nclaims exist in the literature.\nWe propose a systematic way to study the calibration error: by decomposing it\ninto (1) calibration error on the train set, and (2) the calibration\ngeneralization gap. This mirrors the fundamental decomposition of\ngeneralization. We then investigate each of these terms, and give empirical\nevidence that (1) DNNs are typically always calibrated on their train set, and\n(2) the calibration generalization gap is upper-bounded by the standard\ngeneralization gap. Taken together, this implies that models with small\ngeneralization gap (|Test Error - Train Error|) are well-calibrated. This\nperspective unifies many results in the literature, and suggests that\ninterventions which reduce the generalization gap (such as adding data, using\nheavy augmentation, or smaller model size) also improve calibration. We thus\nhope our initial study lays the groundwork for a more systematic and\ncomprehensive understanding of the relation between calibration,\ngeneralization, and optimization.",
    "descriptor": "\nComments: Appeared at ICML 2022 Workshop on Distribution-Free Uncertainty Quantification\n",
    "authors": [
      "Annabelle Carrell",
      "Neil Mallinar",
      "James Lucas",
      "Preetum Nakkiran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01964"
  },
  {
    "id": "arXiv:2210.01966",
    "title": "Joint Reconfigurable Intelligent Surface Location and Passive  Beamforming Optimization for Maximizing the Secrecy-Rate",
    "abstract": "The physical layer security (PLS) is investigated for reconfigurable\nintelligent surface (RIS) assisted wireless networks, where a source transmits\nits confidential information to a legitimate destination with the aid of a\nsingle small RIS in the presence of a malicious eavesdropper. A new joint RIS\nlocation and passive beamforming (J-LPB) optimization scheme is proposed for\nthe sake of maximizing the secrecy rate under the RIS location constraint and\nthe constraint that the modulus of the reflecting coefficient at each RIS's\nunit is not larger than 1. Specifically, we analyze the optimal location of the\nRIS, and conclude that the product involving the source-RIS distance and the\nRIS-destination distance should be minimized. Since the product minimization\nproblem is nonconvex, we then propose a two-tier optimization algorithm for\nsolving it. Based on the near-optimal RIS 3D location obtained, we further\nformulate the passive beamforming optimization problem, and then propose to\napply the Charnes-Cooper transformation along with the sequential rank-one\nconstraint relaxation (SROCR) algorithm to solve it. Our numerical results show\nthat the secrecy rate of the proposed J-LPB optimization scheme is higher than\nthat of the benchmarks. Explicitly, we use the following benchmarks: the\nnear-source-based RIS location and passive beamforming (NSB-LPB) optimization\nscheme, the near-destination-based RIS location and passive beamforming\n(NDB-LPB) optimization scheme, and the random RIS location and passive\nbeamforming (R-LPB) optimization scheme. Finally, the benefits of our J-LPB\nscheme are further increased with the number of RIS units.",
    "descriptor": "",
    "authors": [
      "Haiyan Guo",
      "Zhen Yang",
      "Yulong Zou",
      "Bin Lyu",
      "Yuhan Jiang",
      "Lajos Hanzo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.01966"
  },
  {
    "id": "arXiv:2210.01969",
    "title": "Hierarchical Adversarial Inverse Reinforcement Learning",
    "abstract": "Hierarchical Imitation Learning (HIL) has been proposed to recover\nhighly-complex behaviors in long-horizontal tasks from expert demonstrations by\nmodeling the task hierarchy with the option framework. Existing methods either\noverlook the causal relationship between the subtask and its corresponding\npolicy or fail to learn the policy in an end-to-end fashion, which leads to\nsuboptimality. In this work, we develop a novel HIL algorithm based on\nAdversarial Inverse Reinforcement Learning and adapt it with the\nExpectation-Maximization algorithm in order to directly recover a hierarchical\npolicy from the unannotated demonstrations. Further, we introduce a directed\ninformation term to the objective function to enhance the causality and propose\na Variational Autoencoder framework for learning with our objectives in an\nend-to-end fashion. Theoretical justifications and evaluations on challenging\nrobotic control tasks are provided to show the superiority of our algorithm.\nThe codes are available at https://github.com/LucasCJYSDL/HierAIRL.",
    "descriptor": "",
    "authors": [
      "Jiayu Chen",
      "Tian Lan",
      "Vaneet Aggarwal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.01969"
  },
  {
    "id": "arXiv:2210.01970",
    "title": "Evaluate & Evaluation on the Hub: Better Best Practices for Data and  Model Measurement",
    "abstract": "Evaluation is a key part of machine learning (ML), yet there is a lack of\nsupport and tooling to enable its informed and systematic practice. We\nintroduce Evaluate and Evaluation on the Hub --a set of tools to facilitate the\nevaluation of models and datasets in ML. Evaluate is a library to support best\npractices for measurements, metrics, and comparisons of data and models. Its\ngoal is to support reproducibility of evaluation, centralize and document the\nevaluation process, and broaden evaluation to cover more facets of model\nperformance. It includes over 50 efficient canonical implementations for a\nvariety of domains and scenarios, interactive documentation, and the ability to\neasily share implementations and outcomes. The library is available at\nhttps://github.com/huggingface/evaluate. In addition, we introduce Evaluation\non the Hub, a platform that enables the large-scale evaluation of over 75,000\nmodels and 11,000 datasets on the Hugging Face Hub, for free, at the click of a\nbutton. Evaluation on the Hub is available at\nhttps://huggingface.co/autoevaluate.",
    "descriptor": "",
    "authors": [
      "Leandro von Werra",
      "Lewis Tunstall",
      "Abhishek Thakur",
      "Alexandra Sasha Luccioni",
      "Tristan Thrush",
      "Aleksandra Piktus",
      "Felix Marty",
      "Nazneen Rajani",
      "Victor Mustar",
      "Helen Ngo",
      "Omar Sanseviero",
      "Mario \u0160a\u0161ko",
      "Albert Villanova",
      "Quentin Lhoest",
      "Julien Chaumond",
      "Margaret Mitchell",
      "Alexander M. Rush",
      "Thomas Wolf",
      "Douwe Kiela"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01970"
  },
  {
    "id": "arXiv:2210.01971",
    "title": "Towards Efficient Modularity in Industrial Drying: A Combinatorial  Optimization Viewpoint",
    "abstract": "Approximately 12% of the total energy consumed in manufacturing is used for\nindustrial drying. It is estimated that roughly 40% of this amount can be\nreduced by more efficient process controls and developing new drying\ntechnologies. Different technologies have different operating conditions where\nthey perform the best. Therefore, one way to reduce process costs is to utilize\nmultiple technologies in a single unit in a modular fashion. In such units,\ndetermining the sequence of drying techniques used in a process and the value\nof the control variables associated with each of them significantly affects the\nperformance in terms of energy efficiency and the product quality of food\nsamples. In other words, we are interested in finding the optimal combination\nof these drying technologies and their optimal control parameters in order to\nreduce process costs. The main contribution of this paper is to mathematically\nformulate this class of problems and propose a framework to simultaneously\noptimize both objectives. The above problem can be interpreted as a\ncombinatorial optimization problem that is NP-hard whose cost function is\nriddled with multiple poor local minima. The algorithm we propose in this paper\nis based on the Maximum Entropy Principle (MEP), which guarantees to converge\nto local minima and is heuristically designed to reach the global minimum.\nSimulation results applied to a batch-process drying prototype to dry\ndistillers dried grain (DDG) products demonstrate successful determination of\nprocess configuration and control parameters. Results show as large as 12%\nimprovement in energy consumption compared to the most efficient widely used\nsingle-stage drying process.",
    "descriptor": "",
    "authors": [
      "Alisina Bayati",
      "Amber Srivastava",
      "Amir Malvandi",
      "Hao Feng",
      "Srinivasa Salapaka"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.01971"
  },
  {
    "id": "arXiv:2210.01973",
    "title": "Meta-Ensemble Parameter Learning",
    "abstract": "Ensemble of machine learning models yields improved performance as well as\nrobustness. However, their memory requirements and inference costs can be\nprohibitively high. Knowledge distillation is an approach that allows a single\nmodel to efficiently capture the approximate performance of an ensemble while\nshowing poor scalability as demand for re-training when introducing new teacher\nmodels. In this paper, we study if we can utilize the meta-learning strategy to\ndirectly predict the parameters of a single model with comparable performance\nof an ensemble. Hereto, we introduce WeightFormer, a Transformer-based model\nthat can predict student network weights layer by layer in a forward pass,\naccording to the teacher model parameters. The proprieties of WeightFormer are\ninvestigated on the CIFAR-10, CIFAR-100, and ImageNet datasets for model\nstructures of VGGNet-11, ResNet-50, and ViT-B/32, where it demonstrates that\nour method can achieve approximate classification performance of an ensemble\nand outperforms both the single network and standard knowledge distillation.\nMore encouragingly, we show that WeightFormer results can further exceeds\naverage ensemble with minor fine-tuning. Importantly, our task along with the\nmodel and results can potentially lead to a new, more efficient, and scalable\nparadigm of ensemble networks parameter learning.",
    "descriptor": "\nComments: technique report\n",
    "authors": [
      "Zhengcong Fei",
      "Shuman Tian",
      "Junshi Huang",
      "Xiaoming Wei",
      "Xiaolin Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01973"
  },
  {
    "id": "arXiv:2210.01974",
    "title": "Towards Prototype-Based Self-Explainable Graph Neural Network",
    "abstract": "Graph Neural Networks (GNNs) have shown great ability in modeling\ngraph-structured data for various domains. However, GNNs are known as black-box\nmodels that lack interpretability. Without understanding their inner working,\nwe cannot fully trust them, which largely limits their adoption in high-stake\nscenarios. Though some initial efforts have been taken to interpret the\npredictions of GNNs, they mainly focus on providing post-hoc explanations using\nan additional explainer, which could misrepresent the true inner working\nmechanism of the target GNN. The works on self-explainable GNNs are rather\nlimited. Therefore, we study a novel problem of learning prototype-based\nself-explainable GNNs that can simultaneously give accurate predictions and\nprototype-based explanations on predictions. We design a framework which can\nlearn prototype graphs that capture representative patterns of each class as\nclass-level explanations. The learned prototypes are also used to\nsimultaneously make prediction for for a test instance and provide\ninstance-level explanation. Extensive experiments on real-world and synthetic\ndatasets show the effectiveness of the proposed framework for both prediction\naccuracy and explanation quality.",
    "descriptor": "",
    "authors": [
      "Enyan Dai",
      "Suhang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01974"
  },
  {
    "id": "arXiv:2210.01977",
    "title": "Energy and Time Based Topology Control Approach to Enhance the Lifetime  of WSN in an economic zone",
    "abstract": "An economic zone requires continuous monitoring and controlling by an\nautonomous surveillance system for heightening its production competency and\nsecurity. Wireless sensor network (WSN) has swiftly grown popularity over the\nworld for uninterruptedly monitoring and controlling a system. Sensor devices,\nthe main elements of WSN, are given limited amount of energy, which leads the\nnetwork to limited lifespan. Therefore, the most significant challenge is to\nincrease the lifespan of a WSN system. Topology control mechanism (TCM) is a\nrenowned method to enhance the lifespan of WSN. This paper proposes an approach\nto extend the lifetime of WSN for an economic area, targeting an economic zone\nin Bangladesh. Observations are made on the performance of the network lifetime\nconsidering the individual combinations of the TCM protocols and comparative\ninvestigation between the time and energy triggering strategy of TCM protocols.\nResults reveal the network makes a better performance in the case of A3\nprotocol while using the topology maintenance protocols with both time and\nenergy triggering methods. Moreover, the performance of the A3 and DGETRec is\nsuperior to the other combinations of TCM protocols. Hence, the WSN system can\nbe able to serve better connectivity coverage in the target economic zone.",
    "descriptor": "\nComments: 14 pages, 10 figures\n",
    "authors": [
      "Tanvir Hossain",
      "Md. Ershadul Haque",
      "Abdullah Al Mamun",
      "Samiul Ul Hoque",
      "Al Amin Fahim"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.01977"
  },
  {
    "id": "arXiv:2210.01979",
    "title": "GAPX: Generalized Autoregressive Paraphrase-Identification X",
    "abstract": "Paraphrase Identification is a fundamental task in Natural Language\nProcessing. While much progress has been made in the field, the performance of\nmany state-of-the-art models often suffer from distribution shift during\ninference time. We verify that a major source of this performance drop comes\nfrom biases introduced by negative examples. To overcome these biases, we\npropose in this paper to train two separate models, one that only utilizes the\npositive pairs and the other the negative pairs. This enables us the option of\ndeciding how much to utilize the negative model, for which we introduce a\nperplexity based out-of-distribution metric that we show can effectively and\nautomatically determine how much weight it should be given during inference. We\nsupport our findings with strong empirical results.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Yifei Zhou",
      "Renyu Li",
      "Hayden Housen",
      "Ser-Nam Lim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.01979"
  },
  {
    "id": "arXiv:2210.01981",
    "title": "Cloud removal Using Atmosphere Model",
    "abstract": "Cloud removal is an essential task in remote sensing data analysis. As the\nimage sensors are distant from the earth ground, it is likely that part of the\narea of interests is covered by cloud. Moreover, the atmosphere in between\ncreates a constant haze layer upon the acquired images. To recover the ground\nimage, we propose to use scattering model for temporal sequence of images of\nany scene in the framework of low rank and sparse models. We further develop\nits variant, which is much faster and yet more accurate. To measure the\nperformance of different methods {\\em objectively}, we develop a semi-realistic\nsimulation method to produce cloud cover so that various methods can be\nquantitatively analysed, which enables detailed study of many aspects of cloud\nremoval algorithms, including verifying the effectiveness of proposed models in\ncomparison with the state-of-the-arts, including deep learning models, and\naddressing the long standing problem of the determination of regularisation\nparameters. The latter is companioned with theoretic analysis on the range of\nthe sparsity regularisation parameter and verified numerically.",
    "descriptor": "",
    "authors": [
      "Yi Guo",
      "Feng Li",
      "Zhuo Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01981"
  },
  {
    "id": "arXiv:2210.01984",
    "title": "Manipulation and Peer Mechanisms: A Survey",
    "abstract": "In peer mechanisms, the competitors for a prize also determine who wins. Each\ncompetitor may be asked to rank, grade, or nominate peers for the prize. Since\nthe prize can be valuable, such as financial aid, course grades, or an award at\na conference, competitors may be tempted to manipulate the mechanism. We survey\napproaches to prevent or discourage the manipulation of peer mechanisms. We\nconclude our survey by identifying several important research challenges",
    "descriptor": "",
    "authors": [
      "Matthew Olckers",
      "Toby Walsh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2210.01984"
  },
  {
    "id": "arXiv:2210.01985",
    "title": "A Multi-Stage Automated Online Network Data Stream Analytics Framework  for IIoT Systems",
    "abstract": "Industry 5.0 aims at maximizing the collaboration between humans and\nmachines. Machines are capable of automating repetitive jobs, while humans\nhandle creative tasks. As a critical component of Industrial Internet of Things\n(IIoT) systems for service delivery, network data stream analytics often\nencounter concept drift issues due to dynamic IIoT environments, causing\nperformance degradation and automation difficulties. In this paper, we propose\na novel Multi-Stage Automated Network Analytics (MSANA) framework for concept\ndrift adaptation in IIoT systems, consisting of dynamic data pre-processing,\nthe proposed Drift-based Dynamic Feature Selection (DD-FS) method, dynamic\nmodel learning & selection, and the proposed Window-based Performance Weighted\nProbability Averaging Ensemble (W-PWPAE) model. It is a complete automated data\nstream analytics framework that enables automatic, effective, and efficient\ndata analytics for IIoT systems in Industry 5.0. Experimental results on two\npublic IoT datasets demonstrate that the proposed framework outperforms\nstate-of-the-art methods for IIoT data stream analytics.",
    "descriptor": "\nComments: Published in IEEE Transactions on Industrial Informatics (Q1, IF: 11.648); Code is available at Github link: this https URL\n",
    "authors": [
      "Li Yang",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.01985"
  },
  {
    "id": "arXiv:2210.01986",
    "title": "MAtt: A Manifold Attention Network for EEG Decoding",
    "abstract": "Recognition of electroencephalographic (EEG) signals highly affect the\nefficiency of non-invasive brain-computer interfaces (BCIs). While recent\nadvances of deep-learning (DL)-based EEG decoders offer improved performances,\nthe development of geometric learning (GL) has attracted much attention for\noffering exceptional robustness in decoding noisy EEG data. However, there is a\nlack of studies on the merged use of deep neural networks (DNNs) and geometric\nlearning for EEG decoding. We herein propose a manifold attention network\n(mAtt), a novel geometric deep learning (GDL)-based model, featuring a manifold\nattention mechanism that characterizes spatiotemporal representations of EEG\ndata fully on a Riemannian symmetric positive definite (SPD) manifold. The\nevaluation of the proposed MAtt on both time-synchronous and -asyncronous EEG\ndatasets suggests its superiority over other leading DL methods for general EEG\ndecoding. Furthermore, analysis of model interpretation reveals the capability\nof MAtt in capturing informative EEG features and handling the non-stationarity\nof brain dynamics.",
    "descriptor": "",
    "authors": [
      "Yue-Ting Pan",
      "Jing-Lun Chou",
      "Chun-Shu Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.01986"
  },
  {
    "id": "arXiv:2210.01987",
    "title": "ImpressLearn: Continual Learning via Combined Task Impressions",
    "abstract": "This work proposes a new method to sequentially train a deep neural network\non multiple tasks without suffering catastrophic forgetting, while endowing it\nwith the capability to quickly adapt to unseen tasks. Starting from existing\nwork on network masking (Wortsman et al., 2020), we show that simply learning a\nlinear combination of a small number of task-specific masks (impressions) on a\nrandomly initialized backbone network is sufficient to both retain accuracy on\npreviously learned tasks, as well as achieve high accuracy on new tasks. In\ncontrast to previous methods, we do not require to generate dedicated masks or\ncontexts for each new task, instead leveraging transfer learning to keep\nper-task parameter overhead small. Our work illustrates the power of linearly\ncombining individual impressions, each of which fares poorly in isolation, to\nachieve performance comparable to a dedicated mask. Moreover, even repeated\nimpressions from the same task (homogeneous masks), when combined can approach\nthe performance of heterogeneous combinations if sufficiently many impressions\nare used. Our approach scales more efficiently than existing methods, often\nrequiring orders of magnitude fewer parameters and can function without\nmodification even when task identity is missing. In addition, in the setting\nwhere task labels are not given at inference, our algorithm gives an often\nfavorable alternative to the entropy based task-inference methods proposed in\n(Wortsman et al., 2020). We evaluate our method on a number of well known image\nclassification data sets and architectures.",
    "descriptor": "",
    "authors": [
      "Dhrupad Bhardwaj",
      "Julia Kempe",
      "Artem Vysogorets",
      "Angela M. Teng",
      "Evaristus C. Ezekwem"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01987"
  },
  {
    "id": "arXiv:2210.01988",
    "title": "Bicoptor: Two-round Secure Three-party Non-linear Computation without  Preprocessing for Privacy-preserving Machine Learning",
    "abstract": "The overhead of non-linear functions dominates the performance of the secure\nmultiparty computation (MPC) based privacy-preserving machine learning (PPML).\nThis work introduces two sets of novel secure three-party computation (3PC)\nprotocols, using additive and replicated secret sharing schemes respectively.\nWe name the whole family of protocols as Bicoptor, its basis is a new sign\ndetermination protocol, which relies on a clever use of the truncation protocol\nproposed in SecureML (S&P 2017). Our 3PC sign determination protocol only\nrequires two communication rounds, and does not involve any preprocessing. Such\nsign determination protocol is well-suited for computing non-linear functions\nin PPML, e.g. the activation function ReLU, Maxpool, and their variants. We\ndevelop suitable protocols for these non-linear functions, which form a family\nof GPU-friendly protocols, Bicoptor. All Bicoptor protocols only require two\ncommunication rounds without preprocessing. We evaluate the protocols using\nadditive secret sharing under a 3-party LAN network over a public cloud, and\nachieve 90,000 DReLU/ReLU or 3,200 Maxpool (find the maximum value of nine\ninputs) operations per second. Under the same settings and environment, our\nReLU protocol has a one or even two order(s) of magnitude improvement to the\nstate-of-the-art works, Edabits (CRYPTO 2020) or Falcon (PETS 2021),\nrespectively without batch processing.",
    "descriptor": "\nComments: 18 pages, 7 figures\n",
    "authors": [
      "Lijing Zhou",
      "Ziyu Wang",
      "Hongrui Cui",
      "Qingrui Song",
      "Yu Yu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.01988"
  },
  {
    "id": "arXiv:2210.01989",
    "title": "Waveformer: Linear-Time Attention with Forward and Backward Wavelet  Transform",
    "abstract": "We propose Waveformer that learns attention mechanism in the wavelet\ncoefficient space, requires only linear time complexity, and enjoys universal\napproximating power. Specifically, we first apply forward wavelet transform to\nproject the input sequences to multi-resolution orthogonal wavelet bases, then\nconduct nonlinear transformations (in this case, a random feature kernel) in\nthe wavelet coefficient space, and finally reconstruct the representation in\ninput space via backward wavelet transform. We note that other non-linear\ntransformations may be used, hence we name the learning paradigm Wavelet\ntransformatIon for Sequence lEarning (WISE). We emphasize the importance of\nbackward reconstruction in the WISE paradigm -- without it, one would be mixing\ninformation from both the input space and coefficient space through skip\nconnections, which shall not be considered as mathematically sound. Compared\nwith Fourier transform in recent works, wavelet transform is more efficient in\ntime complexity and better captures local and positional information; we\nfurther support this through our ablation studies. Extensive experiments on\nseven long-range understanding datasets from the Long Range Arena benchmark and\ncode understanding tasks demonstrate that (1) Waveformer achieves competitive\nand even better accuracy than a number of state-of-the-art Transformer variants\nand (2) WISE can boost accuracies of various attention approximation methods\nwithout increasing the time complexity. These together showcase the superiority\nof learning attention in a wavelet coefficient space over the input space.",
    "descriptor": "",
    "authors": [
      "Yufan Zhuang",
      "Zihan Wang",
      "Fangbo Tao",
      "Jingbo Shang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01989"
  },
  {
    "id": "arXiv:2210.02000",
    "title": "Internal Longest Palindrome Queries in Optimal Time",
    "abstract": "Palindromes are strings that read the same forward and backward. Problems of\ncomputing palindromic structures in strings have been studied for many years\nwith a motivation of their application to biology. The longest palindrome\nproblem is one of the most important and classical problems regarding\npalindromic structures, that is, to compute the longest palindrome appearing in\na string $T$ of length $n$. The problem can be solved in $\\mathcal{O}(n)$ time\nby the famous algorithm of Manacher [Journal of the ACM, 1975]. In this paper,\nwe consider the problem in the internal model. The internal longest palindrome\nquery is, given a substring $T[i..j]$ of $T$ as a query, to compute the longest\npalindrome appearing in $T[i.. j]$. The best known data structure for this\nproblem is the one proposed by Amir et al. [Algorithmica, 2020], which can\nanswer any query in $\\mathcal{O}(\\log n)$ time. In this paper, we propose a\nlinear-size data structure that can answer any internal longest palindrome\nquery in constant time. Also, given the input string $T$, our data structure\ncan be constructed in $\\mathcal{O}(n)$ time.",
    "descriptor": "",
    "authors": [
      "Kazuki Mitani",
      "Takuya Mieno",
      "Kazuhisa Seto",
      "Takashi Horiyama"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.02000"
  },
  {
    "id": "arXiv:2210.02009",
    "title": "Multi-Camera Collaborative Depth Prediction via Consistent Structure  Estimation",
    "abstract": "Depth map estimation from images is an important task in robotic systems.\nExisting methods can be categorized into two groups including multi-view stereo\nand monocular depth estimation. The former requires cameras to have large\noverlapping areas and sufficient baseline between cameras, while the latter\nthat processes each image independently can hardly guarantee the structure\nconsistency between cameras. In this paper, we propose a novel multi-camera\ncollaborative depth prediction method that does not require large overlapping\nareas while maintaining structure consistency between cameras. Specifically, we\nformulate the depth estimation as a weighted combination of depth basis, in\nwhich the weights are updated iteratively by a refinement network driven by the\nproposed consistency loss. During the iterative update, the results of depth\nestimation are compared across cameras and the information of overlapping areas\nis propagated to the whole depth maps with the help of basis formulation.\nExperimental results on DDAD and NuScenes datasets demonstrate the superior\nperformance of our method.",
    "descriptor": "",
    "authors": [
      "Jialei Xu",
      "Xianming Liu",
      "Yuanchao Bai",
      "Junjun Jiang",
      "Kaixuan Wang",
      "Xiaozhi Chen",
      "Xiangyang Ji"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02009"
  },
  {
    "id": "arXiv:2210.02016",
    "title": "Multi-task Self-supervised Graph Neural Networks Enable Stronger Task  Generalization",
    "abstract": "Self-supervised learning (SSL) for graph neural networks (GNNs) has attracted\nincreasing attention from the graph machine learning community in recent years,\nowing to its capability to learn performant node embeddings without costly\nlabel information. One weakness of conventional SSL frameworks for GNNs is that\nthey learn through a single philosophy, such as mutual information maximization\nor generative reconstruction. When applied to various downstream tasks, these\nframeworks rarely perform equally well for every task, because one philosophy\nmay not span the extensive knowledge required for all tasks. In light of this,\nwe introduce ParetoGNN, a multi-task SSL framework for node representation\nlearning over graphs. Specifically, ParetoGNN is self-supervised by manifold\npretext tasks observing multiple philosophies. To reconcile different\nphilosophies, we explore a multiple-gradient descent algorithm, such that\nParetoGNN actively learns from every pretext task while minimizing potential\nconflicts. We conduct comprehensive experiments over four downstream tasks\n(i.e., node classification, node clustering, link prediction, and partition\nprediction), and our proposal achieves the best overall performance across\ntasks on 11 widely adopted benchmark datasets. Besides, we observe that\nlearning from multiple philosophies enhances not only the task generalization\nbut also the single task performance, demonstrating that ParetoGNN achieves\nbetter task generalization via the disjoint yet complementary knowledge learned\nfrom different philosophies.",
    "descriptor": "",
    "authors": [
      "Mingxuan Ju",
      "Tong Zhao",
      "Qianlong Wen",
      "Wenhao Yu",
      "Neil Shah",
      "Yanfang Ye",
      "Chuxu Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02016"
  },
  {
    "id": "arXiv:2210.02017",
    "title": "Infectious Probability Analysis on COVID-19 Spreading with Wireless Edge  Networks",
    "abstract": "The emergence of infectious disease COVID-19 has challenged and changed the\nworld in an unprecedented manner. The integration of wireless networks with\nedge computing (namely wireless edge networks) brings opportunities to address\nthis crisis. In this paper, we aim to investigate the prediction of the\ninfectious probability and propose precautionary measures against COVID-19 with\nthe assistance of wireless edge networks. Due to the availability of the\nrecorded detention time and the density of individuals within a wireless edge\nnetwork, we propose a stochastic geometry-based method to analyze the\ninfectious probability of individuals. The proposed method can well keep the\nprivacy of individuals in the system since it does not require to know the\nlocation or trajectory of each individual. Moreover, we also consider three\ntypes of mobility models and the static model of individuals. Numerical results\nshow that analytical results well match with simulation results, thereby\nvalidating the accuracy of the proposed model. Moreover, numerical results also\noffer many insightful implications. Thereafter, we also offer a number of\ncountermeasures against the spread of COVID-19 based on wireless edge networks.\nThis study lays the foundation toward predicting the infectious risk in\nrealistic environment and points out directions in mitigating the spread of\ninfectious diseases with the aid of wireless edge networks.",
    "descriptor": "",
    "authors": [
      "Xuran Li",
      "Shuaishuai Guo",
      "Hong-Ning Dai",
      "Dengwang Li"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.02017"
  },
  {
    "id": "arXiv:2210.02018",
    "title": "InterFace:Adjustable Angular Margin Inter-class Loss for Deep Face  Recognition",
    "abstract": "In the field of face recognition, it is always a hot research topic to\nimprove the loss solution to make the face features extracted by the network\nhave greater discriminative power. Research works in recent years has improved\nthe discriminative power of the face model by normalizing softmax to the cosine\nspace step by step and then adding a fixed penalty margin to reduce the\nintra-class distance to increase the inter-class distance. Although a great\ndeal of previous work has been done to optimize the boundary penalty to improve\nthe discriminative power of the model, adding a fixed margin penalty to the\ndepth feature and the corresponding weight is not consistent with the pattern\nof data in the real scenario. To address this issue, in this paper, we propose\na novel loss function, InterFace, releasing the constraint of adding a margin\npenalty only between the depth feature and the corresponding weight to push the\nseparability of classes by adding corresponding margin penalties between the\ndepth features and all weights. To illustrate the advantages of InterFace over\na fixed penalty margin, we explained geometrically and comparisons on a set of\nmainstream benchmarks. From a wider perspective, our InterFace has advanced the\nstate-of-the-art face recognition performance on five out of thirteen\nmainstream benchmarks. All training codes, pre-trained models, and training\nlogs, are publicly released\n\\footnote{$https://github.com/iamsangmeng/InterFace$}.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2109.09416 by other authors\n",
    "authors": [
      "Meng Sang",
      "Jiaxuan Chen",
      "Mengzhen Li",
      "Pan Tan",
      "Anning Pan",
      "Shang Zhao",
      "Yang Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02018"
  },
  {
    "id": "arXiv:2210.02019",
    "title": "Atari-5: Distilling the Arcade Learning Environment down to Five Games",
    "abstract": "The Arcade Learning Environment (ALE) has become an essential benchmark for\nassessing the performance of reinforcement learning algorithms. However, the\ncomputational cost of generating results on the entire 57-game dataset limits\nALE's use and makes the reproducibility of many results infeasible. We propose\na novel solution to this problem in the form of a principled methodology for\nselecting small but representative subsets of environments within a benchmark\nsuite. We applied our method to identify a subset of five ALE games, called\nAtari-5, which produces 57-game median score estimates within 10% of their true\nvalues. Extending the subset to 10-games recovers 80% of the variance for\nlog-scores for all games within the 57-game set. We show this level of\ncompression is possible due to a high degree of correlation between many of the\ngames in ALE.",
    "descriptor": "",
    "authors": [
      "Matthew Aitchison",
      "Penny Sweetser",
      "Marcus Hutter"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02019"
  },
  {
    "id": "arXiv:2210.02021",
    "title": "Exploring Effective Knowledge Transfer for Few-shot Object Detection",
    "abstract": "Recently, few-shot object detection~(FSOD) has received much attention from\nthe community, and many methods are proposed to address this problem from a\nknowledge transfer perspective. Though promising results have been achieved,\nthese methods fail to achieve shot-stable:~methods that excel in low-shot\nregimes are likely to struggle in high-shot regimes, and vice versa. We believe\nthis is because the primary challenge of FSOD changes when the number of shots\nvaries. In the low-shot regime, the primary challenge is the lack of\ninner-class variation. In the high-shot regime, as the variance approaches the\nreal one, the main hindrance to the performance comes from misalignment between\nlearned and true distributions. However, these two distinct issues remain\nunsolved in most existing FSOD methods. In this paper, we propose to overcome\nthese challenges by exploiting rich knowledge the model has learned and\neffectively transferring them to the novel classes. For the low-shot regime, we\npropose a distribution calibration method to deal with the lack of inner-class\nvariation problem. Meanwhile, a shift compensation method is proposed to\ncompensate for possible distribution shift during fine-tuning. For the\nhigh-shot regime, we propose to use the knowledge learned from ImageNet as\nguidance for the feature learning in the fine-tuning stage, which will\nimplicitly align the distributions of the novel classes. Although targeted\ntoward different regimes, these two strategies can work together to further\nimprove the FSOD performance. Experiments on both the VOC and COCO benchmarks\nshow that our proposed method can significantly outperform the baseline method\nand produce competitive results in both low-shot settings (shot<5) and\nhigh-shot settings (shot>=5). Code is available at\nhttps://github.com/JulioZhao97/EffTrans_Fsdet.git.",
    "descriptor": "\nComments: 9 pages, 6 figures, accepted by ACM Multimedia 2022\n",
    "authors": [
      "Zhiyuan Zhao",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02021"
  },
  {
    "id": "arXiv:2210.02023",
    "title": "DreamShard: Generalizable Embedding Table Placement for Recommender  Systems",
    "abstract": "We study embedding table placement for distributed recommender systems, which\naims to partition and place the tables on multiple hardware devices (e.g.,\nGPUs) to balance the computation and communication costs. Although prior work\nhas explored learning-based approaches for the device placement of\ncomputational graphs, embedding table placement remains to be a challenging\nproblem because of 1) the operation fusion of embedding tables, and 2) the\ngeneralizability requirement on unseen placement tasks with different numbers\nof tables and/or devices. To this end, we present DreamShard, a reinforcement\nlearning (RL) approach for embedding table placement. DreamShard achieves the\nreasoning of operation fusion and generalizability with 1) a cost network to\ndirectly predict the costs of the fused operation, and 2) a policy network that\nis efficiently trained on an estimated Markov decision process (MDP) without\nreal GPU execution, where the states and the rewards are estimated with the\ncost network. Equipped with sum and max representation reductions, the two\nnetworks can directly generalize to any unseen tasks with different numbers of\ntables and/or devices without fine-tuning. Extensive experiments show that\nDreamShard substantially outperforms the existing human expert and RNN-based\nstrategies with up to 19% speedup over the strongest baseline on large-scale\nsynthetic tables and our production tables. The code is available at\nhttps://github.com/daochenzha/dreamshard",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Daochen Zha",
      "Louis Feng",
      "Qiaoyu Tan",
      "Zirui Liu",
      "Kwei-Herng Lai",
      "Bhargav Bhushanam",
      "Yuandong Tian",
      "Arun Kejariwal",
      "Xia Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02023"
  },
  {
    "id": "arXiv:2210.02024",
    "title": "Perfect Reconstruction Two-Channel Filter Banks on Arbitrary Graphs",
    "abstract": "This paper extends the existing theory of perfect reconstruction two-channel\nfilter banks from bipartite graphs to non-bipartite graphs. By generalizing the\nconcept of downsampling/upsampling we establish the frame of two-channel filter\nbank on arbitrary connected, undirected and weighted graphs. Then the equations\nfor perfect reconstruction of the filter banks are presented and solved under\nproper conditions. Algorithms for designing orthogonal and biorthogonal banks\nare given and two typical orthogonal two-channel filter banks are calculated.\nThe locality and approximation properties of such filter banks are discussed\ntheoretically and experimentally.",
    "descriptor": "\nComments: 33 pages,11 figures. This manuscript has been submitted to ScienceDirect Applied and Computational Harmonic Analysis (ACHA) on Jan 27,2022\n",
    "authors": [
      "Junxia You",
      "Lihua Yang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.02024"
  },
  {
    "id": "arXiv:2210.02025",
    "title": "GMMSeg: Gaussian Mixture based Generative Semantic Segmentation Models",
    "abstract": "Prevalent semantic segmentation solutions are, in essence, a dense\ndiscriminative classifier of p(class|pixel feature). Though straightforward,\nthis de facto paradigm neglects the underlying data distribution p(pixel\nfeature|class), and struggles to identify out-of-distribution data. Going\nbeyond this, we propose GMMSeg, a new family of segmentation models that rely\non a dense generative classifier for the joint distribution p(pixel\nfeature,class). For each class, GMMSeg builds Gaussian Mixture Models (GMMs)\nvia Expectation-Maximization (EM), so as to capture class-conditional\ndensities. Meanwhile, the deep dense representation is end-to-end trained in a\ndiscriminative manner, i.e., maximizing p(class|pixel feature). This endows\nGMMSeg with the strengths of both generative and discriminative models. With a\nvariety of segmentation architectures and backbones, GMMSeg outperforms the\ndiscriminative counterparts on three closed-set datasets. More impressively,\nwithout any modification, GMMSeg even performs well on open-world datasets. We\nbelieve this work brings fundamental insights into the related fields.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022; Code: this https URL\n",
    "authors": [
      "Chen Liang",
      "Wenguan Wang",
      "Jiaxu Miao",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02025"
  },
  {
    "id": "arXiv:2210.02029",
    "title": "Inharmonious Region Localization with Auxiliary Style Feature",
    "abstract": "With the prevalence of image editing techniques, users can create fantastic\nsynthetic images, but the image quality may be compromised by the\ncolor/illumination discrepancy between the manipulated region and background.\nInharmonious region localization aims to localize the inharmonious region in a\nsynthetic image. In this work, we attempt to leverage auxiliary style feature\nto facilitate this task. Specifically, we propose a novel color mapping module\nand a style feature loss to extract discriminative style features containing\ntask-relevant color/illumination information. Based on the extracted style\nfeatures, we also propose a novel style voting module to guide the localization\nof inharmonious region. Moreover, we introduce semantic information into the\nstyle voting module to achieve further improvement. Our method surpasses the\nexisting methods by a large margin on the benchmark dataset.",
    "descriptor": "\nComments: BMVC2022\n",
    "authors": [
      "Penghao Wu",
      "Li Niu",
      "Liqing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02029"
  },
  {
    "id": "arXiv:2210.02030",
    "title": "Point Cloud Recognition with Position-to-Structure Attention  Transformers",
    "abstract": "In this paper, we present Position-to-Structure Attention Transformers\n(PS-Former), a Transformer-based algorithm for 3D point cloud recognition.\nPS-Former deals with the challenge in 3D point cloud representation where\npoints are not positioned in a fixed grid structure and have limited feature\ndescription (only 3D coordinates ($x, y, z$) for scattered points). Existing\nTransformer-based architectures in this domain often require a pre-specified\nfeature engineering step to extract point features. Here, we introduce two new\naspects in PS-Former: 1) a learnable condensation layer that performs point\ndownsampling and feature extraction; and 2) a Position-to-Structure Attention\nmechanism that recursively enriches the structural information with the\nposition attention branch. Compared with the competing methods, while being\ngeneric with less heuristics feature designs, PS-Former demonstrates\ncompetitive experimental results on three 3D point cloud tasks including\nclassification, part segmentation, and scene segmentation.",
    "descriptor": "",
    "authors": [
      "Zheng Ding",
      "James Hou",
      "Zhuowen Tu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02030"
  },
  {
    "id": "arXiv:2210.02033",
    "title": "Learning Video-independent Eye Contact Segmentation from In-the-Wild  Videos",
    "abstract": "Human eye contact is a form of non-verbal communication and can have a great\ninfluence on social behavior. Since the location and size of the eye contact\ntargets vary across different videos, learning a generic video-independent eye\ncontact detector is still a challenging task. In this work, we address the task\nof one-way eye contact detection for videos in the wild. Our goal is to build a\nunified model that can identify when a person is looking at his gaze targets in\nan arbitrary input video. Considering that this requires time-series relative\neye movement information, we propose to formulate the task as a temporal\nsegmentation. Due to the scarcity of labeled training data, we further propose\na gaze target discovery method to generate pseudo-labels for unlabeled videos,\nwhich allows us to train a generic eye contact segmentation model in an\nunsupervised way using in-the-wild videos. To evaluate our proposed approach,\nwe manually annotated a test dataset consisting of 52 videos of human\nconversations. Experimental results show that our eye contact segmentation\nmodel outperforms the previous video-dependent eye contact detector and can\nachieve 71.88% framewise accuracy on our annotated test set. Our code and\nevaluation dataset are available at\nhttps://github.com/ut-vision/Video-Independent-ECS.",
    "descriptor": "\nComments: Accepted to ACCV2022\n",
    "authors": [
      "Tianyi Wu",
      "Yusuke Sugano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02033"
  },
  {
    "id": "arXiv:2210.02034",
    "title": "Clustering Semantic Predicates in the Open Research Knowledge Graph",
    "abstract": "When semantically describing knowledge graphs (KGs), users have to make a\ncritical choice of a vocabulary (i.e. predicates and resources). The success of\nKG building is determined by the convergence of shared vocabularies so that\nmeaning can be established. The typical lifecycle for a new KG construction can\nbe defined as follows: nascent phases of graph construction experience\nterminology divergence, while later phases of graph construction experience\nterminology convergence and reuse. In this paper, we describe our approach\ntailoring two AI-based clustering algorithms for recommending predicates (in\nRDF statements) about resources in the Open Research Knowledge Graph (ORKG)\nhttps://orkg.org/. Such a service to recommend existing predicates to semantify\nnew incoming data of scholarly publications is of paramount importance for\nfostering terminology convergence in the ORKG. Our experiments show very\npromising results: a high precision with relatively high recall in linear\nruntime performance. Furthermore, this work offers novel insights into the\npredicate groups that automatically accrue loosely as generic semantification\npatterns for semantification of scholarly knowledge spanning 44 research\nfields.",
    "descriptor": "",
    "authors": [
      "Omar Arab Oghli",
      "Jennifer D'Souza",
      "S\u00f6ren Auer"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02034"
  },
  {
    "id": "arXiv:2210.02036",
    "title": "Inharmonious Region Localization via Recurrent Self-Reasoning",
    "abstract": "Synthetic images created by image editing operations are prevalent, but the\ncolor or illumination inconsistency between the manipulated region and\nbackground may make it unrealistic. Thus, it is important yet challenging to\nlocalize the inharmonious region to improve the quality of synthetic image.\nInspired by the classic clustering algorithm, we aim to group pixels into two\nclusters: inharmonious cluster and background cluster by inserting a novel\nRecurrent Self-Reasoning (RSR) module into the bottleneck of UNet structure.\nThe mask output from RSR module is provided for the decoder as attention\nguidance. Finally, we adaptively combine the masks from RSR and the decoder to\nform our final mask. Experimental results on the image harmonization dataset\ndemonstrate that our method achieves competitive performance both\nquantitatively and qualitatively.",
    "descriptor": "\nComments: BMVC2022\n",
    "authors": [
      "Penghao Wu",
      "Li Niu",
      "Jing Liang",
      "Liqing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02036"
  },
  {
    "id": "arXiv:2210.02038",
    "title": "MOTSLAM: MOT-assisted monocular dynamic SLAM using single-view depth  estimation",
    "abstract": "Visual SLAM systems targeting static scenes have been developed with\nsatisfactory accuracy and robustness. Dynamic 3D object tracking has then\nbecome a significant capability in visual SLAM with the requirement of\nunderstanding dynamic surroundings in various scenarios including autonomous\ndriving, augmented and virtual reality. However, performing dynamic SLAM solely\nwith monocular images remains a challenging problem due to the difficulty of\nassociating dynamic features and estimating their positions. In this paper, we\npresent MOTSLAM, a dynamic visual SLAM system with the monocular configuration\nthat tracks both poses and bounding boxes of dynamic objects. MOTSLAM first\nperforms multiple object tracking (MOT) with associated both 2D and 3D bounding\nbox detection to create initial 3D objects. Then, neural-network-based\nmonocular depth estimation is applied to fetch the depth of dynamic features.\nFinally, camera poses, object poses, and both static, as well as dynamic map\npoints, are jointly optimized using a novel bundle adjustment. Our experiments\non the KITTI dataset demonstrate that our system has reached best performance\non both camera ego-motion and object tracking on monocular dynamic SLAM.",
    "descriptor": "",
    "authors": [
      "Hanwei Zhang",
      "Hideaki Uchiyama",
      "Shintaro Ono",
      "Hiroshi Kawasaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02038"
  },
  {
    "id": "arXiv:2210.02040",
    "title": "GT-GAN: General Purpose Time Series Synthesis with Generative  Adversarial Networks",
    "abstract": "Time series synthesis is an important research topic in the field of deep\nlearning, which can be used for data augmentation. Time series data types can\nbe broadly classified into regular or irregular. However, there are no existing\ngenerative models that show good performance for both types without any model\nchanges. Therefore, we present a general purpose model capable of synthesizing\nregular and irregular time series data. To our knowledge, we are the first\ndesigning a general purpose time series synthesis model, which is one of the\nmost challenging settings for time series synthesis. To this end, we design a\ngenerative adversarial network-based method, where many related techniques are\ncarefully integrated into a single framework, ranging from neural\nordinary/controlled differential equations to continuous time-flow processes.\nOur method outperforms all existing methods.",
    "descriptor": "",
    "authors": [
      "Jinsung Jeon",
      "Jeonghak Kim",
      "Haryong Song",
      "Seunghyeon Cho",
      "Noseong Park"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02040"
  },
  {
    "id": "arXiv:2210.02041",
    "title": "Natural Color Fool: Towards Boosting Black-box Unrestricted Attacks",
    "abstract": "Unrestricted color attacks, which manipulate semantically meaningful color of\nan image, have shown their stealthiness and success in fooling both human eyes\nand deep neural networks. However, current works usually sacrifice the\nflexibility of the uncontrolled setting to ensure the naturalness of\nadversarial examples. As a result, the black-box attack performance of these\nmethods is limited. To boost transferability of adversarial examples without\ndamaging image quality, we propose a novel Natural Color Fool (NCF) which is\nguided by realistic color distributions sampled from a publicly available\ndataset and optimized by our neighborhood search and initialization reset. By\nconducting extensive experiments and visualizations, we convincingly\ndemonstrate the effectiveness of our proposed method. Notably, on average,\nresults show that our NCF can outperform state-of-the-art approaches by\n15.0%$\\sim$32.9% for fooling normally trained models and 10.0%$\\sim$25.3% for\nevading defense methods. Our code is available at\nhttps://github.com/ylhz/Natural-Color-Fool.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Shengming Yuan",
      "Qilong Zhang",
      "Lianli Gao",
      "Yaya Cheng",
      "Jingkuan Song"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02041"
  },
  {
    "id": "arXiv:2210.02042",
    "title": "FedMT: Federated Learning with Mixed-type Labels",
    "abstract": "In federated learning (FL), classifiers (e.g., deep networks) are trained on\ndatasets from multiple centers without exchanging data across them, and thus\nimproves sample efficiency. In the classical setting of FL, the same labeling\ncriterion is usually employed across all centers being involved in training.\nThis constraint greatly limits the applicability of FL. For example, standards\nused for disease diagnosis are more likely to be different across clinical\ncenters, which mismatches the classical FL setting. In this paper, we consider\nan important yet under-explored setting of FL, namely FL with mixed-type labels\nwhere different labeling criteria can be employed by various centers, leading\nto inter-center label space differences and challenging existing FL methods\ndesigned for the classical setting. To effectively and efficiently train models\nwith mixed-type labels, we propose a theory-guided and model-agnostic approach\nthat can make use of the underlying correspondence between those label spaces\nand can be easily combined with various FL methods such as FedAvg. We present\nconvergence analysis based on over-parameterized ReLU networks. We show that\nthe proposed method can achieve linear convergence in label projection, and\ndemonstrate the impact of the parameters of our new setting on the convergence\nrate. The proposed method is evaluated and the theoretical findings are\nvalidated on benchmark and medical datasets.",
    "descriptor": "\nComments: 23 pages\n",
    "authors": [
      "Qiong Zhang",
      "Aline Talhouk",
      "Gang Niu",
      "Xiaoxiao Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.02042"
  },
  {
    "id": "arXiv:2210.02045",
    "title": "Coarse-to-Fine Point Cloud Registration with SE(3)-Equivariant  Representations",
    "abstract": "Point cloud registration is a crucial problem in computer vision and\nrobotics. Existing methods either rely on matching local geometric features,\nwhich are sensitive to the pose differences, or leverage global shapes and\nthereby lead to inconsistency when facing distribution variances such as\npartial overlapping. Combining the advantages of both types of methods, we\nadopt a coarse-to-fine pipeline that concurrently handles both issues. We first\nreduce the pose differences between input point clouds by aligning global\nfeatures; then we match the local features to further refine the inaccurate\nalignments resulting from distribution variances. As global feature alignment\nrequires the features to preserve the poses of input point clouds and local\nfeature matching expects the features to be invariant to these poses, we\npropose an SE(3)-equivariant feature extractor to simultaneously generate two\ntypes of features. In this feature extractor, representations preserving the\nposes are first encoded by our novel SE(3)-equivariant network and then\nconverted into pose-invariant ones by a pose-detaching module. Experiments\ndemonstrate that our proposed method increases the recall rate by 20% compared\nto state-of-the-art methods when facing both pose differences and distribution\nvariances.",
    "descriptor": "",
    "authors": [
      "Cheng-Wei Lin",
      "Tung-I Chen",
      "Hsin-Ying Lee",
      "Wen-Chin Chen",
      "Winston H. Hsu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02045"
  },
  {
    "id": "arXiv:2210.02046",
    "title": "Prototype Design and Efficiency Analysis of a Novel Robot Drive Based on  3K-H-V Topology",
    "abstract": "Robot actuators directly affect the performance of robots, and robot drives\ndirectly affect the performance of robot actuators. With the development of\nrobotics, robots have put higher requirements on robot drives, such as high\nstiffness, high accuracy, high loading, high efficiency, low backlash, compact\nsize, and hollow structure. In order to meet the demand development of robot\nactuators, this research base proposes a new robot drive based on 3K-H-V\ntopology using involute and cycloidal gear shapes, planetary cycloidal drive,\nfrom the perspective of drive topology and through the design idea of\ndecoupling. In this study, the reduction ratio and the efficiency model of the\n3K-H-V topology were analyzed, and a prototype planetary cycloidal actuator was\ndesigned. The feasibility of the drive is initially verified by experimentally\nconcluding that the PCA has a hollow structure, compact size, and high torque\ndensity (69 kg/Nm).",
    "descriptor": "",
    "authors": [
      "Le Qi",
      "Dapeng Yang",
      "Baoshi Cao",
      "Zhiqi Li",
      "Yikun Gu",
      "Zongwu Xie",
      "Hong Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02046"
  },
  {
    "id": "arXiv:2210.02051",
    "title": "Weak error analysis for the stochastic Allen-Cahn equation",
    "abstract": "We prove strong rate resp. weak rate ${\\mathcal O}(\\tau)$ for a structure\npreserving temporal discretization (with $\\tau$ the step size) of the\nstochastic Allen-Cahn equation with additive resp. multiplicative colored noise\nin $d=1,2,3$ dimensions. Direct variational arguments exploit the one-sided\nLipschitz property of the cubic nonlinearity in the first setting to settle\nfirst order strong rate. It is the same property which allows for uniform\nbounds for the derivatives of the solution of the related Kolmogorov equation,\nand then leads to weak rate ${\\mathcal O}(\\tau)$ in the presence of\nmultiplicative noise. Hence, we obtain twice the rate of convergence known for\nthe strong error in the presence of multiplicative noise.",
    "descriptor": "",
    "authors": [
      "Dominic Breit",
      "Andreas Prohl"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.02051"
  },
  {
    "id": "arXiv:2210.02054",
    "title": "Placing by Touching: An empirical study on the importance of tactile  sensing for precise object placing",
    "abstract": "Tactile sensors are promising tools for endowing robots with embodied\nintelligence and increased dexterity. These sensors can provide robotic systems\nwith direct information about physical interactions with the world, which is\ndifficult to obtain from extrinsic perception systems. This work deals with a\npractical everyday living problem: stable object placement on flat surfaces\nstarting from unknown initial poses. Common approaches for object placing\neither require complete scene specifications or indirect sensor measurements,\nsuch as cameras which are prone to suffer from occlusions. Instead, this work\nproposes a novel approach for stable object placing that combines tactile\nfeedback and proprioceptive sensing. We devise a neural architecture that\nestimates a rotation matrix which results in a corrective gripper movement that\naligns the object with the table and paves the way for the subsequent stable\nobject placement. We compare models with different sensing modalities, such as\nforce-torque and an external motion capture system, in real-world object\nplacement tasks with different objects. Our experimental evaluation of the\nplacing policies with a set of unknown everyday objects reveals an impressive\ngeneralization of the tactile-based pipeline and suggests that tactile sensing\nplays a vital role in the intrinsic understanding of dexterous object\nmanipulation. Videos of our approach are available at\nhttps://sites.google.com/view/placing-by-touching.",
    "descriptor": "",
    "authors": [
      "Luca Lach",
      "Niklas Funk",
      "Robert Haschke",
      "Severin Lemaignan",
      "Helge Joachim Ritter",
      "Jan Peters",
      "Georgia Chalvatzaki"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02054"
  },
  {
    "id": "arXiv:2210.02055",
    "title": "Learning to Act: Novel Integration of Algorithms and Models for Epidemic  Preparedness",
    "abstract": "In this work we present a framework which may transform research and praxis\nin epidemic planning. Introduced in the context of the ongoing COVID-19\npandemic, we provide a concrete demonstration of the way algorithms may learn\nfrom epidemiological models to scale their value for epidemic preparedness. Our\ncontributions in this work are two fold: 1) a novel platform which makes it\neasy for decision making stakeholders to interact with epidemiological models\nand algorithms developed within the Machine learning community, and 2) the\nrelease of this work under the Apache-2.0 License. The objective of this paper\nis not to look closely at any particular models or algorithms, but instead to\nhighlight how they can be coupled and shared to empower evidence-based decision\nmaking.",
    "descriptor": "\nComments: Presented at the ICLR 2021 Workshop on AI for Public Health. arXiv admin note: substantial text overlap with arXiv:2111.07779\n",
    "authors": [
      "Sekou L. Remy",
      "Oliver E. Bent"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02055"
  },
  {
    "id": "arXiv:2210.02060",
    "title": "Graph Classification via Discriminative Edge Feature Learning",
    "abstract": "Spectral graph convolutional neural networks (GCNNs) have been producing\nencouraging results in graph classification tasks. However, most spectral GCNNs\nutilize fixed graphs when aggregating node features, while omitting edge\nfeature learning and failing to get an optimal graph structure. Moreover, many\nexisting graph datasets do not provide initialized edge features, further\nrestraining the ability of learning edge features via spectral GCNNs. In this\npaper, we try to address this issue by designing an edge feature scheme and an\nadd-on layer between every two stacked graph convolution layers in GCNN. Both\nare lightweight while effective in filling the gap between edge feature\nlearning and performance enhancement of graph classification. The edge feature\nscheme makes edge features adapt to node representations at different graph\nconvolution layers. The add-on layers help adjust the edge features to an\noptimal graph structure. To test the effectiveness of our method, we take\nEuclidean positions as initial node features and extract graphs with semantic\ninformation from point cloud objects. The node features of our extracted graphs\nare more scalable for edge feature learning than most existing graph datasets\n(in one-hot encoded label format). Three new graph datasets are constructed\nbased on ModelNet40, ModelNet10 and ShapeNet Part datasets. Experimental\nresults show that our method outperforms state-of-the-art graph classification\nmethods on the new datasets by reaching 96.56% overall accuracy on\nGraph-ModelNet40, 98.79% on Graph-ModelNet10 and 97.91% on Graph-ShapeNet Part.\nThe constructed graph datasets will be released to the community.",
    "descriptor": "",
    "authors": [
      "Yang Yi",
      "Xuequan Lu",
      "Shang Gao",
      "Antonio Robles-Kelly",
      "Yuejie Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02060"
  },
  {
    "id": "arXiv:2210.02063",
    "title": "Improving Sentiment Analysis By Emotion Lexicon Approach on Vietnamese  Texts",
    "abstract": "The sentiment analysis task has various applications in practice. In the\nsentiment analysis task, words and phrases that represent positive and negative\nemotions are important. Finding out the words that represent the emotion from\nthe text can improve the performance of the classification models for the\nsentiment analysis task. In this paper, we propose a methodology that combines\nthe emotion lexicon with the classification model for enhancing the accuracy of\nthe models. Our experimental results show that the emotion lexicon combined\nwith the classification model improves the performance of models.",
    "descriptor": "\nComments: Accepted at the International Conference on Asian Language Processing (IALP 2022)\n",
    "authors": [
      "An Long Doan",
      "Son T. Luu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02063"
  },
  {
    "id": "arXiv:2210.02067",
    "title": "Computing maximal generalized palindromes",
    "abstract": "Palindromes are popular and important objects in textual data processing,\nbioinformatics, and combinatorics on words. Let $S = XaY$ be a string, where\n$X$ and $Y$ are of the same length and $a$ is either a single character or the\nempty string. Then, there exist two alternative definitions for palindromes:\n$S$ is said to be a palindrome if: Reversal-based definition: $S$ is equal to\nits reversal $S^R$; Symmetry-based definition: its left-arm $X$ is equal to the\nreversal of its right-arm $Y^R$. It is clear that if the \"equality\" ($\\approx$)\nused in both definitions is exact character matching ($=$), then the two\ndefinitions are the same. However, if we apply other string-equality criteria\n$\\approx$, including the complementary model for biological sequences, the\nparameterized model [Baker, JCSS 1996], the order-preserving model [Kim et al.,\nTCS 2014], the Cartesian-tree model [Park et al., TCS 2020], and the\npalindromic-structure model [I et al., TCS 2013], then are the reversal-based\npalindromes and the symmetry-based palindromes the same? To the best of our\nknowledge, no previous work has considered or answered this natural question.\nIn this paper, we first provide answers to this question, and then present\nefficient algorithms for computing all maximal generalized palindromes that\noccur in a given string. After confirming that Gusfield's offline suffix-tree\nbased algorithm for computing maximal symmetry-based palindromes can be readily\nextended to the aforementioned matching models, we show how to extend\nManacher's online algorithm for computing maximal reversal-based palindromes in\nlinear time for all the aforementioned matching models.",
    "descriptor": "",
    "authors": [
      "Mitsuru Funakoshi",
      "Takuya Mieno",
      "Yuto Nakashima",
      "Shunsuke Inenaga",
      "Hideo Bannai",
      "Masayuki Takeda"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.02067"
  },
  {
    "id": "arXiv:2210.02068",
    "title": "Contextualized Generative Retrieval",
    "abstract": "The text retrieval task is mainly performed in two ways: the bi-encoder\napproach and the generative approach. The bi-encoder approach maps the document\nand query embeddings to common vector space and performs a nearest neighbor\nsearch. It stably shows high performance and efficiency across different\ndomains but has an embedding space bottleneck as it interacts in L2 or inner\nproduct space. The generative retrieval model retrieves by generating a target\nsequence and overcomes the embedding space bottleneck by interacting in the\nparametric space. However, it fails to retrieve the information it has not seen\nduring the training process as it depends solely on the information encoded in\nits own model parameters. To leverage the advantages of both approaches, we\npropose Contextualized Generative Retrieval model, which uses contextualized\nembeddings (output embeddings of a language model encoder) as vocab embeddings\nat the decoding step of generative retrieval. The model uses information\nencoded in both the non-parametric space of contextualized token embeddings and\nthe parametric space of the generative retrieval model. Our approach of\ngenerative retrieval with contextualized vocab embeddings shows higher\nperformance than generative retrieval with only vanilla vocab embeddings in the\ndocument retrieval task, an average of 6% higher performance in KILT (NQ, TQA)\nand 2X higher in NQ-320k, suggesting the benefits of using contextualized\nembedding in generative retrieval models.",
    "descriptor": "",
    "authors": [
      "Hyunji Lee",
      "Jaeyoung Kim",
      "Hoyeon Chang",
      "Hanseok Oh",
      "Sohee Yang",
      "Vlad Karpukhin",
      "Yi Lu",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02068"
  },
  {
    "id": "arXiv:2210.02070",
    "title": "Non-Convergence and Limit Cycles in the Adam optimizer",
    "abstract": "One of the most popular training algorithms for deep neural networks is the\nAdaptive Moment Estimation (Adam) introduced by Kingma and Ba. Despite its\nsuccess in many applications there is no satisfactory convergence analysis:\nonly local convergence can be shown for batch mode under some restrictions on\nthe hyperparameters, counterexamples exist for incremental mode. Recent results\nshow that for simple quadratic objective functions limit cycles of period 2\nexist in batch mode, but only for atypical hyperparameters, and only for the\nalgorithm without bias correction. %More general there are several more\nadaptive gradient methods which try to estimate a fitting learning rate and /\nor search direction from the training data to improve the learning process\ncompared to pure gradient descent with fixed learningrate. We extend the\nconvergence analysis for Adam in the batch mode with bias correction and show\nthat even for quadratic objective functions as the simplest case of convex\nfunctions 2-limit-cycles exist, for all choices of the hyperparameters. We\nanalyze the stability of these limit cycles and relate our analysis to other\nresults where approximate convergence was shown, but under the additional\nassumption of bounded gradients which does not apply to quadratic functions.\nThe investigation heavily relies on the use of computer algebra due to the\ncomplexity of the equations.",
    "descriptor": "",
    "authors": [
      "Sebastian Bock",
      "Martin Georg Wei\u00df"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02070"
  },
  {
    "id": "arXiv:2210.02071",
    "title": "Advanced Deep Learning Architectures for Accurate Detection of  Subsurface Tile Drainage Pipes from Remote Sensing Images",
    "abstract": "Subsurface tile drainage pipes provide agronomic, economic and environmental\nbenefits. By lowering the water table of wet soils, they improve the aeration\nof plant roots and ultimately increase the productivity of farmland. They do\nhowever also provide an entryway of agrochemicals into subsurface water bodies\nand increase nutrition loss in soils. For maintenance and infrastructural\ndevelopment, accurate maps of tile drainage pipe locations and drained\nagricultural land are needed. However, these maps are often outdated or not\npresent. Different remote sensing (RS) image processing techniques have been\napplied over the years with varying degrees of success to overcome these\nrestrictions. Recent developments in deep learning (DL) techniques improve upon\nthe conventional techniques with machine learning segmentation models. In this\nstudy, we introduce two DL-based models: i) improved U-Net architecture; and\nii) Visual Transformer-based encoder-decoder in the framework of tile drainage\npipe detection. Experimental results confirm the effectiveness of both models\nin terms of detection accuracy when compared to a basic U-Net architecture. Our\ncode and models are publicly available at\n\\url{https://git.tu-berlin.de/rsim/drainage-pipes-detection}.",
    "descriptor": "\nComments: Accepted in SPIE Remote Sensing (ESI22R). For code visit: this https URL\n",
    "authors": [
      "Tom L. Breitkopf",
      "Leonard W. Hackel",
      "Mahdyar Ravanbakhsh",
      "Anne-Karin Cooke",
      "Sandra Willkommen",
      "Stefan Broda",
      "Beg\u00fcm Demir"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.02071"
  },
  {
    "id": "arXiv:2210.02074",
    "title": "Two Video Data Sets for Tracking and Retrieval of Out of Distribution  Objects",
    "abstract": "In this work we present two video test data sets for the novel computer\nvision (CV) task of out of distribution tracking (OOD tracking). Here, OOD\nobjects are understood as objects with a semantic class outside the semantic\nspace of an underlying image segmentation algorithm, or an instance within the\nsemantic space which however looks decisively different from the instances\ncontained in the training data. OOD objects occurring on video sequences should\nbe detected on single frames as early as possible and tracked over their time\nof appearance as long as possible. During the time of appearance, they should\nbe segmented as precisely as possible. We present the SOS data set containing\n20 video sequences of street scenes and more than 1000 labeled frames with up\nto two OOD objects. We furthermore publish the synthetic CARLA-WildLife data\nset that consists of 26 video sequences containing up to four OOD objects on a\nsingle frame. We propose metrics to measure the success of OOD tracking and\ndevelop a baseline algorithm that efficiently tracks the OOD objects. As an\napplication that benefits from OOD tracking, we retrieve OOD sequences from\nunlabeled videos of street scenes containing OOD objects.",
    "descriptor": "",
    "authors": [
      "Kira Maag",
      "Robin Chan",
      "Svenja Uhlemeyer",
      "Kamil Kowol",
      "Hanno Gottschalk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02074"
  },
  {
    "id": "arXiv:2210.02075",
    "title": "On the Learning Mechanisms in Physical Reasoning",
    "abstract": "Is dynamics prediction indispensable for physical reasoning? If so, what kind\nof roles do the dynamics prediction modules play during the physical reasoning\nprocess? Most studies focus on designing dynamics prediction networks and\ntreating physical reasoning as a downstream task without investigating the\nquestions above, taking for granted that the designed dynamics prediction would\nundoubtedly help the reasoning process. In this work, we take a closer look at\nthis assumption, exploring this fundamental hypothesis by comparing two\nlearning mechanisms: Learning from Dynamics (LfD) and Learning from Intuition\n(LfI). In the first experiment, we directly examine and compare these two\nmechanisms. Results show a surprising finding: Simple LfI is better than or on\npar with state-of-the-art LfD. This observation leads to the second experiment\nwith Ground-truth Dynamics, the ideal case of LfD wherein dynamics are obtained\ndirectly from a simulator. Results show that dynamics, if directly given\ninstead of approximated, would achieve much higher performance than LfI alone\non physical reasoning; this essentially serves as the performance upper bound.\nYet practically, LfD mechanism can only predict Approximate Dynamics using\ndynamics learning modules that mimic the physical laws, making the following\ndownstream physical reasoning modules degenerate into the LfI paradigm; see the\nthird experiment. We note that this issue is hard to mitigate, as dynamics\nprediction errors inevitably accumulate in the long horizon. Finally, in the\nfourth experiment, we note that LfI, the extremely simpler strategy when done\nright, is more effective in learning to solve physical reasoning problems.\nTaken together, the results on the challenging benchmark of PHYRE show that LfI\nis, if not better, as good as LfD for dynamics prediction. However, the\npotential improvement from LfD, though challenging, remains lucrative.",
    "descriptor": "\nComments: 17 pages, NeurIPS 2022\n",
    "authors": [
      "Shiqian Li",
      "Kewen Wu",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02075"
  },
  {
    "id": "arXiv:2210.02077",
    "title": "Exploring The Role of Mean Teachers in Self-supervised Masked  Auto-Encoders",
    "abstract": "Masked image modeling (MIM) has become a popular strategy for self-supervised\nlearning~(SSL) of visual representations with Vision Transformers. A\nrepresentative MIM model, the masked auto-encoder (MAE), randomly masks a\nsubset of image patches and reconstructs the masked patches given the unmasked\npatches. Concurrently, many recent works in self-supervised learning utilize\nthe student/teacher paradigm which provides the student with an additional\ntarget based on the output of a teacher composed of an exponential moving\naverage (EMA) of previous students. Although common, relatively little is known\nabout the dynamics of the interaction between the student and teacher. Through\nanalysis on a simple linear model, we find that the teacher conditionally\nremoves previous gradient directions based on feature similarities which\neffectively acts as a conditional momentum regularizer. From this analysis, we\npresent a simple SSL method, the Reconstruction-Consistent Masked Auto-Encoder\n(RC-MAE) by adding an EMA teacher to MAE. We find that RC-MAE converges faster\nand requires less memory usage than state-of-the-art self-distillation methods\nduring pre-training, which may provide a way to enhance the practicality of\nprohibitively expensive self-supervised learning of Vision Transformer models.\nAdditionally, we show that RC-MAE achieves more robustness and better\nperformance compared to MAE on downstream tasks such as ImageNet-1K\nclassification, object detection, and instance segmentation.",
    "descriptor": "\nComments: pre-print\n",
    "authors": [
      "Youngwan Lee",
      "Jeffrey Willette",
      "Jonghee Kim",
      "Juho Lee",
      "Sung Ju Hwang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02077"
  },
  {
    "id": "arXiv:2210.02081",
    "title": "Locate before Answering: Answer Guided Question Localization for Video  Question Answering",
    "abstract": "Video question answering (VideoQA) is an essential task in vision-language\nunderstanding, which has attracted numerous research attention recently.\nNevertheless, existing works mostly achieve promising performances on short\nvideos of duration within 15 seconds. For VideoQA on minute-level long-term\nvideos, those methods are likely to fail because of lacking the ability to deal\nwith noise and redundancy caused by scene changes and multiple actions in the\nvideo. Considering the fact that the question often remains concentrated in a\nshort temporal range, we propose to first locate the question to a segment in\nthe video and then infer the answer using the located segment only. Under this\nscheme, we propose \"Locate before Answering\" (LocAns), a novel approach that\nintegrates a question locator and an answer predictor into an end-to-end model.\nDuring the training phase, the available answer label not only serves as the\nsupervision signal of the answer predictor, but also is used to generate pseudo\ntemporal labels for the question locator. Moreover, we design a decoupled\nalternative training strategy to update the two modules separately. In the\nexperiments, LocAns achieves state-of-the-art performance on two modern\nlong-term VideoQA datasets NExT-QA and ActivityNet-QA, and its qualitative\nexamples show the reliable performance of the question localization.",
    "descriptor": "",
    "authors": [
      "Tianwen Qian",
      "Ran Cui",
      "Jingjing Chen",
      "Pai Peng",
      "Xiaowei Guo",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02081"
  },
  {
    "id": "arXiv:2210.02082",
    "title": "Jitter Does Matter: Adapting Gaze Estimation to New Domains",
    "abstract": "Deep neural networks have demonstrated superior performance on\nappearance-based gaze estimation tasks. However, due to variations in person,\nilluminations, and background, performance degrades dramatically when applying\nthe model to a new domain. In this paper, we discover an interesting gaze\njitter phenomenon in cross-domain gaze estimation, i.e., the gaze predictions\nof two similar images can be severely deviated in target domain. This is\nclosely related to cross-domain gaze estimation tasks, but surprisingly, it has\nnot been noticed yet previously. Therefore, we innovatively propose to utilize\nthe gaze jitter to analyze and optimize the gaze domain adaptation task. We\nfind that the high-frequency component (HFC) is an important factor that leads\nto jitter. Based on this discovery, we add high-frequency components to input\nimages using the adversarial attack and employ contrastive learning to\nencourage the model to obtain similar representations between original and\nperturbed data, which reduces the impacts of HFC. We evaluate the proposed\nmethod on four cross-domain gaze estimation tasks, and experimental results\ndemonstrate that it significantly reduces the gaze jitter and improves the gaze\nestimation performance in target domains.",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Ruicong Liu",
      "Yiwei Bao",
      "Mingjie Xu",
      "Haofei Wang",
      "Yunfei Liu",
      "Feng Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02082"
  },
  {
    "id": "arXiv:2210.02083",
    "title": "Multi-View Independent Component Analysis with Shared and Individual  Sources",
    "abstract": "Independent component analysis (ICA) is a blind source separation method for\nlinear disentanglement of independent latent sources from observed data. We\ninvestigate the special setting of noisy linear ICA where the observations are\nsplit among different views, each receiving a mixture of shared and individual\nsources. We prove that the corresponding linear structure is identifiable, and\nthe shared sources can be recovered, provided that sufficiently many diverse\nviews and data points are available. To computationally estimate the sources,\nwe optimize a constrained form of the joint log-likelihood of the observed data\namong all views. We show empirically that our objective recovers the sources in\nhigh dimensional settings, also in the case when the measurements are corrupted\nby noise. Finally, we apply the proposed model in a challenging real-life\napplication, where the estimated shared sources from two large transcriptome\ndatasets (observed data) provided by two different labs (two different views)\nlead to a more plausible representation of the underlying graph structure than\nexisting baselines.",
    "descriptor": "",
    "authors": [
      "Teodora Pandeva",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.02083"
  },
  {
    "id": "arXiv:2210.02085",
    "title": "DooML: A new Database & Object-Oriented Modeling Language for  database-driven web application design and development",
    "abstract": "A database driven web application is a very common software solution to\nrising business problems. Modeling the database and the software architecture\ncan be challenging, hence there not being one combined modeling language for\ndatabase and software architecture, specifically suited for web application\ndevelopment. In this paper we present Database object-oriented Modeling\nLanguage (DooML) and its primary Archetype Diagram: a notation for specifying\nthe design of a database schema and corresponding object-oriented software\narchitecture. It combines the syntax for drawing Entity Relationship Diagrams,\nthe Relational Model and Universal Modeling Language Class Diagrams as well to\ncreate a mixed diagram, stating database design as well as software design\nspecifications. By default, DooML ensures that the approach of advanced web\napplication development is model-driven and both database-oriented as well as\nobject-oriented.",
    "descriptor": "\nComments: 9 pages. International Journal of Software Engineering & Applications (IJSEA), Chennai, India (2022)\n",
    "authors": [
      "Thijs Otter"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.02085"
  },
  {
    "id": "arXiv:2210.02087",
    "title": "Bilinear Exponential Family of MDPs: Frequentist Regret Bound with  Tractable Exploration and Planning",
    "abstract": "We study the problem of episodic reinforcement learning in continuous\nstate-action spaces with unknown rewards and transitions. Specifically, we\nconsider the setting where the rewards and transitions are modeled using\nparametric bilinear exponential families. We propose an algorithm, BEF-RLSVI,\nthat a) uses penalized maximum likelihood estimators to learn the unknown\nparameters, b) injects a calibrated Gaussian noise in the parameter of rewards\nto ensure exploration, and c) leverages linearity of the exponential family\nwith respect to an underlying RKHS to perform tractable planning. We further\nprovide a frequentist regret analysis of BEF-RLSVI that yields an upper bound\nof $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3K})$, where $d$ is the dimension of the\nparameters, $H$ is the episode length, and $K$ is the number of episodes. Our\nanalysis improves the existing bounds for the bilinear exponential family of\nMDPs by $\\sqrt{H}$ and removes the handcrafted clipping deployed in existing\n\\RLSVI-type algorithms. Our regret bound is order-optimal with respect to $H$\nand $K$.",
    "descriptor": "",
    "authors": [
      "Reda Ouhamma",
      "Debabrota Basu",
      "Odalric-Ambrym Maillard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02087"
  },
  {
    "id": "arXiv:2210.02088",
    "title": "WUDA: Unsupervised Domain Adaptation Based on Weak Source Domain Labels",
    "abstract": "Unsupervised domain adaptation (UDA) for semantic segmentation addresses the\ncross-domain problem with fine source domain labels. However, the acquisition\nof semantic labels has always been a difficult step, many scenarios only have\nweak labels (e.g. bounding boxes). For scenarios where weak supervision and\ncross-domain problems coexist, this paper defines a new task: unsupervised\ndomain adaptation based on weak source domain labels (WUDA). To explore\nsolutions for this task, this paper proposes two intuitive frameworks: 1)\nPerform weakly supervised semantic segmentation in the source domain, and then\nimplement unsupervised domain adaptation; 2) Train an object detection model\nusing source domain data, then detect objects in the target domain and\nimplement weakly supervised semantic segmentation. We observe that the two\nframeworks behave differently when the datasets change. Therefore, we construct\ndataset pairs with a wide range of domain shifts and conduct extended\nexperiments to analyze the impact of different domain shifts on the two\nframeworks. In addition, to measure domain shift, we apply the metric\nrepresentation shift to urban landscape image segmentation for the first time.\nThe source code and constructed datasets are available at\n\\url{https://github.com/bupt-ai-cz/WUDA}.",
    "descriptor": "",
    "authors": [
      "Shengjie Liu",
      "Chuang Zhu",
      "Wenqi Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02088"
  },
  {
    "id": "arXiv:2210.02089",
    "title": "Transformer-based conditional generative adversarial network for  multivariate time series generation",
    "abstract": "Conditional generation of time-dependent data is a task that has much\ninterest, whether for data augmentation, scenario simulation, completing\nmissing data, or other purposes. Recent works proposed a Transformer-based Time\nseries generative adversarial network (TTS-GAN) to address the limitations of\nrecurrent neural networks. However, this model assumes a unimodal distribution\nand tries to generate samples around the expectation of the real data\ndistribution. One of its limitations is that it may generate a random\nmultivariate time series; it may fail to generate samples in the presence of\nmultiple sub-components within an overall distribution. One could train models\nto fit each sub-component separately to overcome this limitation. Our work\nextends the TTS-GAN by conditioning its generated output on a particular\nencoded context allowing the use of one model to fit a mixture distribution\nwith multiple sub-components. Technically, it is a conditional generative\nadversarial network that models realistic multivariate time series under\ndifferent types of conditions, such as categorical variables or multivariate\ntime series. We evaluate our model on UniMiB Dataset, which contains\nacceleration data following the XYZ axes of human activities collected using\nSmartphones. We use qualitative evaluations and quantitative metrics such as\nPrincipal Component Analysis (PCA), and we introduce a modified version of the\nFrechet inception distance (FID) to measure the performance of our model and\nthe statistical similarities between the generated and the real data\ndistributions. We show that this transformer-based CGAN can generate realistic\nhigh-dimensional and long data sequences under different kinds of conditions.",
    "descriptor": "",
    "authors": [
      "Abdellah Madane",
      "Mohamed-djallel Dilmi",
      "Florent Forest",
      "Hanane Azzag",
      "Mustapha Lebbah",
      "Jerome Lacaille"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02089"
  },
  {
    "id": "arXiv:2210.02090",
    "title": "Joint Communication and Computation in Hybrid Cloud/Mobile Edge  Computing Networks",
    "abstract": "Facing a vast amount of connections, huge performance demands, and the need\nfor reliable connectivity, the sixth generation of communication networks (6G)\nis envisioned to implement disruptive technologies that jointly spur\nconnectivity, performance, and reliability. In this context, this paper\nproposes, and evaluates the benefit of, a hybrid central cloud (CC) computing\nand mobile edge computing (MEC) platform, especially introduced to balance the\nnetwork resources required for joint computation and communication. Consider a\nhybrid cloud and MEC system, where several power-hungry multi-antenna unmanned\naerial vehicles (UAVs) are deployed at the cell-edge to boost the CC\nconnectivity and relieve part of its computation burden. While the\nmulti-antenna base stations are connected to the cloud via capacity-limited\nfronthaul links, the UAVs serve the cell-edge users with limited power and\ncomputational capabilities. The paper then considers the problem of maximizing\nthe weighted network sum-rate subject to per-user delay, computational\ncapacity, and power constraints, so as to determine the beamforming vectors and\ncomputation allocations. Such intricate non-convex optimization problem is\ntackled using an iterative algorithm that relies on $\\ell_0$-norm relaxation,\nsuccessive convex approximation, and fractional programming, and has the\ncompelling ability to be implemented in a distributed fashion across the\nmultiple UAVs and the CC. The paper results illustrate the numerical prospects\nof the proposed algorithm for enabling joint communication and computation, and\nhighlight the appreciable improvements of data processing delays and\nthroughputs as compared to conventional system strategies.",
    "descriptor": "\nComments: 7 pages, 3 figures, 2 tables, copyright IEEE, the paper has been accepted for publication at the IEEE Global Communications Conference (Globecom) Workshops, 978-1-6654-5975-4/22 c 2022 IEEE\n",
    "authors": [
      "Robert-Jeron Reifert",
      "Hayssam Dahrouj",
      "Basem Shihada",
      "Aydin Sezgin",
      "Tareq Y. Al-Naffouri",
      "Mohamed-Slim Alouini"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.02090"
  },
  {
    "id": "arXiv:2210.02091",
    "title": "Tripletformer for Probabilistic Interpolation of Asynchronous Time  Series",
    "abstract": "Asynchronous time series are often observed in several applications such as\nhealth care, astronomy, and climate science, and pose a significant challenge\nto the standard deep learning architectures. Interpolation of asynchronous time\nseries is vital for many real-world tasks like root cause analysis, and medical\ndiagnosis. In this paper, we propose a novel encoder-decoder architecture\ncalled Tripletformer, which works on the set of observations where each set\nelement is a triple of time, channel, and value, for the probabilistic\ninterpolation of the asynchronous time series. Both the encoder and the decoder\nof the Tripletformer are modeled using attention layers and fully connected\nlayers and are invariant to the order in which set elements are presented. The\nproposed Tripletformer is compared with a range of baselines over multiple\nreal-world and synthetic asynchronous time series datasets, and the\nexperimental results attest that it produces more accurate and certain\ninterpolations. We observe an improvement in negative loglikelihood error up to\n33% over real and 800% over synthetic asynchronous time series datasets\ncompared to the state-of-the-art model using the Tripletformer.",
    "descriptor": "",
    "authors": [
      "Vijaya Krishna Yalavarthi",
      "Johannes Burchert",
      "Lars Schmidt-thieme"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02091"
  },
  {
    "id": "arXiv:2210.02093",
    "title": "Centralized Feature Pyramid for Object Detection",
    "abstract": "Visual feature pyramid has shown its superiority in both effectiveness and\nefficiency in a wide range of applications. However, the existing methods\nexorbitantly concentrate on the inter-layer feature interactions but ignore the\nintra-layer feature regulations, which are empirically proved beneficial.\nAlthough some methods try to learn a compact intra-layer feature representation\nwith the help of the attention mechanism or the vision transformer, they ignore\nthe neglected corner regions that are important for dense prediction tasks. To\naddress this problem, in this paper, we propose a Centralized Feature Pyramid\n(CFP) for object detection, which is based on a globally explicit centralized\nfeature regulation. Specifically, we first propose a spatial explicit visual\ncenter scheme, where a lightweight MLP is used to capture the globally\nlong-range dependencies and a parallel learnable visual center mechanism is\nused to capture the local corner regions of the input images. Based on this, we\nthen propose a globally centralized regulation for the commonly-used feature\npyramid in a top-down fashion, where the explicit visual center information\nobtained from the deepest intra-layer feature is used to regulate frontal\nshallow features. Compared to the existing feature pyramids, CFP not only has\nthe ability to capture the global long-range dependencies, but also efficiently\nobtain an all-round yet discriminative feature representation. Experimental\nresults on the challenging MS-COCO validate that our proposed CFP can achieve\nthe consistent performance gains on the state-of-the-art YOLOv5 and YOLOX\nobject detection baselines.",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Yu Quan",
      "Dong Zhang",
      "Liyan Zhang",
      "Jinhui Tang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02093"
  },
  {
    "id": "arXiv:2210.02095",
    "title": "ChemAlgebra: Algebraic Reasoning on Chemical Reactions",
    "abstract": "While showing impressive performance on various kinds of learning tasks, it\nis yet unclear whether deep learning models have the ability to robustly tackle\nreasoning tasks. than by learning the underlying reasoning process that is\nactually required to solve the tasks. Measuring the robustness of reasoning in\nmachine learning models is challenging as one needs to provide a task that\ncannot be easily shortcut by exploiting spurious statistical correlations in\nthe data, while operating on complex objects and constraints. reasoning task.\nTo address this issue, we propose ChemAlgebra, a benchmark for measuring the\nreasoning capabilities of deep learning models through the prediction of\nstoichiometrically-balanced chemical reactions. ChemAlgebra requires\nmanipulating sets of complex discrete objects -- molecules represented as\nformulas or graphs -- under algebraic constraints such as the mass preservation\nprinciple. We believe that ChemAlgebra can serve as a useful test bed for the\nnext generation of machine reasoning models and as a promoter of their\ndevelopment.",
    "descriptor": "",
    "authors": [
      "Andrea Valenti",
      "Davide Bacciu",
      "Antonio Vergari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.02095"
  },
  {
    "id": "arXiv:2210.02097",
    "title": "Teaching Yourself:c Graph Self-Distillation on Neighborhood for Node  Classification",
    "abstract": "Recent years have witnessed great success in handling graph-related tasks\nwith Graph Neural Networks (GNNs). Despite their great academic success,\nMulti-Layer Perceptrons (MLPs) remain the primary workhorse for practical\nindustrial applications. One reason for this academic-industrial gap is the\nneighborhood-fetching latency incurred by data dependency in GNNs, which make\nit hard to deploy for latency-sensitive applications that require fast\ninference. Conversely, without involving any feature aggregation, MLPs have no\ndata dependency and infer much faster than GNNs, but their performance is less\ncompetitive. Motivated by these complementary strengths and weaknesses, we\npropose a Graph Self-Distillation on Neighborhood (GSDN) framework to reduce\nthe gap between GNNs and MLPs. Specifically, the GSDN framework is based purely\non MLPs, where structural information is only implicitly used as prior to guide\nknowledge self-distillation between the neighborhood and the target,\nsubstituting the explicit neighborhood information propagation as in GNNs. As a\nresult, GSDN enjoys the benefits of graph topology-awareness in training but\nhas no data dependency in inference. Extensive experiments have shown that the\nperformance of vanilla MLPs can be greatly improved with self-distillation,\ne.g., GSDN improves over stand-alone MLPs by 15.54\\% on average and outperforms\nthe state-of-the-art GNNs on six datasets. Regarding inference speed, GSDN\ninfers 75X-89X faster than existing GNNs and 16X-25X faster than other\ninference acceleration methods.",
    "descriptor": "",
    "authors": [
      "Lirong Wu",
      "Jun Xia",
      "Haitao Lin",
      "Zhangyang Gao",
      "Zicheng Liu",
      "Guojiang Zhao",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02097"
  },
  {
    "id": "arXiv:2210.02099",
    "title": "Automated Graph Self-supervised Learning via Multi-teacher Knowledge  Distillation",
    "abstract": "Self-supervised learning on graphs has recently achieved remarkable success\nin graph representation learning. With hundreds of self-supervised pretext\ntasks proposed over the past few years, the research community has greatly\ndeveloped, and the key is no longer to design more powerful but complex pretext\ntasks, but to make more effective use of those already on hand. This paper\nstudies the problem of how to automatically, adaptively, and dynamically learn\ninstance-level self-supervised learning strategies for each node from a given\npool of pretext tasks. In this paper, we propose a novel multi-teacher\nknowledge distillation framework for Automated Graph Self-Supervised Learning\n(AGSSL), which consists of two main branches: (i) Knowledge Extraction:\ntraining multiple teachers with different pretext tasks, so as to extract\ndifferent levels of knowledge with different inductive biases; (ii) Knowledge\nIntegration: integrating different levels of knowledge and distilling them into\nthe student model. Without simply treating different teachers as equally\nimportant, we provide a provable theoretical guideline for how to integrate the\nknowledge of different teachers, i.e., the integrated teacher probability\nshould be close to the true Bayesian class-probability. To approach the\ntheoretical optimum in practice, two adaptive knowledge integration strategies\nare proposed to construct a relatively \"good\" integrated teacher. Extensive\nexperiments on eight datasets show that AGSSL can benefit from multiple pretext\ntasks, outperforming the corresponding individual tasks; by combining a few\nsimple but classical pretext tasks, the resulting performance is comparable to\nother leading counterparts.",
    "descriptor": "",
    "authors": [
      "Lirong Wu",
      "Yufei Huang",
      "Haitao Lin",
      "Zicheng Liu",
      "Tianyu Fan",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02099"
  },
  {
    "id": "arXiv:2210.02101",
    "title": "Unconditional convergence of conservative spectral Galerkin methods for  the coupled fractional nonlinear Klein-Gordon-Schr\u00f6dinger equations",
    "abstract": "In this work, two novel classes of structure-preserving spectral Galerkin\nmethods are proposed which based on the Crank-Nicolson scheme and the\nexponential scalar auxiliary variable method respectively, for solving the\ncoupled fractional nonlinear Klein-Gordon-Schr\\\"odinger equation. The paper\nfocuses on the theoretical analyses and computational efficiency of the\nproposed schemes, the Crank-Nicoloson scheme is proved to be unconditionally\nconvergent and has the maximum-norm boundness of numerical solutions. The\nexponential scalar auxiliary variable scheme is linearly implicit and\ndecoupled, but lack of the maximum-norm boundness, also, the energy structure\nhas been modified. Subsequently, the efficient implementations of the proposed\nschemes are introduced in detail. Both the theoretical analyses and the\nnumerical comparisons show that the proposed spectral Galerkin methods have\nhigh efficiency in long-time computations.",
    "descriptor": "",
    "authors": [
      "Dongdong Hu",
      "Yayun Fu",
      "Wenjun Cai",
      "Yushun Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.02101"
  },
  {
    "id": "arXiv:2210.02102",
    "title": "An Architectural Approach to Creating a Cloud Application for Developing  Microservices",
    "abstract": "The cloud is a new paradigm that is paving the way for new approaches and\nstandards. The architectural styles are evolving in response to the cloud's\nrequirements. In recent years, microservices have emerged as the preferred\narchitectural style for scalable, rapidly evolving cloud applications. The\nadoption of microservices to the detriment of monolithic structures, which are\nincreasingly being phased out, is one of the most significant developments in\nbusiness architecture. Cloud-native architectures make microservices system\ndeployment more productive, adaptable, and cost-effective. Regardless, many\nfirms have begun to transition from one type of architecture to another, though\nthis is still in its early stages. The primary purpose of this article is to\ngain a better understanding of how to design microservices through developing\ncloud apps, as well as current microservices trends, the reason for\nmicroservices research, emerging standards, and prospective research gaps.\nResearchers and practitioners in software engineering can use the data to stay\ncurrent on SOA and cloud computing developments.",
    "descriptor": "\nComments: 12 pages, 5 figures, 1 table\n",
    "authors": [
      "A. N. M. Sajedul Alam",
      "Junaid Bin Kibria",
      "Al Hasib Mahamud",
      "Arnob Kumar Dey",
      "Hasan Muhammed Zahidul Amin",
      "Md Sabbir Hossain",
      "Annajiat Alim Rasel"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.02102"
  },
  {
    "id": "arXiv:2210.02109",
    "title": "ProxNLP: a primal-dual augmented Lagrangian solver for nonlinear  programming in Robotics and beyond",
    "abstract": "Mathematical optimization is the workhorse behind several aspects of modern\nrobotics and control. In these applications, the focus is on constrained\noptimization, and the ability to work on manifolds (such as the classical\nmatrix Lie groups), along with a specific requirement for robustness and speed.\nIn recent years, augmented Lagrangian methods have seen a resurgence due to\ntheir robustness and flexibility, their connections to (inexact) proximal-point\nmethods, and their interoperability with Newton or semismooth Newton methods.\nIn the sequel, we present primal-dual augmented Lagrangian method for\ninequality-constrained problems on manifolds, which we introduced in our recent\nwork, as well as an efficient C++ implementation suitable for use in robotics\napplications and beyond.",
    "descriptor": "\nComments: Workshop paper at the 6th Legged Robots Workshop, at the IEEE International Conference on Robotics and Automation (ICRA) 2022\n",
    "authors": [
      "Wilson Jallet",
      "Antoine Bambade",
      "Nicolas Mansard",
      "Justin Carpentier"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02109"
  },
  {
    "id": "arXiv:2210.02117",
    "title": "Tight Lower Bounds for Problems Parameterized by Rank-width",
    "abstract": "We show that there is no $2^{o(k^2)} n^{O(1)}$ time algorithm for Independent\nSet on $n$-vertex graphs with rank-width $k$, unless the Exponential Time\nHypothesis (ETH) fails. Our lower bound matches the $2^{O(k^2)} n^{O(1)}$ time\nalgorithm given by Bui-Xuan, Telle, and Vatshelle [Discret.~Appl.~Math., 2010]\nand it answers the open question of Bergougnoux and Kant\\'{e} [SIAM J. Discret.\nMath.,~2021]. We also show that the known $2^{O(k^2)} n^{O(1)}$ time algorithms\nfor Weighted Dominating Set, Maximum Induced Matching and Feedback Vertex Set\nparameterized by rank-width $k$ are optimal assuming ETH. Our results are the\nfirst tight ETH lower bounds parameterized by rank-width that do not follow\ndirectly from lower bounds for $n$-vertex graphs.",
    "descriptor": "",
    "authors": [
      "Benjamin Bergougnoux",
      "Tuukka Korhonen",
      "Jesper Nederlof"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.02117"
  },
  {
    "id": "arXiv:2210.02119",
    "title": "ISFL: Trustworthy Federated Learning for Non-i.i.d. Data with Local  Importance Sampling",
    "abstract": "As a promising integrated computation and communication learning paradigm,\nfederated learning (FL) carries a periodic sharing from distributed clients.\nDue to the non-i.i.d. data distribution on clients, FL model suffers from the\ngradient diversity, poor performance, bad convergence, etc. In this work, we\naim to tackle this key issue by adopting data-driven importance sampling (IS)\nfor local training. We propose a trustworthy framework, named importance\nsampling federated learning (ISFL), which is especially compatible with neural\nnetwork (NN) models. The framework is evaluated both theoretically and\nexperimentally. Firstly, we derive the parameter deviation bound between ISFL\nand the centralized full-data training to identify the main factors of the\nnon-i.i.d. dilemmas. We will then formulate the selection of optimal IS weights\nas an optimization problem and obtain theoretical solutions. We also employ\nwater-filling methods to calculate the IS weights and develop the complete ISFL\nalgorithms. The experimental results on CIFAR-10 fit our proposed theories well\nand prove that ISFL reaps higher performance, as well as better convergence on\nnon-i.i.d. data. To the best of our knowledge, ISFL is the first non-i.i.d. FL\nsolution from the local sampling aspect which exhibits theoretical NN\ncompatibility. Furthermore, as a local sampling approach, ISFL can be easily\nmigrated into emerging FL frameworks.",
    "descriptor": "",
    "authors": [
      "Zheqi Zhu",
      "Pingyi Fan",
      "Chenghui Peng",
      "Khaled B. Letaief"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.02119"
  },
  {
    "id": "arXiv:2210.02121",
    "title": "A spectral algorithm for finding maximum cliques in dense random  intersection graphs",
    "abstract": "In a random intersection graph $G_{n,m,p}$, each of $n$ vertices selects a\nrandom subset of a set of $m$ labels by including each label independently with\nprobability $p$ and edges are drawn between vertices that have at least one\nlabel in common. Among other applications, such graphs have been used to model\nsocial networks, in which individuals correspond to vertices and various\nfeatures (e.g. ideas, interests) correspond to labels; individuals sharing at\nleast one common feature are connected and this is abstracted by edges in\nrandom intersection graphs. In this paper, we consider the problem of finding\nmaximum cliques when the input graph is $G_{n,m,p}$. Current algorithms for\nthis problem are successful with high probability only for relatively sparse\ninstances, leaving the dense case mostly unexplored. We present a spectral\nalgorithm for finding large cliques that processes vertices according to\nrespective values in the second largest eigenvector of the adjacency matrix of\ninduced subgraphs of the input graph corresponding to common neighbors of small\ncliques. Leveraging on the Single Label Clique Theorem from [15], we were able\nto construct random instances, without the need to externally plant a large\nclique in the input graph. In particular, we used label choices to determine\nthe maximum clique and then concealed label information by just giving the\nadjacency matrix of $G_{n, m, p}$ as input to the algorithm. Our experimental\nevaluation showed that our spectral algorithm clearly outperforms existing\npolynomial time algorithms, both with respect to the failure probability and\nthe approximation guarantee metrics, especially in the dense regime, thus\nsuggesting that spectral properties of random intersection graphs may be also\nused to construct efficient algorithms for other NP-hard graph theoretical\nproblems as well.",
    "descriptor": "",
    "authors": [
      "Filippos Christodoulou",
      "Sotiris Nikoletseas",
      "Christoforos Raptopoulos",
      "Paul Spirakis"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.02121"
  },
  {
    "id": "arXiv:2210.02123",
    "title": "On the Influence of Cognitive Styles on Users' Understanding of  Explanations",
    "abstract": "Artificial intelligence (AI) is becoming increasingly complex, making it\ndifficult for users to understand how the AI has derived its prediction. Using\nexplainable AI (XAI)-methods, researchers aim to explain AI decisions to users.\nSo far, XAI-based explanations pursue a technology-focused approach -\nneglecting the influence of users' cognitive abilities and differences in\ninformation processing on the understanding of explanations. Hence, this study\ntakes a human-centered perspective and incorporates insights from cognitive\npsychology. In particular, we draw on the psychological construct of cognitive\nstyles that describe humans' characteristic modes of processing information.\nApplying a between-subject experiment design, we investigate how users'\nrational and intuitive cognitive styles affect their objective and subjective\nunderstanding of different types of explanations provided by an AI. Initial\nresults indicate substantial differences in users' understanding depending on\ntheir cognitive style. We expect to contribute to a more nuanced view of the\ninterrelation of human factors and XAI design.",
    "descriptor": "\nComments: Accepted at 43rd International Conference on Information Systems (ICIS 2022)\n",
    "authors": [
      "Lara Riefle",
      "Patrick Hemmer",
      "Carina Benz",
      "Michael V\u00f6ssing",
      "Jannik Pries"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.02123"
  },
  {
    "id": "arXiv:2210.02125",
    "title": "Statistical characterization of the chordal product determinant of  Grassmannian codes",
    "abstract": "We consider the chordal product determinant, a measure of the distance\nbetween two subspaces of the same dimension. In information theory, collections\nof elements in the complex Grassmannian are searched with the property that\ntheir pairwise chordal products are as large as possible. We characterize this\nfunction from an statistical perspective, which allows us to obtain bounds for\nthe minimal chordal product and related energy of such collections.",
    "descriptor": "\nComments: 13 pages, 2 figures\n",
    "authors": [
      "Javier \u00c1lvarez-Vizoso",
      "Carlos Beltr\u00e1n",
      "Diego Cuevas",
      "Ignacio Santamar\u0131a",
      "Vit Tucek",
      "Gunnar Peters"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.02125"
  },
  {
    "id": "arXiv:2210.02127",
    "title": "Visual-Inertial and Leg Odometry Fusion for Dynamic Locomotion",
    "abstract": "Implementing dynamic locomotion behaviors on legged robots requires a\nhigh-quality state estimation module. Especially when the motion includes\nflight phases, state-of-the-art approaches fail to produce reliable estimation\nof the robot posture, in particular base height. In this paper, we propose a\nnovel approach for combining visual-inertial odometry (VIO) with leg odometry\nin an extended Kalman filter (EKF) based state estimator. The VIO module uses a\nstereo camera and IMU to yield low-drift 3D position and yaw orientation and\ndrift-free pitch and roll orientation of the robot base link in the inertial\nframe. However, these values have a considerable amount of latency due to image\nprocessing and optimization, while the rate of update is quite low which is not\nsuitable for low-level control. To reduce the latency, we predict the VIO state\nestimate at the rate of the IMU measurements of the VIO sensor. The EKF module\nuses the base pose and linear velocity predicted by VIO, fuses them further\nwith a second high-rate IMU and leg odometry measurements, and produces robot\nstate estimates with a high frequency and small latency suitable for control.\nWe integrate this lightweight estimation framework with a nonlinear model\npredictive controller and show successful implementation of a set of agile\nlocomotion behaviors, including trotting and jumping at varying horizontal\nspeeds, on a torque-controlled quadruped robot.",
    "descriptor": "\nComments: Submitted to IEEE International Conference on Robotics and Automation (ICRA), 2023\n",
    "authors": [
      "Victor Dh\u00e9din",
      "Haolong Li",
      "Shahram Khorshidi",
      "Lukas Mack",
      "Adithya Kumar Chinnakkonda Ravi",
      "Avadesh Meduri",
      "Paarth Shah",
      "Felix Grimminger",
      "Ludovic Righetti",
      "Majid Khadiv",
      "Joerg Stueckler"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02127"
  },
  {
    "id": "arXiv:2210.02128",
    "title": "Novel Methodologies for Solving the Inverse Unsteady Heat Transfer  Problem of Estimating the Boundary Heat Flux in Continuous Casting Molds",
    "abstract": "In this work, we investigate the estimation of the transient mold-slab heat\nflux in continuous casting molds given some thermocouples measurements in the\nmold plates. Mathematically, we can see this problem as the estimation of a\nNeumann boundary condition given pointwise state observations in the interior\nof the domain. We formulate it in a deterministic inverse problem setting.\nAfter introducing the industrial problem, we present the mold thermal model and\nrelated assumptions. Then, we formulate the boundary heat flux estimation\nproblem in a deterministic inverse problem setting using a sequential approach\naccording to the sequentiality of the temperature measurements. We consider\ndifferent formulations of the inverse problem. For each one, we develop novel\ndirect methodologies exploiting a space parameterization of the heat flux and\nthe linearity of the mold model. We construct these methods to be divided into\na computationally expensive offline phase that can be computed before the\nprocess starts, and a cheaper online phase to be performed during the casting\nprocess. To conclude, we test the performance of the proposed methods in two\nbenchmark cases.",
    "descriptor": "",
    "authors": [
      "Umberto Emil Morelli",
      "Patricia Barral",
      "Peregrina Quintela",
      "Gianluigi Rozza",
      "Giovanni Stabile"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.02128"
  },
  {
    "id": "arXiv:2210.02131",
    "title": "Density Planner: Minimizing Collision Risk in Motion Planning with  Dynamic Obstacles using Density-based Reachability",
    "abstract": "Autonomous systems with uncertainties are prevalent in robotics. However,\nensuring the safety of those systems is challenging due to sophisticated\ndynamics and the hardness to predict future states. Usually, a classical motion\nplanning method considering all possible states will not find a feasible path\nin crowded environments. To overcome this conservativeness, we propose a\ndensity-based method. The proposed method uses a neural network and the\nLiouville equation to learn the density evolution, and by applying a\ngradient-based optimization procedure, we can plan for feasible and probably\nsafe trajectories to minimize the collision risk. We conduct experiments on\nsimulated environments and environments generated from real-world data and\noutperform baseline methods such as model predictive control (MPC) and\nnonlinear programming (NLP). While our method requires planning time in\nadvance, the online computational complexity is very low when compared to other\nmethods.",
    "descriptor": "\nComments: submitted to ICRA2023\n",
    "authors": [
      "Laura L\u00fctzow",
      "Yue Meng",
      "Andres Chavez Armijos",
      "Chuchu Fan"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.02131"
  },
  {
    "id": "arXiv:2210.02137",
    "title": "Internet Service Providers' and Individuals' Attitudes, Barriers, and  Incentives to Secure IoT",
    "abstract": "Internet Service Providers (ISPs) and individual users of Internet of Things\n(IoT) play a vital role in securing IoT. However, encouraging them to do so is\nhard. Our study investigates ISPs' and individuals' attitudes towards the\nsecurity of IoT, the obstacles they face, and their incentives to keep IoT\nsecure, drawing evidence from Japan.\nDue to the complex interactions of the stakeholders, we follow an iterative\nmethodology where we present issues and potential solutions to our stakeholders\nin turn. For ISPs, we survey 27 ISPs in Japan, followed by a workshop with\nrepresentatives from government and 5 ISPs. Based on the findings from this, we\nconduct semi-structured interviews with 20 participants followed by a more\nquantitative survey with 328 participants. We review these results in a second\nworkshop with representatives from government and 7 ISPs. The appreciation of\nchallenges by each party has lead to findings that are supported by all\nstakeholders.\nSecuring IoT devices is neither users' nor ISPs' priority. Individuals are\nkeen on more interventions both from the government as part of regulation and\nfrom ISPs in terms of filtering malicious traffic. Participants are willing to\npay for enhanced monitoring and filtering. While ISPs do want to help users,\nthere appears to be a lack of effective technology to aid them. ISPs would like\nto see more public recognition for their efforts, but internally they struggle\nwith executive buy-in and effective means to communicate with their customers.\nThe majority of barriers and incentives are external to ISPs and individuals,\ndemonstrating the complexity of keeping IoT secure and emphasizing the need for\nrelevant stakeholders in the IoT ecosystem to work in tandem.",
    "descriptor": "\nComments: This is an extended version of our USENIX Security '23 paper\n",
    "authors": [
      "Nissy Sombatruang",
      "Tristan Caulfield",
      "Ingolf Becker",
      "Akira Fujita",
      "Takahiro Kasama",
      "Koji Nakao",
      "Daisuke Inoue"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.02137"
  },
  {
    "id": "arXiv:2210.02143",
    "title": "Common Vulnerability Scoring System Prediction based on Open Source  Intelligence Information Sources",
    "abstract": "The number of newly published vulnerabilities is constantly increasing. Until\nnow, the information available when a new vulnerability is published is\nmanually assessed by experts using a Common Vulnerability Scoring System (CVSS)\nvector and score. This assessment is time consuming and requires expertise.\nVarious works already try to predict CVSS vectors or scores using machine\nlearning based on the textual descriptions of the vulnerability to enable\nfaster assessment. However, for this purpose, previous works only use the texts\navailable in databases such as National Vulnerability Database. With this work,\nthe publicly available web pages referenced in the National Vulnerability\nDatabase are analyzed and made available as sources of texts through web\nscraping. A Deep Learning based method for predicting the CVSS vector is\nimplemented and evaluated. The present work provides a classification of the\nNational Vulnerability Database's reference texts based on the suitability and\ncrawlability of their texts. While we identified the overall influence of the\nadditional texts is negligible, we outperformed the state-of-the-art with our\nDeep Learning prediction models.",
    "descriptor": "\nComments: 12 pages, 6 figures\n",
    "authors": [
      "Philipp Kuehn",
      "David N. Relke",
      "Christian Reuter"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02143"
  },
  {
    "id": "arXiv:2210.02144",
    "title": "SECOE: Alleviating Sensors Failure in Machine Learning-Coupled IoT  Systems",
    "abstract": "Machine learning (ML) applications continue to revolutionize many domains. In\nrecent years, there has been considerable research interest in building novel\nML applications for a variety of Internet of Things (IoT) domains, such as\nprecision agriculture, smart cities, and smart manufacturing. IoT domains are\ncharacterized by continuous streams of data originating from diverse,\ngeographically distributed sensors, and they often require a real-time or\nsemi-real-time response. IoT characteristics pose several fundamental\nchallenges to designing and implementing effective ML applications.\nSensor/network failures that result in data stream interruptions is one such\nchallenge. Unfortunately, the performance of many ML applications quickly\ndegrades when faced with data incompleteness. Current techniques to handle data\nincompleteness are based upon data imputation ( i.e., they try to fill-in\nmissing data). Unfortunately, these techniques may fail, especially when\nmultiple sensors' data streams become concurrently unavailable (due to\nsimultaneous sensor failures). With the aim of building robust IoT-coupled ML\napplications, this paper proposes SECOE, a unique, proactive approach for\nalleviating potentially simultaneous sensor failures. The fundamental idea\nbehind SECOE is to create a carefully chosen ensemble of ML models in which\neach model is trained assuming a set of failed sensors (i.e., the training set\nomits corresponding values). SECOE includes a novel technique to minimize the\nnumber of models in the ensemble by harnessing the correlations among sensors.\nWe demonstrate the efficacy of the SECOE approach through a series of\nexperiments involving three distinct datasets. The experimental findings reveal\nthat SECOE effectively preserves prediction accuracy in the presence of sensor\nfailures.",
    "descriptor": "\nComments: 8 pages, 10 figures\n",
    "authors": [
      "Yousef AlShehri",
      "Lakshmish Ramaswamy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.02144"
  },
  {
    "id": "arXiv:2210.02147",
    "title": "Adaptive Leading Cruise Control in Mixed Traffic Considering Human  Behavioral Diversity",
    "abstract": "This paper presents an adaptive leading cruise control strategy for the\nconnected and automated vehicle (CAV) and first considers its impact on the\nfollowing human-driven vehicle (HDV) with diverse driving characteristics in\nthe unified optimization framework for improved holistic energy efficiency. The\ncar-following behaviors of HDV are statistically calibrated using the Next\nGeneration Simulation dataset. In a typical single-lane car-following scenario\nwhere CAVs and HDVs share the road, the longitudinal speed control of CAVs can\nsubstantially reduce the energy consumption of the following HDV by avoiding\nunnecessary acceleration and braking. Moreover, apart from the objectives\nincluding car-following safety and traffic efficiency, the energy efficiencies\nof both CAV and HDV are incorporated into the reward function of reinforcement\nlearning. The specific driving pattern of the following HDV is learned in\nreal-time from historical speed information to predict its acceleration and\npower consumption in the optimization horizon. A comprehensive simulation is\nconducted to statistically verify the positive impacts of CAV on the holistic\nenergy efficiency of the mixed traffic flow with uncertain and diverse human\ndriving behaviors. Simulation results indicate that the holistic energy\nefficiency is improved by 4.38% on average.",
    "descriptor": "",
    "authors": [
      "Qun Wang",
      "Haoxuan Dong",
      "Fei Ju",
      "Weichao Zhuang",
      "Chen Lv",
      "Liangmo Wang",
      "Ziyou Song"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.02147"
  },
  {
    "id": "arXiv:2210.02149",
    "title": "Relational Proxies: Emergent Relationships as Fine-Grained  Discriminators",
    "abstract": "Fine-grained categories that largely share the same set of parts cannot be\ndiscriminated based on part information alone, as they mostly differ in the way\nthe local parts relate to the overall global structure of the object. We\npropose Relational Proxies, a novel approach that leverages the relational\ninformation between the global and local views of an object for encoding its\nsemantic label. Starting with a rigorous formalization of the notion of\ndistinguishability between fine-grained categories, we prove the necessary and\nsufficient conditions that a model must satisfy in order to learn the\nunderlying decision boundaries in the fine-grained setting. We design\nRelational Proxies based on our theoretical findings and evaluate it on seven\nchallenging fine-grained benchmark datasets and achieve state-of-the-art\nresults on all of them, surpassing the performance of all existing works with a\nmargin exceeding 4% in some cases. We also experimentally validate our theory\non fine-grained distinguishability and obtain consistent results across\nmultiple benchmarks. Implementation is available at\nhttps://github.com/abhrac/relational-proxies.",
    "descriptor": "\nComments: Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Abhra Chaudhuri",
      "Massimiliano Mancini",
      "Zeynep Akata",
      "Anjan Dutta"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02149"
  },
  {
    "id": "arXiv:2210.02156",
    "title": "Fine-Tuning with Differential Privacy Necessitates an Additional  Hyperparameter Search",
    "abstract": "Models need to be trained with privacy-preserving learning algorithms to\nprevent leakage of possibly sensitive information contained in their training\ndata. However, canonical algorithms like differentially private stochastic\ngradient descent (DP-SGD) do not benefit from model scale in the same way as\nnon-private learning. This manifests itself in the form of unappealing\ntradeoffs between privacy and utility (accuracy) when using DP-SGD on complex\ntasks. To remediate this tension, a paradigm is emerging: fine-tuning with\ndifferential privacy from a model pretrained on public (i.e., non-sensitive)\ntraining data.\nIn this work, we identify an oversight of existing approaches for\ndifferentially private fine tuning. They do not tailor the fine-tuning approach\nto the specifics of learning with privacy. Our main result is to show how\ncarefully selecting the layers being fine-tuned in the pretrained neural\nnetwork allows us to establish new state-of-the-art tradeoffs between privacy\nand accuracy. For instance, we achieve 77.9% accuracy for $(\\varepsilon,\n\\delta)=(2, 10^{-5})$ on CIFAR-100 for a model pretrained on ImageNet. Our work\ncalls for additional hyperparameter search to configure the differentially\nprivate fine-tuning procedure itself.",
    "descriptor": "",
    "authors": [
      "Yannis Cattan",
      "Christopher A. Choquette-Choo",
      "Nicolas Papernot",
      "Abhradeep Thakurta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.02156"
  },
  {
    "id": "arXiv:2210.02159",
    "title": "Differentiable Mathematical Programming for Object-Centric  Representation Learning",
    "abstract": "We propose topology-aware feature partitioning into $k$ disjoint partitions\nfor given scene features as a method for object-centric representation\nlearning. To this end, we propose to use minimum $s$-$t$ graph cuts as a\npartitioning method which is represented as a linear program. The method is\ntopologically aware since it explicitly encodes neighborhood relationships in\nthe image graph. To solve the graph cuts our solution relies on an efficient,\nscalable, and differentiable quadratic programming approximation. Optimizations\nspecific to cut problems allow us to solve the quadratic programs and compute\ntheir gradients significantly more efficiently compared with the general\nquadratic programming approach. Our results show that our approach is scalable\nand outperforms existing methods on object discovery tasks with textured scenes\nand objects.",
    "descriptor": "",
    "authors": [
      "Adeel Pervez",
      "Phillip Lippe",
      "Efstratios Gavves"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02159"
  },
  {
    "id": "arXiv:2210.02161",
    "title": "Security and Privacy Concerns in Cloud-based Scientific and Business  Workflows: A Systematic Review",
    "abstract": "Today, the number of data-intensive and compute-intensive applications like\nbusiness and scientific workflows has dramatically increased, which made cloud\ncomputing more popular in the matter of delivering a large amount of computing\nresources on demand. On the other hand, security is a critical issue affecting\nthe wide adoption of cloud technologies, especially for workflows that are\nmostly dealing with sensitive data and tasks. In this paper, we carry out a\nreview of the state-of-the-art on how security and privacy concerns in\nscientific and business workflows in cloud environments are being addressed and\nidentify the limitations and gaps in the current body of knowledge in this\narea. In this extensive literature review, we first present a classification of\nthe state-of-the-art security solutions organized according to the phases of\nthe workflow life cycle they target. Based on our findings, we provide a\ndetailed review and classification of the most relevant available literature\nfocusing on the execution, monitoring, and adaptation phases of workflows.\nFinally, we present a list of open research issues related to the security of\ncloud-based workflows and discuss them.",
    "descriptor": "\nComments: 16 pages, 8 figures, 5 tables\n",
    "authors": [
      "Nafiseh Soveizi",
      "Fatih Turkmen",
      "Dimka Karastoyanova"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.02161"
  },
  {
    "id": "arXiv:2210.02163",
    "title": "Hypergraph patterns and collaboration structure",
    "abstract": "Humans collaborate in different contexts such as in creative or scientific\nprojects, in workplaces and in sports. Depending on the project and external\ncircumstances, a newly formed collaboration may include people that have\ncollaborated before in the past, and people with no collaboration history. Such\nexisting relationships between team members have been reported to influence the\nperformance of teams. However, it is not clear how existing relationships\nbetween team members should be quantified, and whether some relationships are\nmore likely to occur in new collaborations than others. Here we introduce a new\nfamily of structural patterns, m-patterns, which formalize relationships\nbetween collaborators and we study the prevalence of such structures in data\nand a simple random-hypergraph null model. We analyze the frequency with which\ndifferent collaboration structures appear in our null model and show how such\nfrequencies depend on size and hyperedge density in the hypergraphs. Comparing\nthe null model to data of human and non-human collaborations, we find that some\ncollaboration structures are vastly under- and overrepresented in empirical\ndatasets. Finally, we find that structures of scientific collaborations on\nCOVID-19 papers in some cases are statistically significantly different from\nthose of non-COVID-19 papers. Examining citation counts for 4 different\nscientific fields, we also find indications that repeat collaborations are more\nsuccessful for 2-author scientific publications and less successful for\n3-author scientific publications as compared to other collaboration structures.",
    "descriptor": "\nComments: 25 pages, 9 figures\n",
    "authors": [
      "Jonas L. Juul",
      "Austin R. Benson",
      "Jon Kleinberg"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Digital Libraries (cs.DL)",
      "Discrete Mathematics (cs.DM)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.02163"
  },
  {
    "id": "arXiv:2210.02165",
    "title": "Graphie: A network-based visual interface for UK's Primary Legislation",
    "abstract": "We present Graphie, a novel navigational interface to visualize Acts and\nBills included in the UK's legislation digital repository [legislation.gov.uk].\nGraphie provides a network representation of the hierarchical structure of an\nAct of Parliament, which is typically organized in a tree-like fashion\naccording to the content and information contained in each sub-branch. Nodes in\nGraphie represent sections of an Act (or individual provisions), while links\nembody the hierarchical connections between them. The legal map provided by\nGraphie is easily navigable by hovering on nodes, which are also color-coded\nand numbered to provide easily accessible information about the underlying\ncontent. The full textual content of each node is also available on a dedicated\nhyperlinked canvas. The building block of Graphie is Sofia, an offline data\npipeline designed to support different data visualizations by parsing and\nmodelling data provided by [legislation.gov.uk] in open access form. While we\nfocus on the Housing Act 2004 for illustrative purposes, our platform is\nscalable, versatile, and provides users with a unified toolbox to visualize and\nexplore the UK legal corpus in a fast and user-friendly way.",
    "descriptor": "\nComments: 15 pages, 12 figures\n",
    "authors": [
      "Evan Tzanis",
      "Pierpaolo Vivo",
      "Yanik-Pascal F\u00f6rster",
      "Luca Gamberi",
      "Alessia Annibale"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.02165"
  },
  {
    "id": "arXiv:2210.02166",
    "title": "Generalized Moving Horizon Estimation for Nonlinear Systems with  Robustness to Measurement Outliers",
    "abstract": "Moving horizon estimation (MHE) is an effective filtering technique for\ndynamic systems. While there has been noticeable progress in the stability\nanalysis of MHE, there is lack of research on robustifying MHE against\nmeasurement outliers. To bridge this gap, we propose a generalized MHE approach\nby utilizing the loss-theoretic perspective of Generalized Bayesian Inference.\nIn particular, we design a robust loss function by leveraging the\n\\{beta}-divergence and propose the \\{beta} moving horizon estimator to handle\nthe outliers. Analytical influence functions are derived to analyze the\nrobustness of the MHE methods. Based on this, we prove that for the case of\nlinear Gaussian systems, the gross error sensitivity of the proposed estimator\nremains bounded, while for the standard MHE, it is unbounded. The effectiveness\nof the proposed approach is demonstrated in simulations on both linear and\nnonlinear systems.",
    "descriptor": "",
    "authors": [
      "Wenhan Cao",
      "Chang Liu",
      "Zhiqian Lan",
      "Yingxi Piao",
      "Shengbo Eben Li"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.02166"
  },
  {
    "id": "arXiv:2210.02167",
    "title": "Faster parameterized algorithms for modification problems to  minor-closed classes",
    "abstract": "Let ${\\cal G}$ be a minor-closed graph class and let $G$ be an $n$-vertex\ngraph. We say that $G$ is a $k$-apex of ${\\cal G}$ if $G$ contains a set $S$ of\nat most $k$ vertices such that $G\\setminus S$ belongs to ${\\cal G}$. Our first\nresult is an algorithm that decides whether $G$ is a $k$-apex of ${\\cal G}$ in\ntime $2^{{\\sf poly}(k)}\\cdot n^2$, where ${\\sf poly}$ is a polynomial function\ndepending on ${\\cal G}$. This algorithm improves the previous one, given by\nSau, Stamoulis, and Thilikos [ICALP 2020], whose running time was $2^{{\\sf\npoly}(k)}\\cdot n^3$. The elimination distance of $G$ to ${\\cal G}$, denoted by\n${\\sf ed}_{\\cal G}(G)$, is the minimum number of rounds required to reduce each\nconnected component of $G$ to a graph in ${\\cal G}$ by removing one vertex from\neach connected component in each round. Bulian and Dawar [Algorithmica 2017]\nprovided an FPT-algorithm, with parameter $k$, to decide whether ${\\sf\ned}_{\\cal G}(G)\\leq k$. However, its dependence on $k$ is not explicit. We\nextend the techniques used in the first algorithm to decide whether ${\\sf\ned}_{\\cal G}(G)\\leq k$ in time $2^{2^{2^{{\\sf poly}(k)}}}\\cdot n^2$. This is\nthe first algorithm for this problem with an explicit parametric dependence in\n$k$. In the special case where ${\\cal G}$ excludes some apex-graph as a minor,\nwe give two alternative algorithms, running in time $2^{2^{{\\cal O}(k^2\\log\nk)}}\\cdot n^2$ and $2^{{\\sf poly}(k)}\\cdot n^3$ respectively, where $c$ and\n${\\sf poly}$ depend on ${\\cal G}$. As a stepping stone for these algorithms, we\nprovide an algorithm that decides whether ${\\sf ed}_{\\cal G}(G)\\leq k$ in time\n$2^{{\\cal O}({\\sf tw}\\cdot k+{\\sf tw}\\log{\\sf tw})}\\cdot n$, where ${\\sf tw}$\nis the treewidth of $G$. Finally, we provide explicit upper bounds on the size\nof the graphs in the minor-obstruction set of the class of graphs ${\\cal\nE}_k({\\cal G})=\\{G\\mid{\\sf ed}_{\\cal G}(G)\\leq k\\}$.",
    "descriptor": "\nComments: 63 pages, 7 figures, abstract abbreviated to fit arXiv limitation\n",
    "authors": [
      "Laure Morelle",
      "Ignasi Sau",
      "Giannos Stamoulis",
      "Dimitrios M. Thilikos"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.02167"
  },
  {
    "id": "arXiv:2210.02168",
    "title": "Bayesian Quadrature for Probability Threshold Robustness of Partially  Undefined Functions",
    "abstract": "In engineering design, one often wishes to calculate the probability that the\nperformance of a system is satisfactory under uncertainty. State of the art\nalgorithms exist to solve this problem using active learning with Gaussian\nprocess models. However, these algorithms cannot be applied to problems which\noften occur in the autonomous vehicle domain where the performance of a system\nmay be undefined under certain circumstances. Na\\\"ive modification of existing\nalgorithms by simply masking undefined values will introduce a discontinuous\nsystem performance function, and would be unsuccessful because these algorithms\nare known to fail for discontinuous performance functions. We solve this\nproblem using a hierarchical model for the system performance, where undefined\nperformance is classified before the performance is regressed. This enables\nactive learning Gaussian process methods to be applied to problems where the\nperformance of the system is sometimes undefined, and we demonstrate this by\ntesting our methodology on synthetic numerical examples for the autonomous\ndriving domain.",
    "descriptor": "\nComments: The code to generate these experiments is available as an open source repository, see this http URL\n",
    "authors": [
      "Jonathan Sadeghi",
      "Romain Mueller",
      "John Redford"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02168"
  },
  {
    "id": "arXiv:2210.02169",
    "title": "Denotational semantics of general store and polymorphism",
    "abstract": "We contribute the first denotational semantics of polymorphic dependent type\ntheory extended by an equational theory for general (higher-order) reference\ntypes and recursive types, based on a combination of guarded recursion and\nimpredicative polymorphism; because our model is based on recursively defined\nsemantic worlds, it is compatible with polymorphism and relational reasoning\nabout stateful abstract datatypes. We then extend our language with modal\nconstructs for proof-relevant relational reasoning based on the logical\nrelations as types principle, in which equivalences between imperative abstract\ndatatypes can be established synthetically. Finally we decompose our store\nmodel as a general construction that extends an arbitrary polymorphic\ncall-by-push-value adjunction with higher-order store, improving on Levy's\npossible worlds model construction; what is new in relation to prior typed\ndenotational models of higher-order store is that our Kripke worlds need not be\nsyntactically definable, and are thus compatible with relational reasoning in\nthe heap. Our work combines recent advances in the operational semantics of\nstate with the purely denotational viewpoint of synthetic guarded domain\ntheory.",
    "descriptor": "",
    "authors": [
      "Jonathan Sterling",
      "Daniel Gratzer",
      "Lars Birkedal"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.02169"
  },
  {
    "id": "arXiv:2210.02174",
    "title": "CW-ERM: Improving Autonomous Driving Planning with Closed-loop Weighted  Empirical Risk Minimization",
    "abstract": "The imitation learning of self-driving vehicle policies through behavioral\ncloning is often carried out in an open-loop fashion, ignoring the effect of\nactions to future states. Training such policies purely with Empirical Risk\nMinimization (ERM) can be detrimental to real-world performance, as it biases\npolicy networks towards matching only open-loop behavior, showing poor results\nwhen evaluated in closed-loop. In this work, we develop an efficient and\nsimple-to-implement principle called Closed-loop Weighted Empirical Risk\nMinimization (CW-ERM), in which a closed-loop evaluation procedure is first\nused to identify training data samples that are important for practical driving\nperformance and then we these samples to help debias the policy network. We\nevaluate CW-ERM in a challenging urban driving dataset and show that this\nprocedure yields a significant reduction in collisions as well as other\nnon-differentiable closed-loop metrics.",
    "descriptor": "",
    "authors": [
      "Eesha Kumar",
      "Yiming Zhang",
      "Stefano Pini",
      "Simon Stent",
      "Ana Ferreira",
      "Sergey Zagoruyko",
      "Christian S. Perone"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02174"
  },
  {
    "id": "arXiv:2210.02176",
    "title": "Feature Importance for Time Series Data: Improving KernelSHAP",
    "abstract": "Feature importance techniques have enjoyed widespread attention in the\nexplainable AI literature as a means of determining how trained machine\nlearning models make their predictions. We consider Shapley value based\napproaches to feature importance, applied in the context of time series data.\nWe present closed form solutions for the SHAP values of a number of time series\nmodels, including VARMAX. We also show how KernelSHAP can be applied to time\nseries tasks, and how the feature importances that come from this technique can\nbe combined to perform \"event detection\". Finally, we explore the use of Time\nConsistent Shapley values for feature importance.",
    "descriptor": "\nComments: Will appear at ICAIF Workshop on Explainable Artificial Intelligence in Finance, November 2, 2022\n",
    "authors": [
      "Mattia Villani",
      "Joshua Lockhart",
      "Daniele Magazzeni"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02176"
  },
  {
    "id": "arXiv:2210.02177",
    "title": "Multi-objective optimization via equivariant deep hypervolume  approximation",
    "abstract": "Optimizing multiple competing objectives is a common problem across science\nand industry. The inherent inextricable trade-off between those objectives\nleads one to the task of exploring their Pareto front. A meaningful quantity\nfor the purpose of the latter is the hypervolume indicator, which is used in\nBayesian Optimization (BO) and Evolutionary Algorithms (EAs). However, the\ncomputational complexity for the calculation of the hypervolume scales\nunfavorably with increasing number of objectives and data points, which\nrestricts its use in those common multi-objective optimization frameworks. To\novercome these restrictions we propose to approximate the hypervolume function\nwith a deep neural network, which we call DeepHV. For better sample efficiency\nand generalization, we exploit the fact that the hypervolume is\nscale-equivariant in each of the objectives as well as permutation invariant\nw.r.t. both the objectives and the samples, by using a deep neural network that\nis equivariant w.r.t. the combined group of scalings and permutations. We\nevaluate our method against exact, and approximate hypervolume methods in terms\nof accuracy, computation time, and generalization. We also apply and compare\nour methods to state-of-the-art multi-objective BO methods and EAs on a range\nof synthetic benchmark test cases. The results show that our methods are\npromising for such multi-objective optimization tasks.",
    "descriptor": "",
    "authors": [
      "Jim Boelrijk",
      "Bernd Ensing",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.02177"
  },
  {
    "id": "arXiv:2210.02181",
    "title": "DISCOVER: Deep identification of symbolic open-form PDEs via enhanced  reinforcement-learning",
    "abstract": "The working mechanisms of complex natural systems tend to abide by concise\nand profound partial differential equations (PDEs). Methods that directly mine\nequations from data are called PDE discovery, which reveals consistent physical\nlaws and facilitates our interaction with the natural world. In this paper, an\nenhanced deep reinforcement-learning framework is proposed to uncover symbolic\nopen-form PDEs with little prior knowledge. Specifically, (1) we first build a\nsymbol library and define that a PDE can be represented as a tree structure.\nThen, (2) we design a structure-aware recurrent neural network agent by\ncombining structured inputs and monotonic attention to generate the pre-order\ntraversal of PDE expression trees. The expression trees are then split into\nfunction terms, and their coefficients can be calculated by the sparse\nregression method. (3) All of the generated PDE candidates are first filtered\nby some physical and mathematical constraints, and then evaluated by a\nmeticulously designed reward function considering the fitness to data and the\nparsimony of the equation. (4) We adopt the risk-seeking policy gradient to\niteratively update the agent to improve the best-case performance. The\nexperiment demonstrates that our framework is capable of mining the governing\nequations of several canonical systems with great efficiency and scalability.",
    "descriptor": "",
    "authors": [
      "Mengge Du",
      "Yuntian Chen",
      "Dongxiao Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.02181"
  },
  {
    "id": "arXiv:2210.02182",
    "title": "CFL-Net: Image Forgery Localization Using Contrastive Learning",
    "abstract": "Conventional forgery localizing methods usually rely on different forgery\nfootprints such as JPEG artifacts, edge inconsistency, camera noise, etc., with\ncross-entropy loss to locate manipulated regions. However, these methods have\nthe disadvantage of over-fitting and focusing on only a few specific forgery\nfootprints. On the other hand, real-life manipulated images are generated via a\nwide variety of forgery operations and thus, leave behind a wide variety of\nforgery footprints. Therefore, we need a more general approach for image\nforgery localization that can work well on a variety of forgery conditions. A\nkey assumption in underlying forged region localization is that there remains a\ndifference of feature distribution between untampered and manipulated regions\nin each forged image sample, irrespective of the forgery type. In this paper,\nwe aim to leverage this difference of feature distribution to aid in image\nforgery localization. Specifically, we use contrastive loss to learn mapping\ninto a feature space where the features between untampered and manipulated\nregions are well-separated for each image. Also, our method has the advantage\nof localizing manipulated region without requiring any prior knowledge or\nassumption about the forgery type. We demonstrate that our work outperforms\nseveral existing methods on three benchmark image manipulation datasets. Code\nis available at https://github.com/niloy193/CFLNet.",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Fahim Faisal Niloy",
      "Kishor Kumar Bhaumik",
      "Simon S. Woo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02182"
  },
  {
    "id": "arXiv:2210.02186",
    "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series  Analysis",
    "abstract": "Time series analysis is of immense importance in extensive applications, such\nas weather forecasting, anomaly detection, and action recognition. This paper\nfocuses on temporal variation modeling, which is the common key problem of\nextensive analysis tasks. Previous methods attempt to accomplish this directly\nfrom the 1D time series, which is extremely challenging due to the intricate\ntemporal patterns. Based on the observation of multi-periodicity in time\nseries, we ravel out the complex temporal variations into the multiple\nintraperiod- and interperiod-variations. To tackle the limitations of 1D time\nseries in representation capability, we extend the analysis of temporal\nvariations into the 2D space by transforming the 1D time series into a set of\n2D tensors based on multiple periods. This transformation can embed the\nintraperiod- and interperiod-variations into the columns and rows of the 2D\ntensors respectively, making the 2D-variations to be easily modeled by 2D\nkernels. Technically, we propose the TimesNet with TimesBlock as a task-general\nbackbone for time series analysis. TimesBlock can discover the\nmulti-periodicity adaptively and extract the complex temporal variations from\ntransformed 2D tensors by a parameter-efficient inception block. Our proposed\nTimesNet achieves consistent state-of-the-art in five mainstream time series\nanalysis tasks, including short- and long-term forecasting, imputation,\nclassification, and anomaly detection.",
    "descriptor": "",
    "authors": [
      "Haixu Wu",
      "Tengge Hu",
      "Yong Liu",
      "Hang Zhou",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02186"
  },
  {
    "id": "arXiv:2210.02190",
    "title": "Domain Discrepancy Aware Distillation for Model Aggregation in Federated  Learning",
    "abstract": "Knowledge distillation has recently become popular as a method of model\naggregation on the server for federated learning. It is generally assumed that\nthere are abundant public unlabeled data on the server. However, in reality,\nthere exists a domain discrepancy between the datasets of the server domain and\na client domain, which limits the performance of knowledge distillation. How to\nimprove the aggregation under such a domain discrepancy setting is still an\nopen problem. In this paper, we first analyze the generalization bound of the\naggregation model produced from knowledge distillation for the client domains,\nand then describe two challenges, server-to-client discrepancy and\nclient-to-client discrepancy, brought to the aggregation model by the domain\ndiscrepancies. Following our analysis, we propose an adaptive knowledge\naggregation algorithm FedD3A based on domain discrepancy aware distillation to\nlower the bound. FedD3A performs adaptive weighting at the sample level in each\nround of FL. For each sample in the server domain, only the client models of\nits similar domains will be selected for playing the teacher role. To achieve\nthis, we show that the discrepancy between the server-side sample and the\nclient domain can be approximately measured using a subspace projection matrix\ncalculated on each client without accessing its raw data. The server can thus\nleverage the projection matrices from multiple clients to assign weights to the\ncorresponding teacher models for each server-side sample. We validate FedD3A on\ntwo popular cross-domain datasets and show that it outperforms the compared\ncompetitors in both cross-silo and cross-device FL settings.",
    "descriptor": "",
    "authors": [
      "Shangchao Su",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02190"
  },
  {
    "id": "arXiv:2210.02191",
    "title": "On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks",
    "abstract": "In many applications with real-world consequences, it is crucial to develop\nreliable uncertainty estimation for the predictions made by the AI decision\nsystems. Targeting at the goal of estimating uncertainty, various deep neural\nnetwork (DNN) based uncertainty estimation algorithms have been proposed.\nHowever, the robustness of the uncertainty returned by these algorithms has not\nbeen systematically explored. In this work, to raise the awareness of the\nresearch community on robust uncertainty estimation, we show that\nstate-of-the-art uncertainty estimation algorithms could fail catastrophically\nunder our proposed adversarial attack despite their impressive performance on\nuncertainty estimation. In particular, we aim at attacking the out-domain\nuncertainty estimation: under our attack, the uncertainty model would be fooled\nto make high-confident predictions for the out-domain data, which they\noriginally would have rejected. Extensive experimental results on various\nbenchmark image datasets show that the uncertainty estimated by\nstate-of-the-art methods could be easily corrupted by our attack.",
    "descriptor": "",
    "authors": [
      "Huimin Zeng",
      "Zhenrui Yue",
      "Yang Zhang",
      "Ziyi Kou",
      "Lanyu Shang",
      "Dong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02191"
  },
  {
    "id": "arXiv:2210.02192",
    "title": "Are All Losses Created Equal: A Neural Collapse Perspective",
    "abstract": "While cross entropy (CE) is the most commonly used loss to train deep neural\nnetworks for classification tasks, many alternative losses have been developed\nto obtain better empirical performance. Among them, which one is the best to\nuse is still a mystery, because there seem to be multiple factors affecting the\nanswer, such as properties of the dataset, the choice of network architecture,\nand so on. This paper studies the choice of loss function by examining the\nlast-layer features of deep networks, drawing inspiration from a recent line\nwork showing that the global optimal solution of CE and mean-square-error (MSE)\nlosses exhibits a Neural Collapse phenomenon. That is, for sufficiently large\nnetworks trained until convergence, (i) all features of the same class collapse\nto the corresponding class mean and (ii) the means associated with different\nclasses are in a configuration where their pairwise distances are all equal and\nmaximized. We extend such results and show through global solution and\nlandscape analyses that a broad family of loss functions including commonly\nused label smoothing (LS) and focal loss (FL) exhibits Neural Collapse. Hence,\nall relevant losses(i.e., CE, LS, FL, MSE) produce equivalent features on\ntraining data. Based on the unconstrained feature model assumption, we provide\neither the global landscape analysis for LS loss or the local landscape\nanalysis for FL loss and show that the (only!) global minimizers are neural\ncollapse solutions, while all other critical points are strict saddles whose\nHessian exhibit negative curvature directions either in the global scope for LS\nloss or in the local scope for FL loss near the optimal solution. The\nexperiments further show that Neural Collapse features obtained from all\nrelevant losses lead to largely identical performance on test data as well,\nprovided that the network is sufficiently large and trained until convergence.",
    "descriptor": "\nComments: 32 page, 10 figures, NeurIPS 2022\n",
    "authors": [
      "Jinxin Zhou",
      "Chong You",
      "Xiao Li",
      "Kangning Liu",
      "Sheng Liu",
      "Qing Qu",
      "Zhihui Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.02192"
  },
  {
    "id": "arXiv:2210.02194",
    "title": "Unsilencing Colonial Archives via Automated Entity Recognition",
    "abstract": "Colonial archives are at the center of increased interest from a variety of\nperspectives, as they contain traces of historically marginalized people.\nUnfortunately, like most archives, they remain difficult to access due to\nsignificant persisting barriers. We focus here on one of them: the biases to be\nfound in historical findings aids, such as indexes of person names, which\nremain in use to this day. In colonial archives, indexes can perpetuate\nsilences by omitting to include mentions of historically marginalized persons.\nIn order to overcome such limitations and pluralize the scope of existing\nfinding aids, we propose using automated entity recognition. To this end, we\ncontribute a fit-for-purpose annotation typology and apply it on the colonial\narchive of the Dutch East India Company (VOC). We release a corpus of nearly\n70,000 annotations as a shared task, for which we provide baselines using\nstate-of-the-art neural network models. Our work intends to stimulate further\ncontributions in the direction of broadening access to (colonial) archives,\nintegrating automation as a possible means to this end.",
    "descriptor": "",
    "authors": [
      "Mrinalini Luthra",
      "Konstantin Todorov",
      "Charles Jeurgens",
      "Giovanni Colavizza"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.02194"
  },
  {
    "id": "arXiv:2210.02195",
    "title": "A machine learning based algorithm selection method to solve the minimum  cost flow problem",
    "abstract": "The minimum cost flow problem is one of the most studied network optimization\nproblems and appears in numerous applications. Some efficient algorithms exist\nfor this problem, which are freely available in the form of libraries or\nsoftware packages. It is noticeable that none of these solvers is better than\nthe other solution methods on all instances. Thus, the question arises whether\nthe fastest algorithm can be selected for a given instance based on the\ncharacteristics of the instance. To this end, we train several machine learning\nclassifiers to predict the fastest among a given set of solvers. We accomplish\nthis by creating a representative data set of 81,000 instances and\ncharacterizing each of these instances by a vector of relevant features. To\nachieve better performance, we conduct a grid search to optimize the\nhyperparameters of the classifiers. Finally, we evaluate the different\nclassifiers by means of accuracy. It is shown that tree-based models appear to\nadapt and exploit the relevant structures of the minimum-cost flow problem\nparticularly well on a large number of instances, predicting the fastest solver\nwith an accuracy of more than 90%.",
    "descriptor": "",
    "authors": [
      "Philipp Herrmann",
      "Anna Meyer",
      "Stefan Ruzika",
      "Luca E. Sch\u00e4fer",
      "Fabian von der Warth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.02195"
  },
  {
    "id": "arXiv:2210.02197",
    "title": "Hierarchical Neyman-Pearson Classification for Prioritizing Severe  Disease Categories in COVID-19 Patient Data",
    "abstract": "COVID-19 has a spectrum of disease severity, ranging from asymptomatic to\nrequiring hospitalization. Providing appropriate medical care to severe\npatients is crucial to reduce mortality risks. Hence, in classifying patients\ninto severity categories, the more important classification errors are\n\"under-diagnosis\", in which patients are misclassified into less severe\ncategories and thus receive insufficient medical care. The Neyman-Pearson (NP)\nclassification paradigm has been developed to prioritize the designated type of\nerror. However, current NP procedures are either for binary classification or\ndo not provide high probability controls on the prioritized errors in\nmulti-class classification. Here, we propose a hierarchical NP (H-NP) framework\nand an umbrella algorithm that generally adapts to popular classification\nmethods and controls the under-diagnosis errors with high probability. On an\nintegrated collection of single-cell RNA-seq (scRNA-seq) datasets for 740\npatients, we explore ways of featurization and demonstrate the efficacy of the\nH-NP algorithm in controlling the under-diagnosis errors regardless of\nfeaturization. Beyond COVID-19 severity classification, the H-NP algorithm\ngenerally applies to multi-class classification problems, where classes have a\npriority order.",
    "descriptor": "",
    "authors": [
      "Lijia Wang",
      "Y. X. Rachel Wang",
      "Jingyi Jessica Li",
      "Xin Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.02197"
  },
  {
    "id": "arXiv:2210.02199",
    "title": "MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting",
    "abstract": "Large-scale self-supervised pre-training Transformer architecture have\nsignificantly boosted the performance for various tasks in natural language\nprocessing (NLP) and computer vision (CV). However, there is a lack of\nresearches on processing multivariate time-series by pre-trained Transformer,\nand especially, current study on masking time-series for self-supervised\nlearning is still a gap. Different from language and image processing, the\ninformation density of time-series increases the difficulty of research. The\nchallenge goes further with the invalidity of the previous patch embedding and\nmask methods. In this paper, according to the data characteristics of\nmultivariate time-series, a patch embedding method is proposed, and we present\nan self-supervised pre-training approach based on Masked Autoencoders (MAE),\ncalled MTSMAE, which can improve the performance significantly over supervised\nlearning without pre-training. Evaluating our method on several common\nmultivariate time-series datasets from different fields and with different\ncharacteristics, experiment results demonstrate that the performance of our\nmethod is significantly better than the best method currently available.",
    "descriptor": "",
    "authors": [
      "Peiwang Tang",
      "Xianchao Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02199"
  },
  {
    "id": "arXiv:2210.02200",
    "title": "Machine learning in bioprocess development: From promise to practice",
    "abstract": "Fostered by novel analytical techniques, digitalization and automation,\nmodern bioprocess development provides high amounts of heterogeneous\nexperimental data, containing valuable process information. In this context,\ndata-driven methods like machine learning (ML) approaches have a high potential\nto rationally explore large design spaces while exploiting experimental\nfacilities most efficiently. The aim of this review is to demonstrate how ML\nmethods have been applied so far in bioprocess development, especially in\nstrain engineering and selection, bioprocess optimization, scale-up, monitoring\nand control of bioprocesses. For each topic, we will highlight successful\napplication cases, current challenges and point out domains that can\npotentially benefit from technology transfer and further progress in the field\nof ML.",
    "descriptor": "\nComments: Submitted to \"Trends in Biotechnology\"\n",
    "authors": [
      "Laura Marie Helleckes",
      "Johannes Hemmerich",
      "Wolfgang Wiechert",
      "Eric von Lieres",
      "Alexander Gr\u00fcnberger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Other Quantitative Biology (q-bio.OT)"
    ],
    "url": "https://arxiv.org/abs/2210.02200"
  },
  {
    "id": "arXiv:2210.02202",
    "title": "A new family of Constitutive Artificial Neural Networks towards  automated model discovery",
    "abstract": "For more than 100 years, chemical, physical, and material scientists have\nproposed competing constitutive models to best characterize the behavior of\nnatural and man-made materials in response to mechanical loading. Now, computer\nscience offers a universal solution: Neural Networks. Neural Networks are\npowerful function approximators that can learn constitutive relations from\nlarge data without any knowledge of the underlying physics. However, classical\nNeural Networks entirely ignore a century of research in constitutive modeling,\nviolate thermodynamic considerations, and fail to predict the behavior outside\nthe training regime. Here we design a new family of Constitutive Artificial\nNeural Networks that inherently satisfy common kinematic, thermodynamic, and\nphysic constraints and, at the same time, constrain the design space of\nadmissible functions to create robust approximators, even in the presence of\nsparse data. Towards this goal we revisit the non-linear field theories of\nmechanics and reverse-engineer the network input to account for material\nobjectivity, symmetry, and incompressibility; the network output to enforce\nthermodynamic consistency; the activation functions to implement physically\nreasonable restrictions; and the network architecture to ensure polyconvexity.\nWe demonstrate that this new class of models is a generalization of the\nclassical neo Hooke, Blatz Ko, Mooney Rivlin, Yeoh, and Demiray models and that\nthe network weights have a clear physical interpretation. When trained with\nclassical benchmark data for rubber under uniaxial tension, biaxial extension,\nand pure shear, our network autonomously selects the best constitutive model\nand learns its set of parameters. Our findings suggests that Constitutive\nArtificial Neural Networks have the potential to induce a paradigm shift in\nconstitutive modeling, from user-defined model selection to automated model\ndiscovery.",
    "descriptor": "\nComments: 31 pages, 14 figures\n",
    "authors": [
      "Kevin Linka",
      "Ellen Kuhl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ],
    "url": "https://arxiv.org/abs/2210.02202"
  },
  {
    "id": "arXiv:2210.02205",
    "title": "Game Theoretic Rating in N-player general-sum games with Equilibria",
    "abstract": "Rating strategies in a game is an important area of research in game theory\nand artificial intelligence, and can be applied to any real-world competitive\nor cooperative setting. Traditionally, only transitive dependencies between\nstrategies have been used to rate strategies (e.g. Elo), however recent work\nhas expanded ratings to utilize game theoretic solutions to better rate\nstrategies in non-transitive games. This work generalizes these ideas and\nproposes novel algorithms suitable for N-player, general-sum rating of\nstrategies in normal-form games according to the payoff rating system. This\nenables well-established solution concepts, such as equilibria, to be leveraged\nto efficiently rate strategies in games with complex strategic interactions,\nwhich arise in multiagent training and real-world interactions between many\nagents. We empirically validate our methods on real world normal-form data\n(Premier League) and multiagent reinforcement learning agent evaluation.",
    "descriptor": "",
    "authors": [
      "Luke Marris",
      "Marc Lanctot",
      "Ian Gemp",
      "Shayegan Omidshafiei",
      "Stephen McAleer",
      "Jerome Connor",
      "Karl Tuyls",
      "Thore Graepel"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.02205"
  },
  {
    "id": "arXiv:2210.02206",
    "title": "Improving Visual-Semantic Embedding with Adaptive Pooling and  Optimization Objective",
    "abstract": "Visual-Semantic Embedding (VSE) aims to learn an embedding space where\nrelated visual and semantic instances are close to each other. Recent VSE\nmodels tend to design complex structures to pool visual and semantic features\ninto fixed-length vectors and use hard triplet loss for optimization. However,\nwe find that: (1) combining simple pooling methods is no worse than these\nsophisticated methods; and (2) only considering the most\ndifficult-to-distinguish negative sample leads to slow convergence and poor\nRecall@K improvement. To this end, we propose an adaptive pooling strategy that\nallows the model to learn how to aggregate features through a combination of\nsimple pooling methods. We also introduce a strategy to dynamically select a\ngroup of negative samples to make the optimization converge faster and perform\nbetter. Experimental results on Flickr30K and MS-COCO demonstrate that a\nstandard VSE using our pooling and optimization strategies outperforms current\nstate-of-the-art systems (at least 1.0% on the metrics of recall) in\nimage-to-text and text-to-image retrieval. Source code of our experiments is\navailable at https://github.com/96-Zachary/vse_2ad.",
    "descriptor": "",
    "authors": [
      "Zijian Zhang",
      "Chang Shu",
      "Ya Xiao",
      "Yuan Shen",
      "Di Zhu",
      "Jing Xiao",
      "Youxin Chen",
      "Jey Han Lau",
      "Qian Zhang",
      "Zheng Lu"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.02206"
  },
  {
    "id": "arXiv:2210.02207",
    "title": "New results of $0$-APN power functions over $\\mathbb{F}_{2^n}$",
    "abstract": "Partially APN functions attract researchers' particular interest recently. It\nplays an important role in studying APN functions. In this paper, based on the\nmultivariate method and resultant elimination, we propose several new infinite\nclasses of $0$-APN power functions over $\\mathbb{F}_{2^n}$. Furthermore, two\ninfinite classes of $0$-APN power functions $x^d$ over $\\mathbb{F}_{2^n}$ are\ncharacterized completely where $(2^k-1)d\\equiv 2^m-1~({\\rm mod}\\ 2^n-1)$ or\n$(2^k+1)d\\equiv 2^m+1~({\\rm mod}\\ 2^n-1)$ for some positive integers $n, m, k$.\nThese infinite classes of $0$-APN power functions can explain some examples of\nexponents of Table $1$ in \\cite{BKRS2020}.",
    "descriptor": "",
    "authors": [
      "Yan-Ping Wang",
      "Zhengbang Zha"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.02207"
  },
  {
    "id": "arXiv:2210.02215",
    "title": "On the Statistical Complexity of Estimation and Testing under Privacy  Constraints",
    "abstract": "Producing statistics that respect the privacy of the samples while still\nmaintaining their accuracy is an important topic of research. We study minimax\nlower bounds when the class of estimators is restricted to the differentially\nprivate ones. In particular, we show that characterizing the power of a\ndistributional test under differential privacy can be done by solving a\ntransport problem. With specific coupling constructions, this observation\nallows us to derivate Le Cam-type and Fano-type inequalities for both regular\ndefinitions of differential privacy and for divergence-based ones (based on\nRenyi divergence). We then proceed to illustrate our results on three simple,\nfully worked out examples. In particular, we show that the problem class has a\nhuge importance on the provable degradation of utility due to privacy. For some\nproblems, privacy leads to a provable degradation only when the rate of the\nprivacy parameters is small enough whereas for other problem, the degradation\nsystematically occurs under much looser hypotheses on the privacy parametters.\nFinally, we show that the known privacy guarantees of DP-SGLD, a private convex\nsolver, when used to perform maximum likelihood, leads to an algorithm that is\nnear-minimax optimal in both the sample size and the privacy tuning parameters\nof the problem for a broad class of parametric estimation procedures that\nincludes exponential families.",
    "descriptor": "",
    "authors": [
      "Cl\u00e9ment Lalanne",
      "Aur\u00e9lien Garivier",
      "R\u00e9mi Gribonval"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.02215"
  },
  {
    "id": "arXiv:2210.02217",
    "title": "Maximum likelihood estimation of distribution grid topology and  parameters from smart meter data",
    "abstract": "This paper defines a Maximum Likelihood Estimator (MLE) for the admittance\nmatrix estimation of distribution grids, utilising voltage magnitude and power\nmeasurements collected only from common, unsychronised measuring devices (Smart\nMeters). First, we present a model of the grid, as well as the existing MLE\nbased on voltage and current phasor measurements. Then, this problem\nformulation is adjusted for phase-less measurements using common assumptions.\nThe effect of these assumptions is compared to the initial problem in various\nscenarios. Finally, numerical experiments on a popular IEEE benchmark network\nindicate promising results. Missing data can greatly disrupt estimation\nmethods. Not measuring the voltage phase only adds 30\\% of error to the\nadmittance matrix estimate in realistic conditions. Moreover, the sensitivity\nto measurement noise is similar with and without the phase.",
    "descriptor": "\nComments: 5 pages, Grid Edge 2023 conference\n",
    "authors": [
      "Lisa Laurent",
      "Jean-S\u00e9bastien Brouillon",
      "Giancarlo Ferrari-Trecate"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.02217"
  },
  {
    "id": "arXiv:2210.02223",
    "title": "CorefDiffs: Co-referential and Differential Knowledge Flow in Document  Grounded Conversations",
    "abstract": "Knowledge-grounded dialog systems need to incorporate smooth transitions\namong knowledge selected for generating responses, to ensure that dialog flows\nnaturally. For document-grounded dialog systems, the inter- and intra-document\nknowledge relations can be used to model such conversational flows. We develop\na novel Multi-Document Co-Referential Graph (Coref-MDG) to effectively capture\nthe inter-document relationships based on commonsense and similarity and the\nintra-document co-referential structures of knowledge segments within the\ngrounding documents. We propose CorefDiffs, a Co-referential and Differential\nflow management method, to linearize the static Coref-MDG into conversational\nsequence logic. CorefDiffs performs knowledge selection by accounting for\ncontextual graph structures and the knowledge difference sequences. CorefDiffs\nsignificantly outperforms the state-of-the-art by 9.5\\%, 7.4\\%, and 8.2\\% on\nthree public benchmarks. This demonstrates that the effective modeling of\nco-reference and knowledge difference for dialog flows are critical for\ntransitions in document-grounded conversation",
    "descriptor": "",
    "authors": [
      "Lin Xu",
      "Qixian Zhou",
      "Jinlan Fu",
      "Min-Yen Kan",
      "See-Kiong Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02223"
  },
  {
    "id": "arXiv:2210.02224",
    "title": "Neural Distillation as a State Representation Bottleneck in  Reinforcement Learning",
    "abstract": "Learning a good state representation is a critical skill when dealing with\nmultiple tasks in Reinforcement Learning as it allows for transfer and better\ngeneralization between tasks. However, defining what constitute a useful\nrepresentation is far from simple and there is so far no standard method to\nfind such an encoding. In this paper, we argue that distillation -- a process\nthat aims at imitating a set of given policies with a single neural network --\ncan be used to learn a state representation displaying favorable\ncharacteristics. In this regard, we define three criteria that measure\ndesirable features of a state encoding: the ability to select important\nvariables in the input space, the ability to efficiently separate states\naccording to their corresponding optimal action, and the robustness of the\nstate encoding on new tasks. We first evaluate these criteria and verify the\ncontribution of distillation on state representation on a toy environment based\non the standard inverted pendulum problem, before extending our analysis on\nmore complex visual tasks from the Atari and Procgen benchmarks.",
    "descriptor": "\nComments: Published at the 1st Conference on Lifelong Learning Agents (CoLLAs), 2022\n",
    "authors": [
      "Valentin Guillet",
      "Dennis G. Wilson",
      "Carlos Aguilar-Melchor",
      "Emmanuel Rachelson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02224"
  },
  {
    "id": "arXiv:2210.02227",
    "title": "Comprint: Image Forgery Detection and Localization using Compression  Fingerprints",
    "abstract": "Manipulation tools that realistically edit images are widely available,\nmaking it easy for anyone to create and spread misinformation. In an attempt to\nfight fake news, forgery detection and localization methods were designed.\nHowever, existing methods struggle to accurately reveal manipulations found in\nimages on the internet, i.e., in the wild. That is because the type of forgery\nis typically unknown, in addition to the tampering traces being damaged by\nrecompression. This paper presents Comprint, a novel forgery detection and\nlocalization method based on the compression fingerprint or comprint. It is\ntrained on pristine data only, providing generalization to detect different\ntypes of manipulation. Additionally, we propose a fusion of Comprint with the\nstate-of-the-art Noiseprint, which utilizes a complementary camera model\nfingerprint. We carry out an extensive experimental analysis and demonstrate\nthat Comprint has a high level of accuracy on five evaluation datasets that\nrepresent a wide range of manipulation types, mimicking in-the-wild\ncircumstances. Most notably, the proposed fusion significantly outperforms\nstate-of-the-art reference methods. As such, Comprint and the fusion\nComprint+Noiseprint represent a promising forensics tool to analyze in-the-wild\ntampered images.",
    "descriptor": "\nComments: Presented at the Workshop on MultiMedia FORensics in the WILD 2022, held in conjunction with the International Conference on Pattern Recognition (ICPR) 2022\n",
    "authors": [
      "Hannes Mareen",
      "Dante Vanden Bussche",
      "Fabrizio Guillaro",
      "Davide Cozzolino",
      "Glenn Van Wallendael",
      "Peter Lambert",
      "Luisa Verdoliva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.02227"
  },
  {
    "id": "arXiv:2210.02231",
    "title": "Decanus to Legatus: Synthetic training for 2D-3D human pose lifting",
    "abstract": "3D human pose estimation is a challenging task because of the difficulty to\nacquire ground-truth data outside of controlled environments. A number of\nfurther issues have been hindering progress in building a universal and robust\nmodel for this task, including domain gaps between different datasets, unseen\nactions between train and test datasets, various hardware settings and high\ncost of annotation, etc. In this paper, we propose an algorithm to generate\ninfinite 3D synthetic human poses (Legatus) from a 3D pose distribution based\non 10 initial handcrafted 3D poses (Decanus) during the training of a 2D to 3D\nhuman pose lifter neural network. Our results show that we can achieve 3D pose\nestimation performance comparable to methods using real data from specialized\ndatasets but in a zero-shot setup, showing the generalization potential of our\nframework.",
    "descriptor": "\nComments: Accepted by ACCV 2022\n",
    "authors": [
      "Yue Zhu",
      "David Picard"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02231"
  },
  {
    "id": "arXiv:2210.02234",
    "title": "Thermal (and Hybrid Thermal/Audio) Side-Channel Attacks on Keyboard  Input",
    "abstract": "To date, there has been no systematic investigation of thermal profiles of\nkeyboards, and thus no efforts have been made to secure them. This serves as\nour main motivation for constructing a means for password harvesting from\nkeyboard thermal emanations. Specifically, we introduce Thermanator: a new\npost-factum insider attack based on heat transfer caused by a user typing a\npassword on a typical external (plastic) keyboard.\nWe conduct and describe a user study that collected thermal residues from 30\nusers entering 10 unique passwords (both weak and strong) on 4 popular\ncommodity keyboards. Results show that entire sets of key-presses can be\nrecovered by non-expert users as late as 30 seconds after initial password\nentry, while partial sets can be recovered as late as 1 minute after entry.\nHowever, the thermal residue side-channel lacks information about password\nlength, duplicate key-presses, and key-press ordering. To overcome these\nlimitations, we leverage keyboard acoustic emanations and combine the two to\nyield AcuTherm, the first hybrid side-channel attack on keyboards. AcuTherm\nsignificantly reduces password search without the need for any training on the\nvictim's typing. We report results gathered for many representative passwords\nbased on a user study involving 19 subjects.\nThe takeaway of this work is three-fold: (1) using plastic keyboards to enter\nsecrets (such as passwords and PINs) is even less secure than previously\nrecognized, (2) post-factum thermal imaging attacks are realistic, and (3)\nhybrid (multiple side-channel) attacks are both realistic and effective.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1806.10189\n",
    "authors": [
      "Tyler Kaczmarek",
      "Ercan Ozturk",
      "Pier Paolo Tricomi",
      "Gene Tsudik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02234"
  },
  {
    "id": "arXiv:2210.02235",
    "title": "Over-the-Air Federated Learning with Privacy Protection via Correlated  Additive Perturbations",
    "abstract": "In this paper, we consider privacy aspects of wireless federated learning\n(FL) with Over-the-Air (OtA) transmission of gradient updates from multiple\nusers/agents to an edge server. By exploiting the waveform superposition\nproperty of multiple access channels, OtA FL enables the users to transmit\ntheir updates simultaneously with linear processing techniques, which improves\nresource efficiency. However, this setting is vulnerable to privacy leakage\nsince an adversary node can hear directly the uncoded message. Traditional\nperturbation-based methods provide privacy protection while sacrificing the\ntraining accuracy due to the reduced signal-to-noise ratio. In this work, we\naim at minimizing privacy leakage to the adversary and the degradation of model\naccuracy at the edge server at the same time. More explicitly, spatially\ncorrelated perturbations are added to the gradient vectors at the users before\ntransmission. Using the zero-sum property of the correlated perturbations, the\nside effect of the added perturbation on the aggregated gradients at the edge\nserver can be minimized. In the meanwhile, the added perturbation will not be\ncanceled out at the adversary, which prevents privacy leakage. Theoretical\nanalysis of the perturbation covariance matrix, differential privacy, and model\nconvergence is provided, based on which an optimization problem is formulated\nto jointly design the covariance matrix and the power scaling factor to balance\nbetween privacy protection and convergence performance. Simulation results\nvalidate the correlated perturbation approach can provide strong defense\nability while guaranteeing high learning accuracy.",
    "descriptor": "\nComments: 8 pages, 4 figures, Allerton 2022\n",
    "authors": [
      "Jialing Liao",
      "Zheng Chen",
      "Erik G. Larsson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.02235"
  },
  {
    "id": "arXiv:2210.02236",
    "title": "On the Use of Deep Learning in Software Defect Prediction",
    "abstract": "Context: Automated software defect prediction (SDP) methods are increasingly\napplied, often with the use of machine learning (ML) techniques. Yet, the\nexisting ML-based approaches require manually extracted features, which are\ncumbersome, time consuming and hardly capture the semantic information reported\nin bug reporting tools. Deep learning (DL) techniques provide practitioners\nwith the opportunities to automatically extract and learn from more complex and\nhigh-dimensional data. Objective: The purpose of this study is to\nsystematically identify, analyze, summarize, and synthesize the current state\nof the utilization of DL algorithms for SDP in the literature. Method: We\nsystematically selected a pool of 102 peer-reviewed studies and then conducted\na quantitative and qualitative analysis using the data extracted from these\nstudies. Results: Main highlights include: (1) most studies applied supervised\nDL; (2) two third of the studies used metrics as an input to DL algorithms; (3)\nConvolutional Neural Network is the most frequently used DL algorithm.\nConclusion: Based on our findings, we propose to (1) develop more comprehensive\nDL approaches that automatically capture the needed features; (2) use diverse\nsoftware artifacts other than source code; (3) adopt data augmentation\ntechniques to tackle the class imbalance problem; (4) publish replication\npackages.",
    "descriptor": "\nComments: 45 pages, 12 figures\n",
    "authors": [
      "G\u00f6rkem Giray",
      "Kwabena Ebo Bennin",
      "\u00d6mer K\u00f6ksal",
      "\u00d6nder Babur",
      "Bedir Tekinerdogan"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.02236"
  },
  {
    "id": "arXiv:2210.02237",
    "title": "Dimensional Data KNN-Based Imputation",
    "abstract": "Data Warehouses (DWs) are core components of Business Intelligence (BI).\nMissing data in DWs have a great impact on data analyses. Therefore, missing\ndata need to be completed. Unlike other existing data imputation methods mainly\nadapted for facts, we propose a new imputation method for dimensions. This\nmethod contains two steps: 1) a hierarchical imputation and 2) a k-nearest\nneighbors (KNN) based imputation. Our solution has the advantage of taking into\naccount the DW structure and dependency constraints. Experimental assessments\nvalidate our method in terms of effectiveness and efficiency.",
    "descriptor": "",
    "authors": [
      "Yuzhao Yang",
      "J\u00e9r\u00f4me Darmont",
      "Franck Ravat",
      "Olivier Teste"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.02237"
  },
  {
    "id": "arXiv:2210.02240",
    "title": "On Neural Consolidation for Transfer in Reinforcement Learning",
    "abstract": "Although transfer learning is considered to be a milestone in deep\nreinforcement learning, the mechanisms behind it are still poorly understood.\nIn particular, predicting if knowledge can be transferred between two given\ntasks is still an unresolved problem. In this work, we explore the use of\nnetwork distillation as a feature extraction method to better understand the\ncontext in which transfer can occur. Notably, we show that distillation does\nnot prevent knowledge transfer, including when transferring from multiple tasks\nto a new one, and we compare these results with transfer without prior\ndistillation. We focus our work on the Atari benchmark due to the variability\nbetween different games, but also to their similarities in terms of visual\nfeatures.",
    "descriptor": "\nComments: Published at the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (IEEE ADPRL), 2022\n",
    "authors": [
      "Valentin Guillet",
      "Dennis G. Wilson",
      "Carlos Aguilar-Melchor",
      "Emmanuel Rachelson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02240"
  },
  {
    "id": "arXiv:2210.02248",
    "title": "Crowding out the truth? A simple model of misinformation, polarization  and meaningful social interactions",
    "abstract": "This paper provides a simple theoretical framework to evaluate the effect of\nkey parameters of ranking algorithms, namely popularity and personalization\nparameters, on measures of platform engagement, misinformation and\npolarization. The results show that an increase in the weight assigned to\nonline social interactions (e.g., likes and shares) and to personalized content\nmay increase engagement on the social media platform, while at the same time\nincreasing misinformation and/or polarization. By exploiting Facebook's 2018\n\"Meaningful Social Interactions\" algorithmic ranking update, we also provide\ndirect empirical support for some of the main predictions of the model.",
    "descriptor": "\nComments: 21 pages + appendices, 11 figures, preprint\n",
    "authors": [
      "Fabrizio Germano",
      "Vicen\u00e7 G\u00f3mez",
      "Francesco Sobbrio"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.02248"
  },
  {
    "id": "arXiv:2210.02249",
    "title": "LDEdit: Towards Generalized Text Guided Image Manipulation via Latent  Diffusion Models",
    "abstract": "Research in vision-language models has seen rapid developments off-late,\nenabling natural language-based interfaces for image generation and\nmanipulation. Many existing text guided manipulation techniques are restricted\nto specific classes of images, and often require fine-tuning to transfer to a\ndifferent style or domain. Nevertheless, generic image manipulation using a\nsingle model with flexible text inputs is highly desirable. Recent work\naddresses this task by guiding generative models trained on the generic image\ndatasets using pretrained vision-language encoders. While promising, this\napproach requires expensive optimization for each input. In this work, we\npropose an optimization-free method for the task of generic image manipulation\nfrom text prompts. Our approach exploits recent Latent Diffusion Models (LDM)\nfor text to image generation to achieve zero-shot text guided manipulation. We\nemploy a deterministic forward diffusion in a lower dimensional latent space,\nand the desired manipulation is achieved by simply providing the target text to\ncondition the reverse diffusion process. We refer to our approach as LDEdit. We\ndemonstrate the applicability of our method on semantic image manipulation and\nartistic style transfer. Our method can accomplish image manipulation on\ndiverse domains and enables editing multiple attributes in a straightforward\nfashion. Extensive experiments demonstrate the benefit of our approach over\ncompeting baselines.",
    "descriptor": "\nComments: Accepted BMVC 2022\n",
    "authors": [
      "Paramanand Chandramouli",
      "Kanchana Vaishnavi Gandikota"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02249"
  },
  {
    "id": "arXiv:2210.02254",
    "title": "Granularity-aware Adaptation for Image Retrieval over Multiple Tasks",
    "abstract": "Strong image search models can be learned for a specific domain, ie. set of\nlabels, provided that some labeled images of that domain are available. A\npractical visual search model, however, should be versatile enough to solve\nmultiple retrieval tasks simultaneously, even if those cover very different\nspecialized domains. Additionally, it should be able to benefit from even\nunlabeled images from these various retrieval tasks. This is the more practical\nscenario that we consider in this paper. We address it with the proposed\nGrappa, an approach that starts from a strong pretrained model, and adapts it\nto tackle multiple retrieval tasks concurrently, using only unlabeled images\nfrom the different task domains. We extend the pretrained model with multiple\nindependently trained sets of adaptors that use pseudo-label sets of different\nsizes, effectively mimicking different pseudo-granularities. We reconcile all\nadaptor sets into a single unified model suited for all retrieval tasks by\nlearning fusion layers that we guide by propagating pseudo-granularity\nattentions across neighbors in the feature space. Results on a benchmark\ncomposed of six heterogeneous retrieval tasks show that the unsupervised Grappa\nmodel improves the zero-shot performance of a state-of-the-art self-supervised\nlearning model, and in some places reaches or improves over a task label-aware\noracle that selects the most fitting pseudo-granularity per task.",
    "descriptor": "\nComments: ECCV 2022\n",
    "authors": [
      "Jon Almaz\u00e1n",
      "Byungsoo Ko",
      "Geonmo Gu",
      "Diane Larlus",
      "Yannis Kalantidis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02254"
  },
  {
    "id": "arXiv:2210.02257",
    "title": "Hiding Images in Deep Probabilistic Models",
    "abstract": "Data hiding with deep neural networks (DNNs) has experienced impressive\nsuccesses in recent years. A prevailing scheme is to train an autoencoder,\nconsisting of an encoding network to embed (or transform) secret messages in\n(or into) a carrier, and a decoding network to extract the hidden messages.\nThis scheme may suffer from several limitations regarding practicability,\nsecurity, and embedding capacity. In this work, we describe a different\ncomputational framework to hide images in deep probabilistic models.\nSpecifically, we use a DNN to model the probability density of cover images,\nand hide a secret image in one particular location of the learned distribution.\nAs an instantiation, we adopt a SinGAN, a pyramid of generative adversarial\nnetworks (GANs), to learn the patch distribution of one cover image. We hide\nthe secret image by fitting a deterministic mapping from a fixed set of noise\nmaps (generated by an embedding key) to the secret image during patch\ndistribution learning. The stego SinGAN, behaving as the original SinGAN, is\npublicly communicated; only the receiver with the embedding key is able to\nextract the secret image. We demonstrate the feasibility of our SinGAN approach\nin terms of extraction accuracy and model security. Moreover, we show the\nflexibility of the proposed method in terms of hiding multiple images for\ndifferent receivers and obfuscating the secret image.",
    "descriptor": "",
    "authors": [
      "Haoyu Chen",
      "Linqi Song",
      "Zhenxing Qian",
      "Xinpeng Zhang",
      "Kede Ma"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.02257"
  },
  {
    "id": "arXiv:2210.02259",
    "title": "Cost Aware Asynchronous Multi-Agent Active Search",
    "abstract": "Multi-agent active search requires autonomous agents to choose sensing\nactions that efficiently locate targets. In a realistic setting, agents also\nmust consider the costs that their decisions incur. Previously proposed active\nsearch algorithms simplify the problem by ignoring uncertainty in the agent's\nenvironment, using myopic decision making, and/or overlooking costs. In this\npaper, we introduce an online active search algorithm to detect targets in an\nunknown environment by making adaptive cost-aware decisions regarding the\nagent's actions. Our algorithm combines principles from Thompson Sampling (for\nsearch space exploration and decentralized multi-agent decision making), Monte\nCarlo Tree Search (for long horizon planning) and pareto-optimal confidence\nbounds (for multi-objective optimization in an unknown environment) to propose\nan online lookahead planner that removes all the simplifications. We analyze\nthe algorithm's performance in simulation to show its efficacy in cost aware\nactive search.",
    "descriptor": "",
    "authors": [
      "Arundhati Banerjee",
      "Ramina Ghods",
      "Jeff Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02259"
  },
  {
    "id": "arXiv:2210.02260",
    "title": "From Intelligent Agents to Trustworthy Human-Centred Multiagent Systems",
    "abstract": "The Agents, Interaction and Complexity research group at the University of\nSouthampton has a long track record of research in multiagent systems (MAS). We\nhave made substantial scientific contributions across learning in MAS,\ngame-theoretic techniques for coordinating agent systems, and formal methods\nfor representation and reasoning. We highlight key results achieved by the\ngroup and elaborate on recent work and open research challenges in developing\ntrustworthy autonomous systems and deploying human-centred AI systems that aim\nto support societal good.",
    "descriptor": "\nComments: Appears in the Special Issue on Multi-Agent Systems Research in the United Kingdom\n",
    "authors": [
      "Mohammad Divband Soorati",
      "Enrico H. Gerding",
      "Enrico Marchioni",
      "Pavel Naumov",
      "Timothy J. Norman",
      "Sarvapali D. Ramchurn",
      "Bahar Rastegari",
      "Adam Sobey",
      "Sebastian Stein",
      "Danesh Tarpore",
      "Vahid Yazdanpanah",
      "Jie Zhang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02260"
  },
  {
    "id": "arXiv:2210.02270",
    "title": "Weak-shot Semantic Segmentation via Dual Similarity Transfer",
    "abstract": "Semantic segmentation is an important and prevalent task, but severely\nsuffers from the high cost of pixel-level annotations when extending to more\nclasses in wider applications. To this end, we focus on the problem named\nweak-shot semantic segmentation, where the novel classes are learnt from\ncheaper image-level labels with the support of base classes having\noff-the-shelf pixel-level labels. To tackle this problem, we propose SimFormer,\nwhich performs dual similarity transfer upon MaskFormer. Specifically,\nMaskFormer disentangles the semantic segmentation task into two sub-tasks:\nproposal classification and proposal segmentation for each proposal. Proposal\nsegmentation allows proposal-pixel similarity transfer from base classes to\nnovel classes, which enables the mask learning of novel classes. We also learn\npixel-pixel similarity from base classes and distill such class-agnostic\nsemantic similarity to the semantic masks of novel classes, which regularizes\nthe segmentation model with pixel-level semantic relationship across images. In\naddition, we propose a complementary loss to facilitate the learning of novel\nclasses. Comprehensive experiments on the challenging COCO-Stuff-10K and ADE20K\ndatasets demonstrate the effectiveness of our method. Codes are available at\nhttps://github.com/bcmi/SimFormer-Weak-Shot-Semantic-Segmentation.",
    "descriptor": "\nComments: accepted by NeurIPS2022\n",
    "authors": [
      "Junjie Chen",
      "Li Niu",
      "Siyuan Zhou",
      "Jianlou Si",
      "Chen Qian",
      "Liqing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02270"
  },
  {
    "id": "arXiv:2210.02272",
    "title": "Numerical Modelling of the Brain Poromechanics by High-Order  Discontinuous Galerkin Methods",
    "abstract": "We introduce and analyze a discontinuous Galerkin method for the numerical\nmodelling of the equations of Multiple-Network Poroelastic Theory (MPET) in the\ndynamic formulation. The MPET model can comprehensively describe functional\nchanges in the brain considering multiple scales of fluids. Concerning the\nspatial discretization, we employ a high-order discontinuous Galerkin method on\npolygonal and polyhedral grids and we derive stability and a priori error\nestimates. The temporal discretization is based on a coupling between a Newmark\n$\\beta$-method for the momentum equation and a $\\theta$-method for the pressure\nequations. After the presentation of some verification numerical tests, we\nperform a convergence analysis using an agglomerated mesh of a geometry of a\nbrain slice. Finally we present a simulation in a three dimensional\npatient-specific brain reconstructed from magnetic resonance images. The model\npresented in this paper can be regarded as a preliminary attempt to model the\nperfusion in the brain.",
    "descriptor": "",
    "authors": [
      "Mattia Corti",
      "Paola F. Antonietti",
      "Luca Dede'",
      "Alfio Maria Quarteroni"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02272"
  },
  {
    "id": "arXiv:2210.02279",
    "title": "A Reduced Basis Ensemble Kalman Method",
    "abstract": "In the process of reproducing the state dynamics of parameter dependent\ndistributed systems, data from physical measurements can be incorporated into\nthe mathematical model to reduce the parameter uncertainty and, consequently,\nimprove the state prediction. Such a Data Assimilation process must deal with\nthe data and model misfit arising from experimental noise as well as model\ninaccuracies and uncertainties. In this work, we focus on the ensemble Kalman\nmethod (EnKM), a particle-based iterative regularization method designed for\n\\textit{a posteriori} analysis of time series. The method is gradient free and,\nlike the ensemble Kalman filter (EnKF), relies on a sample of parameters or\nparticle ensemble to identify the state that better reproduces the physical\nobservations, while preserving the physics of the system as described by the\nbest knowledge model. We consider systems described by parameterized parabolic\npartial differential equations and employ model order reduction (MOR)\ntechniques to generate surrogate models of different accuracy with uncertain\nparameters. Their use in combination with the EnKM involves the introduction of\nthe model bias which constitutes a new source of systematic error. To mitigate\nits impact, an algorithm adjustment is proposed accounting for a prior\nestimation of the bias in the data. The resulting RB-EnKM is tested in\ndifferent conditions, including different ensemble sizes and increasing levels\nof experimental noise. The results are compared to those obtained with the\nstandard EnKF and with the unadjusted algorithm.",
    "descriptor": "",
    "authors": [
      "Francesco A. B. Silva",
      "Cecilia Pagliantini",
      "Martin Grepl",
      "Karen Veroy"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02279"
  },
  {
    "id": "arXiv:2210.02282",
    "title": "Covering Properties of Sum-Rank Metric Codes",
    "abstract": "The sum-rank metric can be seen as a generalization of both, the rank and the\nHamming metric. It is well known that sum-rank metric codes outperform rank\nmetric codes in terms of the required field size to construct maximum distance\nseparable codes (i.e., the codes achieving the Singleton bound in the\ncorresponding metric). In this work, we investigate the covering property of\nsum-rank metric codes to enrich the theory of sum-rank metric codes. We intend\nto answer the question: what is the minimum cardinality of a code given a\nsum-rank covering radius? We show the relations of this quantity between\ndifferent metrics and provide several lower and upper bounds for sum-rank\nmetric codes.",
    "descriptor": "\nComments: 7 pages. This work has been presented in 58th Annual Allerton Conference on Communication, Control, and Computing\n",
    "authors": [
      "Cornelia Ott",
      "Hedongliang Liu",
      "Antonia Wachter-Zeh"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.02282"
  },
  {
    "id": "arXiv:2210.02284",
    "title": "Unsupervised Sentence Textual Similarity with Compositional Phrase  Semantics",
    "abstract": "Measuring Sentence Textual Similarity (STS) is a classic task that can be\napplied to many downstream NLP applications such as text generation and\nretrieval. In this paper, we focus on unsupervised STS that works on various\ndomains but only requires minimal data and computational resources.\nTheoretically, we propose a light-weighted Expectation-Correction (EC)\nformulation for STS computation. EC formulation unifies unsupervised STS\napproaches including the cosine similarity of Additively Composed (AC) sentence\nembeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose\nthe Recursive Optimal Transport Similarity (ROTS) algorithm to capture the\ncompositional phrase semantics by composing multiple recursive EC formulations.\nROTS finishes in linear time and is faster than its predecessors. ROTS is\nempirically more effective and scalable than previous approaches. Extensive\nexperiments on 29 STS tasks under various settings show the clear advantage of\nROTS over existing approaches. Detailed ablation studies demonstrate the\neffectiveness of our approaches.",
    "descriptor": "\nComments: COLING 2022; Github repository this https URL ; Partially overlapped with arXiv:2002.00745 ; 20 pages, 5 figures, 17 tables\n",
    "authors": [
      "Zihao Wang",
      "Jiaheng Dou",
      "Yong Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02284"
  },
  {
    "id": "arXiv:2210.02287",
    "title": "TC-SKNet with GridMask for Low-complexity Classification of Acoustic  scene",
    "abstract": "Convolution neural networks (CNNs) have good performance in low-complexity\nclassification tasks such as acoustic scene classifications (ASCs). However,\nthere are few studies on the relationship between the length of target speech\nand the size of the convolution kernels. In this paper, we combine Selective\nKernel Network with Temporal-Convolution (TC-SKNet) to adjust the receptive\nfield of convolution kernels to solve the problem of variable length of target\nvoice while keeping low-complexity. GridMask is a data augmentation strategy by\nmasking part of the raw data or feature area. It can enhance the generalization\nof the model as the role of dropout. In our experiments, the performance gain\nbrought by GridMask is stronger than spectrum augmentation in ASCs. Finally, we\nadopt AutoML to search best structure of TC-SKNet and hyperparameters of\nGridMask for improving the classification performance. As a result, a peak\naccuracy of 59.87% TC-SKNet is equivalent to that of SOTA, but the parameters\nonly use 20.9 K.",
    "descriptor": "\nComments: Accepted to APSIPA ASC 2022\n",
    "authors": [
      "Luyuan Xie",
      "Yan Zhong",
      "Lin Yang",
      "Zhaoyu Yan",
      "Zhonghai Wu",
      "Junjie Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.02287"
  },
  {
    "id": "arXiv:2210.02288",
    "title": "On Convexity in Split graphs: Complexity of Steiner tree and Domination",
    "abstract": "Given a graph $G$ with a terminal set $R \\subseteq V(G)$, the Steiner tree\nproblem (STREE) asks for a set $S\\subseteq V(G) \\setminus R$ such that the\ngraph induced on $S\\cup R$ is connected. A split graph is a graph which can be\npartitioned into a clique and an independent set. It is known that STREE is\nNP-complete on split graphs \\cite{white1985steiner}. To strengthen this result,\nwe introduce convex ordering on one of the partitions (clique or independent\nset), and prove that STREE is polynomial-time solvable for tree-convex split\ngraphs with convexity on clique ($K$), whereas STREE is NP-complete on\ntree-convex split graphs with convexity on independent set ($I$). We further\nstrengthen our NP-complete result by establishing a dichotomy which says that\nfor unary-tree-convex split graphs (path-convex split graphs), STREE is\npolynomial-time solvable, and NP-complete for binary-tree-convex split graphs\n(comb-convex split graphs). We also show that STREE is polynomial-time solvable\nfor triad-convex split graphs with convexity on $I$, and circular-convex split\ngraphs. Further, we show that STREE can be used as a framework for the\ndominating set problem (DS) on split graphs, and hence the classical complexity\n(P vs NPC) of STREE and DS is the same for all these subclasses of split\ngraphs. Furthermore, it is important to highlight that in\n\\cite{CHLEBIK20081264}, it is incorrectly claimed that the problem of finding a\nminimum dominating set on split graphs cannot be approximated within\n$(1-\\epsilon)\\ln |V(G)|$ in polynomial-time for any $\\epsilon >0$ unless NP\n$\\subseteq$ DTIME $n^{O(\\log \\log n)}$. When the input is restricted to split\ngraphs, we show that the minimum dominating set problem has\n$2-\\frac{1}{|I|}$-approximation algorithm that runs in polynomial time.",
    "descriptor": "",
    "authors": [
      "A Mohanapriya",
      "P Renjith",
      "N Sadagopan"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.02288"
  },
  {
    "id": "arXiv:2210.02289",
    "title": "Coverage and Capacity of Joint Communication and Sensing in Wireless  Networks",
    "abstract": "From an information theoretic perspective, joint communication and sensing\n(JCAS) represents a natural generalization of communication network\nfunctionality. However, it requires the reevaluation of network performance\nfrom a multi-objective perspective. We develop a novel mathematical framework\nfor characterizing the sensing and communication coverage probability and\nergodic capacity in JCAS networks. We employ an information theoretic\nformulation of radar tracking to extend the notions of coverage probability and\nergodic capacity to the radar setting. Using this framework, we analyze the\ndownlink sensing and communication coverage and capacity of a JCAS network\nemploying a shared multicarrier waveform and analog beamforming. Leveraging\ntools from stochastic geometry, we derive upper and lower bounds for these\nquantities. We also develop several general technical results including: i) a\nmethod for obtaining closed form bounds on the Laplace Transform of a shot\nnoise process, ii) an analog of H\\\"older's Inequality to the setting of\nharmonic means, and iii) a relation between the Laplace and Mellin Transforms\nof a non-negative random variable. We use the derived bounds to investigate the\nperformance of JCAS networks under varying base station and blockage density.\nAmong several insights, our analysis indicates that network densification\nimproves sensing performance - in contrast to communications.",
    "descriptor": "\nComments: 77 pages, 5 figures. Submitted to IEEE Transactions on Information Theory\n",
    "authors": [
      "Nicholas R. Olson",
      "Jeffrey G. Andrews",
      "Robert W. Heath Jr"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.02289"
  },
  {
    "id": "arXiv:2210.02291",
    "title": "Progressive Denoising Model for Fine-Grained Text-to-Image Generation",
    "abstract": "Recently, vector quantized autoregressive (VQ-AR) models have shown\nremarkable results in text-to-image synthesis by equally predicting discrete\nimage tokens from the top left to bottom right in the latent space. Although\nthe simple generative process surprisingly works well, is this the best way to\ngenerate the image? For instance, human creation is more inclined to the\noutline-to-fine of an image, while VQ-AR models themselves do not consider any\nrelative importance of each component. In this paper, we present a progressive\ndenoising model for high-fidelity text-to-image image generation. The proposed\nmethod takes effect by creating new image tokens from coarse to fine based on\nthe existing context in a parallel manner and this procedure is recursively\napplied until an image sequence is completed. The resulting coarse-to-fine\nhierarchy makes the image generation process intuitive and interpretable.\nExtensive experiments demonstrate that the progressive model produces\nsignificantly better results when compared with the previous VQ-AR method in\nFID score across a wide variety of categories and aspects. Moreover, the\ntext-to-image generation time of traditional AR increases linearly with the\noutput image resolution and hence is quite time-consuming even for normal-size\nimages. In contrast, our approach allows achieving a better trade-off between\ngeneration quality and speed.",
    "descriptor": "\nComments: Technique report\n",
    "authors": [
      "Zhengcong Fei",
      "Mingyuan Fan",
      "Junshi Huang",
      "Xiaoming Wei",
      "Xiaolin Wei"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02291"
  },
  {
    "id": "arXiv:2210.02292",
    "title": "Double-Ended Palindromic Trees: A Linear-Time Data Structure and Its  Applications",
    "abstract": "The palindromic tree (a.k.a. eertree) is a linear-size data structure that\nprovides access to all palindromic substrings of a string. In this paper, we\npropose a generalized version of eertree, called double-ended eertree, which\nsupports linear-time online double-ended queue operations on the stored string.\nAt the heart of our construction, is a class of substrings, called surfaces, of\nindependent interest. Namely, surfaces are neither prefixes nor suffixes of any\nother palindromic substrings and characterize the link structure of all\npalindromic substrings in the eertree.\nAs an application, we develop a framework for range queries involving\npalindromes on strings, including counting distinct palindromic substrings, and\nfinding the longest palindromic substring, shortest unique palindromic\nsubstring and shortest absent palindrome of any substring. In particular,\noffline queries only use linear space. Apart from range queries, we enumerate\npalindromic rich strings with a given word in linear time on the length of the\ngiven word.",
    "descriptor": "\nComments: 66 pages, 3 tables, 15 algorithms\n",
    "authors": [
      "Qisheng Wang",
      "Ming Yang",
      "Xinrui Zhu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.02292"
  },
  {
    "id": "arXiv:2210.02297",
    "title": "Multiclass Learnability Beyond the PAC Framework: Universal Rates and  Partial Concept Classes",
    "abstract": "In this paper we study the problem of multiclass classification with a\nbounded number of different labels $k$, in the realizable setting. We extend\nthe traditional PAC model to a) distribution-dependent learning rates, and b)\nlearning rates under data-dependent assumptions. First, we consider the\nuniversal learning setting (Bousquet, Hanneke, Moran, van Handel and\nYehudayoff, STOC '21), for which we provide a complete characterization of the\nachievable learning rates that holds for every fixed distribution. In\nparticular, we show the following trichotomy: for any concept class, the\noptimal learning rate is either exponential, linear or arbitrarily slow.\nAdditionally, we provide complexity measures of the underlying hypothesis class\nthat characterize when these rates occur. Second, we consider the problem of\nmulticlass classification with structured data (such as data lying on a low\ndimensional manifold or satisfying margin conditions), a setting which is\ncaptured by partial concept classes (Alon, Hanneke, Holzman and Moran, FOCS\n'21). Partial concepts are functions that can be undefined in certain parts of\nthe input space. We extend the traditional PAC learnability of total concept\nclasses to partial concept classes in the multiclass setting and investigate\ndifferences between partial and total concepts.",
    "descriptor": "",
    "authors": [
      "Alkis Kalavasis",
      "Grigoris Velegkas",
      "Amin Karbasi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.02297"
  },
  {
    "id": "arXiv:2210.02299",
    "title": "SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit  Neural Representations",
    "abstract": "Accurate mapping of large-scale environments is an essential building block\nof most outdoor autonomous systems. Challenges of traditional mapping methods\ninclude the balance between memory consumption and mapping accuracy. This paper\naddresses the problems of achieving large-scale 3D reconstructions with\nimplicit representations using 3D LiDAR measurements. We learn and store\nimplicit features through an octree-based hierarchical structure, which is\nsparse and extensible. The features can be turned into signed distance values\nthrough a shallow neural network. We leverage binary cross entropy loss to\noptimize the local features with the 3D measurements as supervision. Based on\nour implicit representation, we design an incremental mapping system with\nregularization to tackle the issue of catastrophic forgetting in continual\nlearning. Our experiments show that our 3D reconstructions are more accurate,\ncomplete, and memory-efficient than current state-of-the-art 3D mapping\nmethods.",
    "descriptor": "\nComments: 6+1 pages, submit to ICRA'23\n",
    "authors": [
      "Xingguang Zhong",
      "Yue Pan",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02299"
  },
  {
    "id": "arXiv:2210.02300",
    "title": "Spatial-Temporal-Aware Safe Multi-Agent Reinforcement Learning of  Connected Autonomous Vehicles in Challenging Scenarios",
    "abstract": "Communication technologies enable coordination among connected and autonomous\nvehicles (CAVs). However, it remains unclear how to utilize shared information\nto improve the safety and efficiency of the CAV system. In this work, we\npropose a framework of constrained multi-agent reinforcement learning (MARL)\nwith a parallel safety shield for CAVs in challenging driving scenarios. The\ncoordination mechanisms of the proposed MARL include information sharing and\ncooperative policy learning, with Graph Convolutional Network (GCN)-Transformer\nas a spatial-temporal encoder that enhances the agent's environment awareness.\nThe safety shield module with Control Barrier Functions (CBF)-based safety\nchecking protects the agents from taking unsafe actions. We design a\nconstrained multi-agent advantage actor-critic (CMAA2C) algorithm to train safe\nand cooperative policies for CAVs. With the experiment deployed in the CARLA\nsimulator, we verify the effectiveness of the safety checking, spatial-temporal\nencoder, and coordination mechanisms designed in our method by comparative\nexperiments in several challenging scenarios with the defined hazard vehicles\n(HAZV). Results show that our proposed methodology significantly increases\nsystem safety and efficiency in challenging scenarios.",
    "descriptor": "\nComments: 6 pages, 5 figures\n",
    "authors": [
      "Zhili Zhang",
      "Songyang Han",
      "Jiangwei Wang",
      "Fei Miao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.02300"
  },
  {
    "id": "arXiv:2210.02302",
    "title": "GLAD: Grounded Layered Autonomous Driving for Complex Service Tasks",
    "abstract": "Given the current point-to-point navigation capabilities of autonomous\nvehicles, researchers are looking into complex service requests that require\nthe vehicles to visit multiple points of interest. In this paper, we develop a\nlayered planning framework, called GLAD, for complex service requests in\nautonomous urban driving. There are three layers for service-level,\nbehavior-level, and motion-level planning. The layered framework is unique in\nits tight coupling, where the different layers communicate user preferences,\nsafety estimates, and motion costs for system optimization. GLAD is visually\ngrounded by perceptual learning from a dataset of 13.8k instances collected\nfrom driving behaviors. GLAD enables autonomous vehicles to efficiently and\nsafely fulfill complex service requests. Experimental results from abstract and\nfull simulation show that our system outperforms a few competitive baselines\nfrom the literature.",
    "descriptor": "",
    "authors": [
      "Yan Ding",
      "Cheng Cui",
      "Xiaohan Zhang",
      "Shiqi Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02302"
  },
  {
    "id": "arXiv:2210.02303",
    "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
    "abstract": "We present Imagen Video, a text-conditional video generation system based on\na cascade of video diffusion models. Given a text prompt, Imagen Video\ngenerates high definition videos using a base video generation model and a\nsequence of interleaved spatial and temporal video super-resolution models. We\ndescribe how we scale up the system as a high definition text-to-video model\nincluding design decisions such as the choice of fully-convolutional temporal\nand spatial super-resolution models at certain resolutions, and the choice of\nthe v-parameterization of diffusion models. In addition, we confirm and\ntransfer findings from previous work on diffusion-based image generation to the\nvideo generation setting. Finally, we apply progressive distillation to our\nvideo models with classifier-free guidance for fast, high quality sampling. We\nfind Imagen Video not only capable of generating videos of high fidelity, but\nalso having a high degree of controllability and world knowledge, including the\nability to generate diverse videos and text animations in various artistic\nstyles and with 3D object understanding. See\nhttps://imagen.research.google/video/ for samples.",
    "descriptor": "\nComments: See accompanying website: this https URL\n",
    "authors": [
      "Jonathan Ho",
      "William Chan",
      "Chitwan Saharia",
      "Jay Whang",
      "Ruiqi Gao",
      "Alexey Gritsenko",
      "Diederik P. Kingma",
      "Ben Poole",
      "Mohammad Norouzi",
      "David J. Fleet",
      "Tim Salimans"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02303"
  },
  {
    "id": "arXiv:2210.02305",
    "title": "Neuro-Planner: A 3D Visual Navigation Method for MAV with Depth Camera  based on Neuromorphic Reinforcement Learning",
    "abstract": "Traditional visual navigation methods of micro aerial vehicle (MAV) usually\ncalculate a passable path that satisfies the constraints depending on a prior\nmap. However, these methods have issues such as high demand for computing\nresources and poor robustness in face of unfamiliar environments. Aiming to\nsolve the above problems, we propose a neuromorphic reinforcement learning\nmethod (Neuro-Planner) that combines spiking neural network (SNN) and deep\nreinforcement learning (DRL) to realize MAV 3D visual navigation with depth\ncamera. Specifically, we design spiking actor network based on two-state LIF\n(TS-LIF) neurons and its encoding-decoding schemes for efficient inference.\nThen our improved hybrid deep deterministic policy gradient (HDDPG) and\nTS-LIF-based spatio-temporal back propagation (STBP) algorithms are used as the\ntraining framework for actor-critic network architecture. To verify the\neffectiveness of the proposed Neuro-Planner, we carry out detailed comparison\nexperiments with various SNN training algorithm (STBP, BPTT and SLAYER) in the\nsoftware-in-the-loop (SITL) simulation framework. The navigation success rate\nof our HDDPG-STBP is 4.3\\% and 5.3\\% higher than that of the original DDPG in\nthe two evaluation environments. To the best of our knowledge, this is the\nfirst work combining neuromorphic computing and deep reinforcement learning for\nMAV 3D visual navigation task.",
    "descriptor": "",
    "authors": [
      "Junjie Jiang",
      "Delei Kong",
      "Kuanxv Hou",
      "Xinjie Huang",
      "Hao Zhuang",
      "Fang Zheng"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02305"
  },
  {
    "id": "arXiv:2210.02313",
    "title": "Multi-stream Fusion for Class Incremental Learning in Pill Image  Classification",
    "abstract": "Classifying pill categories from real-world images is crucial for various\nsmart healthcare applications. Although existing approaches in image\nclassification might achieve a good performance on fixed pill categories, they\nfail to handle novel instances of pill categories that are frequently presented\nto the learning algorithm. To this end, a trivial solution is to train the\nmodel with novel classes. However, this may result in a phenomenon known as\ncatastrophic forgetting, in which the system forgets what it learned in\nprevious classes. In this paper, we address this challenge by introducing the\nclass incremental learning (CIL) ability to traditional pill image\nclassification systems. Specifically, we propose a novel incremental\nmulti-stream intermediate fusion framework enabling incorporation of an\nadditional guidance information stream that best matches the domain of the\nproblem into various state-of-the-art CIL methods. From this framework, we\nconsider color-specific information of pill images as a guidance stream and\ndevise an approach, namely \"Color Guidance with Multi-stream intermediate\nfusion\"(CG-IMIF) for solving CIL pill image classification task. We conduct\ncomprehensive experiments on real-world incremental pill image classification\ndataset, namely VAIPE-PCIL, and find that the CG-IMIF consistently outperforms\nseveral state-of-the-art methods by a large margin in different task settings.\nOur code, data, and trained model are available at\nhttps://github.com/vinuni-vishc/CG-IMIF.",
    "descriptor": "\nComments: Accepted for publication in the Asian Conference on Computer Vision (ACCV 2022)\n",
    "authors": [
      "Trong-Tung Nguyen",
      "Hieu H. Pham",
      "Phi Le Nguyen",
      "Thanh Hung Nguyen",
      "Minh Do"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02313"
  },
  {
    "id": "arXiv:2210.02317",
    "title": "Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing  Local and Remote Computers",
    "abstract": "Real-time learning is crucial for robotic agents adapting to ever-changing,\nnon-stationary environments. A common setup for a robotic agent is to have two\ndifferent computers simultaneously: a resource-limited local computer tethered\nto the robot and a powerful remote computer connected wirelessly. Given such a\nsetup, it is unclear to what extent the performance of a learning system can be\naffected by resource limitations and how to efficiently use the wirelessly\nconnected powerful computer to compensate for any performance loss. In this\npaper, we implement a real-time learning system called the Remote-Local\nDistributed (ReLoD) system to distribute computations of two deep reinforcement\nlearning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy\nOptimization (PPO), between a local and a remote computer. The performance of\nthe system is evaluated on two vision-based control tasks developed using a\nrobotic arm and a mobile robot. Our results show that SAC's performance\ndegrades heavily on a resource-limited local computer. Strikingly, when all\ncomputations of the learning system are deployed on a remote workstation, SAC\nfails to compensate for the performance loss, indicating that, without careful\nconsideration, using a powerful remote computer may not result in performance\nimprovement. However, a carefully chosen distribution of computations of SAC\nconsistently and substantially improves its performance on both tasks. On the\nother hand, the performance of PPO remains largely unaffected by the\ndistribution of computations. In addition, when all computations happen solely\non a powerful tethered computer, the performance of our system remains on par\nwith an existing system that is well-tuned for using a single machine. ReLoD is\nthe only publicly available system for real-time RL that applies to multiple\nrobots for vision-based tasks.",
    "descriptor": "\nComments: Submitted to the 2023 International Conference on Robotics and Automation (ICRA). Source code at this https URL and companion video at this https URL\n",
    "authors": [
      "Yan Wang",
      "Gautham Vasan",
      "A. Rupam Mahmood"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02317"
  },
  {
    "id": "arXiv:2210.02318",
    "title": "FQDet: Fast-converging Query-based Detector",
    "abstract": "Recently, two-stage Deformable DETR introduced the query-based two-stage\nhead, a new type of two-stage head different from the region-based two-stage\nheads of classical detectors as Faster R-CNN. In query-based two-stage heads,\nthe second stage selects one feature per detection, called the query, as\nopposed to pooling a rectangular grid of features as in region-based detectors.\nIn this work, we further improve the query-based head from Deformable DETR,\nsignificantly speeding up the convergence while increasing its performance.\nThis is achieved by incorporating classical techniques such as anchor\ngeneration within the query-based paradigm. By combining the best of both the\nclassical and the query-based worlds, our FQDet head peaks at 45.4 AP on the\n2017 COCO validation set when using a ResNet-50+TPN backbone, only after\ntraining for 12 epochs using the 1x schedule. We outperform other\nhigh-performing two-stage heads such as e.g. Cascade R-CNN, while using the\nsame backbone and while often being computationally cheaper. Additionally, when\nusing the large ResNeXt-101-DCN+TPN backbone and multi-scale testing, our FQDet\nhead achieves 52.9 AP on the 2017 COCO test-dev set after only 12 epochs of\ntraining. Code will be released.",
    "descriptor": "",
    "authors": [
      "C\u00e9dric Picron",
      "Punarjay Chakravarty",
      "Tinne Tuytelaars"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02318"
  },
  {
    "id": "arXiv:2210.02324",
    "title": "Promising or Elusive? Unsupervised Object Segmentation from Real-world  Single Images",
    "abstract": "In this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically\ninvestigate the effectiveness of existing unsupervised models on challenging\nreal-world images. We firstly introduce four complexity factors to\nquantitatively measure the distributions of object- and scene-level biases in\nappearance and geometry for datasets with human annotations. With the aid of\nthese factors, we empirically find that, not surprisingly, existing\nunsupervised models catastrophically fail to segment generic objects in\nreal-world images, although they can easily achieve excellent performance on\nnumerous simple synthetic datasets, due to the vast gap in objectness biases\nbetween synthetic and real images. By conducting extensive experiments on\nmultiple groups of ablated real-world datasets, we ultimately find that the key\nfactors underlying the colossal failure of existing unsupervised models on\nreal-world images are the challenging distributions of object- and scene-level\nbiases in appearance and geometry. Because of this, the inductive biases\nintroduced in existing unsupervised models can hardly capture the diverse\nobject distributions. Our research results suggest that future work should\nexploit more explicit objectness biases in the network design.",
    "descriptor": "\nComments: NeurIPS 2022. Code and data are available at project page: this https URL\n",
    "authors": [
      "Yafei Yang",
      "Bo Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02324"
  },
  {
    "id": "arXiv:2210.02326",
    "title": "Learning Across Domains and Devices: Style-Driven Source-Free Domain  Adaptation in Clustered Federated Learning",
    "abstract": "Federated Learning (FL) has recently emerged as a possible way to tackle the\ndomain shift in real-world Semantic Segmentation (SS) without compromising the\nprivate nature of the collected data. However, most of the existing works on FL\nunrealistically assume labeled data in the remote clients. Here we propose a\nnovel task (FFREEDA) in which the clients' data is unlabeled and the server\naccesses a source labeled dataset for pre-training only. To solve FFREEDA, we\npropose LADD, which leverages the knowledge of the pre-trained model by\nemploying self-supervision with ad-hoc regularization techniques for local\ntraining and introducing a novel federated clustered aggregation scheme based\non the clients' style. Our experiments show that our algorithm is able to\nefficiently tackle the new task outperforming existing approaches. The code is\navailable at https://github.com/Erosinho13/LADD.",
    "descriptor": "\nComments: WACV 2023; 11 pages manuscript, 6 pages supplemental material\n",
    "authors": [
      "Donald Shenaj",
      "Eros Fan\u00ec",
      "Marco Toldo",
      "Debora Caldarola",
      "Antonio Tavera",
      "Umberto Michieli",
      "Marco Ciccone",
      "Pietro Zanuttigh",
      "Barbara Caputo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02326"
  },
  {
    "id": "arXiv:2210.02328",
    "title": "Geometric discretization of diffeomorphisms",
    "abstract": "Many partial differential equations in mathematical physics describe the\nevolution of time-dependent (smooth) vector fields on a fixed domain. Examples\ninclude compressible fluid dynamics, shape analysis, optimal transport, and\nshallow water equations. The flow of the vector field generates a\ndiffeomorphism, which in turn can be used to act on for instance functions or\ndensities. Here, we consider a geometric discretization of diffeomorphisms on\nthe sphere, based on quantization theory. We provide numerical examples and\ndiscuss potential applications of the discretization method.",
    "descriptor": "\nComments: 17 pages, 6 figures\n",
    "authors": [
      "Erik Jansson",
      "Klas Modin"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Differential Geometry (math.DG)"
    ],
    "url": "https://arxiv.org/abs/2210.02328"
  },
  {
    "id": "arXiv:2210.02329",
    "title": "Non-closure under complementation for unambiguous linear grammars",
    "abstract": "The paper demonstrates the non-closure of the family of unambiguous linear\nlanguages (that is, those defined by unambiguous linear context-free grammars)\nunder complementation. To be precise, a particular unambiguous linear grammar\nis presented, and it is proved that the complement of this language is not\ndefined by any context-free grammar. This also constitutes an alternative proof\nfor the result of Hibbard and Ullian (\"The independence of inherent ambiguity\nfrom complementedness among context-free languages\", J.ACM, 1966) on the\nnon-closure of the unambiguous languages under complementation.",
    "descriptor": "",
    "authors": [
      "Olga Martynova",
      "Alexander Okhotin"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2210.02329"
  },
  {
    "id": "arXiv:2210.02330",
    "title": "Revisiting Graph Contrastive Learning from the Perspective of Graph  Spectrum",
    "abstract": "Graph Contrastive Learning (GCL), learning the node representations by\naugmenting graphs, has attracted considerable attentions. Despite the\nproliferation of various graph augmentation strategies, some fundamental\nquestions still remain unclear: what information is essentially encoded into\nthe learned representations by GCL? Are there some general graph augmentation\nrules behind different augmentations? If so, what are they and what insights\ncan they bring? In this paper, we answer these questions by establishing the\nconnection between GCL and graph spectrum. By an experimental investigation in\nspectral domain, we firstly find the General grAph augMEntation (GAME) rule for\nGCL, i.e., the difference of the high-frequency parts between two augmented\ngraphs should be larger than that of low-frequency parts. This rule reveals the\nfundamental principle to revisit the current graph augmentations and design new\neffective graph augmentations. Then we theoretically prove that GCL is able to\nlearn the invariance information by contrastive invariance theorem, together\nwith our GAME rule, for the first time, we uncover that the learned\nrepresentations by GCL essentially encode the low-frequency information, which\nexplains why GCL works. Guided by this rule, we propose a spectral graph\ncontrastive learning module (SpCo), which is a general and GCL-friendly\nplug-in. We combine it with different existing GCL models, and extensive\nexperiments well demonstrate that it can further improve the performances of a\nwide variety of different GCL methods.",
    "descriptor": "\nComments: This paper has been accepted by NeurIPS 2022\n",
    "authors": [
      "Nian Liu",
      "Xiao Wang",
      "Deyu Bo",
      "Chuan Shi",
      "Jian Pei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02330"
  },
  {
    "id": "arXiv:2210.02334",
    "title": "Using Full-Text Content to Characterize and Identify Best Seller Books",
    "abstract": "Artistic pieces can be studied from several perspectives, one example being\ntheir reception among readers over time. In the present work, we approach this\ninteresting topic from the standpoint of literary works, particularly assessing\nthe task of predicting whether a book will become a best seller. Dissimilarly\nfrom previous approaches, we focused on the full content of books and\nconsidered visualization and classification tasks. We employed visualization\nfor the preliminary exploration of the data structure and properties, involving\nSemAxis and linear discriminant analyses. Then, to obtain quantitative and more\nobjective results, we employed various classifiers. Such approaches were used\nalong with a dataset containing (i) books published from 1895 to 1924 and\nconsecrated as best sellers by the \\emph{Publishers Weekly Bestseller Lists}\nand (ii) literary works published in the same period but not being mentioned in\nthat list. Our comparison of methods revealed that the best-achieved result -\ncombining a bag-of-words representation with a logistic regression classifier -\nled to an average accuracy of 0.75 both for the leave-one-out and 10-fold\ncross-validations. Such an outcome suggests that it is unfeasible to predict\nthe success of books with high accuracy using only the full content of the\ntexts. Nevertheless, our findings provide insights into the factors leading to\nthe relative success of a literary work.",
    "descriptor": "",
    "authors": [
      "Giovana D. da Silva",
      "Filipi N. Silva",
      "Henrique F. de Arruda",
      "B\u00e1rbara C. e Souza",
      "Luciano da F. Costa",
      "Diego R. Amancio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02334"
  },
  {
    "id": "arXiv:2210.02336",
    "title": "An Integrated Web Platform for the Mizar Mathematical Library",
    "abstract": "This paper reports on the development of a Web platform to host the Mizar\nMathematical Library (MML). In recent years, the size of formalized\nmathematical libraries has been drastically increasing, and this has led to a\ngrowing demand for tools that support efficient and comprehensive browsing,\nsearching, and annotation of these libraries. This platform implements a Wiki\nfunction to add comments to the HTMLized MML, three types of search function\n(article, symbol, and theorem), and a function to show the dependency graph of\nthe MML. This platform is designed with consistency, scalability, and\ninteroperability as top priorities for long-term use.",
    "descriptor": "",
    "authors": [
      "Hideharu Furushima",
      "Daichi Yamamichi",
      "Seigo Shigenaka",
      "Kazuhisa Nakasho",
      "Katsumi Wasaki"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.02336"
  },
  {
    "id": "arXiv:2210.02337",
    "title": "When Physical Layer Key Generation Meets RIS: Opportunities, Challenges,  and Road Ahead",
    "abstract": "Physical layer key generation (PLKG) is a promising technology to obtain\nsymmetric keys between a pair of wireless communication users in a\nplug-and-play manner. The shared entropy source almost entirely comes from the\nintrinsic randomness of the radio channel, which is highly dependent on the\nwireless environment. However, in some static wireless environments, the\nintrinsic randomness of wireless channel is hard to be guaranteed. Very\nrecently, thanks to reconfigurable intelligent surfaces (RISs) with their\nexcellent ability on electromagnetic wave control, the wireless channel\nenvironment can be customized. In this article, we overview the RIS-aided PLKG\nin a static indoor environment, including its potential application scenarios,\nchannel model and hardware architectures. Then, we analyze the design\nchallenges of RIS-aided PLKG, including channel reciprocity, RIS switch speed\nand RIS deployment via proof-of-concept experiments on a RIS-aided PLKG\nprototype system. In particular, our experimental results show that the key\ngeneration rate is 15-fold higher than that without RIS in a static indoor\nenvironment. Next, we design a RIS flip attack via a prototype experiment and\ndiscuss its possible attack-defense countermeasures. Finally, several\nconclusions and future directions are identified.",
    "descriptor": "",
    "authors": [
      "Ning Gao",
      "Yu Han",
      "Nannan Li",
      "Shi Jin",
      "Michail Matthaiou"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.02337"
  },
  {
    "id": "arXiv:2210.02343",
    "title": "Visual Backtracking Teleoperation: A Data Collection Protocol for  Offline Image-Based Reinforcement Learning",
    "abstract": "We consider how to most efficiently leverage teleoperator time to collect\ndata for learning robust image-based value functions and policies for sparse\nreward robotic tasks. To accomplish this goal, we modify the process of data\ncollection to include more than just successful demonstrations of the desired\ntask. Instead we develop a novel protocol that we call Visual Backtracking\nTeleoperation (VBT), which deliberately collects a dataset of visually similar\nfailures, recoveries, and successes. VBT data collection is particularly useful\nfor efficiently learning accurate value functions from small datasets of\nimage-based observations. We demonstrate VBT on a real robot to perform\ncontinuous control from image observations for the deformable manipulation task\nof T-shirt grasping. We find that by adjusting the data collection process we\nimprove the quality of both the learned value functions and policies over a\nvariety of baseline methods for data collection. Specifically, we find that\noffline reinforcement learning on VBT data outperforms standard behavior\ncloning on successful demonstration data by 13% when both methods are given\nequal-sized datasets of 60 minutes of data from the real robot.",
    "descriptor": "",
    "authors": [
      "David Brandfonbrener",
      "Stephen Tu",
      "Avi Singh",
      "Stefan Welker",
      "Chad Boodoo",
      "Nikolai Matni",
      "Jake Varley"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02343"
  },
  {
    "id": "arXiv:2210.02345",
    "title": "Spatially Constrained Time-Optimal Motion Planning",
    "abstract": "This paper focuses on spatial time-optimal motion planning, a generalization\nof the exact time-optimal path following problem that allows the system to plan\nwithin a predefined space. In contrast to state-of-the-art methods, we drop the\nassumption that a collision-free geometric reference is given. Instead, we\npresent a two-stage motion planning method that solely relies on a goal\nlocation and a geometric representation of the environment to compute a\ntime-optimal trajectory that is compliant with system dynamics and constraints.\nTo do so, the proposed scheme first computes an obstacle-free Pythagorean\nHodograph parametric spline, and second solves a spatially reformulated\nminimum-time optimization problem. The spline obtained in the first stage is\nnot a geometric reference, but an extension of the environment representation,\nand thus, time-optimality of the solution is guaranteed. The efficacy of the\nproposed approach is benchmarked by a known planar example and validated in a\nmore complex spatial system, illustrating its versatility and applicability.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Jon Arrizabalaga",
      "Markus Ryll"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.02345"
  },
  {
    "id": "arXiv:2210.02347",
    "title": "clip2latent: Text driven sampling of a pre-trained StyleGAN using  denoising diffusion and CLIP",
    "abstract": "We introduce a new method to efficiently create text-to-image models from a\npre-trained CLIP and StyleGAN. It enables text driven sampling with an existing\ngenerative model without any external data or fine-tuning. This is achieved by\ntraining a diffusion model conditioned on CLIP embeddings to sample latent\nvectors of a pre-trained StyleGAN, which we call clip2latent. We leverage the\nalignment between CLIP's image and text embeddings to avoid the need for any\ntext labelled data for training the conditional diffusion model. We demonstrate\nthat clip2latent allows us to generate high-resolution (1024x1024 pixels)\nimages based on text prompts with fast sampling, high image quality, and low\ntraining compute and data requirements. We also show that the use of the well\nstudied StyleGAN architecture, without further fine-tuning, allows us to\ndirectly apply existing methods to control and modify the generated images\nadding a further layer of control to our text-to-image pipeline.",
    "descriptor": "\nComments: Accepted to BMVC 2022\n",
    "authors": [
      "Justin N. M. Pinkney",
      "Chuan Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02347"
  },
  {
    "id": "arXiv:2210.02348",
    "title": "Structure preserving transport stabilized compatible finite element  methods for magnetohydrodynamics",
    "abstract": "We present compatible finite element space discretizations for the ideal\ncompressible magnetohydrodynamic equations. The magnetic field is considered\nboth in div- and curl-conforming spaces, leading to a strongly or weakly\npreserved zero-divergence condition, respectively. The equations are\ndiscretized in space such that transfers between the kinetic, internal, and\nmagnetic energies are consistent, leading to a preserved total energy. We also\ndiscuss further adjustments to the discretization required to additionally\nachieve magnetic helicity preservation. Finally, we describe new transport\nstabilization methods for the magnetic field equation which maintain the\nzero-divergence and energy conservation properties, including one method which\nalso preserves magnetic helicity. The methods' preservation and improved\nstability properties are confirmed numerically using a steady state and a\nmagnetic dynamo test case.",
    "descriptor": "",
    "authors": [
      "Golo A. Wimmer",
      "Xianzhu Tang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02348"
  },
  {
    "id": "arXiv:2210.02350",
    "title": "Crowdsourcing and Sidewalk Data: A Preliminary Study on the  Trustworthiness of OpenStreetMap Data in the US",
    "abstract": "Sidewalks play a pivotal role in urban mobility of everyday life. Ideally,\nsidewalks provide a safe walkway for pedestrians, link public transportation\nfacilities, and equip people with routing and navigation services. However,\nthere is a scarcity of open sidewalk data, which not only impacts the\naccessibility and walkability of cities but also limits policymakers in\ngenerating insightful measures to improve the current state of pedestrian\nfacilities. As one of the most famous crowdsourced data repositories,\nOpenStreetMap (OSM) could aid the lack of open sidewalk data to a large extent.\nHowever, completeness and quality of OSM data have long been a major issue. In\nthis paper, we offer a preliminary study on the availability and\ntrustworthiness of OSM sidewalk data. First, we compare OSM sidewalk data\ncoverage in over 50 major cities in the United States. Then, we select three\nmajor cities (Seattle, Chicago, and New York City) to further analyze the\ncompleteness of sidewalk data and its features, and to compute a\ntrustworthiness index leveraging historical OSM sidewalk data.",
    "descriptor": "\nComments: ASSETS 2022 UrbanAccess Workshop\n",
    "authors": [
      "Kazi Shahrukh Omar",
      "Gustavo Moreira",
      "Daniel Hodczak",
      "Maryam Hosseini",
      "Fabio Miranda"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.02350"
  },
  {
    "id": "arXiv:2210.02351",
    "title": "Schema Encoding for Transferable Dialogue State Tracking",
    "abstract": "Dialogue state tracking (DST) is an essential sub-task for task-oriented\ndialogue systems. Recent work has focused on deep neural models for DST.\nHowever, the neural models require a large dataset for training. Furthermore,\napplying them to another domain needs a new dataset because the neural models\nare generally trained to imitate the given dataset. In this paper, we propose\nSchema Encoding for Transferable Dialogue State Tracking (SETDST), which is a\nneural DST method for effective transfer to new domains. Transferable DST could\nassist developments of dialogue systems even with few dataset on target\ndomains. We use a schema encoder not just to imitate the dataset but to\ncomprehend the schema of the dataset. We aim to transfer the model to new\ndomains by encoding new schemas and using them for DST on multi-domain\nsettings. As a result, SET-DST improved the joint accuracy by 1.46 points on\nMultiWOZ 2.1.",
    "descriptor": "\nComments: Accepted to COLING 2022\n",
    "authors": [
      "Hyunmin Jeon",
      "Gary Geunbae Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02351"
  },
  {
    "id": "arXiv:2210.02352",
    "title": "Fast Untethered Soft Robotic Crawler with Elastic Instability",
    "abstract": "High-speed locomotion of animals gives them tremendous advantages in\nexploring, hunting, and escaping from predators in varying environments.\nEnlightened by the fast-running gait of mammals like cheetahs and wolves, we\ndesigned and fabricated a single-servo-driving untethered soft robot that is\ncapable of galloping at a speed of 313 mm/s or 1.56 body length per second\n(BL/s), 5.2 times and 2.6 times faster than the reported fastest predecessors\nin mm/s and BL/s, respectively, in literature. An in-plane prestressed hair\nclip mechanism (HCM) made up of semi-rigid materials like plastic is used as\nthe supporting chassis, the compliant spine, and the muscle force amplifier of\nthe robot at the same time, enabling the robot to be rapid and strong. The\ninfluence of factors including actuation frequency, substrates,\ntethering/untethering, and symmetric/asymmetric actuation is explored with\nexperiments. Based on previous work, this paper further demonstrated the\npotential of HCM in addressing the speed problem of soft robots.",
    "descriptor": "",
    "authors": [
      "Zechen Xiong",
      "Yufeng Su",
      "Hod Lipson"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02352"
  },
  {
    "id": "arXiv:2210.02356",
    "title": "A Liquid Democracy System for Human-Computer Societies",
    "abstract": "Problem of reliable democratic governance is critical for survival of any\ncommunity, and it will be critical for communities powered with Artificial\nIntelligence (AI) systems upon developments of the latter. Apparently, it will\nbe getting more and more critical because of increasing speeds and scales of\nelectronic communications and decreasing latencies in system responses. In\norder to address this need, we present design and implementation of a\nreputation system supporting \"liquid democracy\" principle. The system is based\non \"weighted liquid rank\" algorithm employing different sorts of explicit and\nimplicit ratings being exchanged by members of the society as well as implicit\nassessments of of the members based on measures of their activity in the\nsociety. The system is evaluated against live social network data with help of\nsimulation modelling for an online marketplace case.",
    "descriptor": "\nComments: 5 pages, 3 figures, presented at AI for Social Good at IJCAI-19 conference\n",
    "authors": [
      "Anton Kolonin",
      "Ben Goertzel",
      "Cassio Pennachin",
      "Deborah Duong",
      "Marco Argentieri",
      "Matt Ikl\u00e9",
      "Nejc Znidar"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.02356"
  },
  {
    "id": "arXiv:2210.02357",
    "title": "Image Masking for Robust Self-Supervised Monocular Depth Estimation",
    "abstract": "Self-supervised monocular depth estimation is a salient task for 3D scene\nunderstanding. Learned jointly with monocular ego-motion estimation, several\nmethods have been proposed to predict accurate pixel-wise depth without using\nlabeled data. Nevertheless, these methods focus on improving performance under\nideal conditions without natural or digital corruptions. A general absence of\nocclusions is assumed even for object-specific depth estimation. These methods\nare also vulnerable to adversarial attacks, which is a pertinent concern for\ntheir reliable deployment on robots and autonomous driving systems. We propose\nMIMDepth, a method that adapts masked image modeling (MIM) for self-supervised\nmonocular depth estimation. While MIM has been used to learn generalizable\nfeatures during pre-training, we show how it could be adapted for direct\ntraining of monocular depth estimation. Our experiments show that MIMDepth is\nmore robust to noise, blur, weather conditions, digital artifacts, occlusions,\nas well as untargeted and targeted adversarial attacks.",
    "descriptor": "",
    "authors": [
      "Hemang Chawla",
      "Kishaan Jeeveswaran",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02357"
  },
  {
    "id": "arXiv:2210.02360",
    "title": "Differentially Private Propensity Scores for Bias Correction",
    "abstract": "In surveys, it is typically up to the individuals to decide if they want to\nparticipate or not, which leads to participation bias: the individuals willing\nto share their data might not be representative of the entire population.\nSimilarly, there are cases where one does not have direct access to any data of\nthe target population and has to resort to publicly available proxy data\nsampled from a different distribution. In this paper, we present Differentially\nPrivate Propensity Scores for Bias Correction (DiPPS), a method for\napproximating the true data distribution of interest in both of the above\nsettings. We assume that the data analyst has access to a dataset $\\tilde{D}$\nthat was sampled from the distribution of interest in a biased way. As\nindividuals may be more willing to share their data when given a privacy\nguarantee, we further assume that the analyst is allowed locally differentially\nprivate access to a set of samples $D$ from the true, unbiased distribution.\nEach data point from the private, unbiased dataset $D$ is mapped to a\nprobability distribution over clusters (learned from the biased dataset\n$\\tilde{D}$), from which a single cluster is sampled via the exponential\nmechanism and shared with the data analyst. This way, the analyst gathers a\ndistribution over clusters, which they use to compute propensity scores for the\npoints in the biased $\\tilde{D}$, which are in turn used to reweight the points\nin $\\tilde{D}$ to approximate the true data distribution. It is now possible to\ncompute any function on the resulting reweighted dataset without further access\nto the private $D$. In experiments on datasets from various domains, we show\nthat DiPPS successfully brings the distribution of the available dataset closer\nto the distribution of interest in terms of Wasserstein distance. We further\nshow that this results in improved estimates for different statistics.",
    "descriptor": "\nComments: 11 pages, 2 figures\n",
    "authors": [
      "Liangwei Chen",
      "Valentin Hartmann",
      "Robert West"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.02360"
  },
  {
    "id": "arXiv:2210.02361",
    "title": "The Power of Duality: Response Time Analysis meets Integer Programming",
    "abstract": "We study a mutually enriching connection between response time analysis in\nreal-time systems and the mixing set problem. Thereby generalizing over known\nresults we present a new approach to the computation of response times in\nfixed-priority uniprocessor real-time scheduling. We even allow that the tasks\nare delayed by some period-constrained release jitter. By studying a dual\nproblem formulation of the decision problem as an integer linear program we\nshow that worst-case response times can be computed by algorithmically\nexploiting a conditional reduction to an instance of the mixing set problem. In\nthe important case of harmonic periods our new technique admits a\nnear-quadratic algorithm to the exact computation of worst-case response times.\nWe show that generally, a smaller utilization leads to more efficient\nalgorithms even in fixed-priority scheduling. Our technique can be reversed to\nsolve the mixing set problem by computing worst-case response times to\nassociated real-time scheduling task systems. Finally, we also apply our\noptimization technique to solve 4-block integer programs with simple objective\nfunctions.",
    "descriptor": "",
    "authors": [
      "Max A. Deppert",
      "Klaus Jansen"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.02361"
  },
  {
    "id": "arXiv:2210.02362",
    "title": "A Reputation System for Market Security and Equity",
    "abstract": "We simulate a reputation system in a market to optimise the balance between\nmarket security and market equity. We introduce a method of using a reputation\nsystem that will stabilise the distribution of wealth in a market in a fair\nmanner. We also introduce metrics of a modified Gini that takes production\nquality into account, a way to use a weighted Pearson as a tool to optimise\nbalance.",
    "descriptor": "\nComments: 4 pages, 1 figure, presented at AI for Social Good workshop at IJCAI-2019 conference\n",
    "authors": [
      "Anton Kolonin",
      "Deborah Duong",
      "Ben Goertzel",
      "Cassio Pennachin",
      "Matt Ikl\u00e9",
      "Nejc Znidar",
      "Marco Argentieri"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.02362"
  },
  {
    "id": "arXiv:2210.02365",
    "title": "SoccerNet 2022 Challenges Results",
    "abstract": "The SoccerNet 2022 challenges were the second annual video understanding\nchallenges organized by the SoccerNet team. In 2022, the challenges were\ncomposed of 6 vision-based tasks: (1) action spotting, focusing on retrieving\naction timestamps in long untrimmed videos, (2) replay grounding, focusing on\nretrieving the live moment of an action shown in a replay, (3) pitch\nlocalization, focusing on detecting line and goal part elements, (4) camera\ncalibration, dedicated to retrieving the intrinsic and extrinsic camera\nparameters, (5) player re-identification, focusing on retrieving the same\nplayers across multiple views, and (6) multiple object tracking, focusing on\ntracking players and the ball through unedited video streams. Compared to last\nyear's challenges, tasks (1-2) had their evaluation metrics redefined to\nconsider tighter temporal accuracies, and tasks (3-6) were novel, including\ntheir underlying data and annotations. More information on the tasks,\nchallenges and leaderboards are available on https://www.soccer-net.org.\nBaselines and development kits are available on https://github.com/SoccerNet.",
    "descriptor": "\nComments: Accepted at ACM MMSports 2022\n",
    "authors": [
      "Silvio Giancola",
      "Anthony Cioppa",
      "Adrien Deli\u00e8ge",
      "Floriane Magera",
      "Vladimir Somers",
      "Le Kang",
      "Xin Zhou",
      "Olivier Barnich",
      "Christophe De Vleeschouwer",
      "Alexandre Alahi",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck",
      "Abdulrahman Darwish",
      "Adrien Maglo",
      "Albert Clap\u00e9s",
      "Andreas Luyts",
      "Andrei Boiarov",
      "Artur Xarles",
      "Astrid Orcesi",
      "Avijit Shah",
      "Baoyu Fan",
      "Bharath Comandur",
      "Chen Chen",
      "Chen Zhang",
      "Chen Zhao",
      "Chengzhi Lin",
      "Cheuk-Yiu Chan",
      "Chun Chuen Hui",
      "Dengjie Li",
      "Fan Yang",
      "Fan Liang",
      "Fang Da",
      "Feng Yan",
      "Fufu Yu",
      "Guanshuo Wang",
      "H. Anthony Chan",
      "He Zhu",
      "Hongwei Kan",
      "Jiaming Chu",
      "Jianming Hu",
      "Jianyang Gu",
      "Jin Chen",
      "Jo\u00e3o V. B. Soares",
      "Jonas Theiner",
      "Jorge De Corte",
      "Jos\u00e9 Henrique Brito",
      "Jun Zhang",
      "Junjie Li",
      "Junwei Liang",
      "Leqi Shen",
      "Lin Ma",
      "Lingchi Chen",
      "Miguel Santos Marques",
      "Mike Azatov",
      "Nikita Kasatkin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02365"
  },
  {
    "id": "arXiv:2210.02368",
    "title": "Spatio-Temporal Learnable Proposals for End-to-End Video Object  Detection",
    "abstract": "This paper presents the novel idea of generating object proposals by\nleveraging temporal information for video object detection. The feature\naggregation in modern region-based video object detectors heavily relies on\nlearned proposals generated from a single-frame RPN. This imminently introduces\nadditional components like NMS and produces unreliable proposals on low-quality\nframes. To tackle these restrictions, we present SparseVOD, a novel video\nobject detection pipeline that employs Sparse R-CNN to exploit temporal\ninformation. In particular, we introduce two modules in the dynamic head of\nSparse R-CNN. First, the Temporal Feature Extraction module based on the\nTemporal RoI Align operation is added to extract the RoI proposal features.\nSecond, motivated by sequence-level semantic aggregation, we incorporate the\nattention-guided Semantic Proposal Feature Aggregation module to enhance object\nfeature representation before detection. The proposed SparseVOD effectively\nalleviates the overhead of complicated post-processing methods and makes the\noverall pipeline end-to-end trainable. Extensive experiments show that our\nmethod significantly improves the single-frame Sparse RCNN by 8%-9% in mAP.\nFurthermore, besides achieving state-of-the-art 80.3% mAP on the ImageNet VID\ndataset with ResNet-50 backbone, our SparseVOD outperforms existing\nproposal-based methods by a significant margin on increasing IoU thresholds\n(IoU > 0.5).",
    "descriptor": "\nComments: BMVC 2022\n",
    "authors": [
      "Khurram Azeem Hashmi",
      "Didier Stricker",
      "Muhammamd Zeshan Afzal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02368"
  },
  {
    "id": "arXiv:2210.02373",
    "title": "Dynamical systems' based neural networks",
    "abstract": "Neural networks have gained much interest because of their effectiveness in\nmany applications. However, their mathematical properties are generally not\nwell understood. If there is some underlying geometric structure inherent to\nthe data or to the function to approximate, it is often desirable to take this\ninto account in the design of the neural network. In this work, we start with a\nnon-autonomous ODE and build neural networks using a suitable,\nstructure-preserving, numerical time-discretisation. The structure of the\nneural network is then inferred from the properties of the ODE vector field.\nBesides injecting more structure into the network architectures, this modelling\nprocedure allows a better theoretical understanding of their behaviour. We\npresent two universal approximation results and demonstrate how to impose some\nparticular properties on the neural networks. A particular focus is on\n1-Lipschitz architectures including layers that are not 1-Lipschitz. These\nnetworks are expressive and robust against adversarial attacks, as shown for\nthe CIFAR-10 dataset.",
    "descriptor": "\nComments: 25 pages with 2 appendices\n",
    "authors": [
      "Elena Celledoni",
      "Davide Murari",
      "Brynjulf Owren",
      "Carola-Bibiane Sch\u00f6nlieb",
      "Ferdia Sherry"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02373"
  },
  {
    "id": "arXiv:2210.02374",
    "title": "Axon: A Language for Dynamic Shapes in Deep Learning Graphs",
    "abstract": "Axon is a language that enables shape and rank inference for tensors in a\nDeep Learning graphs. It aims to make shapes implicit and inferred, in a\nsimilar manner to how types are implicit and inferred in many functional\nprogramming languages. Tensor dimensions are represented by expressions\nconsisting of symbolic variables, constants, and arithmetic operators. Tensor\nshapes can be expressed as either a sequence of these dimension expressions, as\na symbolic variable, or as an appending of other shapes. This allows complex\nconstraints on shapes to be expressed. Axon is functional in style, with a type\nsystem similar in to Standard ML, extended to include shape information. It\nprovides a suite of built in operators over tensors, including pointwise\narithmetic operators, maps, reduction, loops and user defined functions. We\ndescribe a shape inference algorithm based on constraint solving which infers\ninformation about shapes, from both shape information provided by the\nprogrammer and the structure of the program. This allows fully automatic\ninference of the shapes of tensors for complex Deep Learning graphs. This\napproach reduces programmer effort when specifying graphs, as tensor shapes are\nnot explicit, allows composition of Deep Learning graphs while maintaining\ninput and output tensor shape compatibility, and aids in automated error\ndetection by identifying shape mismatches at runtime.",
    "descriptor": "",
    "authors": [
      "Alexander Collins",
      "Vinod Grover"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.02374"
  },
  {
    "id": "arXiv:2210.02375",
    "title": "Feedback reconstruction techniques for optimal control problems on a  tree structure",
    "abstract": "The computation of feedback control using Dynamic Programming equation is a\ndifficult task due the curse of dimensionality. The tree structure algorithm is\none the methods introduced recently that mitigate this problem. The method\ncomputes the value function avoiding the construction of a space grid and the\nneed for interpolation techniques using a discrete set of controls. However,\nthe computation of the control is strictly linked to control set chosen in the\ncomputation of the tree. Here, we extend and complete the method selecting a\nfiner control set in the computation of the feedback. This requires to use an\ninterpolation method for scattered data which allows us to reconstruct the\nvalue function for nodes not belonging to the tree. The effectiveness of the\nmethod is shown via a numerical example.",
    "descriptor": "",
    "authors": [
      "Alessandro Alla",
      "Luca Saluzzi"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02375"
  },
  {
    "id": "arXiv:2210.02377",
    "title": "Goal Recognition as a Deep Learning Task: the GRNet Approach",
    "abstract": "In automated planning, recognising the goal of an agent from a trace of\nobservations is an important task with many applications. The state-of-the-art\napproaches to goal recognition rely on the application of planning techniques,\nwhich requires a model of the domain actions and of the initial domain state\n(written, e.g., in PDDL). We study an alternative approach where goal\nrecognition is formulated as a classification task addressed by machine\nlearning. Our approach, called GRNet, is primarily aimed at making goal\nrecognition more accurate as well as faster by learning how to solve it in a\ngiven domain. Given a planning domain specified by a set of propositions and a\nset of action names, the goal classification instances in the domain are solved\nby a Recurrent Neural Network (RNN). A run of the RNN processes a trace of\nobserved actions to compute how likely it is that each domain proposition is\npart of the agent's goal, for the problem instance under considerations. These\npredictions are then aggregated to choose one of the candidate goals. The only\ninformation required as input of the trained RNN is a trace of action labels,\neach one indicating just the name of an observed action. An experimental\nanalysis confirms that \\our achieves good performance in terms of both goal\nclassification accuracy and runtime, obtaining better performance w.r.t. a\nstate-of-the-art goal recognition system over the considered benchmarks.",
    "descriptor": "",
    "authors": [
      "Mattia Chiari",
      "Alfonso E. Gerevini",
      "Luca Putelli",
      "Francesco Percassi",
      "Ivan Serina"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02377"
  },
  {
    "id": "arXiv:2210.02381",
    "title": "A Novel Entropy-Maximizing TD3-based Reinforcement Learning for  Automatic PID Tuning",
    "abstract": "Proportional-integral-derivative (PID) controllers have been widely used in\nthe process industry. However, the satisfactory control performance of a PID\ncontroller depends strongly on the tuning parameters. Conventional PID tuning\nmethods require extensive knowledge of the system model, which is not always\nknown especially in the case of complex dynamical systems. In contrast,\nreinforcement learning-based PID tuning has gained popularity since it can\ntreat PID tuning as a black-box problem and deliver the optimal PID parameters\nwithout requiring explicit process models. In this paper, we present a novel\nentropy-maximizing twin-delayed deep deterministic policy gradient (EMTD3)\nmethod for automating the PID tuning. In the proposed method, an\nentropy-maximizing stochastic actor is employed at the beginning to encourage\nthe exploration of the action space. Then a deterministic actor is deployed to\nfocus on local exploitation and discover the optimal solution. The\nincorporation of the entropy-maximizing term can significantly improve the\nsample efficiency and assist in fast convergence to the global solution. Our\nproposed method is applied to the PID tuning of a second-order system to verify\nits effectiveness in improving the sample efficiency and discovering the\noptimal PID parameters compared to traditional TD3.",
    "descriptor": "\nComments: 6 pages, 7 figures\n",
    "authors": [
      "Myisha A. Chowdhury",
      "Qiugang Lu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.02381"
  },
  {
    "id": "arXiv:2210.02382",
    "title": "NeuralMeshing: Differentiable Meshing of Implicit Neural Representations",
    "abstract": "The generation of triangle meshes from point clouds, i.e. meshing, is a core\ntask in computer graphics and computer vision. Traditional techniques directly\nconstruct a surface mesh using local decision heuristics, while some recent\nmethods based on neural implicit representations try to leverage data-driven\napproaches for this meshing process. However, it is challenging to define a\nlearnable representation for triangle meshes of unknown topology and size and\nfor this reason, neural implicit representations rely on non-differentiable\npost-processing in order to extract the final triangle mesh. In this work, we\npropose a novel differentiable meshing algorithm for extracting surface meshes\nfrom neural implicit representations. Our method produces the mesh in an\niterative fashion, which makes it applicable to shapes of various scales and\nadaptive to the local curvature of the shape. Furthermore, our method produces\nmeshes with regular tessellation patterns and fewer triangle faces compared to\nexisting methods. Experiments demonstrate the comparable reconstruction\nperformance and favorable mesh properties over baselines.",
    "descriptor": "\nComments: This preprint has not undergone any post-submission improvements or corrections. The Version of Record of this contribution is published in \"44th DAGM German Conference on Pattern Recognition (GCPR 2022), Konstanz, Germany, September 27-30, 2022, Proceedings\", and is available at this https URL\n",
    "authors": [
      "Mathias Vetsch",
      "Sandro Lombardi",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02382"
  },
  {
    "id": "arXiv:2210.02390",
    "title": "Variational prompt tuning improves generalization of vision-language  models",
    "abstract": "Prompt tuning provides an efficient mechanism to adapt large vision-language\nmodels to downstream tasks by treating part of the input language prompts as\nlearnable parameters while freezing the rest of the model. Existing works for\nprompt tuning are however prone to damaging the generalization capabilities of\nthe foundation models, because the learned prompts lack the capacity of\ncovering certain concepts within the language model. To avoid such limitation,\nwe propose a probabilistic modeling of the underlying distribution of prompts,\nallowing prompts within the support of an associated concept to be derived\nthrough stochastic sampling. This results in a more complete and richer\ntransfer of the information captured by the language model, providing better\ngeneralization capabilities for downstream tasks. The resulting algorithm\nrelies on a simple yet powerful variational framework that can be directly\nintegrated with other developments. We show our approach is seamlessly\nintegrated into both standard and conditional prompt learning frameworks,\nimproving the performance on both cases considerably, especially with regards\nto preserving the generalization capability of the original model. Our method\nprovides the current state-of-the-art for prompt learning, surpassing CoCoOp by\n1.6% average Top-1 accuracy on the standard benchmark. Remarkably, it even\nsurpasses the original CLIP model in terms of generalization to new classes.\nImplementation code will be released.",
    "descriptor": "",
    "authors": [
      "Mohammad Mahdi Derakhshani",
      "Enrique Sanchez",
      "Adrian Bulat",
      "Victor Guilherme Turrisi da Costa",
      "Cees G. M. Snoek",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02390"
  },
  {
    "id": "arXiv:2210.02391",
    "title": "Geometry Driven Progressive Warping for One-Shot Face Animation",
    "abstract": "Face animation aims at creating photo-realistic portrait videos with animated\nposes and expressions. A common practice is to generate displacement fields\nthat are used to warp pixels and features from source to target. However, prior\nattempts often produce sub-optimal displacements. In this work, we present a\ngeometry driven model and propose two geometric patterns as guidance: 3D face\nrendered displacement maps and posed neural codes. The model can optionally use\none of the patterns as guidance for displacement estimation. To model\ndisplacements at locations not covered by the face model (e.g., hair), we\nresort to source image features for contextual information and propose a\nprogressive warping module that alternates between feature warping and\ndisplacement estimation at increasing resolutions. We show that the proposed\nmodel can synthesize portrait videos with high fidelity and achieve the new\nstate-of-the-art results on the VoxCeleb1 and VoxCeleb2 datasets for both cross\nidentity and same identity reconstruction.",
    "descriptor": "",
    "authors": [
      "Yatao Zhong",
      "Faezeh Amjadi",
      "Ilya Zharkov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.02391"
  },
  {
    "id": "arXiv:2210.02394",
    "title": "Social Balance on Networks: Local Minima and Best Edge Dynamics",
    "abstract": "Structural balance theory is an established framework for studying social\nrelationships of friendship and enmity. These relationships are modeled by a\nsigned network whose energy potential measures the level of imbalance, while\nstochastic dynamics drives the network towards a state of minimum energy that\ncaptures social balance. It is known that this energy landscape has local\nminima that can trap socially-aware dynamics, preventing it from reaching\nbalance. Here we first study the robustness and attractor properties of these\nlocal minima. We show that a stochastic process can reach them from an\nabundance of initial states, and that some local minima cannot be escaped by\nmild perturbations of the network. Motivated by these anomalies, we introduce\nBest Edge Dynamics (BED), a new plausible stochastic process. We prove that BED\nalways reaches balance, and that it does so fast in various interesting\nsettings.",
    "descriptor": "\nComments: 13 pages, 14 figures\n",
    "authors": [
      "Krishnendu Chatterjee",
      "Jakub Svoboda",
      "\u00d0or\u0111e \u017dikeli\u0107",
      "Andreas Pavlogiannis",
      "Josef Tkadlec"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.02394"
  },
  {
    "id": "arXiv:2210.02396",
    "title": "Temporally Consistent Video Transformer for Long-Term Video Prediction",
    "abstract": "Generating long, temporally consistent video remains an open challenge in\nvideo generation. Primarily due to computational limitations, most prior\nmethods limit themselves to training on a small subset of frames that are then\nextended to generate longer videos through a sliding window fashion. Although\nthese techniques may produce sharp videos, they have difficulty retaining\nlong-term temporal consistency due to their limited context length. In this\nwork, we present Temporally Consistent Video Transformer (TECO), a\nvector-quantized latent dynamics video prediction model that learns compressed\nrepresentations to efficiently condition on long videos of hundreds of frames\nduring both training and generation. We use a MaskGit prior for dynamics\nprediction which enables both sharper and faster generations compared to prior\nwork. Our experiments show that TECO outperforms SOTA baselines in a variety of\nvideo prediction benchmarks ranging from simple mazes in DMLab, large 3D worlds\nin Minecraft, and complex real-world videos from Kinetics-600. In addition, to\nbetter understand the capabilities of video prediction models in modeling\ntemporal consistency, we introduce several challenging video prediction tasks\nconsisting of agents randomly traversing 3D scenes of varying difficulty. This\npresents a challenging benchmark for video prediction in partially observable\nenvironments where a model must understand what parts of the scenes to\nre-create versus invent depending on its past observations or generations.\nGenerated videos are available at https://wilson1yan.github.io/teco",
    "descriptor": "\nComments: Project website: this https URL\n",
    "authors": [
      "Wilson Yan",
      "Danijar Hafner",
      "Stephen James",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02396"
  },
  {
    "id": "arXiv:2210.02397",
    "title": "EU cost action on future generation optical wireless communication  technologies -- newfocus ca19111, a white paper",
    "abstract": "The EU COST Action NEWFOCUS is focused on investigating radical solutions\nwith the potential to impact the design of future wireless networks. It aims to\naddress some of the challenges in OWC and establish it as an efficient\ntechnology that can satisfy the demanding requirements of backhaul and access\nnetwork levels in 5G networks. This also includes the use of hybrid links that\nassociate OWC with radiofrequency or wired/fiber-based technologies. The focus\nof this White Paper is on the use of optical wireless communication (OWC) as\nenabling technology in a range of areas outlined in HE's Pillar II including\nHealth, Manufacturing, Intelligent Transportation Systems (ITS), Unmanned\nAerial Vehicles and Network and Protocol.",
    "descriptor": "",
    "authors": [
      "M A Khalighi",
      "Z Ghassemlooy",
      "S Zvanovec",
      "N Stevens",
      "L N Alves",
      "A Shrestha",
      "M Uysal",
      "A M Vegni",
      "P D Diamantoulakis",
      "V K Papanikolaou",
      "G K Karagiannidis",
      "B Ortega",
      "V Almenar",
      "O Bouchet",
      "L Ladid"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.02397"
  },
  {
    "id": "arXiv:2210.02399",
    "title": "Phenaki: Variable Length Video Generation From Open Domain Textual  Description",
    "abstract": "We present Phenaki, a model capable of realistic video synthesis, given a\nsequence of textual prompts. Generating videos from text is particularly\nchallenging due to the computational cost, limited quantities of high quality\ntext-video data and variable length of videos. To address these issues, we\nintroduce a new model for learning video representation which compresses the\nvideo to a small representation of discrete tokens. This tokenizer uses causal\nattention in time, which allows it to work with variable-length videos. To\ngenerate video tokens from text we are using a bidirectional masked transformer\nconditioned on pre-computed text tokens. The generated video tokens are\nsubsequently de-tokenized to create the actual video. To address data issues,\nwe demonstrate how joint training on a large corpus of image-text pairs as well\nas a smaller number of video-text examples can result in generalization beyond\nwhat is available in the video datasets. Compared to the previous video\ngeneration methods, Phenaki can generate arbitrary long videos conditioned on a\nsequence of prompts (i.e. time variable text or a story) in open domain. To the\nbest of our knowledge, this is the first time a paper studies generating videos\nfrom time variable prompts. In addition, compared to the per-frame baselines,\nthe proposed video encoder-decoder computes fewer tokens per video but results\nin better spatio-temporal consistency.",
    "descriptor": "",
    "authors": [
      "Ruben Villegas",
      "Mohammad Babaeizadeh",
      "Pieter-Jan Kindermans",
      "Hernan Moraldo",
      "Han Zhang",
      "Mohammad Taghi Saffar",
      "Santiago Castro",
      "Julius Kunze",
      "Dumitru Erhan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02399"
  },
  {
    "id": "arXiv:2210.02400",
    "title": "Emotion Twenty Questions Dialog System for Lexical Emotional  Intelligence",
    "abstract": "This paper presents a web-based demonstration of Emotion Twenty Questions\n(EMO20Q), a dialog game whose purpose is to study how people describe emotions.\nEMO20Q can also be used to develop artificially intelligent dialog agents that\ncan play the game. In previous work, an EMO20Q agent used a sequential Bayesian\nmachine learning model and could play the question-asking role. Newer\ntransformer-based neural machine learning models have made it possible to\ndevelop an agent for the question-answering role.\nThis demo paper describes the recent developments in the question-answering\nrole of the EMO20Q game, which requires the agent to respond to more open-ended\ninputs. Furthermore, we also describe the design of the system, including the\nweb-based front-end, agent architecture and programming, and updates to earlier\nsoftware used.\nThe demo system will be available to collect pilot data during the ACII\nconference and this data will be used to inform future experiments and system\ndesign.",
    "descriptor": "\nComments: 2022 10th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)\n",
    "authors": [
      "Abe Kazemzadeh",
      "Adedamola Sanusi",
      "Huihui"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.02400"
  },
  {
    "id": "arXiv:2210.02401",
    "title": "Medical Image Retrieval via Nearest Neighbor Search on Pre-trained Image  Features",
    "abstract": "Nearest neighbor search (NNS) aims to locate the points in high-dimensional\nspace that is closest to the query point. The brute-force approach for finding\nthe nearest neighbor becomes computationally infeasible when the number of\npoints is large. The NNS has multiple applications in medicine, such as\nsearching large medical imaging databases, disease classification, diagnosis,\netc. With a focus on medical imaging, this paper proposes DenseLinkSearch an\neffective and efficient algorithm that searches and retrieves the relevant\nimages from heterogeneous sources of medical images. Towards this, given a\nmedical database, the proposed algorithm builds the index that consists of\npre-computed links of each point in the database. The search algorithm utilizes\nthe index to efficiently traverse the database in search of the nearest\nneighbor. We extensively tested the proposed NNS approach and compared the\nperformance with state-of-the-art NNS approaches on benchmark datasets and our\ncreated medical image datasets. The proposed approach outperformed the existing\napproach in terms of retrieving accurate neighbors and retrieval speed. We also\nexplore the role of medical image feature representation in content-based\nmedical image retrieval tasks. We propose a Transformer-based feature\nrepresentation technique that outperformed the existing pre-trained Transformer\napproach on CLEF 2011 medical image retrieval task. The source code of our\nexperiments are available at https://github.com/deepaknlp/DLS.",
    "descriptor": "",
    "authors": [
      "Deepak Gupta",
      "Russell Loane",
      "Soumya Gayen",
      "Dina Demner-Fushman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02401"
  },
  {
    "id": "arXiv:2210.02404",
    "title": "ciDATGAN: Conditional Inputs for Tabular GANs",
    "abstract": "Conditionality has become a core component for Generative Adversarial\nNetworks (GANs) for generating synthetic images. GANs are usually using latent\nconditionality to control the generation process. However, tabular data only\ncontains manifest variables. Thus, latent conditionality either restricts the\ngenerated data or does not produce sufficiently good results. Therefore, we\npropose a new methodology to include conditionality in tabular GANs inspired by\nimage completion methods. This article presents ciDATGAN, an evolution of the\nDirected Acyclic Tabular GAN (DATGAN) that has already been shown to outperform\nstate-of-the-art tabular GAN models. First, we show that the addition of\nconditional inputs does hinder the model's performance compared to its\npredecessor. Then, we demonstrate that ciDATGAN can be used to unbias datasets\nwith the help of well-chosen conditional inputs. Finally, it shows that\nciDATGAN can learn the logic behind the data and, thus, be used to complete\nlarge synthetic datasets using data from a smaller feeder dataset.",
    "descriptor": "\nComments: Technical report, 21 pages\n",
    "authors": [
      "Gael Lederrey",
      "Tim Hillel",
      "Michel Bierlaire"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02404"
  },
  {
    "id": "arXiv:2210.02406",
    "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
    "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks.",
    "descriptor": "\nComments: Datasets, Code and Prompts available at this https URL\n",
    "authors": [
      "Tushar Khot",
      "Harsh Trivedi",
      "Matthew Finlayson",
      "Yao Fu",
      "Kyle Richardson",
      "Peter Clark",
      "Ashish Sabharwal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02406"
  },
  {
    "id": "arXiv:2210.02407",
    "title": "The Influence of Explainable Artificial Intelligence: Nudging Behaviour  or Boosting Capability?",
    "abstract": "This article aims to provide a theoretical account and corresponding paradigm\nfor analysing how explainable artificial intelligence (XAI) influences people's\nbehaviour and cognition. It uses insights from research on behaviour change.\nTwo notable frameworks for thinking about behaviour change techniques are\nnudges - aimed at influencing behaviour - and boosts - aimed at fostering\ncapability. It proposes that local and concept-based explanations are more\nadjacent to nudges, while global and counterfactual explanations are more\nadjacent to boosts. It outlines a method for measuring XAI influence and argues\nfor the benefits of understanding it for optimal, safe and ethical human-AI\ncollaboration.",
    "descriptor": "\nComments: Accepted at the ICML-22 Workshop on Human-Machine Collaboration and Teaming held at the Thirty-ninth International Conference on Machine Learning (ICML-22), 7 pages\n",
    "authors": [
      "Matija Franklin"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02407"
  },
  {
    "id": "arXiv:2210.02410",
    "title": "The Vendi Score: A Diversity Evaluation Metric for Machine Learning",
    "abstract": "Diversity is an important criterion for many areas of machine learning (ML),\nincluding generative modeling and dataset curation. Yet little work has gone\ninto understanding, formalizing, and measuring diversity in ML. In this paper,\nwe address the diversity evaluation problem by proposing the Vendi Score, which\nconnects and extends ideas from ecology and quantum statistical mechanics to\nML. The Vendi Score is defined as the exponential of the Shannon entropy of the\neigenvalues of a similarity matrix. This matrix is induced by a user-defined\nsimilarity function applied to the sample to be evaluated for diversity. In\ntaking a similarity function as input, the Vendi Score enables its user to\nspecify any desired form of diversity. Importantly, unlike many existing\nmetrics in ML, the Vendi Score doesn't require a reference dataset or\ndistribution over samples or labels, it is therefore general and applicable to\nany generative model, decoding algorithm, and dataset from any domain where\nsimilarity can be defined. We showcased the Vendi Score on molecular generative\nmodeling, a domain where diversity plays an important role in enabling the\ndiscovery of novel molecules. We found that the Vendi Score addresses\nshortcomings of the current diversity metric of choice in that domain. We also\napplied the Vendi Score to generative models of images and decoding algorithms\nof text and found it confirms known results about diversity in those domains.\nFurthermore, we used the Vendi Score to measure mode collapse, a known\nlimitation of generative adversarial networks (GANs). In particular, the Vendi\nScore revealed that even GANs that capture all the modes of a labeled dataset\ncan be less diverse than the original dataset. Finally, the interpretability of\nthe Vendi Score allowed us to diagnose several benchmark ML datasets for\ndiversity, opening the door for diversity-informed data augmentation.",
    "descriptor": "\nComments: The Vendi Score is available as a pip package (this https URL) and as part of HuggingFace Evaluate (this https URL)\n",
    "authors": [
      "Dan Friedman",
      "Adji Bousso Dieng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.02410"
  },
  {
    "id": "arXiv:2210.02411",
    "title": "Dynamical Isometry for Residual Networks",
    "abstract": "The training success, training speed and generalization ability of neural\nnetworks rely crucially on the choice of random parameter initialization. It\nhas been shown for multiple architectures that initial dynamical isometry is\nparticularly advantageous. Known initialization schemes for residual blocks,\nhowever, miss this property and suffer from degrading separability of different\ninputs for increasing depth and instability without Batch Normalization or lack\nfeature diversity. We propose a random initialization scheme, RISOTTO, that\nachieves perfect dynamical isometry for residual networks with ReLU activation\nfunctions even for finite depth and width. It balances the contributions of the\nresidual and skip branches unlike other schemes, which initially bias towards\nthe skip connections. In experiments, we demonstrate that in most cases our\napproach outperforms initialization schemes proposed to make Batch\nNormalization obsolete, including Fixup and SkipInit, and facilitates stable\ntraining. Also in combination with Batch Normalization, we find that RISOTTO\noften achieves the overall best result.",
    "descriptor": "\nComments: 22 pages, 5 figures\n",
    "authors": [
      "Advait Gadhikar",
      "Rebekka Burkholz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02411"
  },
  {
    "id": "arXiv:2210.02412",
    "title": "How Erd\u00f6s and R\u00e9nyi Win the Lottery",
    "abstract": "Random masks define surprisingly effective sparse neural network models, as\nhas been shown empirically. The resulting Erd\\\"os-R\\'enyi (ER) random graphs\ncan often compete with dense architectures and state-of-the-art lottery ticket\npruning algorithms struggle to outperform them, even though the random\nbaselines do not rely on computationally expensive pruning-training iterations\nbut can be drawn initially without significant computational overhead. We offer\na theoretical explanation of how such ER masks can approximate arbitrary target\nnetworks if they are wider by a logarithmic factor in the inverse sparsity $1 /\n\\log(1/\\text{sparsity})$. While we are the first to show theoretically and\nexperimentally that random ER source networks contain strong lottery tickets,\nwe also prove the existence of weak lottery tickets that require a lower degree\nof overparametrization than strong lottery tickets. These unusual results are\nbased on the observation that ER masks are well trainable in practice, which we\nverify in experiments with varied choices of random masks. Some of these\ndata-free choices outperform previously proposed random approaches on standard\nimage classification benchmark datasets.",
    "descriptor": "\nComments: 27 pages, 6 figures\n",
    "authors": [
      "Advait Gadhikar",
      "Sohum Mukherjee",
      "Rebekka Burkholz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02412"
  },
  {
    "id": "arXiv:2210.02414",
    "title": "GLM-130B: An Open Bilingual Pre-trained Model",
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 and unveil how models of such a scale can be\nsuccessfully pre-trained. Over the course of this effort, we face numerous\nunexpected technical and engineering challenges, particularly on loss spikes\nand disconvergence. In this paper, we introduce the training process of\nGLM-130B including its design choices, training strategies for both efficiency\nand stability, and engineering efforts. The resultant GLM-130B model offers\nsignificant outperformance over GPT-3 175B on a wide range of popular English\nbenchmarks while the performance advantage is not observed in OPT-175B and\nBLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0\n260B -- the largest Chinese language model -- across related benchmarks.\nFinally, we leverage a unique scaling property of GLM-130B to reach INT4\nquantization, without quantization aware training and with almost no\nperformance loss, making it the first among 100B-scale models. More\nimportantly, the property allows its effective inference on 4$\\times$RTX 3090\n(24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs\nrequired for using 100B-scale models. The GLM-130B model weights are publicly\naccessible and its code, training logs, related toolkit, and lessons learned\nare open-sourced at https://github.com/THUDM/GLM-130B .",
    "descriptor": "\nComments: 47 pages\n",
    "authors": [
      "Aohan Zeng",
      "Xiao Liu",
      "Zhengxiao Du",
      "Zihan Wang",
      "Hanyu Lai",
      "Ming Ding",
      "Zhuoyi Yang",
      "Yifan Xu",
      "Wendi Zheng",
      "Xiao Xia",
      "Weng Lam Tam",
      "Zixuan Ma",
      "Yufei Xue",
      "Jidong Zhai",
      "Wenguang Chen",
      "Peng Zhang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02414"
  },
  {
    "id": "arXiv:2210.02415",
    "title": "A Fourier Approach to Mixture Learning",
    "abstract": "We revisit the problem of learning mixtures of spherical Gaussians. Given\nsamples from mixture $\\frac{1}{k}\\sum_{j=1}^{k}\\mathcal{N}(\\mu_j, I_d)$, the\ngoal is to estimate the means $\\mu_1, \\mu_2, \\ldots, \\mu_k \\in \\mathbb{R}^d$ up\nto a small error. The hardness of this learning problem can be measured by the\nseparation $\\Delta$ defined as the minimum distance between all pairs of means.\nRegev and Vijayaraghavan (2017) showed that with $\\Delta = \\Omega(\\sqrt{\\log\nk})$ separation, the means can be learned using $\\mathrm{poly}(k, d)$ samples,\nwhereas super-polynomially many samples are required if $\\Delta = o(\\sqrt{\\log\nk})$ and $d = \\Omega(\\log k)$. This leaves open the low-dimensional regime\nwhere $d = o(\\log k)$.\nIn this work, we give an algorithm that efficiently learns the means in $d =\nO(\\log k/\\log\\log k)$ dimensions under separation $d/\\sqrt{\\log k}$ (modulo\ndoubly logarithmic factors). This separation is strictly smaller than\n$\\sqrt{\\log k}$, and is also shown to be necessary. Along with the results of\nRegev and Vijayaraghavan (2017), our work almost pins down the critical\nseparation threshold at which efficient parameter learning becomes possible for\nspherical Gaussian mixtures. More generally, our algorithm runs in time\n$\\mathrm{poly}(k)\\cdot f(d, \\Delta, \\epsilon)$, and is thus fixed-parameter\ntractable in parameters $d$, $\\Delta$ and $\\epsilon$.\nOur approach is based on estimating the Fourier transform of the mixture at\ncarefully chosen frequencies, and both the algorithm and its analysis are\nsimple and elementary. Our positive results can be easily extended to learning\nmixtures of non-Gaussian distributions, under a mild condition on the Fourier\nspectrum of the distribution.",
    "descriptor": "\nComments: To appear at NeurIPS 2022\n",
    "authors": [
      "Mingda Qiao",
      "Guru Guruganesh",
      "Ankit Singh Rawat",
      "Kumar Avinava Dubey",
      "Manzil Zaheer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.02415"
  },
  {
    "id": "arXiv:2210.02419",
    "title": "Explanation Uncertainty with Decision Boundary Awareness",
    "abstract": "Post-hoc explanation methods have become increasingly depended upon for\nunderstanding black-box classifiers in high-stakes applications, precipitating\na need for reliable explanations. While numerous explanation methods have been\nproposed, recent works have shown that many existing methods can be\ninconsistent or unstable. In addition, high-performing classifiers are often\nhighly nonlinear and can exhibit complex behavior around the decision boundary,\nleading to brittle or misleading local explanations. Therefore, there is an\nimpending need to quantify the uncertainty of such explanation methods in order\nto understand when explanations are trustworthy. We introduce a novel\nuncertainty quantification method parameterized by a Gaussian Process model,\nwhich combines the uncertainty approximation of existing methods with a novel\ngeodesic-based similarity which captures the complexity of the target black-box\ndecision boundary. The proposed framework is highly flexible; it can be used\nwith any black-box classifier and feature attribution method to amortize\nuncertainty estimates for explanations. We show theoretically that our proposed\ngeodesic-based kernel similarity increases with the complexity of the decision\nboundary. Empirical results on multiple tabular and image datasets show that\nour decision boundary-aware uncertainty estimate improves understanding of\nexplanations as compared to existing methods.",
    "descriptor": "",
    "authors": [
      "Davin Hill",
      "Aria Masoomi",
      "Sandesh Ghimire",
      "Max Torop",
      "Jennifer Dy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02419"
  },
  {
    "id": "arXiv:2210.02426",
    "title": "Pebble minimization: the last theorems",
    "abstract": "Pebble transducers are nested two-way transducers which can drop marks (named\n\"pebbles\") on their input word. Such machines can compute functions whose\noutput size is polynomial in the size of their input. They can be seen as\nsimple recursive programs whose recursion height is bounded. A natural problem\nis, given a pebble transducer, to compute an equivalent pebble transducer with\nminimal recursion height. This problem is open since the introduction of the\nmodel.\nIn this paper, we study two restrictions of pebble transducers, that cannot\nsee the marks (\"blind pebble transducers\" introduced by Nguy\\^en et al.), or\nthat can only see the last mark dropped (\"last pebble transducers\" introduced\nby Engelfriet et al.). For both models, we provide an effective algorithm for\nminimizing the recursion height. The key property used in both cases is that a\nfunction whose output size is linear (resp. quadratic, cubic, etc.) can always\nbe computed by a machine whose recursion height is 1 (resp. 2, 3, etc.). We\nfinally show that this key property fails as soon as we consider machines that\ncan see more than one mark.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Ga\u00ebtan Dou\u00e9neau-Tabot"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2210.02426"
  },
  {
    "id": "arXiv:2210.02428",
    "title": "Gradual C0: Symbolic Execution for Efficient Gradual Verification",
    "abstract": "Current static verification techniques have allowed users to specify and\nverify more code than ever before. However, such techniques only support\ncomplete and detailed specifications, which places an undue burden on users. To\nsolve this problem, prior work proposed gradual verification, which handles\ncomplete, partial, or missing specifications by soundly combining static and\ndynamic checking. Gradual verification has also been extended to programs that\nmanipulate recursive, mutable data structures on the heap. Unfortunately, this\nextension does not exhibit a pay-as-you-go cost model, which rewards users with\nincreased static correctness guarantees and decreased dynamic checking as\nspecifications are refined. In fact, all properties are checked dynamically\nregardless of any static guarantees, resulting in significant run-time\noverhead.\nIn this paper, we present the first gradual verifier for recursive heap data\nstructures that can be used on real programs, called Gradual C0. Additionally,\nGradual C0 reasons with symbolic execution and supports a pay-as-you-go cost\nmodel. Our approach addresses technical challenges related to symbolic\nexecution with imprecise specifications, heap ownership, and that branches\nacross program statements and specifications. Finally, we empirically evaluated\nthe pay-as-you-go cost model of Gradual C0 and found that, on average, Gradual\nC0 decreases run-time overhead between 50-75% compared to the fully-dynamic\napproach used in prior work. Further, the worst-case scenarios for performance\nare predictable and avoidable.",
    "descriptor": "\nComments: 26 pages without appendix supplement, preprint\n",
    "authors": [
      "Jenna DiVincenzo",
      "Ian McCormack",
      "Hemant Gouni",
      "Jacob Gorenburg",
      "Mona Zhang",
      "Conrad Zimmerman",
      "Joshua Sunshine",
      "\u00c9ric Tanter",
      "Jonathan Aldrich"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.02428"
  },
  {
    "id": "arXiv:2210.02432",
    "title": "Coercive second-kind boundary integral equations for the Laplace  Dirichlet problem on Lipschitz domains",
    "abstract": "We present new second-kind integral-equation formulations of the interior and\nexterior Dirichlet problems for Laplace's equation. The operators in these\nformulations are both continuous and coercive on general Lipschitz domains in\n$\\mathbb{R}^d$, $d\\geq 2$, in the space $L^2(\\Gamma)$, where $\\Gamma$ denotes\nthe boundary of the domain. These properties of continuity and coercivity\nimmediately imply that (i) the Galerkin method converges when applied to these\nformulations; and (ii) the Galerkin matrices are well-conditioned as the\ndiscretisation is refined, without the need for operator preconditioning. The\nmain significance of these results is that it was recently proved (see\nChandler-Wilde and Spence, Numer. Math., 150(2):299-271, 2022) that there exist\n2- and 3-d Lipschitz domains and 3-d starshaped Lipschitz polyhedra for which\nthe operators in the standard second-kind integral-equation formulations for\nLaplace's equation (involving the double-layer potential and its adjoint)\n$\\textit{cannot}$ be written as the sum of a coercive operator and a compact\noperator in the space $L^2(\\Gamma)$. Therefore there exist 2- and 3-d Lipschitz\ndomains and 3-d starshaped Lipschitz polyhedra for which Galerkin methods in\n$L^2(\\Gamma)$ do $\\textit{not}$ converge when applied to the standard\nsecond-kind formulations, but $\\textit{do}$ converge for the new formulations.",
    "descriptor": "",
    "authors": [
      "Simon N. Chandler-Wilde",
      "Euan A. Spence"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.02432"
  },
  {
    "id": "arXiv:2210.02434",
    "title": "A flow-based formulation for parallel machine scheduling using decision  diagrams",
    "abstract": "We present a new flow-based formulation for identical parallel machine\nscheduling with a regular objective function and without idle time. The\nformulation is constructed with the help of a decision diagram that represents\nall job sequences that respect specific ordering rules. These rules rely on a\npartition of the planning horizon into, generally non-uniform, periods and do\nnot exclude all optimal solutions, but they constrain solutions to adhere to a\ncanonical form. The new formulation has numerous variables and constraints, and\nhence we apply a Dantzig-Wolfe decomposition in order to compute the linear\nprogramming relaxation in reasonable time; the resulting lower bound is\nstronger than the bound from the classical time-indexed formulation. We develop\na branch-and-price framework that solves several instances from the literature\nfor the first time. We compare the new formulation with the time-indexed and\narc-time-indexed formulation by means of a series of computational experiments.",
    "descriptor": "",
    "authors": [
      "Daniel Kowalczyk",
      "Roel Leus",
      "Christopher Hojny",
      "Stefan R\u00f8pke"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.02434"
  },
  {
    "id": "arXiv:2210.02435",
    "title": "IRJIT -- An Information Retrieval Technique for Just-in-time Defect  Identification",
    "abstract": "Defect identification at commit check-in time prevents the introduction of\ndefects into software. Current defect identification approaches either rely on\nmanually crafted features such as change metrics or involve training expensive\nmachine learning or deep learning models. By relying on a complex underlying\nmodel, these approaches are not often explainable, which means the models'\npredictions cannot be understood by the developers. An approach that is not\nexplainable might not be adopted in real-life development environments because\nof developers' lack of trust in its results. Furthermore, because of an\nextensive training process, these approaches cannot readily learn from new\nexamples when they arrive, making them unsuitable for fast online prediction.\nTo address these limitations, we propose an approach called IRJIT that employs\ninformation retrieval on source code, and labels new commits as buggy or clean\nbased on their similarity to past buggy or clean commits. Our approach is\nonline and explainable as it can learn from new data without retraining, and\ndevelopers can see the documents that support a prediction. Through an\nevaluation of 8 open-source projects, we show that IRJIT achieves AUC and F1\nscore close to the state-of-the-art machine learning approach JITLine, without\nconsiderable re-training.",
    "descriptor": "",
    "authors": [
      "Hareem Sahar",
      "Yuxin Liu",
      "Abdul Ali Bangash",
      "Abram Hindle",
      "Denilson Barbosa"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.02435"
  },
  {
    "id": "arXiv:2210.02437",
    "title": "ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild",
    "abstract": "Benchmarking initiatives support the meaningful comparison of competing\nsolutions to prominent problems in speech and language processing. Successive\nbenchmarking evaluations typically reflect a progressive evolution from ideal\nlab conditions towards to those encountered in the wild. ASVspoof, the spoofing\nand deepfake detection initiative and challenge series, has followed the same\ntrend. This article provides a summary of the ASVspoof 2021 challenge and the\nresults of 37 participating teams. For the logical access task, results\nindicate that countermeasures solutions are robust to newly introduced encoding\nand transmission effects. Results for the physical access task indicate the\npotential to detect replay attacks in real, as opposed to simulated physical\nspaces, but a lack of robustness to variations between simulated and real\nacoustic environments. The DF task, new to the 2021 edition, targets solutions\nto the detection of manipulated, compressed speech data posted online. While\ndetection solutions offer some resilience to compression effects, they lack\ngeneralization across different source datasets. In addition to a summary of\nthe top-performing systems for each task, new analyses of influential data\nfactors and results for hidden data subsets, the article includes a review of\npost-challenge results, an outline of the principal challenge limitations and a\nroad-map for the future of ASVspoof. Link to the ASVspoof challenge and related\nresources: https://www.asvspoof.org/index2021.html",
    "descriptor": "\nComments: Submitted to IEEE/ACM Transactions on Audio, Speech and Language Processing\n",
    "authors": [
      "Xuechen Liu",
      "Xin Wang",
      "Md Sahidullah",
      "Jose Patino",
      "H\u00e9ctor Delgado",
      "Tomi Kinnunen",
      "Massimiliano Todisco",
      "Junichi Yamagishi",
      "Nicholas Evans",
      "Andreas Nautsch",
      "Kong Aik Lee"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.02437"
  },
  {
    "id": "arXiv:2210.02438",
    "title": "DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics",
    "abstract": "We introduce the first work to explore web-scale diffusion models for\nrobotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first\ninferring a text description of those objects, then generating an image\nrepresenting a natural, human-like arrangement of those objects, and finally\nphysically arranging the objects according to that image. The significance is\nthat we achieve this zero-shot using DALL-E, without needing any further data\ncollection or training. Encouraging real-world results with human studies show\nthat this is an exciting direction for the future of web-scale robot learning\nalgorithms. We also propose a list of recommendations to the text-to-image\ncommunity, to align further developments of these models with applications to\nrobotics. Videos are available at: https://www.robot-learning.uk/dall-e-bot",
    "descriptor": "\nComments: Webpage and videos: www.robot-learning.uk/dall-e-bot\n",
    "authors": [
      "Ivan Kapelyukh",
      "Vitalis Vosylius",
      "Edward Johns"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02438"
  },
  {
    "id": "arXiv:2210.02441",
    "title": "Ask Me Anything: A simple strategy for prompting language models",
    "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt that demonstrates how to perform the task and\nno additional training. Prompting is a brittle process wherein small\nmodifications to the prompt can cause large variations in the model\npredictions, and therefore significant effort is dedicated towards designing a\npainstakingly \"perfect prompt\" for a task. To mitigate the high degree of\neffort involved in prompt-design, we instead ask whether producing multiple\neffective, yet imperfect, prompts and aggregating them can lead to a high\nquality prompting strategy. Our observations motivate our proposed prompting\nmethod, ASK ME ANYTHING (AMA). We first develop an understanding of the\neffective prompt formats, finding that question-answering (QA) prompts, which\nencourage open-ended generation (\"Who went to the park?\") tend to outperform\nthose that restrict the model outputs (\"John went to the park. Output True or\nFalse.\"). Our approach recursively uses the LLM itself to transform task inputs\nto the effective QA format. We apply the collected prompts to obtain several\nnoisy votes for the input's true label. We find that the prompts can have very\ndifferent accuracies and complex dependencies and thus propose to use weak\nsupervision, a procedure for combining the noisy predictions, to produce the\nfinal predictions for the inputs. We evaluate AMA across open-source model\nfamilies (e.g., Neo, BLOOM, OPT, and T0) and model sizes (125M-175B\nparameters), demonstrating an average performance lift of 10.2% over the\nfew-shot baseline. This simple strategy enables the open-source GPT-Neo-6B\nmodel to match and exceed the performance of few-shot GPT3-175B on 15 of 20\npopular benchmarks. Averaged across these tasks, the GPT-Neo-6B model\noutperforms few-shot GPT3-175B. We release our code here:\nhttps://github.com/HazyResearch/ama_prompting",
    "descriptor": "",
    "authors": [
      "Simran Arora",
      "Avanika Narayan",
      "Mayee F. Chen",
      "Laurel J. Orr",
      "Neel Guha",
      "Kush Bhatia",
      "Ines Chami",
      "Frederic Sala",
      "Christopher R\u00e9"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02441"
  },
  {
    "id": "arXiv:2210.02442",
    "title": "Making Your First Choice: To Address Cold Start Problem in Vision Active  Learning",
    "abstract": "Active learning promises to improve annotation efficiency by iteratively\nselecting the most important data to be annotated first. However, we uncover a\nstriking contradiction to this promise: active learning fails to select data as\nefficiently as random selection at the first few choices. We identify this as\nthe cold start problem in vision active learning, caused by a biased and\noutlier initial query. This paper seeks to address the cold start problem by\nexploiting the three advantages of contrastive learning: (1) no annotation is\nrequired; (2) label diversity is ensured by pseudo-labels to mitigate bias; (3)\ntypical data is determined by contrastive features to reduce outliers.\nExperiments are conducted on CIFAR-10-LT and three medical imaging datasets\n(i.e. Colon Pathology, Abdominal CT, and Blood Cell Microscope). Our initial\nquery not only significantly outperforms existing active querying strategies\nbut also surpasses random selection by a large margin. We foresee our solution\nto the cold start problem as a simple yet strong baseline to choose the initial\nquery for vision active learning. Code is available:\nhttps://github.com/c-liangyu/CSVAL",
    "descriptor": "",
    "authors": [
      "Liangyu Chen",
      "Yutong Bai",
      "Siyu Huang",
      "Yongyi Lu",
      "Bihan Wen",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02442"
  },
  {
    "id": "arXiv:2210.02443",
    "title": "Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D  Object Detection",
    "abstract": "While recent camera-only 3D detection methods leverage multiple timesteps,\nthe limited history they use significantly hampers the extent to which temporal\nfusion can improve object perception. Observing that existing works' fusion of\nmulti-frame images are instances of temporal stereo matching, we find that\nperformance is hindered by the interplay between 1) the low granularity of\nmatching resolution and 2) the sub-optimal multi-view setup produced by limited\nhistory usage. Our theoretical and empirical analysis demonstrates that the\noptimal temporal difference between views varies significantly for different\npixels and depths, making it necessary to fuse many timesteps over long-term\nhistory. Building on our investigation, we propose to generate a cost volume\nfrom a long history of image observations, compensating for the coarse but\nefficient matching resolution with a more optimal multi-view matching setup.\nFurther, we augment the per-frame monocular depth predictions used for\nlong-term, coarse matching with short-term, fine-grained matching and find that\nlong and short term temporal fusion are highly complementary. While maintaining\nhigh efficiency, our framework sets new state-of-the-art on nuScenes, achieving\nfirst place on the test set and outperforming previous best art by 5.2% mAP and\n3.7% NDS on the validation set. Code will be released\n$\\href{https://github.com/Divadi/SOLOFusion}{here.}$",
    "descriptor": "\nComments: Code will be released at this https URL\n",
    "authors": [
      "Jinhyung Park",
      "Chenfeng Xu",
      "Shijia Yang",
      "Kurt Keutzer",
      "Kris Kitani",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02443"
  },
  {
    "id": "arXiv:2210.01341",
    "title": "Safe and Stable Control Synthesis for Uncertain System Models via  Distributionally Robust Optimization",
    "abstract": "This paper considers enforcing safety and stability of dynamical systems in\nthe presence of model uncertainty. Safety and stability constraints may be\nspecified using a control barrier function (CBF) and a control Lyapunov\nfunction (CLF), respectively. To take model uncertainty into account, robust\nand chance formulations of the constraints are commonly considered. However,\nthis requires known error bounds or a known distribution for the model\nuncertainty, and the resulting formulations may suffer from over-conservatism\nor over-confidence. In this paper, we assume that only a finite set of model\nparametric uncertainty samples is available and formulate a distributionally\nrobust chance-constrained program (DRCCP) for control synthesis with CBF safety\nand CLF stability guarantees. To enable the efficient computation of control\ninputs during online execution, we provide a reformulation of the DRCCP as a\nsecond-order cone program (SOCP). Our formulation is evaluated in an adaptive\ncruise control example in comparison to 1) a baseline CLF-CBF quadratic\nprogramming approach, 2) a robust approach that assumes known error bounds of\nthe system uncertainty, and 3) a chance-constrained approach that assumes a\nknown Gaussian Process distribution of the uncertainty.",
    "descriptor": "",
    "authors": [
      "Kehan Long",
      "Yinzhuang Yi",
      "Jorge Cortes",
      "Nikolay Atanasov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.01341"
  },
  {
    "id": "arXiv:2210.01806",
    "title": "Low-Light Image Restoration Based on Retina Model using Neural Networks",
    "abstract": "We report the possibility of using a simple neural network for effortless\nrestoration of low-light images inspired by the retina model, which mimics the\nneurophysiological principles and dynamics of various types of optical neurons.\nThe proposed neural network model saves the cost of computational overhead in\ncontrast with traditional signal-processing models, and generates results\ncomparable with complicated deep learning models from the subjective perceptual\nperspective. This work shows that to directly simulate the functionalities of\nretinal neurons using neural networks not only avoids the manually seeking for\nthe optimal parameters, but also paves the way to build corresponding\nartificial versions for certain neurobiological organizations.",
    "descriptor": "",
    "authors": [
      "Yurui Ming",
      "Yuanyuan Liang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01806"
  },
  {
    "id": "arXiv:2210.01863",
    "title": "Group Personalized Federated Learning",
    "abstract": "Federated learning (FL) can help promote data privacy by training a shared\nmodel in a de-centralized manner on the physical devices of clients. In the\npresence of highly heterogeneous distributions of local data, personalized FL\nstrategy seeks to mitigate the potential client drift. In this paper, we\npresent the group personalization approach for applications of FL in which\nthere exist inherent partitions among clients that are significantly distinct.\nIn our method, the global FL model is fine-tuned through another FL training\nprocess over each homogeneous group of clients, after which each group-specific\nFL model is further adapted and personalized for any client. The proposed\nmethod can be well interpreted from a Bayesian hierarchical modeling\nperspective. With experiments on two real-world datasets, we demonstrate this\napproach can achieve superior personalization performance than other FL\ncounterparts.",
    "descriptor": "",
    "authors": [
      "Zhe Liu",
      "Yue Hui",
      "Fuchun Peng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01863"
  },
  {
    "id": "arXiv:2210.01948",
    "title": "Game-theoretic statistics and safe anytime-valid inference",
    "abstract": "Safe anytime-valid inference (SAVI) provides measures of statistical evidence\nand certainty -- e-processes for testing and confidence sequences for\nestimation -- that remain valid at all stopping times, accommodating continuous\nmonitoring and analysis of accumulating data and optional stopping or\ncontinuation for any reason. These measures are based on test martingales,\nwhich are nonnegative martingales starting at one. Since a test martingale is\nthe wealth process of a player in a betting game, SAVI uses game-theoretic\nintuition, language and mathematics. This survey reports some recent advances\nin testing composite hypotheses and estimating functionals in nonparametric\nsettings, leading to new methods even for nonsequential problems.",
    "descriptor": "\nComments: 27 pages. Submitted for review in July\n",
    "authors": [
      "Aaditya Ramdas",
      "Peter Gr\u00fcnwald",
      "Vladimir Vovk",
      "Glenn Shafer"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Computer Science and Game Theory (cs.GT)",
      "Information Theory (cs.IT)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.01948"
  },
  {
    "id": "arXiv:2210.02015",
    "title": "Conformalized Fairness via Quantile Regression",
    "abstract": "Algorithmic fairness has received increased attention in socially sensitive\ndomains. While rich literature on mean fairness has been established, research\non quantile fairness remains sparse but vital. To fulfill great needs and\nadvocate the significance of quantile fairness, we propose a novel framework to\nlearn a real-valued quantile function under the fairness requirement of\nDemographic Parity with respect to sensitive attributes, such as race or\ngender, and thereby derive a reliable fair prediction interval. Using optimal\ntransport and functional synchronization techniques, we establish theoretical\nguarantees of distribution-free coverage and exact fairness for the induced\nprediction interval constructed by fair quantiles. A hands-on pipeline is\nprovided to incorporate flexible quantile regressions with an efficient\nfairness adjustment post-processing algorithm. We demonstrate the superior\nempirical performance of this approach on several benchmark datasets. Our\nresults show the model's ability to uncover the mechanism underlying the\nfairness-accuracy trade-off in a wide range of societal and medical\napplications.",
    "descriptor": "\nComments: 5 figures, 2 tables\n",
    "authors": [
      "Meichen Liu",
      "Lei Ding",
      "Dengdeng Yu",
      "Wulong Liu",
      "Linglong Kong",
      "Bei Jiang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02015"
  },
  {
    "id": "arXiv:2210.02035",
    "title": "A Counterexample to a Directed KKL Inequality",
    "abstract": "We show that the natural directed analogues of the KKL theorem [KKL88] and\nthe Eldan--Gross inequality [EG20] from the analysis of Boolean functions fail\nto hold. This is in contrast to several other isoperimetric inequalities on the\nBoolean hypercube (such as the Poincare inequality, Margulis's inequality\n[Mar74] and Talagrand's inequality [Tal93]) for which directed strengthenings\nhave recently been established.",
    "descriptor": "\nComments: 5 pages. Comments welcome\n",
    "authors": [
      "Quentin Dubroff",
      "Shivam Nadimpalli",
      "Bhargav Narayanan"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.02035"
  },
  {
    "id": "arXiv:2210.02092",
    "title": "Functional Central Limit Theorem and Strong Law of Large Numbers for  Stochastic Gradient Langevin Dynamics",
    "abstract": "We study the mixing properties of an important optimization algorithm of\nmachine learning: the stochastic gradient Langevin dynamics (SGLD) with a fixed\nstep size. The data stream is not assumed to be independent hence the SGLD is\nnot a Markov chain, merely a \\emph{Markov chain in a random environment}, which\ncomplicates the mathematical treatment considerably. We derive a strong law of\nlarge numbers and a functional central limit theorem for SGLD.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Attila Lovas",
      "Mikl\u00f3s R\u00e1sonyi"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.02092"
  },
  {
    "id": "arXiv:2210.02107",
    "title": "On a discrete framework of hypocoercivity for kinetic equations",
    "abstract": "We propose and study a fully discrete finite volume scheme for the\nVlasov-Fokker-Planck equation written as an hyperbolic system using Hermite\npolynomials in velocity. This approach naturally preserves the stationary\nsolution and the weighted L 2 relative entropy. Then, we adapt the arguments\ndeveloped in [12] based the hypocoercivity method to get quantitative estimates\non the convergence to equilibrium of the discrete solution. Finally, we prove\nthat in the diffusive limit, the scheme is asymptotic preserving with respect\nto both the time variable and the scaling parameter at play.",
    "descriptor": "",
    "authors": [
      "Alain Blaustein",
      "Francis Filbet"
    ],
    "subjectives": [
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02107"
  },
  {
    "id": "arXiv:2210.02113",
    "title": "Optimization-Informed Neural Networks",
    "abstract": "Solving constrained nonlinear optimization problems (CNLPs) is a longstanding\nproblem that arises in various fields, e.g., economics, computer science, and\nengineering. We propose optimization-informed neural networks (OINN), a deep\nlearning approach to solve CNLPs. By neurodynamic optimization methods, a CNLP\nis first reformulated as an initial value problem (IVP) involving an ordinary\ndifferential equation (ODE) system. A neural network model is then used as an\napproximate solution for this IVP, with the endpoint being the prediction to\nthe CNLP. We propose a novel training algorithm that directs the model to hold\nthe best prediction during training. In a nutshell, OINN transforms a CNLP into\na neural network training problem. By doing so, we can solve CNLPs based on\ndeep learning infrastructure only, without using standard optimization solvers\nor numerical integration solvers. The effectiveness of the proposed approach is\ndemonstrated through a collection of classical problems, e.g., variational\ninequalities, nonlinear complementary problems, and standard CNLPs.",
    "descriptor": "",
    "authors": [
      "Dawen Wu",
      "Abdel Lisser"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02113"
  },
  {
    "id": "arXiv:2210.02120",
    "title": "Development and validation of deep learning based embryo selection  across multiple days of transfer",
    "abstract": "This work describes the development and validation of a fully automated deep\nlearning model, iDAScore v2.0, for the evaluation of embryos incubated for 2,\n3, and 5 or more days. The model is trained and evaluated on an extensive and\ndiverse dataset including 181,428 embryos from 22 IVF clinics across the world.\nFor discriminating transferred embryos with known outcome (KID), we show AUCs\nranging from 0.621 to 0.708 depending on the day of transfer. Predictive\nperformance increased over time and showed a strong correlation with\nmorphokinetic parameters. The model has equivalent performance to KIDScore D3\non day 3 embryos while significantly surpassing the performance of KIDScore D5\nv3 on day 5+ embryos. This model provides an analysis of time-lapse sequences\nwithout the need for user input, and provides a reliable method for ranking\nembryos for likelihood to implant, at both cleavage and blastocyst stages. This\ngreatly improves embryo grading consistency and saves time compared to\ntraditional embryo evaluation methods.",
    "descriptor": "",
    "authors": [
      "Jacob Theilgaard Lassen",
      "Mikkel Fly Kragh",
      "Jens Rimestad",
      "Martin Nyg\u00e5rd Johansen",
      "J\u00f8rgen Berntsen"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.02120"
  },
  {
    "id": "arXiv:2210.02126",
    "title": "Stock Volatility Prediction using Time Series and Deep Learning Approach",
    "abstract": "Volatility clustering is a crucial property that has a substantial impact on\nstock market patterns. Nonetheless, developing robust models for accurately\npredicting future stock price volatility is a difficult research topic. For\npredicting the volatility of three equities listed on India's national stock\nmarket (NSE), we propose multiple volatility models depending on the\ngeneralized autoregressive conditional heteroscedasticity (GARCH),\nGlosten-Jagannathan-GARCH (GJR-GARCH), Exponential general autoregressive\nconditional heteroskedastic (EGARCH), and LSTM framework. Sector-wise stocks\nhave been chosen in our study. The sectors which have been considered are\nbanking, information technology (IT), and pharma. yahoo finance has been used\nto obtain stock price data from Jan 2017 to Dec 2021. Among the pulled-out\nrecords, the data from Jan 2017 to Dec 2020 have been taken for training, and\ndata from 2021 have been chosen for testing our models. The performance of\npredicting the volatility of stocks of three sectors has been evaluated by\nimplementing three different types of GARCH models as well as by the LSTM model\nare compared. It has been observed the LSTM performed better in predicting\nvolatility in pharma over banking and IT sectors. In tandem, it was also\nobserved that E-GARCH performed better in the case of the banking sector and\nfor IT and pharma, GJR-GARCH performed better.",
    "descriptor": "\nComments: This is the accepted version of the paper in the 2022 IEEE 2nd Mysore Sub Section International Conference, MysuruCon22. The conference will be organized in Mysuore, during October 16-17, 2022. The paper is 6 pages long, and it contains 10 figures and 8 tables\n",
    "authors": [
      "Ananda Chatterjee",
      "Hrisav Bhowmick",
      "Jaydip Sen"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02126"
  },
  {
    "id": "arXiv:2210.02129",
    "title": "Personalized Decentralized Bilevel Optimization over Stochastic and  Directed Networks",
    "abstract": "While personalization in distributed learning has been extensively studied,\nexisting approaches employ dedicated algorithms to optimize their specific type\nof parameters (e.g., client clusters or model interpolation weights), making it\ndifficult to simultaneously optimize different types of parameters to yield\nbetter performance. Moreover, their algorithms require centralized or static\nundirected communication networks, which can be vulnerable to center-point\nfailures or deadlocks. This study proposes optimizing various types of\nparameters using a single algorithm that runs on more practical communication\nenvironments. First, we propose a gradient-based bilevel optimization that\nreduces most personalization approaches to the optimization of client-wise\nhyperparameters. Second, we propose a decentralized algorithm to estimate\ngradients with respect to the hyperparameters, which can run even on stochastic\nand directed communication networks. Our empirical results demonstrated that\nthe gradient-based bilevel optimization enabled combining existing\npersonalization approaches which led to state-of-the-art performance,\nconfirming it can perform on multiple simulated communication environments\nincluding a stochastic and directed network.",
    "descriptor": "",
    "authors": [
      "Naoyuki Terashita",
      "Satoshi Hara"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02129"
  },
  {
    "id": "arXiv:2210.02142",
    "title": "Sequential sum-of-squares programming for analysis of nonlinear systems",
    "abstract": "Numerous interesting properties in nonlinear systems analysis can be written\nas polynomial optimization problems with nonconvex sum-of-squares problems. To\nsolve those problems efficiently, we propose a sequential approach of local\nlinearizations leading to tractable, convex sum-of-squares problems. Local\nconvergence is proven under the assumption of strong regularity and the new\napproach is applied to estimate the region of attraction of a polynomial\naircraft model.",
    "descriptor": "\nComments: Submitted to 2023 American Control Conference\n",
    "authors": [
      "Torbj\u00f8rn Cunis",
      "Beno\u00eet Legat"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.02142"
  },
  {
    "id": "arXiv:2210.02157",
    "title": "The Influence of Learning Rule on Representation Dynamics in Wide Neural  Networks",
    "abstract": "It is unclear how changing the learning rule of a deep neural network alters\nits learning dynamics and representations. To gain insight into the\nrelationship between learned features, function approximation, and the learning\nrule, we analyze infinite-width deep networks trained with gradient descent\n(GD) and biologically-plausible alternatives including feedback alignment (FA),\ndirect feedback alignment (DFA), and error modulated Hebbian learning (Hebb),\nas well as gated linear networks (GLN). We show that, for each of these\nlearning rules, the evolution of the output function at infinite width is\ngoverned by a time varying effective neural tangent kernel (eNTK). In the lazy\ntraining limit, this eNTK is static and does not evolve, while in the rich\nmean-field regime this kernel's evolution can be determined self-consistently\nwith dynamical mean field theory (DMFT). This DMFT enables comparisons of the\nfeature and prediction dynamics induced by each of these learning rules. In the\nlazy limit, we find that DFA and Hebb can only learn using the last layer\nfeatures, while full FA can utilize earlier layers with a scale determined by\nthe initial correlation between feedforward and feedback weight matrices. In\nthe rich regime, DFA and FA utilize a temporally evolving and depth-dependent\nNTK. Counterintuitively, we find that FA networks trained in the rich regime\nexhibit more feature learning if initialized with smaller correlation between\nthe forward and backward pass weights. GLNs admit a very simple formula for\ntheir lazy limit kernel and preserve conditional Gaussianity of their\npreactivations under gating functions. Error modulated Hebb rules show very\nsmall task-relevant alignment of their kernels and perform most task relevant\nlearning in the last layer.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "Blake Bordelon",
      "Cengiz Pehlevan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02157"
  },
  {
    "id": "arXiv:2210.02175",
    "title": "Boundary-safe PINNs extension: Application to non-linear parabolic PDEs  in counterparty credit risk",
    "abstract": "The goal of this work is to develop deep learning numerical methods for\nsolving option XVA pricing problems given by non-linear PDE models. A novel\nstrategy for the treatment of the boundary conditions is proposed, which allows\nto get rid of the heuristic choice of the weights for the different addends\nthat appear in the loss function related to the training process. It is based\non defining the losses associated to the boundaries by means of the PDEs that\narise from substituting the related conditions into the model equation itself.\nFurther, automatic differentiation is employed to obtain accurate approximation\nof the partial derivatives.",
    "descriptor": "",
    "authors": [
      "Joel P. Villarino",
      "\u00c1lvaro Leitao",
      "Jos\u00e9 A. Garc\u00eda-Rodr\u00edguez"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.02175"
  },
  {
    "id": "arXiv:2210.02184",
    "title": "Rediscovery of Numerical L\u00fcscher's Formula from the Neural Network",
    "abstract": "We present that by predicting the spectrum in discrete space from the phase\nshift in continuous space, the neural network can remarkably reproduce the\nnumerical L\\\"uscher's formula to a high precision. The model-independent\nproperty of the L\\\"uscher's formula is naturally realized by the\ngeneralizability of the neural network. This exhibits the great potential of\nthe neural network to extract model-independent relation between\nmodel-dependent quantities, and this data-driven approach could greatly\nfacilitate the discovery of the physical principles underneath the intricate\ndata.",
    "descriptor": "\nComments: 8 figures\n",
    "authors": [
      "Yu Lu",
      "Yi-Jia Wang",
      "Ying Chen",
      "Jia-Jun Wu"
    ],
    "subjectives": [
      "High Energy Physics - Lattice (hep-lat)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2210.02184"
  },
  {
    "id": "arXiv:2210.02189",
    "title": "A Generalizable Artificial Intelligence Model for COVID-19  Classification Task Using Chest X-ray Radiographs: Evaluated Over Four  Clinical Datasets with 15,097 Patients",
    "abstract": "Purpose: To answer the long-standing question of whether a model trained from\na single clinical site can be generalized to external sites.\nMaterials and Methods: 17,537 chest x-ray radiographs (CXRs) from 3,264\nCOVID-19-positive patients and 4,802 COVID-19-negative patients were collected\nfrom a single site for AI model development. The generalizability of the\ntrained model was retrospectively evaluated using four different real-world\nclinical datasets with a total of 26,633 CXRs from 15,097 patients (3,277\nCOVID-19-positive patients). The area under the receiver operating\ncharacteristic curve (AUC) was used to assess diagnostic performance.\nResults: The AI model trained using a single-source clinical dataset achieved\nan AUC of 0.82 (95% CI: 0.80, 0.84) when applied to the internal temporal test\nset. When applied to datasets from two external clinical sites, an AUC of 0.81\n(95% CI: 0.80, 0.82) and 0.82 (95% CI: 0.80, 0.84) were achieved. An AUC of\n0.79 (95% CI: 0.77, 0.81) was achieved when applied to a multi-institutional\nCOVID-19 dataset collected by the Medical Imaging and Data Resource Center\n(MIDRC). A power-law dependence, N^(k )(k is empirically found to be -0.21 to\n-0.25), indicates a relatively weak performance dependence on the training data\nsizes.\nConclusion: COVID-19 classification AI model trained using well-curated data\nfrom a single clinical site is generalizable to external clinical sites without\na significant drop in performance.",
    "descriptor": "",
    "authors": [
      "Ran Zhang",
      "Xin Tie",
      "John W. Garrett",
      "Dalton Griner",
      "Zhihua Qi",
      "Nicholas B. Bevins",
      "Scott B. Reeder",
      "Guang-Hong Chen"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02189"
  },
  {
    "id": "arXiv:2210.02203",
    "title": "PriorNet: lesion segmentation in PET-CT including prior tumor appearance  information",
    "abstract": "Tumor segmentation in PET-CT images is challenging due to the dual nature of\nthe acquired information: low metabolic information in CT and low spatial\nresolution in PET. U-Net architecture is the most common and widely recognized\napproach when developing a fully automatic image segmentation method in the\nmedical field. We proposed a two-step approach, aiming to refine and improve\nthe segmentation performances of tumoral lesions in PET-CT. The first step\ngenerates a prior tumor appearance map from the PET-CT volumes, regarded as\nprior tumor information. The second step, consisting of a standard U-Net,\nreceives the prior tumor appearance map and PET-CT images to generate the\nlesion mask. We evaluated the method on the 1014 cases available for the\nAutoPET 2022 challenge, and the results showed an average Dice score of 0.701\non the positive cases.",
    "descriptor": "",
    "authors": [
      "Simone Bendazzoli",
      "Mehdi Astaraki"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.02203"
  },
  {
    "id": "arXiv:2210.02216",
    "title": "Correspondence Theory for Modal Fairtlough-Mendler Semantics of  Intuitionistic Modal Logic",
    "abstract": "We study the correspondence theory of intuitionistic modal logic in modal\nFairtlough-Mendler semantics (modal FM semantics) \\cite{FaMe97}, which is the\nintuitionistic modal version of possibility semantics \\cite{Ho16}. We identify\nthe fragment of inductive formulas \\cite{GorankoV06} in this language and give\nthe algorithm $\\mathsf{ALBA}$ \\cite{CoPa12} in this semantic setting. There are\ntwo major features in the paper: one is that in the expanded modal language,\nthe nominal variables, which are interpreted as atoms in perfect Boolean\nalgebras, complete join-prime elements in perfect distributive lattices and\ncomplete join-irreducible elements in perfect lattices, are interpreted as the\nrefined regular open closures of singletons in the present setting, similar to\nthe possibility semantics for classical normal modal logic \\cite{Zh21d}; the\nother feature is that we do not use conominals or diamond, which restricts the\nfragment of inductive formulas significantly. We prove the soundness of the\n$\\mathsf{ALBA}$ with respect to modal FM frames and show that the\n$\\mathsf{ALBA}$ succeeds on inductive formulas, similar to existing settings\nlike \\cite{CoPa12,Zh21d,Zh22a}.",
    "descriptor": "",
    "authors": [
      "Zhiguang Zhao"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.02216"
  },
  {
    "id": "arXiv:2210.02226",
    "title": "Null Hypothesis Test for Anomaly Detection",
    "abstract": "We extend the use of Classification Without Labels for anomaly detection with\na hypothesis test designed to exclude the background-only hypothesis. By\ntesting for statistical independence of the two discriminating dataset regions,\nwe are able exclude the background-only hypothesis without relying on fixed\nanomaly score cuts or extrapolations of background estimates between regions.\nThe method relies on the assumption of conditional independence of anomaly\nscore features and dataset regions, which can be ensured using existing\ndecorrelation techniques. As a benchmark example, we consider the LHC Olympics\ndataset where we show that mutual information represents a suitable test for\nstatistical independence and our method exhibits excellent and robust\nperformance at different signal fractions even in presence of realistic feature\ncorrelations.",
    "descriptor": "\nComments: 8 pages, 3 figures. Comments welcome!\n",
    "authors": [
      "Jernej F. Kamenik",
      "Manuel Szewc"
    ],
    "subjectives": [
      "High Energy Physics - Phenomenology (hep-ph)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)"
    ],
    "url": "https://arxiv.org/abs/2210.02226"
  },
  {
    "id": "arXiv:2210.02241",
    "title": "HeartSpot: Privatized and Explainable Data Compression for Cardiomegaly  Detection",
    "abstract": "Advances in data-driven deep learning for chest X-ray image analysis\nunderscore the need for explainability, privacy, large datasets and significant\ncomputational resources. We frame privacy and explainability as a lossy\nsingle-image compression problem to reduce both computational and data\nrequirements without training. For Cardiomegaly detection in chest X-ray\nimages, we propose HeartSpot and four spatial bias priors. HeartSpot priors\ndefine how to sample pixels based on domain knowledge from medical literature\nand from machines. HeartSpot privatizes chest X-ray images by discarding up to\n97% of pixels, such as those that reveal the shape of the thoracic cage, bones,\nsmall lesions and other sensitive features. HeartSpot priors are ante-hoc\nexplainable and give a human-interpretable image of the preserved spatial\nfeatures that clearly outlines the heart. HeartSpot offers strong compression,\nwith up to 32x fewer pixels and 11x smaller filesize. Cardiomegaly detectors\nusing HeartSpot are up to 9x faster to train or at least as accurate (up to\n+.01 AUC ROC) when compared to a baseline DenseNet121. HeartSpot is post-hoc\nexplainable by re-using existing attribution methods without requiring access\nto the original non-privatized image. In summary, HeartSpot improves speed and\naccuracy, reduces image size, improves privacy and ensures explainability.\nSource code: https://www.github.com/adgaudio/HeartSpot",
    "descriptor": "\nComments: Accepted to IEEE-EMBS International Conference on Biomedical and Health Informatics 2022. IEEE copyrights may apply\n",
    "authors": [
      "Elvin Johnson",
      "Shreshta Mohan",
      "Alex Gaudio",
      "Asim Smailagic",
      "Christos Faloutsos",
      "Aur\u00e9lio Campilho"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02241"
  },
  {
    "id": "arXiv:2210.02271",
    "title": "Extending Conformal Prediction to Hidden Markov Models with Exact  Validity via de Finetti's Theorem for Markov Chains",
    "abstract": "Conformal prediction is a widely used method to quantify uncertainty in\nsettings where the data is independent and identically distributed (IID), or\nmore generally, exchangeable. Conformal prediction takes in a pre-trained\nclassifier, a calibration dataset and a confidence level as inputs, and returns\na function which maps feature vectors to subsets of classes. The output of the\nreturned function for a new feature vector (i.e., a test data point) is\nguaranteed to contain the true class with the pre-specified confidence. Despite\nits success and usefulness in IID settings, extending conformal prediction to\nnon-exchangeable (e.g., Markovian) data in a manner that provably preserves all\ndesirable theoretical properties has largely remained an open problem. As a\nsolution, we extend conformal prediction to the setting of a Hidden Markov\nModel (HMM) with unknown parameters. The key idea behind the proposed method is\nto partition the non-exchangeable Markovian data from the HMM into exchangeable\nblocks by exploiting the de Finetti's Theorem for Markov Chains discovered by\nDiaconis and Freedman (1980). The permutations of the exchangeable blocks are\nthen viewed as randomizations of the observed Markovian data from the HMM. The\nproposed method provably retains all desirable theoretical guarantees offered\nby the classical conformal prediction framework and is general enough to be\nuseful in many sequential prediction problems.",
    "descriptor": "",
    "authors": [
      "Buddhika Nettasinghe",
      "Samrat Chatterjee",
      "Ramakrishna Tipireddy",
      "Mahantesh Halappanavar"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.02271"
  },
  {
    "id": "arXiv:2210.02273",
    "title": "Novel Radiomic Measurements of Tumor- Associated Vasculature Morphology  on Clinical Imaging as a Biomarker of Treatment Response in Multiple Cancers",
    "abstract": "Purpose: Tumor-associated vasculature differs from healthy blood vessels by\nits chaotic architecture and twistedness, which promotes treatment resistance.\nMeasurable differences in these attributes may help stratify patients by likely\nbenefit of systemic therapy (e.g. chemotherapy). In this work, we present a new\ncategory of radiomic biomarkers called quantitative tumor-associated\nvasculature (QuanTAV) features, and demonstrate their ability to predict\nresponse and survival across multiple cancers, imaging modalities, and\ntreatment regimens.\nExperimental Design: We segmented tumor vessels and computed mathematical\nmeasurements of twistedness and organization on routine pre-treatment radiology\n(CT or contrast-enhanced MRI) from 558 patients, who received one of four\nfirst-line chemotherapy-based therapeutic intervention strategies for breast\n(n=371) or non-small cell lung cancer (NSCLC, n=187).\nResults: Across 4 chemotherapy-based treatment strategies, classifiers of\nQuanTAV measurements significantly (p<.05) predicted response in held out\ntesting cohorts alone (AUC=0.63-0.71) and increased AUC by 0.06-0.12 when added\nto models of significant clinical variables alone. QuanTAV risk scores were\nprognostic of recurrence free survival in treatment cohorts chemotherapy for\nbreast cancer (p=0.002, HR=1.25, 95% CI 1.08-1.44, C-index=.66) and\nchemoradiation for NSCLC (p=0.039, HR=1.28, 95% CI 1.01-1.62, C-index=0.66).\nCategorical QuanTAV risk groups were independently prognostic among all\ntreatment groups, including NSCLC patients receiving chemotherapy (p=0.034,\nHR=2.29, 95% CI 1.07-4.94, C-index=0.62).\nConclusions: Across these domains, we observed an association of vascular\nmorphology on radiology with treatment outcome. Our findings suggest the\npotential of tumor-associated vasculature shape and structure as a prognostic\nand predictive biomarker for multiple cancers and treatments.",
    "descriptor": "\nComments: This manuscript has been accepted for publication in Clinical Cancer Research, which is published by the American Association for Cancer Research\n",
    "authors": [
      "Nathaniel Braman",
      "Prateek Prasanna",
      "Kaustav Bera",
      "Mehdi Alilou",
      "Mohammadhadi Khorrami",
      "Patrick Leo",
      "Maryam Etesami",
      "Manasa Vulchi",
      "Paulette Turk",
      "Amit Gupta",
      "Prantesh Jain",
      "Pingfu Fu",
      "Nathan Pennell",
      "Vamsidhar Velcheti",
      "Jame Abraham",
      "Donna Plecha",
      "Anant Madabhushi"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Tissues and Organs (q-bio.TO)"
    ],
    "url": "https://arxiv.org/abs/2210.02273"
  },
  {
    "id": "arXiv:2210.02286",
    "title": "Probabilistic reconciliation of forecasts via importance sampling",
    "abstract": "Hierarchical time series are common in several applied fields. Forecasts are\nrequired to be coherent, that is, to satisfy the constraints given by the\nhierarchy. The most popular technique to enforce coherence is called\nreconciliation, which adjusts the base forecasts computed for each time series.\nHowever, recent works on probabilistic reconciliation present several\nlimitations. In this paper, we propose a new approach based on conditioning to\nreconcile any type of forecast distribution. We then introduce a new algorithm,\ncalled Bottom-Up Importance Sampling, to efficiently sample from the reconciled\ndistribution. It can be used for any base forecast distribution: discrete,\ncontinuous, or even in the form of samples. The method was tested on several\ntemporal hierarchies showing that our reconciliation effectively improves the\nquality of probabilistic forecasts. Moreover, our algorithm is up to 3 orders\nof magnitude faster than vanilla MCMC methods.",
    "descriptor": "\nComments: 28 pages, 7 figures\n",
    "authors": [
      "Lorenzo Zambon",
      "Dario Azzimonti",
      "Giorgio Corani"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02286"
  },
  {
    "id": "arXiv:2210.02335",
    "title": "Informed sampling-based trajectory planner for automated driving in  dynamic urban environments",
    "abstract": "The urban environment is amongst the most difficult domains for autonomous\nvehicles. The vehicle must be able to plan a safe route on challenging road\nlayouts, in the presence of various dynamic traffic participants such as\nvehicles, cyclists and pedestrians and in various environmental conditions. The\nchallenge remains to have motion planners that are computationally fast and\nthat account for future movements of other road users proactively. This paper\ndescribes an computationally efficient sampling-based trajectory planner for\nsafe and comfortable driving in urban environments. The planner improves the\nStable-Sparse-RRT algorithm by adding initial exploration branches to the\nsearch tree based on road layout information and reiterating the previous\nsolution. Furthermore, the trajectory planner accounts for the predicted motion\nof other traffic participants to allow for safe driving in urban traffic.\nSimulation studies show that the planner is capable of planning collision-free,\ncomfortable trajectories in several urban traffic scenarios. Adding the\ndomain-knowledge-based exploration branches increases the efficiency of\nexploration of highly interesting areas, thereby increasing the overall\nplanning performance.",
    "descriptor": "",
    "authors": [
      "Robin Smit",
      "Chris van der Ploeg",
      "Arjan Teerhuis",
      "Emilia Silvas"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.02335"
  },
  {
    "id": "arXiv:2210.02339",
    "title": "Particle clustering in turbulence: Prediction of spatial and statistical  properties with deep learning",
    "abstract": "We demonstrate the utility of deep learning for modeling the clustering of\nparticles that are aerodynamically coupled to turbulent fluids. Using a\nLagrangian particle module within the ATHENA++ hydrodynamics code, we simulate\nthe dynamics of particles in the Epstein drag regime within a periodic domain\nof isotropic forced hydrodynamic turbulence. This setup is an idealized model\nrelevant to the collisional growth of micron to mmsized dust particles in early\nstage planet formation. The simulation data is used to train a U-Net deep\nlearning model to predict gridded three-dimensional representations of the\nparticle density and velocity fields, given as input the corresponding fluid\nfields. The trained model qualitatively captures the filamentary structure of\nclustered particles in a highly non-linear regime. We assess model fidelity by\ncalculating metrics of the density structure (the radial distribution function)\nand of the velocity field (the relative velocity and the relative radial\nvelocity between particles). Although trained only on the spatial fields, the\nmodel predicts these statistical quantities with errors that are typically <\n10%. Our results suggest that, given appropriately expanded training data, deep\nlearning could be used to accelerate calculations of particle clustering and\ncollision outcomes both in protoplanetary disks, and in related two-fluid\nturbulence problems that arise in other disciplines.",
    "descriptor": "\nComments: 19 pages, 13 figures, submitted to ApJ\n",
    "authors": [
      "Yan-Mong Chan",
      "Natascha Manger",
      "Yin Li",
      "Chao-Chin Yang",
      "Zhaohuan Zhu",
      "Philip J. Armitage",
      "Shirley Ho"
    ],
    "subjectives": [
      "Earth and Planetary Astrophysics (astro-ph.EP)",
      "Machine Learning (cs.LG)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2210.02339"
  },
  {
    "id": "arXiv:2210.02341",
    "title": "A distributed Gibbs Sampler with Hypergraph Structure for  High-Dimensional Inverse Problems",
    "abstract": "Sampling-based algorithms are classical approaches to perform Bayesian\ninference in inverse problems. They provide estimators with the associated\ncredibility intervals to quantify the uncertainty on the estimators. Although\nthese methods hardly scale to high dimensional problems, they have recently\nbeen paired with optimization techniques, such as proximal and splitting\napproaches, to address this issue. Such approaches pave the way to distributed\nsamplers, splitting computations to make inference more scalable and faster. We\nintroduce a distributed Gibbs sampler to efficiently solve such problems,\nconsidering posterior distributions with multiple smooth and non-smooth\nfunctions composed with linear operators. The proposed approach leverages a\nrecent approximate augmentation technique reminiscent of primal-dual\noptimization methods. It is further combined with a block-coordinate approach\nto split the primal and dual variables into blocks, leading to a distributed\nblock-coordinate Gibbs sampler. The resulting algorithm exploits the hypergraph\nstructure of the involved linear operators to efficiently distribute the\nvariables over multiple workers under controlled communication costs. It\naccommodates several distributed architectures, such as the Single Program\nMultiple Data and client-server architectures. Experiments on a large image\ndeblurring problem show the performance of the proposed approach to produce\nhigh quality estimates with credibility intervals in a small amount of time.",
    "descriptor": "",
    "authors": [
      "Pierre-Antoine Thouvenin",
      "Audrey Repetti",
      "Pierre Chainais"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.02341"
  },
  {
    "id": "arXiv:2210.02349",
    "title": "Fitting a Directional Microstructure Model to Diffusion-Relaxation MRI  Data with Self-Supervised Machine Learning",
    "abstract": "Machine learning is a powerful approach for fitting microstructural models to\ndiffusion MRI data. Early machine learning microstructure imaging\nimplementations trained regressors to estimate model parameters in a supervised\nway, using synthetic training data with known ground truth. However, a drawback\nof this approach is that the choice of training data impacts fitted parameter\nvalues. Self-supervised learning is emerging as an attractive alternative to\nsupervised learning in this context. Thus far, both supervised and\nself-supervised learning have typically been applied to isotropic models, such\nas intravoxel incoherent motion (IVIM), as opposed to models where the\ndirectionality of anisotropic structures is also estimated. In this paper, we\ndemonstrate self-supervised machine learning model fitting for a directional\nmicrostructural model. In particular, we fit a combined T1-ball-stick model to\nthe multidimensional diffusion (MUDI) challenge diffusion-relaxation dataset.\nOur self-supervised approach shows clear improvements in parameter estimation\nand computational time, for both simulated and in-vivo brain data, compared to\nstandard non-linear least squares fitting. Code for the artificial neural net\nconstructed for this study is available for public use from the following\nGitHub repository: https://github.com/jplte/deep-T1-ball-stick",
    "descriptor": "\nComments: Oral Presentation in: Computational Diffusion MRI Workshop (CDMRI) at Medical Image Computing and Computer Assisted Intervention (MICCAI) 2022\n",
    "authors": [
      "Jason P. Lim",
      "Stefano B. Blumberg",
      "Neil Narayan",
      "Sean C. Epstein",
      "Daniel C. Alexander",
      "Marco Palombo",
      "Paddy J. Slator"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.02349"
  },
  {
    "id": "arXiv:2210.02355",
    "title": "A kernel-based quantum random forest for improved classification",
    "abstract": "The emergence of Quantum Machine Learning (QML) to enhance traditional\nclassical learning methods has seen various limitations to its realisation.\nThere is therefore an imperative to develop quantum models with unique model\nhypotheses to attain expressional and computational advantage. In this work we\nextend the linear quantum support vector machine (QSVM) with kernel function\ncomputed through quantum kernel estimation (QKE), to form a decision tree\nclassifier constructed from a decision directed acyclic graph of QSVM nodes -\nthe ensemble of which we term the quantum random forest (QRF). To limit\noverfitting, we further extend the model to employ a low-rank Nystr\\\"{o}m\napproximation to the kernel matrix. We provide generalisation error bounds on\nthe model and theoretical guarantees to limit errors due to finite sampling on\nthe Nystr\\\"{o}m-QKE strategy. In doing so, we show that we can achieve lower\nsampling complexity when compared to QKE. We numerically illustrate the effect\nof varying model hyperparameters and finally demonstrate that the QRF is able\nobtain superior performance over QSVMs, while also requiring fewer kernel\nestimations.",
    "descriptor": "\nComments: 33 pages, 11 figures\n",
    "authors": [
      "Maiyuren Srikumar",
      "Charles D. Hill",
      "Lloyd C.L. Hollenberg"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02355"
  },
  {
    "id": "arXiv:2210.02416",
    "title": "A deep learning model for brain vessel segmentation in 3DRA with  arteriovenous malformations",
    "abstract": "Segmentation of brain arterio-venous malformations (bAVMs) in 3D rotational\nangiographies (3DRA) is still an open problem in the literature, with high\nrelevance for clinical practice. While deep learning models have been applied\nfor segmenting the brain vasculature in these images, they have never been used\nin cases with bAVMs. This is likely caused by the difficulty to obtain\nsufficiently annotated data to train these approaches. In this paper we\nintroduce a first deep learning model for blood vessel segmentation in 3DRA\nimages of patients with bAVMs. To this end, we densely annotated 5 3DRA volumes\nof bAVM cases and used these to train two alternative 3DUNet-based\narchitectures with different segmentation objectives. Our results show that the\nnetworks reach a comprehensive coverage of relevant structures for bAVM\nanalysis, much better than what is obtained using standard methods. This is\npromising for achieving a better topological and morphological characterisation\nof the bAVM structures of interest. Furthermore, the models have the ability to\nsegment venous structures even when missing in the ground truth labelling,\nwhich is relevant for planning interventional treatments. Ultimately, these\nresults could be used as more reliable first initial guesses, alleviating the\ncumbersome task of creating manual labels.",
    "descriptor": "\nComments: 9 pages, 4 figures, submitted to SIPAIM 2022, to be published in the SPIE Digital Library\n",
    "authors": [
      "Camila Garc\u00eda",
      "Yibin Fang",
      "Jianmin Liu",
      "Ana Paula Narata",
      "Jos\u00e9 Ignacio Orlando",
      "Ignacio Larrabide"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02416"
  },
  {
    "id": "arXiv:1910.08635",
    "title": "Tree-based Intelligent Intrusion Detection System in Internet of  Vehicles",
    "abstract": "Comments: Published in IEEE Global Communications Conference (GLOBECOM) 2019; Code is available at Github link: this https URL",
    "descriptor": "\nComments: Published in IEEE Global Communications Conference (GLOBECOM) 2019; Code is available at Github link: this https URL\n",
    "authors": [
      "Li Yang",
      "Abdallah Moubayed",
      "Ismail Hamieh",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1910.08635"
  },
  {
    "id": "arXiv:2005.08483",
    "title": "Improving Reverse k Nearest Neighbors Queries",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1911.02788 by other authors",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1911.02788 by other authors\n",
    "authors": [
      "Lixin Ye"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2005.08483"
  },
  {
    "id": "arXiv:2005.12979",
    "title": "Seamlessly Unifying Attributes and Items: Conversational Recommendation  for Cold-Start Users",
    "abstract": "Comments: TOIS 2021",
    "descriptor": "\nComments: TOIS 2021\n",
    "authors": [
      "Shijun Li",
      "Wenqiang Lei",
      "Qingyun Wu",
      "Xiangnan He",
      "Peng Jiang",
      "Tat-Seng Chua"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2005.12979"
  },
  {
    "id": "arXiv:2006.15578",
    "title": "Universal Medical Image Segmentation using 3D Fabric Image  Representation Encoding Networks",
    "abstract": "Universal Medical Image Segmentation using 3D Fabric Image  Representation Encoding Networks",
    "descriptor": "",
    "authors": [
      "Siyu Liu",
      "Wei Dai",
      "Craig Engstrom",
      "Jurgen Fripp",
      "Stuart Crozier",
      "Jason A. Dowling",
      "Shekhar S. Chandra"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2006.15578"
  },
  {
    "id": "arXiv:2007.06686",
    "title": "A Systematic Survey on Deep Generative Models for Graph Generation",
    "abstract": "Comments: Accepted in TPAMI",
    "descriptor": "\nComments: Accepted in TPAMI\n",
    "authors": [
      "Xiaojie Guo",
      "Liang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.06686"
  },
  {
    "id": "arXiv:2007.15745",
    "title": "On Hyperparameter Optimization of Machine Learning Algorithms: Theory  and Practice",
    "abstract": "Comments: Published in Neurocomputing (Elsevier's journal, Q1, IF: 5.779). Tutorial code has got 1000+ stars. Github link: this https URL",
    "descriptor": "\nComments: Published in Neurocomputing (Elsevier's journal, Q1, IF: 5.779). Tutorial code has got 1000+ stars. Github link: this https URL\n",
    "authors": [
      "Li Yang",
      "Abdallah Shami"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.15745"
  },
  {
    "id": "arXiv:2009.08632",
    "title": "Approximately Socially-Optimal Decentralized Coalition Formation with  Application to P2P Energy Sharing",
    "abstract": "Comments: To appear in ACM SIGENERGY Energy Informatics Review, 2022",
    "descriptor": "\nComments: To appear in ACM SIGENERGY Energy Informatics Review, 2022\n",
    "authors": [
      "Sid Chi-Kin Chau",
      "Khaled Elbassioni",
      "Yue Zhou"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2009.08632"
  },
  {
    "id": "arXiv:2012.01102",
    "title": "An Analytic Propositional Proof System on Graphs",
    "abstract": "An Analytic Propositional Proof System on Graphs",
    "descriptor": "",
    "authors": [
      "Matteo Acclavio",
      "Ross Horne",
      "Lutz Stra\u00dfburger"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2012.01102"
  },
  {
    "id": "arXiv:2012.09422",
    "title": "The Variational Method of Moments",
    "abstract": "The Variational Method of Moments",
    "descriptor": "",
    "authors": [
      "Andrew Bennett",
      "Nathan Kallus"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2012.09422"
  },
  {
    "id": "arXiv:2102.04097",
    "title": "Effects of Layer Freezing on Transferring a Speech Recognition System to  Under-resourced Languages",
    "abstract": "Comments: Published at KONVENS 2021",
    "descriptor": "\nComments: Published at KONVENS 2021\n",
    "authors": [
      "Onno Eberhard",
      "Torsten Zesch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2102.04097"
  },
  {
    "id": "arXiv:2102.06463",
    "title": "A more accurate view of the Flat Wall Theorem",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2004.12692",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2004.12692\n",
    "authors": [
      "Ignasi Sau",
      "Giannos Stamoulis",
      "Dimitrios M. Thilikos"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2102.06463"
  },
  {
    "id": "arXiv:2103.00882",
    "title": "K-apices Of Minor-closed Graph Classes. I. Bounding The Obstructions",
    "abstract": "Comments: 46 pages and 12 figures. arXiv admin note: text overlap with arXiv:2004.12692",
    "descriptor": "\nComments: 46 pages and 12 figures. arXiv admin note: text overlap with arXiv:2004.12692\n",
    "authors": [
      "Ignasi Sau",
      "Giannos Stamoulis",
      "Dimitrios M. Thilikos"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2103.00882"
  },
  {
    "id": "arXiv:2104.03158",
    "title": "Beyond Impute-Then-Regress: Adapting Prediction to Missing Data",
    "abstract": "Beyond Impute-Then-Regress: Adapting Prediction to Missing Data",
    "descriptor": "",
    "authors": [
      "Dimitris Bertsimas",
      "Arthur Delarue",
      "Jean Pauphilet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.03158"
  },
  {
    "id": "arXiv:2104.06970",
    "title": "Understanding the Eluder Dimension",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Gene Li",
      "Pritish Kamath",
      "Dylan J. Foster",
      "Nathan Srebro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2104.06970"
  },
  {
    "id": "arXiv:2105.00114",
    "title": "Improved Real-Time Monocular SLAM Using Semantic Segmentation on  Selective Frames",
    "abstract": "Improved Real-Time Monocular SLAM Using Semantic Segmentation on  Selective Frames",
    "descriptor": "",
    "authors": [
      "Jinkyu Lee",
      "Muhyun Back",
      "Sung Soo Hwang",
      "Il Yong Chun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.00114"
  },
  {
    "id": "arXiv:2106.01543",
    "title": "Ver: View Discovery in the Wild",
    "abstract": "Ver: View Discovery in the Wild",
    "descriptor": "",
    "authors": [
      "Yue Gong",
      "Zhiru Zhu",
      "Sainyam Galhotra",
      "Raul Castro Fernandez"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2106.01543"
  },
  {
    "id": "arXiv:2106.03007",
    "title": "Truthful Self-Play",
    "abstract": "Truthful Self-Play",
    "descriptor": "",
    "authors": [
      "Shohei Ohsawa"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2106.03007"
  },
  {
    "id": "arXiv:2106.06312",
    "title": "A Coupled Design of Exploiting Record Similarity for Practical Vertical  Federated Learning",
    "abstract": "A Coupled Design of Exploiting Record Similarity for Practical Vertical  Federated Learning",
    "descriptor": "",
    "authors": [
      "Zhaomin Wu",
      "Qinbin Li",
      "Bingsheng He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2106.06312"
  },
  {
    "id": "arXiv:2106.09907",
    "title": "The dihedral hidden subgroup problem",
    "abstract": "Comments: 19 pages, expanded survey content and additional no-go theorems added",
    "descriptor": "\nComments: 19 pages, expanded survey content and additional no-go theorems added\n",
    "authors": [
      "Imin Chen",
      "David Sun"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2106.09907"
  },
  {
    "id": "arXiv:2106.15214",
    "title": "Joint Majorization-Minimization for Nonnegative Matrix Factorization  with the $\u03b2$-divergence",
    "abstract": "Joint Majorization-Minimization for Nonnegative Matrix Factorization  with the $\u03b2$-divergence",
    "descriptor": "",
    "authors": [
      "Arthur Marmin",
      "Jos\u00e9 Henrique de Morais Goulart",
      "C\u00e9dric F\u00e9votte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.15214"
  },
  {
    "id": "arXiv:2107.01305",
    "title": "Maximum likelihood for high-noise group orbit estimation and  single-particle cryo-EM",
    "abstract": "Maximum likelihood for high-noise group orbit estimation and  single-particle cryo-EM",
    "descriptor": "",
    "authors": [
      "Zhou Fan",
      "Roy R. Lederman",
      "Yi Sun",
      "Tianhao Wang",
      "Sheng Xu"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2107.01305"
  },
  {
    "id": "arXiv:2107.07154",
    "title": "What and When to Look?: Temporal Span Proposal Network for Video  Relation Detection",
    "abstract": "Comments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Sangmin Woo",
      "Junhyug Noh",
      "Kangil Kim"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.07154"
  },
  {
    "id": "arXiv:2107.09667",
    "title": "Human Perception of Audio Deepfakes",
    "abstract": "Comments: Published at ACM Multimedia 2022 Workshop DDAM",
    "descriptor": "\nComments: Published at ACM Multimedia 2022 Workshop DDAM\n",
    "authors": [
      "Nicolas M. M\u00fcller",
      "Karla Markert",
      "Jennifer Williams"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2107.09667"
  },
  {
    "id": "arXiv:2108.00373",
    "title": "SPEAR : Semi-supervised Data Programming in Python",
    "abstract": "Comments: EMNLP Demonstrations - 2022",
    "descriptor": "\nComments: EMNLP Demonstrations - 2022\n",
    "authors": [
      "Guttu Sai Abhishek",
      "Harshad Ingole",
      "Parth Laturia",
      "Vineeth Dorna",
      "Ayush Maheshwari",
      "Rishabh Iyer",
      "Ganesh Ramakrishnan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2108.00373"
  },
  {
    "id": "arXiv:2109.02553",
    "title": "Broken-FEEC discretizations and Hodge Laplace problems",
    "abstract": "Broken-FEEC discretizations and Hodge Laplace problems",
    "descriptor": "",
    "authors": [
      "Martin Campos-Pinto",
      "Yaman G\u00fc\u00e7l\u00fc"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2109.02553"
  },
  {
    "id": "arXiv:2109.06495",
    "title": "Error analysis for 2D stochastic Navier--Stokes equations in bounded  domains with Dirichlet data",
    "abstract": "Comments: Statement and proof of Lemma 3.1. (c) have been revised",
    "descriptor": "\nComments: Statement and proof of Lemma 3.1. (c) have been revised\n",
    "authors": [
      "Dominic Breit",
      "Andreas Prohl"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2109.06495"
  },
  {
    "id": "arXiv:2109.06906",
    "title": "Recovering individual emotional states from sparse ratings using  collaborative filtering",
    "abstract": "Comments: 21 pages, 8 figures",
    "descriptor": "\nComments: 21 pages, 8 figures\n",
    "authors": [
      "Eshin Jolly",
      "Max Farrens",
      "Nathan Greenstein",
      "Hedwig Eisenbarth",
      "Marianne Reddan",
      "Eric Andrews",
      "Tor D. Wager",
      "Luke J. Chang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2109.06906"
  },
  {
    "id": "arXiv:2109.08965",
    "title": "PCNN: A physics-constrained neural network for multiphase flows",
    "abstract": "Comments: 29 pages, 9 figures",
    "descriptor": "\nComments: 29 pages, 9 figures\n",
    "authors": [
      "Haoyang Zheng",
      "Ziyang Huang",
      "Guang Lin"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.08965"
  },
  {
    "id": "arXiv:2109.12643",
    "title": "Quantum Money from Quaternion Algebras",
    "abstract": "Comments: 33 pages. This paper can be viewed as an extended version of \"Quantum Money from Modular Forms\" by Daniel M. Kane (arXiv:1809.05925v2). Updated statements and proofs. To appear in Mathematical Cryptology",
    "descriptor": "\nComments: 33 pages. This paper can be viewed as an extended version of \"Quantum Money from Modular Forms\" by Daniel M. Kane (arXiv:1809.05925v2). Updated statements and proofs. To appear in Mathematical Cryptology\n",
    "authors": [
      "Daniel M. Kane",
      "Shahed Sharif",
      "Alice Silverberg"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2109.12643"
  },
  {
    "id": "arXiv:2110.01729",
    "title": "Stochastic coordinate transformations with applications to robust  machine learning",
    "abstract": "Stochastic coordinate transformations with applications to robust  machine learning",
    "descriptor": "",
    "authors": [
      "Julio Enrique Castrillon-Candas",
      "Dingning Liu",
      "Mark Kon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01729"
  },
  {
    "id": "arXiv:2110.05456",
    "title": "Rome was built in 1776: A Case Study on Factual Correctness in  Knowledge-Grounded Response Generation",
    "abstract": "Rome was built in 1776: A Case Study on Factual Correctness in  Knowledge-Grounded Response Generation",
    "descriptor": "",
    "authors": [
      "Sashank Santhanam",
      "Behnam Hedayatnia",
      "Spandana Gella",
      "Aishwarya Padmakumar",
      "Seokhwan Kim",
      "Yang Liu",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.05456"
  },
  {
    "id": "arXiv:2110.06763",
    "title": "Efficient Estimation in NPIV Models: A Comparison of Various Neural  Networks-Based Estimators",
    "abstract": "Efficient Estimation in NPIV Models: A Comparison of Various Neural  Networks-Based Estimators",
    "descriptor": "",
    "authors": [
      "Jiafeng Chen",
      "Xiaohong Chen",
      "Elie Tamer"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2110.06763"
  },
  {
    "id": "arXiv:2110.07307",
    "title": "FocusNet: Classifying Better by Focusing on Confusing Classes",
    "abstract": "Comments: Accepted by Pattern Recognition 2022",
    "descriptor": "\nComments: Accepted by Pattern Recognition 2022\n",
    "authors": [
      "Xue Zhang",
      "Zehua Sheng",
      "Hui-Liang Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.07307"
  },
  {
    "id": "arXiv:2110.13413",
    "title": "Does your graph need a confidence boost? Convergent boosted smoothing on  graphs with tabular node features",
    "abstract": "Does your graph need a confidence boost? Convergent boosted smoothing on  graphs with tabular node features",
    "descriptor": "",
    "authors": [
      "Jiuhai Chen",
      "Jonas Mueller",
      "Vassilis N. Ioannidis",
      "Soji Adeshina",
      "Yangkun Wang",
      "Tom Goldstein",
      "David Wipf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.13413"
  },
  {
    "id": "arXiv:2111.01374",
    "title": "A Game of Primes",
    "abstract": "A Game of Primes",
    "descriptor": "",
    "authors": [
      "Raghavendra Bhat"
    ],
    "subjectives": [
      "General Mathematics (math.GM)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2111.01374"
  },
  {
    "id": "arXiv:2111.02098",
    "title": "Distributed Extended Object Tracking Information Filter Over Sensor  Networks",
    "abstract": "Comments: This paper contains 23 pages with single-column, 24 figures",
    "descriptor": "\nComments: This paper contains 23 pages with single-column, 24 figures\n",
    "authors": [
      "Zhifei Li",
      "Yan Liang",
      "Linfeng Xu",
      "Shuli Ma"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2111.02098"
  },
  {
    "id": "arXiv:2111.04867",
    "title": "TACCL: Guiding Collective Algorithm Synthesis using Communication  Sketches",
    "abstract": "Comments: Accepted at NSDI'23. Contains 20 pages, 11 figures, including Appendix",
    "descriptor": "\nComments: Accepted at NSDI'23. Contains 20 pages, 11 figures, including Appendix\n",
    "authors": [
      "Aashaka Shah",
      "Vijay Chidambaram",
      "Meghan Cowan",
      "Saeed Maleki",
      "Madan Musuvathi",
      "Todd Mytkowicz",
      "Jacob Nelson",
      "Olli Saarikivi",
      "Rachee Singh"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.04867"
  },
  {
    "id": "arXiv:2111.04920",
    "title": "PopBlends: Strategies for Conceptual Blending with Large Language Models",
    "abstract": "PopBlends: Strategies for Conceptual Blending with Large Language Models",
    "descriptor": "",
    "authors": [
      "Sitong Wang",
      "Savvas Petridis",
      "Taeahn Kwon",
      "Xiaojuan Ma",
      "Lydia B. Chilton"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2111.04920"
  },
  {
    "id": "arXiv:2111.07283",
    "title": "Intensity Mapping Functions For HDR Panorama Imaging: Weighted Histogram  Averaging",
    "abstract": "Comments: 11 pages, 5 figures",
    "descriptor": "\nComments: 11 pages, 5 figures\n",
    "authors": [
      "Yilun Xu",
      "Zhengguo Li",
      "Weihai Chen",
      "Changyun Wen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.07283"
  },
  {
    "id": "arXiv:2111.10541",
    "title": "Graph-augmented Learning to Rank for Querying Large-scale Knowledge  Graph",
    "abstract": "Comments: Accepted by AACL 2022",
    "descriptor": "\nComments: Accepted by AACL 2022\n",
    "authors": [
      "Hanning Gao",
      "Lingfei Wu",
      "Po Hu",
      "Zhihua Wei",
      "Fangli Xu",
      "Bo Long"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2111.10541"
  },
  {
    "id": "arXiv:2111.11154",
    "title": "Efficient formulation of a two-noded geometrically exact curved beam  element",
    "abstract": "Efficient formulation of a two-noded geometrically exact curved beam  element",
    "descriptor": "",
    "authors": [
      "Martin Hor\u00e1k",
      "Emma La Malfa Ribolla",
      "Milan Jir\u00e1sek"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2111.11154"
  },
  {
    "id": "arXiv:2112.11941",
    "title": "CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning  of Large Language Models",
    "abstract": "Comments: 10 pages including references, plus 5 pages appendix. Edits for version 3 vs LREC 2022: Point out human baseline in abstract (also to match arxiv abstract), fix affiliation apergo.ai, and fix a recurring typo",
    "descriptor": "\nComments: 10 pages including references, plus 5 pages appendix. Edits for version 3 vs LREC 2022: Point out human baseline in abstract (also to match arxiv abstract), fix affiliation apergo.ai, and fix a recurring typo\n",
    "authors": [
      "J\u00f6rg Frohberg",
      "Frank Binder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.11941"
  },
  {
    "id": "arXiv:2201.04329",
    "title": "Neural Residual Flow Fields for Efficient Video Representations",
    "abstract": "Comments: Accepted for ACCV 2022, codes are available at this https URL",
    "descriptor": "\nComments: Accepted for ACCV 2022, codes are available at this https URL\n",
    "authors": [
      "Daniel Rho",
      "Junwoo Cho",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.04329"
  },
  {
    "id": "arXiv:2201.09049",
    "title": "LTC-SUM: Lightweight Client-driven Personalized Video Summarization  Framework Using 2D CNN",
    "abstract": "Comments: 14",
    "descriptor": "\nComments: 14\n",
    "authors": [
      "Ghulam Mujtaba",
      "Adeel Malik",
      "Eun-Seok Ryu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.09049"
  },
  {
    "id": "arXiv:2201.09825",
    "title": "Supported Sets -- A New Foundation For Nominal Sets And Automata",
    "abstract": "Supported Sets -- A New Foundation For Nominal Sets And Automata",
    "descriptor": "",
    "authors": [
      "Thorsten Wi\u00dfmann"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2201.09825"
  },
  {
    "id": "arXiv:2201.11932",
    "title": "Deep Generative Model for Periodic Graphs",
    "abstract": "Comments: This paper has been accepted by NeurIPS 2022",
    "descriptor": "\nComments: This paper has been accepted by NeurIPS 2022\n",
    "authors": [
      "Shiyu Wang",
      "Xiaojie Guo",
      "Liang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11932"
  },
  {
    "id": "arXiv:2201.13329",
    "title": "Can Adversarial Training Be Manipulated By Non-Robust Features?",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Lue Tao",
      "Lei Feng",
      "Hongxin Wei",
      "Jinfeng Yi",
      "Sheng-Jun Huang",
      "Songcan Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2201.13329"
  },
  {
    "id": "arXiv:2202.03406",
    "title": "Dependence model assessment and selection with DecoupleNets",
    "abstract": "Dependence model assessment and selection with DecoupleNets",
    "descriptor": "",
    "authors": [
      "Marius Hofert",
      "Avinash Prasad",
      "Mu Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)",
      "Risk Management (q-fin.RM)",
      "Applications (stat.AP)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2202.03406"
  },
  {
    "id": "arXiv:2202.05189",
    "title": "Understanding Rare Spurious Correlations in Neural Networks",
    "abstract": "Understanding Rare Spurious Correlations in Neural Networks",
    "descriptor": "",
    "authors": [
      "Yao-Yuan Yang",
      "Chi-Ning Chou",
      "Kamalika Chaudhuri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.05189"
  },
  {
    "id": "arXiv:2202.05423",
    "title": "Understanding Curriculum Learning in Policy Optimization for Solving  Combinatorial Optimization Problems",
    "abstract": "Comments: 31 pages, 8 figures, 1 table",
    "descriptor": "\nComments: 31 pages, 8 figures, 1 table\n",
    "authors": [
      "Runlong Zhou",
      "Yuandong Tian",
      "Yi Wu",
      "Simon S. Du"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05423"
  },
  {
    "id": "arXiv:2202.05488",
    "title": "Fast Adversarial Training with Noise Augmentation: A Unified Perspective  on RandStart and GradAlign",
    "abstract": "Fast Adversarial Training with Noise Augmentation: A Unified Perspective  on RandStart and GradAlign",
    "descriptor": "",
    "authors": [
      "Axi Niu",
      "Kang Zhang",
      "Chaoning Zhang",
      "Chenshuang Zhang",
      "In So Kweon",
      "Chang D. Yoo",
      "Yanning Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.05488"
  },
  {
    "id": "arXiv:2202.05687",
    "title": "Towards Adversarially Robust Deepfake Detection: An Ensemble Approach",
    "abstract": "Towards Adversarially Robust Deepfake Detection: An Ensemble Approach",
    "descriptor": "",
    "authors": [
      "Ashish Hooda",
      "Neal Mangaokar",
      "Ryan Feng",
      "Kassem Fawaz",
      "Somesh Jha",
      "Atul Prakash"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.05687"
  },
  {
    "id": "arXiv:2202.06679",
    "title": "Liveness and Latency of Byzantine State-Machine Replication (Extended  Version)",
    "abstract": "Comments: Extended version of a paper from the 2022 Symposium on Distributed Computing (DISC'22)",
    "descriptor": "\nComments: Extended version of a paper from the 2022 Symposium on Distributed Computing (DISC'22)\n",
    "authors": [
      "Manuel Bravo",
      "Gregory Chockler",
      "Alexey Gotsman"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.06679"
  },
  {
    "id": "arXiv:2202.11124",
    "title": "Learning with Free Object Segments for Long-Tailed Instance Segmentation",
    "abstract": "Comments: Accepted to ECCV 2022",
    "descriptor": "\nComments: Accepted to ECCV 2022\n",
    "authors": [
      "Cheng Zhang",
      "Tai-Yu Pan",
      "Tianle Chen",
      "Jike Zhong",
      "Wenjin Fu",
      "Wei-Lun Chao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.11124"
  },
  {
    "id": "arXiv:2202.12570",
    "title": "Multi-Instance Causal Representation Learning for Instance Label  Prediction and Out-of-Distribution Generalization",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Weijia Zhang",
      "Xuanhui Zhang",
      "Han-Wen Deng",
      "Min-Ling Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.12570"
  },
  {
    "id": "arXiv:2203.00307",
    "title": "Temporal Perceiver: A General Architecture for Arbitrary Boundary  Detection",
    "abstract": "Temporal Perceiver: A General Architecture for Arbitrary Boundary  Detection",
    "descriptor": "",
    "authors": [
      "Jing Tan",
      "Yuhong Wang",
      "Gangshan Wu",
      "Limin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.00307"
  },
  {
    "id": "arXiv:2203.00763",
    "title": "Multi-Sentence Knowledge Selection in Open-Domain Dialogue",
    "abstract": "Comments: Accepted at INLG 2021. 11 pages, 5 tables, 8 figures",
    "descriptor": "\nComments: Accepted at INLG 2021. 11 pages, 5 tables, 8 figures\n",
    "authors": [
      "Mihail Eric",
      "Nicole Chartier",
      "Behnam Hedayatnia",
      "Karthik Gopalakrishnan",
      "Pankaj Rajan",
      "Yang Liu",
      "Dilek Hakkani-Tur"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.00763"
  },
  {
    "id": "arXiv:2203.01121",
    "title": "VaiPhy: a Variational Inference Based Algorithm for Phylogeny",
    "abstract": "Comments: NeurIPS-22 conference paper",
    "descriptor": "\nComments: NeurIPS-22 conference paper\n",
    "authors": [
      "Hazal Koptagel",
      "Oskar Kviman",
      "Harald Melin",
      "Negar Safinianaini",
      "Jens Lagergren"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2203.01121"
  },
  {
    "id": "arXiv:2203.01589",
    "title": "Reconfigurable Intelligent Surface Assisted OFDM Relaying: Subcarrier  Matching with Balanced SNR",
    "abstract": "Comments: Accepted by IEEE Transactions on Vehicular Technology",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Vehicular Technology\n",
    "authors": [
      "Tong Zhang",
      "Shuai Wang",
      "Yufan Zhuang",
      "Changsheng You",
      "Miaowen Wen",
      "Yik-Chung Wu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2203.01589"
  },
  {
    "id": "arXiv:2203.01631",
    "title": "Fully-Connected Network on Noncompact Symmetric Space and Ridgelet  Transform based on Helgason-Fourier Analysis",
    "abstract": "Comments: replaced with the published version (ICML2022)",
    "descriptor": "\nComments: replaced with the published version (ICML2022)\n",
    "authors": [
      "Sho Sonoda",
      "Isao Ishikawa",
      "Masahiro Ikeda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.01631"
  },
  {
    "id": "arXiv:2203.04306",
    "title": "Diffusion Models for Medical Anomaly Detection",
    "abstract": "Diffusion Models for Medical Anomaly Detection",
    "descriptor": "",
    "authors": [
      "Julia Wolleb",
      "Florentin Bieder",
      "Robin Sandk\u00fchler",
      "Philippe C. Cattin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.04306"
  },
  {
    "id": "arXiv:2203.04560",
    "title": "Training from a Better Start Point: Active Self-Semi-Supervised Learning  for Few Labeled Samples",
    "abstract": "Comments: 12 pages, 8 figures",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Ziting Wen",
      "Oscar Pizarro",
      "Stefan Williams"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.04560"
  },
  {
    "id": "arXiv:2203.06768",
    "title": "Probabilistically Robust Recourse: Navigating the Trade-offs between  Costs and Robustness in Algorithmic Recourse",
    "abstract": "Probabilistically Robust Recourse: Navigating the Trade-offs between  Costs and Robustness in Algorithmic Recourse",
    "descriptor": "",
    "authors": [
      "Martin Pawelczyk",
      "Teresa Datta",
      "Johannes van-den-Heuvel",
      "Gjergji Kasneci",
      "Himabindu Lakkaraju"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2203.06768"
  },
  {
    "id": "arXiv:2203.06832",
    "title": "Semi-Discrete Normalizing Flows through Differentiable Tessellation",
    "abstract": "Semi-Discrete Normalizing Flows through Differentiable Tessellation",
    "descriptor": "",
    "authors": [
      "Ricky T. Q. Chen",
      "Brandon Amos",
      "Maximilian Nickel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.06832"
  },
  {
    "id": "arXiv:2203.09174",
    "title": "Nearest Neighbor Classifier with Margin Penalty for Active Learning",
    "abstract": "Nearest Neighbor Classifier with Margin Penalty for Active Learning",
    "descriptor": "",
    "authors": [
      "Yuan Cao",
      "Zhiqiao Gao",
      "Jie Hu",
      "Mingchuan Yang",
      "Jinpeng Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.09174"
  },
  {
    "id": "arXiv:2203.10761",
    "title": "Decoupled Mixup for Data-efficient Learning",
    "abstract": "Comments: The preprint revision, 15 pages, 6 figures. The source code is available at this https URL",
    "descriptor": "\nComments: The preprint revision, 15 pages, 6 figures. The source code is available at this https URL\n",
    "authors": [
      "Zicheng Liu",
      "Siyuan Li",
      "Ge Wang",
      "Cheng Tan",
      "Lirong Wu",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.10761"
  },
  {
    "id": "arXiv:2203.11512",
    "title": "Gradient Vector Fields of Discrete Morse Functions and Watershed-cuts",
    "abstract": "Gradient Vector Fields of Discrete Morse Functions and Watershed-cuts",
    "descriptor": "",
    "authors": [
      "Nicolas Boutry",
      "Gilles Bertrand",
      "Laurent Najman"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Algebraic Topology (math.AT)",
      "Geometric Topology (math.GT)"
    ],
    "url": "https://arxiv.org/abs/2203.11512"
  },
  {
    "id": "arXiv:2203.11793",
    "title": "A Perspective on Neural Capacity Estimation: Viability and Reliability",
    "abstract": "Comments: 33 pages, 9 figures, under revison for possible journal publication",
    "descriptor": "\nComments: 33 pages, 9 figures, under revison for possible journal publication\n",
    "authors": [
      "Farhad Mirkarimi",
      "Stefano Rini",
      "Nariman Farsad"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2203.11793"
  },
  {
    "id": "arXiv:2203.11882",
    "title": "Linear-depth quantum circuits for multiqubit controlled gates",
    "abstract": "Linear-depth quantum circuits for multiqubit controlled gates",
    "descriptor": "",
    "authors": [
      "Adenilton J. da Silva",
      "Daniel K. Park"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2203.11882"
  },
  {
    "id": "arXiv:2203.11991",
    "title": "Joint Feature Learning and Relation Modeling for Tracking: A One-Stream  Framework",
    "abstract": "Comments: Accepted by ECCV 2022",
    "descriptor": "\nComments: Accepted by ECCV 2022\n",
    "authors": [
      "Botao Ye",
      "Hong Chang",
      "Bingpeng Ma",
      "Shiguang Shan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.11991"
  },
  {
    "id": "arXiv:2204.00352",
    "title": "On the Efficiency of Integrating Self-supervised Learning and  Meta-learning for User-defined Few-shot Keyword Spotting",
    "abstract": "Comments: Accepted by SLT 2022",
    "descriptor": "\nComments: Accepted by SLT 2022\n",
    "authors": [
      "Wei-Tsung Kao",
      "Yuan-Kuei Wu",
      "Chia-Ping Chen",
      "Zhi-Sheng Chen",
      "Yu-Pao Tsai",
      "Hung-Yi Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2204.00352"
  },
  {
    "id": "arXiv:2204.00673",
    "title": "Learnable latent embeddings for joint behavioral and neural analysis",
    "abstract": "Comments: Website: cebra.ai",
    "descriptor": "\nComments: Website: cebra.ai\n",
    "authors": [
      "Steffen Schneider",
      "Jin Hwa Lee",
      "Mackenzie Weygandt Mathis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2204.00673"
  },
  {
    "id": "arXiv:2204.02016",
    "title": "Existence, uniqueness and approximation of solutions to Carath\u00e9odory  delay differential equations",
    "abstract": "Existence, uniqueness and approximation of solutions to Carath\u00e9odory  delay differential equations",
    "descriptor": "",
    "authors": [
      "Fabio V. Difonzo",
      "Pawe\u0142 Przyby\u0142owicz",
      "Yue Wu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2204.02016"
  },
  {
    "id": "arXiv:2204.02311",
    "title": "PaLM: Scaling Language Modeling with Pathways",
    "abstract": "PaLM: Scaling Language Modeling with Pathways",
    "descriptor": "",
    "authors": [
      "Aakanksha Chowdhery",
      "Sharan Narang",
      "Jacob Devlin",
      "Maarten Bosma",
      "Gaurav Mishra",
      "Adam Roberts",
      "Paul Barham",
      "Hyung Won Chung",
      "Charles Sutton",
      "Sebastian Gehrmann",
      "Parker Schuh",
      "Kensen Shi",
      "Sasha Tsvyashchenko",
      "Joshua Maynez",
      "Abhishek Rao",
      "Parker Barnes",
      "Yi Tay",
      "Noam Shazeer",
      "Vinodkumar Prabhakaran",
      "Emily Reif",
      "Nan Du",
      "Ben Hutchinson",
      "Reiner Pope",
      "James Bradbury",
      "Jacob Austin",
      "Michael Isard",
      "Guy Gur-Ari",
      "Pengcheng Yin",
      "Toju Duke",
      "Anselm Levskaya",
      "Sanjay Ghemawat",
      "Sunipa Dev",
      "Henryk Michalewski",
      "Xavier Garcia",
      "Vedant Misra",
      "Kevin Robinson",
      "Liam Fedus",
      "Denny Zhou",
      "Daphne Ippolito",
      "David Luan",
      "Hyeontaek Lim",
      "Barret Zoph",
      "Alexander Spiridonov",
      "Ryan Sepassi",
      "David Dohan",
      "Shivani Agrawal",
      "Mark Omernick",
      "Andrew M. Dai",
      "Thanumalayan Sankaranarayana Pillai",
      "Marie Pellat",
      "Aitor Lewkowycz",
      "Erica Moreira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.02311"
  },
  {
    "id": "arXiv:2204.02643",
    "title": "Compositional pre-processing for automated reasoning in dependent type  theory",
    "abstract": "Compositional pre-processing for automated reasoning in dependent type  theory",
    "descriptor": "",
    "authors": [
      "Valentin Blot",
      "Denis Cousineau",
      "Enzo Crance",
      "Louise Dubois de Prisque",
      "Chantal Keller",
      "Assia Mahboubi",
      "Pierre Vial"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2204.02643"
  },
  {
    "id": "arXiv:2204.02765",
    "title": "Code Search: A Survey of Techniques for Finding Code",
    "abstract": "Code Search: A Survey of Techniques for Finding Code",
    "descriptor": "",
    "authors": [
      "Luca Di Grazia",
      "Michael Pradel"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2204.02765"
  },
  {
    "id": "arXiv:2205.00463",
    "title": "A Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction",
    "abstract": "A Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction",
    "descriptor": "",
    "authors": [
      "Qiaoqiao Ding",
      "Hui Ji",
      "Yuhui Quan",
      "Xiaoqun Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.00463"
  },
  {
    "id": "arXiv:2205.09332",
    "title": "Accelerated Training of Physics-Informed Neural Networks (PINNs) using  Meshless Discretizations",
    "abstract": "Comments: Accepted at the 36th Annual Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Accepted at the 36th Annual Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Ramansh Sharma",
      "Varun Shankar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.09332"
  },
  {
    "id": "arXiv:2205.10183",
    "title": "Prototypical Calibration for Few-shot Learning of Language Models",
    "abstract": "Prototypical Calibration for Few-shot Learning of Language Models",
    "descriptor": "",
    "authors": [
      "Zhixiong Han",
      "Yaru Hao",
      "Li Dong",
      "Yutao Sun",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10183"
  },
  {
    "id": "arXiv:2205.10671",
    "title": "Pessimism for Offline Linear Contextual Bandits using $\\ell_p$  Confidence Sets",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Gene Li",
      "Cong Ma",
      "Nathan Srebro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10671"
  },
  {
    "id": "arXiv:2205.11890",
    "title": "A Quadrature Rule combining Control Variates and Adaptive Importance  Sampling",
    "abstract": "A Quadrature Rule combining Control Variates and Adaptive Importance  Sampling",
    "descriptor": "",
    "authors": [
      "R\u00e9mi Leluc",
      "Fran\u00e7ois Portier",
      "Johan Segers",
      "Aigerim Zhuman"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2205.11890"
  },
  {
    "id": "arXiv:2205.12796",
    "title": "Non-rigid Point Cloud Registration with Neural Deformation Pyramid",
    "abstract": "Comments: NeurIPS'2022 camera ready. Code: this https URL",
    "descriptor": "\nComments: NeurIPS'2022 camera ready. Code: this https URL\n",
    "authors": [
      "Yang Li",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12796"
  },
  {
    "id": "arXiv:2205.14327",
    "title": "Efficient Policy Iteration for Robust Markov Decision Processes via  Regularization",
    "abstract": "Efficient Policy Iteration for Robust Markov Decision Processes via  Regularization",
    "descriptor": "",
    "authors": [
      "Navdeep Kumar",
      "Kfir Levy",
      "Kaixin Wang",
      "Shie Mannor"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14327"
  },
  {
    "id": "arXiv:2205.15674",
    "title": "Generalised Implicit Neural Representations",
    "abstract": "Comments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Daniele Grattarola",
      "Pierre Vandergheynst"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.15674"
  },
  {
    "id": "arXiv:2205.15856",
    "title": "coVariance Neural Networks",
    "abstract": "coVariance Neural Networks",
    "descriptor": "",
    "authors": [
      "Saurabh Sihag",
      "Gonzalo Mateos",
      "Corey McMillan",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15856"
  },
  {
    "id": "arXiv:2206.00152",
    "title": "Human-AI Shared Control via Policy Dissection",
    "abstract": "Human-AI Shared Control via Policy Dissection",
    "descriptor": "",
    "authors": [
      "Quanyi Li",
      "Zhenghao Peng",
      "Haibin Wu",
      "Lan Feng",
      "Bolei Zhou"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00152"
  },
  {
    "id": "arXiv:2206.01310",
    "title": "Learning a Restricted Boltzmann Machine using biased Monte Carlo  sampling",
    "abstract": "Comments: 24 pages, 12 figures",
    "descriptor": "\nComments: 24 pages, 12 figures\n",
    "authors": [
      "Nicolas B\u00e9reux",
      "Aur\u00e9lien Decelle",
      "Cyril Furtlehner",
      "Beatriz Seoane"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.01310"
  },
  {
    "id": "arXiv:2206.02574",
    "title": "On the duality between contrastive and non-contrastive self-supervised  learning",
    "abstract": "On the duality between contrastive and non-contrastive self-supervised  learning",
    "descriptor": "",
    "authors": [
      "Quentin Garrido",
      "Yubei Chen",
      "Adrien Bardes",
      "Laurent Najman",
      "Yann Lecun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.02574"
  },
  {
    "id": "arXiv:2206.02780",
    "title": "GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions",
    "abstract": "GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions",
    "descriptor": "",
    "authors": [
      "Gene Chou",
      "Ilya Chugunov",
      "Felix Heide"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02780"
  },
  {
    "id": "arXiv:2206.02873",
    "title": "No Parameter Left Behind: How Distillation and Model Size Affect  Zero-Shot Retrieval",
    "abstract": "No Parameter Left Behind: How Distillation and Model Size Affect  Zero-Shot Retrieval",
    "descriptor": "",
    "authors": [
      "Guilherme Moraes Rosa",
      "Luiz Bonifacio",
      "Vitor Jeronymo",
      "Hugo Abonizio",
      "Marzieh Fadaee",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Computation and Language (cs.CL)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2206.02873"
  },
  {
    "id": "arXiv:2206.03491",
    "title": "EiX-GNN : Concept-level eigencentrality explainer for graph neural  networks",
    "abstract": "EiX-GNN : Concept-level eigencentrality explainer for graph neural  networks",
    "descriptor": "",
    "authors": [
      "Adrien Raison",
      "Pascal Bourdon",
      "David Helbert"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03491"
  },
  {
    "id": "arXiv:2206.04183",
    "title": "High-order implicit time integration scheme with controllable numerical  dissipation based on mixed-order Pad\u00e9 expansions",
    "abstract": "Comments: 37 pages, 36 figures, 89 equations",
    "descriptor": "\nComments: 37 pages, 36 figures, 89 equations\n",
    "authors": [
      "Chongmin Song",
      "Xiaoran Zhang",
      "Sascha Eisentr\u00e4ger",
      "Ankit Ankit"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.04183"
  },
  {
    "id": "arXiv:2206.04767",
    "title": "A Programmatic Definition of Visualization Insights, Objectives, and  Tasks",
    "abstract": "A Programmatic Definition of Visualization Insights, Objectives, and  Tasks",
    "descriptor": "",
    "authors": [
      "Leilani Battle",
      "Alvitta Ottley"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2206.04767"
  },
  {
    "id": "arXiv:2206.05224",
    "title": "A Multi-Task Benchmark for Korean Legal Language Understanding and  Judgement Prediction",
    "abstract": "Comments: Accepted at NeurIPS 2022 Datasets and Benchmarks track",
    "descriptor": "\nComments: Accepted at NeurIPS 2022 Datasets and Benchmarks track\n",
    "authors": [
      "Wonseok Hwang",
      "Dongjun Lee",
      "Kyoungyeon Cho",
      "Hanuhl Lee",
      "Minjoon Seo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.05224"
  },
  {
    "id": "arXiv:2206.06424",
    "title": "Look, Radiate, and Learn: Self-supervised Localisation via Radio-Visual  Correspondence",
    "abstract": "Look, Radiate, and Learn: Self-supervised Localisation via Radio-Visual  Correspondence",
    "descriptor": "",
    "authors": [
      "Mohammed Alloulah",
      "Maximilian Arnold"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2206.06424"
  },
  {
    "id": "arXiv:2206.06615",
    "title": "MDS Codes with Euclidean and Hermitian Hulls of Flexible Dimensions and  Their Applications to EAQECCs",
    "abstract": "Comments: 25 pages, 5 tables",
    "descriptor": "\nComments: 25 pages, 5 tables\n",
    "authors": [
      "Yang Li",
      "Ruhao Wan",
      "Shixin Zhu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2206.06615"
  },
  {
    "id": "arXiv:2206.08916",
    "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
    "abstract": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
    "descriptor": "",
    "authors": [
      "Jiasen Lu",
      "Christopher Clark",
      "Rowan Zellers",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08916"
  },
  {
    "id": "arXiv:2206.09457",
    "title": "All you need is feedback: Communication with block attention feedback  codes",
    "abstract": "All you need is feedback: Communication with block attention feedback  codes",
    "descriptor": "",
    "authors": [
      "Emre Ozfatura",
      "Yulin Shao",
      "Alberto Perotti",
      "Branislav Popovic",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.09457"
  },
  {
    "id": "arXiv:2206.09674",
    "title": "EAGER: Asking and Answering Questions for Automatic Reward Shaping in  Language-guided RL",
    "abstract": "Comments: 24 pages, 16 figures, 5 tables",
    "descriptor": "\nComments: 24 pages, 16 figures, 5 tables\n",
    "authors": [
      "Thomas Carta",
      "Sylvain Lamprier",
      "Pierre-Yves Oudeyer",
      "Olivier Sigaud"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.09674"
  },
  {
    "id": "arXiv:2206.10207",
    "title": "SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Gang Li",
      "Heliang Zheng",
      "Daqing Liu",
      "Chaoyue Wang",
      "Bing Su",
      "Changwen Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.10207"
  },
  {
    "id": "arXiv:2206.10292",
    "title": "Artificial Neural Network evaluation of Poincar\u00e9 constant for Voronoi  polygons",
    "abstract": "Artificial Neural Network evaluation of Poincar\u00e9 constant for Voronoi  polygons",
    "descriptor": "",
    "authors": [
      "Beatrice Crippa",
      "Silvia Bertoluzza",
      "Micol Pennacchio"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2206.10292"
  },
  {
    "id": "arXiv:2206.13728",
    "title": "Boosting R-CNN: Reweighting R-CNN Samples by RPN's Error for Underwater  Object Detection",
    "abstract": "Boosting R-CNN: Reweighting R-CNN Samples by RPN's Error for Underwater  Object Detection",
    "descriptor": "",
    "authors": [
      "Pinhao Song",
      "Pengteng Li",
      "Linhui Dai",
      "Tao Wang",
      "Zhan Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.13728"
  },
  {
    "id": "arXiv:2207.00614",
    "title": "Integral Probability Metrics PAC-Bayes Bounds",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Ron Amit",
      "Baruch Epstein",
      "Shay Moran",
      "Ron Meir"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.00614"
  },
  {
    "id": "arXiv:2207.02910",
    "title": "Ant Hill Colonization optimization algorithm(AHCOA) for controlling the  side lobe of a uniform linear array",
    "abstract": "Comments: major modification in paper required, withdraw immediately",
    "descriptor": "\nComments: major modification in paper required, withdraw immediately\n",
    "authors": [
      "Sunit Shantanu Digamber Fulari",
      "Harbinder Singh"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2207.02910"
  },
  {
    "id": "arXiv:2207.04046",
    "title": "Optimal Pattern synthesis of linear antenna array using Ant Hill  Colonization Optimization algorithm(AHCOA)",
    "abstract": "Comments: major modification required, I want to add few major data and stuff",
    "descriptor": "\nComments: major modification required, I want to add few major data and stuff\n",
    "authors": [
      "Sunit Shantanu Digamber Fulari",
      "Harbinder Singh"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2207.04046"
  },
  {
    "id": "arXiv:2207.04994",
    "title": "Uncertainty-Aware Mixed-Variable Machine Learning for Materials Design",
    "abstract": "Uncertainty-Aware Mixed-Variable Machine Learning for Materials Design",
    "descriptor": "",
    "authors": [
      "Hengrui Zhang",
      "Wei Wayne Chen",
      "Akshay Iyer",
      "Daniel W. Apley",
      "Wei Chen"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.04994"
  },
  {
    "id": "arXiv:2207.06127",
    "title": "MM-ALT: A Multimodal Automatic Lyric Transcription System",
    "abstract": "Comments: Accepted by ACM Multimedia 2022. Camera ready version and correct some typos",
    "descriptor": "\nComments: Accepted by ACM Multimedia 2022. Camera ready version and correct some typos\n",
    "authors": [
      "Xiangming Gu",
      "Longshen Ou",
      "Danielle Ong",
      "Ye Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2207.06127"
  },
  {
    "id": "arXiv:2207.06131",
    "title": "Continual Meta-Reinforcement Learning for UAV-Aided Vehicular Wireless  Networks",
    "abstract": "Comments: submitted for conference publication",
    "descriptor": "\nComments: submitted for conference publication\n",
    "authors": [
      "Riccardo Marini",
      "Sangwoo Park",
      "Osvaldo Simeone",
      "Chiara Buratti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2207.06131"
  },
  {
    "id": "arXiv:2207.06229",
    "title": "Stochastic Functional Analysis and Multilevel Vector Field Anomaly  Detection",
    "abstract": "Stochastic Functional Analysis and Multilevel Vector Field Anomaly  Detection",
    "descriptor": "",
    "authors": [
      "Julio E Castrillon-Candas",
      "Mark Kon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Functional Analysis (math.FA)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2207.06229"
  },
  {
    "id": "arXiv:2207.06343",
    "title": "TCT: Convexifying Federated Learning using Bootstrapped Neural Tangent  Kernels",
    "abstract": "Comments: Accepted at Neural Information Processing Systems (NeurIPS) 2022. V2 releases code",
    "descriptor": "\nComments: Accepted at Neural Information Processing Systems (NeurIPS) 2022. V2 releases code\n",
    "authors": [
      "Yaodong Yu",
      "Alexander Wei",
      "Sai Praneeth Karimireddy",
      "Yi Ma",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.06343"
  },
  {
    "id": "arXiv:2207.06489",
    "title": "A Data-Efficient Deep Learning Framework for Segmentation and  Classification of Histopathology Images",
    "abstract": "Comments: Originally published at the ECCV 2022 Medical Computer Vision Workshop (ECCV-MCV 2022)",
    "descriptor": "\nComments: Originally published at the ECCV 2022 Medical Computer Vision Workshop (ECCV-MCV 2022)\n",
    "authors": [
      "Pranav Singh",
      "Jacopo Cirrone"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.06489"
  },
  {
    "id": "arXiv:2207.06926",
    "title": "Single Level Importance Sampling for McKean-Vlasov Stochastic  Differential Equation",
    "abstract": "Single Level Importance Sampling for McKean-Vlasov Stochastic  Differential Equation",
    "descriptor": "",
    "authors": [
      "Nadhir Ben Rached",
      "Abdul-Lateef Haji-Ali",
      "Shyam Mohan Subbiah Pillai",
      "Ra\u00fal Tempone"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2207.06926"
  },
  {
    "id": "arXiv:2207.07110",
    "title": "Fine-grained Few-shot Recognition by Deep Object Parsing",
    "abstract": "Fine-grained Few-shot Recognition by Deep Object Parsing",
    "descriptor": "",
    "authors": [
      "Ruizhao Zhu",
      "Pengkai Zhu",
      "Samarth Mishra",
      "Venkatesh Saligrama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.07110"
  },
  {
    "id": "arXiv:2207.08323",
    "title": "PlaneSDF-based Change Detection for Long-term Dense Mapping",
    "abstract": "Comments: 8 pages, 7 figures, and 1 table. To be published in Robotics and Automation Letters and IROS 2022. Link to supplementary video added in the abstract: this https URL",
    "descriptor": "\nComments: 8 pages, 7 figures, and 1 table. To be published in Robotics and Automation Letters and IROS 2022. Link to supplementary video added in the abstract: this https URL\n",
    "authors": [
      "Jiahui Fu",
      "Chengyuan Lin",
      "Yuichi Taguchi",
      "Andrea Cohen",
      "Yifu Zhang",
      "Stephen Mylabathula",
      "John J. Leonard"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.08323"
  },
  {
    "id": "arXiv:2207.12392",
    "title": "Self-Distilled Vision Transformer for Domain Generalization",
    "abstract": "Comments: 23 pages, 12 figures",
    "descriptor": "\nComments: 23 pages, 12 figures\n",
    "authors": [
      "Maryam Sultana",
      "Muzammal Naseer",
      "Muhammad Haris Khan",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.12392"
  },
  {
    "id": "arXiv:2208.00386",
    "title": "Robotic Dough Shaping",
    "abstract": "Comments: To be published in International Conference on Control, Automation and Systems (ICCAS), 2022",
    "descriptor": "\nComments: To be published in International Conference on Control, Automation and Systems (ICCAS), 2022\n",
    "authors": [
      "Jan Ondras",
      "Di Ni",
      "Xi Deng",
      "Zeqi Gu",
      "Henry Zheng",
      "Tapomayukh Bhattacharjee"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.00386"
  },
  {
    "id": "arXiv:2208.00789",
    "title": "Self-supervised learning with rotation-invariant kernels",
    "abstract": "Self-supervised learning with rotation-invariant kernels",
    "descriptor": "",
    "authors": [
      "L\u00e9on Zheng",
      "Gilles Puy",
      "Elisa Riccietti",
      "Patrick P\u00e9rez",
      "R\u00e9mi Gribonval"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.00789"
  },
  {
    "id": "arXiv:2208.01421",
    "title": "T4DT: Tensorizing Time for Learning Temporal 3D Visual Data",
    "abstract": "T4DT: Tensorizing Time for Learning Temporal 3D Visual Data",
    "descriptor": "",
    "authors": [
      "Mikhail Usvyatsov",
      "Rafael Ballester-Rippoll",
      "Lina Bashaeva",
      "Konrad Schindler",
      "Gonzalo Ferrer",
      "Ivan Oseledets"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.01421"
  },
  {
    "id": "arXiv:2208.01587",
    "title": "Learning to Incorporate Texture Saliency Adaptive Attention to Image  Cartoonization",
    "abstract": "Comments: Proceedings of the 39th International Conference on Machine Learning, PMLR 162:7183-7207, 2022",
    "descriptor": "\nComments: Proceedings of the 39th International Conference on Machine Learning, PMLR 162:7183-7207, 2022\n",
    "authors": [
      "Xiang Gao",
      "Yuqi Zhang",
      "Yingjie Tian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.01587"
  },
  {
    "id": "arXiv:2208.03225",
    "title": "Multilevel Importance Sampling for McKean-Vlasov Stochastic Differential  Equation",
    "abstract": "Comments: Follow-up to arXiv:2207.06926",
    "descriptor": "\nComments: Follow-up to arXiv:2207.06926\n",
    "authors": [
      "Nadhir Ben Rached",
      "Abdul-Lateef Haji-Ali",
      "Shyam Mohan Subbiah Pillai",
      "Ra\u00fal Tempone"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2208.03225"
  },
  {
    "id": "arXiv:2208.03998",
    "title": "I Learn to Diffuse, or Data Alchemy 101: a Mnemonic Manifesto",
    "abstract": "Comments: Submission to IEEE alt.vis 2022. Short paper containing a 4-page comic, and an immense amount of content in a linked miro board",
    "descriptor": "\nComments: Submission to IEEE alt.vis 2022. Short paper containing a 4-page comic, and an immense amount of content in a linked miro board\n",
    "authors": [
      "Victor Schetinger",
      "Velitchko Filipov",
      "Ignacio P\u00e9rez-Messina",
      "Ethan Smith",
      "Rodrigo Oliveira de Oliveira"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2208.03998"
  },
  {
    "id": "arXiv:2208.04842",
    "title": "AOC; Assembling Overlapping Communities",
    "abstract": "Comments: This version submitted to Quantitative Science Studies",
    "descriptor": "\nComments: This version submitted to Quantitative Science Studies\n",
    "authors": [
      "Akhil Jakatdar",
      "Baqiao Liu",
      "Tandy Warnow",
      "George Chacko"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2208.04842"
  },
  {
    "id": "arXiv:2208.05433",
    "title": "Detecting COVID-19 from digitized ECG printouts using 1D convolutional  neural networks",
    "abstract": "Comments: Accepted with minor revision by Plos One",
    "descriptor": "\nComments: Accepted with minor revision by Plos One\n",
    "authors": [
      "Thao Nguyen",
      "Hieu H. Pham",
      "Huy Khiem Le",
      "Anh Tu Nguyen",
      "Ngoc Tien Thanh",
      "Cuong Do"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.05433"
  },
  {
    "id": "arXiv:2208.07220",
    "title": "PatchDropout: Economizing Vision Transformers Using Patch Dropout",
    "abstract": "Comments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
    "descriptor": "\nComments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n",
    "authors": [
      "Yue Liu",
      "Christos Matsoukas",
      "Fredrik Strand",
      "Hossein Azizpour",
      "Kevin Smith"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.07220"
  },
  {
    "id": "arXiv:2208.07489",
    "title": "Single Round-trip Hierarchical ORAM via Succinct Indices",
    "abstract": "Comments: 22 pages, 3 Figures, 5 Tables",
    "descriptor": "\nComments: 22 pages, 3 Figures, 5 Tables\n",
    "authors": [
      "William Holland",
      "Olga Ohrimenko",
      "Anthony Wirth"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2208.07489"
  },
  {
    "id": "arXiv:2208.07541",
    "title": "Social Interactions for Autonomous Driving: A Review and Perspectives",
    "abstract": "Comments: 183 pages, 36 figures",
    "descriptor": "\nComments: 183 pages, 36 figures\n",
    "authors": [
      "Wenshuo Wang",
      "Letian Wang",
      "Chengyuan Zhang",
      "Changliu Liu",
      "Lijun Sun"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2208.07541"
  },
  {
    "id": "arXiv:2208.08797",
    "title": "Exploiting Sentiment and Common Sense for Zero-shot Stance Detection",
    "abstract": "Exploiting Sentiment and Common Sense for Zero-shot Stance Detection",
    "descriptor": "",
    "authors": [
      "Yun Luo",
      "Zihan Liu",
      "Yuefeng Shi",
      "Stan Z Li",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2208.08797"
  },
  {
    "id": "arXiv:2208.08910",
    "title": "Learned Indexing in Proteins: Extended Work on Substituting Complex  Distance Calculations with Embedding and Clustering Techniques",
    "abstract": "Comments: 14 pages, 7 figures, short version published in SISAP 2022 as a short paper",
    "descriptor": "\nComments: 14 pages, 7 figures, short version published in SISAP 2022 as a short paper\n",
    "authors": [
      "Jaroslav O\u013eha",
      "Ter\u00e9zia Slanin\u00e1kov\u00e1",
      "Martin Gendiar",
      "Matej Antol",
      "Vlastislav Dohnal"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.08910"
  },
  {
    "id": "arXiv:2208.09656",
    "title": "A Domain Generalization Approach for Out-Of-Distribution 12-lead ECG  Classification with Convolutional Neural Networks",
    "abstract": "Comments: This paper has been accepted at: IEEE BigDataService2022 (this http URL)",
    "descriptor": "\nComments: This paper has been accepted at: IEEE BigDataService2022 (this http URL)\n",
    "authors": [
      "Aristotelis Ballas",
      "Christos Diou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.09656"
  },
  {
    "id": "arXiv:2208.10847",
    "title": "Latent Variable Models in the Era of Industrial Big Data: Extension and  Beyond",
    "abstract": "Latent Variable Models in the Era of Industrial Big Data: Extension and  Beyond",
    "descriptor": "",
    "authors": [
      "Xiangyin Kong",
      "Xiaoyu Jiang",
      "Bingxin Zhang",
      "Jinsong Yuan",
      "Zhiqiang Ge"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.10847"
  },
  {
    "id": "arXiv:2208.13628",
    "title": "Efficient Vision-Language Pretraining with Visual Concepts and  Hierarchical Alignment",
    "abstract": "Comments: BMVC 2022",
    "descriptor": "\nComments: BMVC 2022\n",
    "authors": [
      "Mustafa Shukor",
      "Guillaume Couairon",
      "Matthieu Cord"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.13628"
  },
  {
    "id": "arXiv:2208.14137",
    "title": "On the Trade-Off between Actionable Explanations and the Right to be  Forgotten",
    "abstract": "On the Trade-Off between Actionable Explanations and the Right to be  Forgotten",
    "descriptor": "",
    "authors": [
      "Martin Pawelczyk",
      "Tobias Leemann",
      "Asia Biega",
      "Gjergji Kasneci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2208.14137"
  },
  {
    "id": "arXiv:2209.00507",
    "title": "A Dataset for Detecting Real-World Environmental Claims",
    "abstract": "A Dataset for Detecting Real-World Environmental Claims",
    "descriptor": "",
    "authors": [
      "Dominik Stammbach",
      "Nicolas Webersinke",
      "Julia Anna Bingler",
      "Mathias Kraus",
      "Markus Leippold"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.00507"
  },
  {
    "id": "arXiv:2209.04234",
    "title": "Retinal Image Restoration and Vessel Segmentation using Modified  Cycle-CBAM and CBAM-UNet",
    "abstract": "Comments: 6 pages, 7 figures, conference",
    "descriptor": "\nComments: 6 pages, 7 figures, conference\n",
    "authors": [
      "Alnur Alimanov",
      "Md Baharul Islam"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.04234"
  },
  {
    "id": "arXiv:2209.05040",
    "title": "SANCL: Multimodal Review Helpfulness Prediction with Selective Attention  and Natural Contrastive Learning",
    "abstract": "Comments: Accepted as a long paper at COLING 2022",
    "descriptor": "\nComments: Accepted as a long paper at COLING 2022\n",
    "authors": [
      "Wei Han",
      "Hui Chen",
      "Zhen Hai",
      "Soujanya Poria",
      "Lidong Bing"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.05040"
  },
  {
    "id": "arXiv:2209.05442",
    "title": "Soft Diffusion: Score Matching for General Corruptions",
    "abstract": "Comments: 21 pages, 12 figures, work in progress",
    "descriptor": "\nComments: 21 pages, 12 figures, work in progress\n",
    "authors": [
      "Giannis Daras",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Alexandros G. Dimakis",
      "Peyman Milanfar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.05442"
  },
  {
    "id": "arXiv:2209.06869",
    "title": "On the State of the Art in Authorship Attribution and Authorship  Verification",
    "abstract": "On the State of the Art in Authorship Attribution and Authorship  Verification",
    "descriptor": "",
    "authors": [
      "Jacob Tyo",
      "Bhuwan Dhingra",
      "Zachary C. Lipton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.06869"
  },
  {
    "id": "arXiv:2209.08966",
    "title": "Will It Blend? Mixing Training Paradigms & Prompting for Argument  Quality Prediction",
    "abstract": "Comments: Accepted at the 9th Workshop on Argument Mining (2022)",
    "descriptor": "\nComments: Accepted at the 9th Workshop on Argument Mining (2022)\n",
    "authors": [
      "Michiel van der Meer",
      "Myrthe Reuver",
      "Urja Khurana",
      "Lea Krause",
      "Selene B\u00e1ez Santamar\u00eda"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.08966"
  },
  {
    "id": "arXiv:2209.09194",
    "title": "Differentiable Frequency-based Disentanglement for Aerial Video Action  Recognition",
    "abstract": "Differentiable Frequency-based Disentanglement for Aerial Video Action  Recognition",
    "descriptor": "",
    "authors": [
      "Divya Kothandaraman",
      "Ming Lin",
      "Dinesh Manocha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.09194"
  },
  {
    "id": "arXiv:2209.09314",
    "title": "Nonlinear approximation spaces for inverse problems",
    "abstract": "Comments: 29 pages, 6 figures",
    "descriptor": "\nComments: 29 pages, 6 figures\n",
    "authors": [
      "Albert Cohen",
      "Matthieu Dolbeault",
      "Olga Mula",
      "Agustin Somacal"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.09314"
  },
  {
    "id": "arXiv:2209.10391",
    "title": "IoU-Enhanced Attention for End-to-End Task Specific Object Detection",
    "abstract": "Comments: ACCV2022",
    "descriptor": "\nComments: ACCV2022\n",
    "authors": [
      "Jing Zhao",
      "Shengjian Wu",
      "Li Sun",
      "Qingli Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.10391"
  },
  {
    "id": "arXiv:2209.13684",
    "title": "Online Search-based Collision-inclusive Motion Planning and Control for  Impact-resilient Mobile Robots",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2108.01802. text overlap with arXiv:1709.05401 by other authors",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2108.01802. text overlap with arXiv:1709.05401 by other authors\n",
    "authors": [
      "Zhouyu Lu",
      "Zhichao Liu",
      "Merrick Campbell",
      "Konstantinos Karydis"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.13684"
  },
  {
    "id": "arXiv:2209.13940",
    "title": "Multilingual Transitivity and Bidirectional Multilingual Agreement for  Multilingual Document-level Machine Translation",
    "abstract": "Multilingual Transitivity and Bidirectional Multilingual Agreement for  Multilingual Document-level Machine Translation",
    "descriptor": "",
    "authors": [
      "Hongyuan Lu",
      "Haoyang Huang",
      "Shuming Ma",
      "Dongdong Zhang",
      "Furu Wei",
      "Wai Lam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.13940"
  },
  {
    "id": "arXiv:2209.14170",
    "title": "Adjoint System in the Shooting Method to Solve Boundary Value Problems",
    "abstract": "Comments: 11 pages, 1 figure with 6 subfigures",
    "descriptor": "\nComments: 11 pages, 1 figure with 6 subfigures\n",
    "authors": [
      "Ernest Scheiber"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.14170"
  },
  {
    "id": "arXiv:2209.14558",
    "title": "Computational Complexity of Sub-Linear Convergent Algorithms",
    "abstract": "Comments: 8 Pages",
    "descriptor": "\nComments: 8 Pages\n",
    "authors": [
      "Hilal AlQuabeh",
      "Farha AlBreiki",
      "Dilshod Azizov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14558"
  },
  {
    "id": "arXiv:2209.14609",
    "title": "Dataset Distillation using Parameter Pruning",
    "abstract": "Dataset Distillation using Parameter Pruning",
    "descriptor": "",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14609"
  },
  {
    "id": "arXiv:2209.14649",
    "title": "Factor Graph Fusion of Raw GNSS Sensing with IMU and Lidar for Precise  Robot Localization without a Base Station",
    "abstract": "Comments: 7 pages, 4 figures, accompanying video: this https URL",
    "descriptor": "\nComments: 7 pages, 4 figures, accompanying video: this https URL\n",
    "authors": [
      "Jonas Beuchert",
      "Marco Camurri",
      "Maurice Fallon"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.14649"
  },
  {
    "id": "arXiv:2209.15055",
    "title": "Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear  Functions",
    "abstract": "Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear  Functions",
    "descriptor": "",
    "authors": [
      "Arthur Jacot"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15055"
  },
  {
    "id": "arXiv:2210.00037",
    "title": "A Complete Set of Connectivity-aware Local Topology Manipulation  Operations for Robot Swarms",
    "abstract": "A Complete Set of Connectivity-aware Local Topology Manipulation  Operations for Robot Swarms",
    "descriptor": "",
    "authors": [
      "Koresh Khateri",
      "Karthik Soma",
      "Mahdi Pourgholi",
      "Mohsen Montazeri",
      "Lorenzo Sabattini",
      "Giovanni Beltrame"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.00037"
  },
  {
    "id": "arXiv:2210.00146",
    "title": "FAST-LIO, Then Bayesian ICP, Then GTSFM",
    "abstract": "FAST-LIO, Then Bayesian ICP, Then GTSFM",
    "descriptor": "",
    "authors": [
      "Jerred Chen",
      "Xiangcheng Hu",
      "Shicong Ma",
      "Jianhao Jiao",
      "Ming Liu",
      "Frank Dellaert"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.00146"
  },
  {
    "id": "arXiv:2210.00465",
    "title": "Assessing the impact of contextual information in hate speech detection",
    "abstract": "Assessing the impact of contextual information in hate speech detection",
    "descriptor": "",
    "authors": [
      "Juan Manuel P\u00e9rez",
      "Franco Luque",
      "Demian Zayat",
      "Mart\u00edn Kondratzky",
      "Agust\u00edn Moro",
      "Pablo Serrati",
      "Joaqu\u00edn Zajac",
      "Paula Miguel",
      "Natalia Debandi",
      "Agust\u00edn Gravano",
      "Viviana Cotik"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.00465"
  },
  {
    "id": "arXiv:2210.00581",
    "title": "PrivTrace: Differentially Private Trajectory Synthesis by Adaptive  Markov Model",
    "abstract": "Comments: To Appear in 2023 USENIX Security Symposium, August 9-11, 2023. Please cite our USENIX Security version",
    "descriptor": "\nComments: To Appear in 2023 USENIX Security Symposium, August 9-11, 2023. Please cite our USENIX Security version\n",
    "authors": [
      "Haiming Wang",
      "Zhikun Zhang",
      "Tianhao Wang",
      "Shibo He",
      "Michael Backes",
      "Jiming Chen",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.00581"
  },
  {
    "id": "arXiv:2210.00597",
    "title": "Composition of Differential Privacy & Privacy Amplification by  Subsampling",
    "abstract": "Composition of Differential Privacy & Privacy Amplification by  Subsampling",
    "descriptor": "",
    "authors": [
      "Thomas Steinke"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.00597"
  },
  {
    "id": "arXiv:2210.00721",
    "title": "Efficient acoustic feature transformation in mismatched environments  using a Guided-GAN",
    "abstract": "Comments: Final published version available at: Efficient acoustic feature transformation in mismatched environments using a Guided-GAN. Speech Communication, 143, pp.10-20",
    "descriptor": "\nComments: Final published version available at: Efficient acoustic feature transformation in mismatched environments using a Guided-GAN. Speech Communication, 143, pp.10-20\n",
    "authors": [
      "Walter Heymans",
      "Marelie H. Davel",
      "Charl van Heerden"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.00721"
  },
  {
    "id": "arXiv:2210.00737",
    "title": "FedDig: Robust Federated Learning Using Data Digest to Represent Absent  Clients",
    "abstract": "FedDig: Robust Federated Learning Using Data Digest to Represent Absent  Clients",
    "descriptor": "",
    "authors": [
      "Chih-Fan Hsu",
      "Ming-Ching Chang",
      "Wei-Chao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.00737"
  },
  {
    "id": "arXiv:2210.01274",
    "title": "Random Weight Factorization Improves the Training of Continuous Neural  Representations",
    "abstract": "Comments: 33 pages, 21 figures, 12 tables",
    "descriptor": "\nComments: 33 pages, 21 figures, 12 tables\n",
    "authors": [
      "Sifan Wang",
      "Hanwen Wang",
      "Jacob H. Seidman",
      "Paris Perdikaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01274"
  },
  {
    "id": "arXiv:2210.01351",
    "title": "Less is More: Task-aware Layer-wise Distillation for Language Model  Compression",
    "abstract": "Less is More: Task-aware Layer-wise Distillation for Language Model  Compression",
    "descriptor": "",
    "authors": [
      "Chen Liang",
      "Simiao Zuo",
      "Qingru Zhang",
      "Pengcheng He",
      "Weizhu Chen",
      "Tuo Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.01351"
  },
  {
    "id": "arXiv:2210.01353",
    "title": "Pay Self-Attention to Audio-Visual Navigation",
    "abstract": "Comments: Main paper (10 pages and 7 figures) and appendix (21 figures and 4 tables). Accepted for publication by BMVC 2022. For data and code, see this https URL",
    "descriptor": "\nComments: Main paper (10 pages and 7 figures) and appendix (21 figures and 4 tables). Accepted for publication by BMVC 2022. For data and code, see this https URL\n",
    "authors": [
      "Yinfeng Yu",
      "Lele Cao",
      "Fuchun Sun",
      "Xiaohong Liu",
      "Liejun Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.01353"
  },
  {
    "id": "arXiv:2210.01448",
    "title": "Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with  Hierarchical Neural Embeddings",
    "abstract": "Comments: SIGGRAPH Asia 2022 (Journal Track); Project Page: this https URL",
    "descriptor": "\nComments: SIGGRAPH Asia 2022 (Journal Track); Project Page: this https URL\n",
    "authors": [
      "Tenglong Ao",
      "Qingzhe Gao",
      "Yuke Lou",
      "Baoquan Chen",
      "Libin Liu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Graphics (cs.GR)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.01448"
  },
  {
    "id": "arXiv:2210.01597",
    "title": "ROAD-R: The Autonomous Driving Dataset with Logical Requirements",
    "abstract": "ROAD-R: The Autonomous Driving Dataset with Logical Requirements",
    "descriptor": "",
    "authors": [
      "Eleonora Giunchiglia",
      "Mihaela C\u0103t\u0103lina Stoian",
      "Salman Khan",
      "Fabio Cuzzolin",
      "Thomas Lukasiewicz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.01597"
  },
  {
    "id": "arXiv:2210.01699",
    "title": "Robust feedback stabilization of interacting multi-agent systems under  uncertainty",
    "abstract": "Comments: 27 pages, 10 figures",
    "descriptor": "\nComments: 27 pages, 10 figures\n",
    "authors": [
      "Giacomo Albi",
      "Michael Herty",
      "Chiara Segala"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.01699"
  },
  {
    "id": "arXiv:2210.01719",
    "title": "Learning the Spectrogram Temporal Resolution for Audio Classification",
    "abstract": "Comments: Under review. Code open-sourced at this https URL",
    "descriptor": "\nComments: Under review. Code open-sourced at this https URL\n",
    "authors": [
      "Haohe Liu",
      "Xubo Liu",
      "Qiuqiang Kong",
      "Wenwu Wang",
      "Mark D. Plumbley"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.01719"
  },
  {
    "id": "arXiv:2210.01736",
    "title": "Using Entropy Measures for Monitoring the Evolution of Activity Patterns",
    "abstract": "Using Entropy Measures for Monitoring the Evolution of Activity Patterns",
    "descriptor": "",
    "authors": [
      "Yushan Huang",
      "Yuchen Zhao",
      "Hamed Haddadi",
      "Payam Barnaghi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.01736"
  }
]