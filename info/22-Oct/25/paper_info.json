[
  {
    "id": "arXiv:2210.12154",
    "title": "Use of BNNM for interference wave solutions of the gBS-like equation and  comparison with PINNs",
    "abstract": "In this work, the generalized broken soliton-like (gBS-like) equation is\nderived through the generalized bilinear method. The neural network model,\nwhich can fit the explicit solution with zero error, is found. The interference\nwave solution of the gBS-like equation is obtained by using the bilinear neural\nnetwork method (BNNM) and physical informed neural networks (PINNs).\nInterference waves are shown well via three-dimensional plots and density\nplots. Compared with PINNs, the bilinear neural network method is not only more\naccurate but also faster.",
    "descriptor": "",
    "authors": [
      "Shashank Reddy Vadyala",
      "Sai Nethra Betgeri"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12154"
  },
  {
    "id": "arXiv:2210.12155",
    "title": "Non-Functional Testing of Runtime Enforcers in Android",
    "abstract": "Runtime enforcers can be used to ensure that running applications satisfy\ndesired correctness properties. Although runtime enforcers that are\ncorrect-by-construction with respect to abstract behavioral models are\nrelatively easy to specify, the concrete software enforcers generated from\nthese specifications may easily introduce issues in the target application.\nIndeed developers can generate test suites to verify the functional behavior of\nthe enforcers, for instance exploiting the same models used to specify them.\nHowever, it remains challenging and tedious to verify the behavior of enforcers\nin terms of non-functional performance characteristics. This paper describes a\npractical approach to reveal runtime enforcers that may introduce\ninefficiencies in the target application. The approach relies on a combination\nof automatic test generation and runtime monitoring of multiple key performance\nindicators. We designed our approach to reveal issues in four indicators for\nmobile systems: responsiveness, launch time, memory, and energy consumption.\nExperimental results show that our approach can detect performance issues that\nmight be introduced by automatically generated enforcers.",
    "descriptor": "\nComments: paper accepted at the 11th International Symposium On Leveraging Applications of Formal Methods, Verification and Validation (ISoLA 2022). arXiv admin note: text overlap with arXiv:2010.04258\n",
    "authors": [
      "Oliviero Riganelli",
      "Daniela Micucci",
      "Leonardo Mariani"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2210.12155"
  },
  {
    "id": "arXiv:2210.12156",
    "title": "Improving Medical Predictions by Irregular Multimodal Electronic Health  Records Modeling",
    "abstract": "Health conditions among patients in intensive care units (ICUs) are monitored\nvia electronic health records (EHRs), composed of numerical time series and\nlengthy clinical note sequences, both taken at irregular time intervals.\nDealing with such irregularity in every modality, and integrating irregularity\ninto multimodal representations to improve medical predictions, is a\nchallenging problem. In this paper, we address this problem by (1) modeling\nirregular time series by incorporating hand-crafted imputation embeddings into\nlearned interpolation embeddings via a gating mechanism; (2) casting a series\nof clinical note representations as multivariate irregular time series and\ntackling irregularity via a time attention mechanism; and (3) fusing\nmultimodalities with an interleaved attention mechanism across temporal steps\nto integrate irregularity into multimodal representations. To the best of our\nknowledge, this is the first work to thoroughly model irregularity in\nmultimodalities and to take into account temporal knowledge in multimodal\nfusion, for improving medical predictions. The results on two medical\nprediction tasks show that our proposed methods outperform the state-of-the-art\n(SOTA) methods in both every single modality and multimodal fusion scenarios,\nillustrating the effectiveness of our methods and the value of modeling\nirregularity in multimodal fusion.",
    "descriptor": "",
    "authors": [
      "Xinlu Zhang",
      "Shiyang Li",
      "Zhiyu Chen",
      "Xifeng Yan",
      "Linda Petzold"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12156"
  },
  {
    "id": "arXiv:2210.12157",
    "title": "Error-Covariance Analysis of Monocular Pose Estimation Using Total Least  Squares",
    "abstract": "This study presents a theoretical structure for the monocular pose estimation\nproblem using the total least squares. The unit-vector line-of-sight\nobservations of the features are extracted from the monocular camera images.\nFirst, the optimization framework is formulated for the pose estimation problem\nwith observation vectors extracted from unit vectors from the camera\ncenter-of-projection, pointing towards the image features. The attitude and\nposition solutions obtained via the derived optimization framework are proven\nto reach the Cram\\'er-Rao lower bound under the small angle approximation of\nthe attitude errors. Specifically, The Fisher Information Matrix and the\nCram\\'er-Rao bounds are evaluated and compared to the analytical derivations of\nthe error-covariance expressions to rigorously prove the optimality of the\nestimates. The sensor data for the measurement model is provided through a\nseries of vector observations, and two fully populated noise-covariance\nmatrices are assumed for the body and reference observation data. The inverse\nof the former matrices appear in terms of a series of weight matrices in the\ncost function. The proposed solution is simulated in a Monte-Carlo framework\nwith 10,000 samples to validate the error-covariance analysis.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2106.11522. text overlap with arXiv:2210.11697\n",
    "authors": [
      "Saeed Maleki",
      "John Crassidis",
      "Yang Cheng",
      "Matthias Schmid"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.12157"
  },
  {
    "id": "arXiv:2210.12160",
    "title": "On the connection between Bregman divergence and value in regularized  Markov decision processes",
    "abstract": "In this short note we derive a relationship between the Bregman divergence\nfrom the current policy to the optimal policy and the suboptimality of the\ncurrent value function in a regularized Markov decision process. This result\nhas implications for multi-task reinforcement learning, offline reinforcement\nlearning, and regret analysis under function approximation, among others.",
    "descriptor": "",
    "authors": [
      "Brendan O'Donoghue"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12160"
  },
  {
    "id": "arXiv:2210.12169",
    "title": "Joint Coreference Resolution for Zeros and non-Zeros in Arabic",
    "abstract": "Most existing proposals about anaphoric zero pronoun (AZP) resolution regard\nfull mention coreference and AZP resolution as two independent tasks, even\nthough the two tasks are clearly related. The main issues that need tackling to\ndevelop a joint model for zero and non-zero mentions are the difference between\nthe two types of arguments (zero pronouns, being null, provide no nominal\ninformation) and the lack of annotated datasets of a suitable size in which\nboth types of arguments are annotated for languages other than Chinese and\nJapanese. In this paper, we introduce two architectures for jointly resolving\nAZPs and non-AZPs, and evaluate them on Arabic, a language for which, as far as\nwe know, there has been no prior work on joint resolution. Doing this also\nrequired creating a new version of the Arabic subset of the standard\ncoreference resolution dataset used for the CoNLL-2012 shared task (Pradhan et\nal.,2012) in which both zeros and non-zeros are included in a single dataset.",
    "descriptor": "",
    "authors": [
      "Abdulrahman Aloraini",
      "Sameer Pradhan",
      "Massimo Poesio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12169"
  },
  {
    "id": "arXiv:2210.12170",
    "title": "Discovering Differences in the Representation of People using  Contextualized Semantic Axes",
    "abstract": "A common paradigm for identifying semantic differences across social and\ntemporal contexts is the use of static word embeddings and their distances. In\nparticular, past work has compared embeddings against \"semantic axes\" that\nrepresent two opposing concepts. We extend this paradigm to BERT embeddings,\nand construct contextualized axes that mitigate the pitfall where antonyms have\nneighboring representations. We validate and demonstrate these axes on two\npeople-centric datasets: occupations from Wikipedia, and multi-platform\ndiscussions in extremist, men's communities over fourteen years. In both\nstudies, contextualized semantic axes can characterize differences among\ninstances of the same word type. In the latter study, we show that references\nto women and the contexts around them have become more detestable over time.",
    "descriptor": "\nComments: 10 pages, 6 figures, EMNLP 2022\n",
    "authors": [
      "Li Lucy",
      "Divya Tadimeti",
      "David Bamman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.12170"
  },
  {
    "id": "arXiv:2210.12177",
    "title": "An unsupervised latent/output physics-informed convolutional-LSTM  network for solving partial differential equations using peridynamic  differential operator",
    "abstract": "This study presents a novel unsupervised convolutional Neural Network (NN)\narchitecture with nonlocal interactions for solving Partial Differential\nEquations (PDEs). The nonlocal Peridynamic Differential Operator (PDDO) is\nemployed as a convolutional filter for evaluating derivatives the field\nvariable. The NN captures the time-dynamics in smaller latent space through\nencoder-decoder layers with a Convolutional Long-short Term Memory (ConvLSTM)\nlayer between them. The ConvLSTM architecture is modified by employing a novel\nactivation function to improve the predictive capability of the learning\narchitecture for physics with periodic behavior. The physics is invoked in the\nform of governing equations at the output of the NN and in the latent (reduced)\nspace. By considering a few benchmark PDEs, we demonstrate the training\nperformance and extrapolation capability of this novel NN architecture by\ncomparing against Physics Informed Neural Networks (PINN) type solvers. It is\nmore capable of extrapolating the solution for future timesteps than the other\nexisting architectures.",
    "descriptor": "\nComments: 23 pages, 15 figures, submitted to Computer Methods in Applied Mechanics and Engineering\n",
    "authors": [
      "A. Mavi",
      "A.C. Bekar",
      "E. Haghighat",
      "E. Madenci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12177"
  },
  {
    "id": "arXiv:2210.12179",
    "title": "The Dark Side of AutoML: Towards Architectural Backdoor Search",
    "abstract": "This paper asks the intriguing question: is it possible to exploit neural\narchitecture search (NAS) as a new attack vector to launch previously\nimprobable attacks? Specifically, we present EVAS, a new attack that leverages\nNAS to find neural architectures with inherent backdoors and exploits such\nvulnerability using input-aware triggers. Compared with existing attacks, EVAS\ndemonstrates many interesting properties: (i) it does not require polluting\ntraining data or perturbing model parameters; (ii) it is agnostic to downstream\nfine-tuning or even re-training from scratch; (iii) it naturally evades\ndefenses that rely on inspecting model parameters or training data. With\nextensive evaluation on benchmark datasets, we show that EVAS features high\nevasiveness, transferability, and robustness, thereby expanding the adversary's\ndesign spectrum. We further characterize the mechanisms underlying EVAS, which\nare possibly explainable by architecture-level ``shortcuts'' that recognize\ntrigger patterns. This work raises concerns about the current practice of NAS\nand points to potential directions to develop effective countermeasures.",
    "descriptor": "",
    "authors": [
      "Ren Pang",
      "Changjiang Li",
      "Zhaohan Xi",
      "Shouling Ji",
      "Ting Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12179"
  },
  {
    "id": "arXiv:2210.12181",
    "title": "Urban Socio-Technical Systems: An Autonomy and Mobility Perspective",
    "abstract": "The future of the human race is urban. The world's population is projected to\ngrow an additional 2.5 billion by 2050, with all expected to live in urban\nareas. This will increase the percentage of urban population from 55% today to\n70% within three decades and further strengthen the role of cities as the hub\nfor information, transportation, and overall socio-economic development. Unlike\nany other time in human history, the increasing levels of autonomy and machine\nintelligence are transforming cities to be no longer just human agglomerations\nbut a fusion of humans, machines, and algorithms making collective decisions,\nthus complex socio-technical systems. This manuscript summarizes and discusses\nmy efforts from the urban autonomy and mobility perspective to develop the\nurban socio-technical system.",
    "descriptor": "",
    "authors": [
      "Weizi Li"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.12181"
  },
  {
    "id": "arXiv:2210.12184",
    "title": "A New Perspective for Understanding Generalization Gap of Deep Neural  Networks Trained with Large Batch Sizes",
    "abstract": "Deep neural networks (DNNs) are typically optimized using various forms of\nmini-batch gradient descent algorithm. A major motivation for mini-batch\ngradient descent is that with a suitably chosen batch size, available computing\nresources can be optimally utilized (including parallelization) for fast model\ntraining. However, many works report the progressive loss of model\ngeneralization when the training batch size is increased beyond some limits.\nThis is a scenario commonly referred to as generalization gap. Although several\nworks have proposed different methods for alleviating the generalization gap\nproblem, a unanimous account for understanding generalization gap is still\nlacking in the literature. This is especially important given that recent works\nhave observed that several proposed solutions for generalization gap problem\nsuch learning rate scaling and increased training budget do not indeed resolve\nit. As such, our main exposition in this paper is to investigate and provide\nnew perspectives for the source of generalization loss for DNNs trained with a\nlarge batch size. Our analysis suggests that large training batch size results\nin increased near-rank loss of units' activation (i.e. output) tensors, which\nconsequently impacts model optimization and generalization. Extensive\nexperiments are performed for validation on popular DNN models such as VGG-16,\nresidual network (ResNet-56) and LeNet-5 using CIFAR-10, CIFAR-100,\nFashion-MNIST and MNIST datasets.",
    "descriptor": "",
    "authors": [
      "Oyebade K. Oyedotun",
      "Konstantinos Papadopoulos",
      "Djamila Aouada"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12184"
  },
  {
    "id": "arXiv:2210.12185",
    "title": "Attention-Based Scattering Network for Satellite Imagery",
    "abstract": "Multi-channel satellite imagery, from stacked spectral bands or\nspatiotemporal data, have meaningful representations for various atmospheric\nproperties. Combining these features in an effective manner to create a\nperformant and trustworthy model is of utmost importance to forecasters. Neural\nnetworks show promise, yet suffer from unintuitive computations, fusion of\nhigh-level features, and may be limited by the quantity of available data. In\nthis work, we leverage the scattering transform to extract high-level features\nwithout additional trainable parameters and introduce a separation scheme to\nbring attention to independent input channels. Experiments show promising\nresults on estimating tropical cyclone intensity and predicting the occurrence\nof lightning from satellite imagery.",
    "descriptor": "\nComments: NeurIPS 2022 Workshop - Tackling Climate Change with Machine Learning, 4 page limit w/ appendix\n",
    "authors": [
      "Jason Stock",
      "Chuck Anderson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12185"
  },
  {
    "id": "arXiv:2210.12186",
    "title": "Improving the Factual Correctness of Radiology Report Generation with  Semantic Rewards",
    "abstract": "Neural image-to-text radiology report generation systems offer the potential\nto improve radiology reporting by reducing the repetitive process of report\ndrafting and identifying possible medical errors. These systems have achieved\npromising performance as measured by widely used NLG metrics such as BLEU and\nCIDEr. However, the current systems face important limitations. First, they\npresent an increased complexity in architecture that offers only marginal\nimprovements on NLG metrics. Secondly, these systems that achieve high\nperformance on these metrics are not always factually complete or consistent\ndue to both inadequate training and evaluation. Recent studies have shown the\nsystems can be substantially improved by using new methods encouraging 1) the\ngeneration of domain entities consistent with the reference and 2) describing\nthese entities in inferentially consistent ways. So far, these methods rely on\nweakly-supervised approaches (rule-based) and named entity recognition systems\nthat are not specific to the chest X-ray domain. To overcome this limitation,\nwe propose a new method, the RadGraph reward, to further improve the factual\ncompleteness and correctness of generated radiology reports. More precisely, we\nleverage the RadGraph dataset containing annotated chest X-ray reports with\nentities and relations between entities. On two open radiology report datasets,\nour system substantially improves the scores up to 14.2% and 25.3% on metrics\nevaluating the factual correctness and completeness of reports.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Jean-Benoit Delbrouck",
      "Pierre Chambon",
      "Christian Bluethgen",
      "Emily Tsai",
      "Omar Almusa",
      "Curtis P. Langlotz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12186"
  },
  {
    "id": "arXiv:2210.12187",
    "title": "Syntactic Surprisal From Neural Models Predicts, But Underestimates,  Human Processing Difficulty From Syntactic Ambiguities",
    "abstract": "Humans exhibit garden path effects: When reading sentences that are\ntemporarily structurally ambiguous, they slow down when the structure is\ndisambiguated in favor of the less preferred alternative. Surprisal theory\n(Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes\nthat these slowdowns are due to the unpredictability of each of the words that\noccur in these sentences. Challenging this hypothesis, van Schijndel & Linzen\n(2021) find that estimates of the cost of word predictability derived from\nlanguage models severely underestimate the magnitude of human garden path\neffects. In this work, we consider whether this underestimation is due to the\nfact that humans weight syntactic factors in their predictions more highly than\nlanguage models do. We propose a method for estimating syntactic predictability\nfrom a language model, allowing us to weigh the cost of lexical and syntactic\npredictability independently. We find that treating syntactic predictability\nindependently from lexical predictability indeed results in larger estimates of\ngarden path. At the same time, even when syntactic predictability is\nindependently weighted, surprisal still greatly underestimate the magnitude of\nhuman garden path effects. Our results support the hypothesis that\npredictability is not the only factor responsible for the processing cost\nassociated with garden path sentences.",
    "descriptor": "\nComments: 13 pages (4 references + appendix), 6 figures. To appear in the proceedings of the 2022 SIGNLL Conference on Computational Natural Language Learning\n",
    "authors": [
      "Suhas Arehalli",
      "Brian Dillon",
      "Tal Linzen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12187"
  },
  {
    "id": "arXiv:2210.12192",
    "title": "Conditional Diffusion with Less Explicit Guidance via Model Predictive  Control",
    "abstract": "How much explicit guidance is necessary for conditional diffusion? We\nconsider the problem of conditional sampling using an unconditional diffusion\nmodel and limited explicit guidance (e.g., a noised classifier, or a\nconditional diffusion model) that is restricted to a small number of time\nsteps. We explore a model predictive control (MPC)-like approach to approximate\nguidance by simulating unconditional diffusion forward, and backpropagating\nexplicit guidance feedback. MPC-approximated guides have high cosine similarity\nto real guides, even over large simulation distances. Adding MPC steps improves\ngenerative quality when explicit guidance is limited to five time steps.",
    "descriptor": "",
    "authors": [
      "Max W. Shen",
      "Ehsan Hajiramezanali",
      "Gabriele Scalia",
      "Alex Tseng",
      "Nathaniel Diamant",
      "Tommaso Biancalani",
      "Andreas Loukas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12192"
  },
  {
    "id": "arXiv:2210.12193",
    "title": "A Trainable Sequence Learner that Learns and Recognizes Two-Input  Sequence Patterns",
    "abstract": "We present two designs for an analog circuit that can learn to detect a\ntemporal sequence of two inputs. The training phase is done by feeding the\ncircuit with the desired sequence and, after the training is completed, each\ntime the trained sequence is encountered again the circuit will emit a signal\nof correct recognition. Sequences are in the order of tens of nanoseconds. The\nfirst design can reset the trained sequence on runtime but assumes very strict\ntiming of the inputs. The second design can only be trained once but is lenient\nin the input's timing.",
    "descriptor": "\nComments: Submitted to IEEE TENCON 2022\n",
    "authors": [
      "Jan Hohenheim",
      "Zhaoyu Devon Liu",
      "Tommaso Stecconi",
      "Pietro Palopoli"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.12193"
  },
  {
    "id": "arXiv:2210.12195",
    "title": "Just Mix Once: Worst-group Generalization by Group Interpolation",
    "abstract": "Advances in deep learning theory have revealed how average generalization\nrelies on superficial patterns in data. The consequences are brittle models\nwith poor performance with shift in group distribution at test time. When group\nannotation is available, we can use robust optimization tools to tackle the\nproblem. However, identification and annotation are time-consuming, especially\non large datasets. A recent line of work leverages self-supervision and\noversampling to improve generalization on minority groups without group\nannotation. We propose to unify and generalize these approaches using a\nclass-conditional variant of mixup tailored for worst-group generalization. Our\napproach, Just Mix Once (JM1), interpolates samples during learning, augmenting\nthe training distribution with a continuous mixture of groups. JM1 is domain\nagnostic and computationally efficient, can be used with any level of group\nannotation, and performs on par or better than the state-of-the-art on\nworst-group generalization. Additionally, we provide a simple explanation of\nwhy JM1 works.",
    "descriptor": "\nComments: preprint\n",
    "authors": [
      "Giorgio Giannone",
      "Serhii Havrylov",
      "Jordan Massiah",
      "Emine Yilmaz",
      "Yunlong Jiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12195"
  },
  {
    "id": "arXiv:2210.12196",
    "title": "Augmentation by Counterfactual Explanation -- Fixing an Overconfident  Classifier",
    "abstract": "A highly accurate but overconfident model is ill-suited for deployment in\ncritical applications such as healthcare and autonomous driving. The\nclassification outcome should reflect a high uncertainty on ambiguous\nin-distribution samples that lie close to the decision boundary. The model\nshould also refrain from making overconfident decisions on samples that lie far\noutside its training distribution, far-out-of-distribution (far-OOD), or on\nunseen samples from novel classes that lie near its training distribution\n(near-OOD). This paper proposes an application of counterfactual explanations\nin fixing an over-confident classifier. Specifically, we propose to fine-tune a\ngiven pre-trained classifier using augmentations from a counterfactual\nexplainer (ACE) to fix its uncertainty characteristics while retaining its\npredictive performance. We perform extensive experiments with detecting\nfar-OOD, near-OOD, and ambiguous samples. Our empirical results show that the\nrevised model have improved uncertainty measures, and its performance is\ncompetitive to the state-of-the-art methods.",
    "descriptor": "\nComments: Accepted in WACV 2023\n",
    "authors": [
      "Sumedha Singla",
      "Nihal Murali",
      "Forough Arabshahi",
      "Sofia Triantafyllou",
      "Kayhan Batmanghelich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12196"
  },
  {
    "id": "arXiv:2210.12197",
    "title": "Life is a Circus and We are the Clowns: Automatically Finding Analogies  between Situations and Processes",
    "abstract": "Analogy-making gives rise to reasoning, abstraction, flexible categorization\nand counterfactual inference -- abilities lacking in even the best AI systems\ntoday. Much research has suggested that analogies are key to non-brittle\nsystems that can adapt to new domains. Despite their importance, analogies\nreceived little attention in the NLP community, with most research focusing on\nsimple word analogies. Work that tackled more complex analogies relied heavily\non manually constructed, hard-to-scale input representations. In this work, we\nexplore a more realistic, challenging setup: our input is a pair of natural\nlanguage procedural texts, describing a situation or a process (e.g., how the\nheart works/how a pump works). Our goal is to automatically extract entities\nand their relations from the text and find a mapping between the different\ndomains based on relational similarity (e.g., blood is mapped to water). We\ndevelop an interpretable, scalable algorithm and demonstrate that it identifies\nthe correct mappings 87% of the time for procedural texts and 94% for stories\nfrom cognitive-psychology literature. We show it can extract analogies from a\nlarge dataset of procedural texts, achieving 79% precision (analogy prevalence\nin data: 3%). Lastly, we demonstrate that our algorithm is robust to\nparaphrasing the input texts.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference (long paper)\n",
    "authors": [
      "Oren Sultan",
      "Dafna Shahaf"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12197"
  },
  {
    "id": "arXiv:2210.12198",
    "title": "Anonymous Bandits for Multi-User Systems",
    "abstract": "In this work, we present and study a new framework for online learning in\nsystems with multiple users that provide user anonymity. Specifically, we\nextend the notion of bandits to obey the standard $k$-anonymity constraint by\nrequiring each observation to be an aggregation of rewards for at least $k$\nusers. This provides a simple yet effective framework where one can learn a\nclustering of users in an online fashion without observing any user's\nindividual decision. We initiate the study of anonymous bandits and provide the\nfirst sublinear regret algorithms and lower bounds for this setting.",
    "descriptor": "",
    "authors": [
      "Hossein Esfandiari",
      "Vahab Mirrokni",
      "Jon Schneider"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12198"
  },
  {
    "id": "arXiv:2210.12201",
    "title": "A computational analysis on the relationship between melodic originality  and thematic fame in classical music from the Romantic period",
    "abstract": "In this work, the researcher presents a novel approach to calculating melodic\noriginality based on the research by Simonton (1994). This novel formula is\nthen applied to a dataset of 428 classical music pieces from the Romantic\nperiod to analyze the relationship between melodic originality and thematic\nfame.",
    "descriptor": "",
    "authors": [
      "Hudson Griffith"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.12201"
  },
  {
    "id": "arXiv:2210.12202",
    "title": "High-Quality RGB-D Reconstruction via Multi-View Uncalibrated  Photometric Stereo and Gradient-SDF",
    "abstract": "Fine-detailed reconstructions are in high demand in many applications.\nHowever, most of the existing RGB-D reconstruction methods rely on\npre-calculated accurate camera poses to recover the detailed surface geometry,\nwhere the representation of a surface needs to be adapted when optimizing\ndifferent quantities. In this paper, we present a novel multi-view RGB-D based\nreconstruction method that tackles camera pose, lighting, albedo, and surface\nnormal estimation via the utilization of a gradient signed distance field\n(gradient-SDF). The proposed method formulates the image rendering process\nusing specific physically-based model(s) and optimizes the surface's quantities\non the actual surface using its volumetric representation, as opposed to other\nworks which estimate surface quantities only near the actual surface. To\nvalidate our method, we investigate two physically-based image formation models\nfor natural light and point light source applications. The experimental results\non synthetic and real-world datasets demonstrate that the proposed method can\nrecover high-quality geometry of the surface more faithfully than the\nstate-of-the-art and further improves the accuracy of estimated camera poses.",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Lu Sang",
      "Bjoern Haefner",
      "Xingxing Zuo",
      "Daniel Cremers"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12202"
  },
  {
    "id": "arXiv:2210.12206",
    "title": "Probing with Noise: Unpicking the Warp and Weft of Embeddings",
    "abstract": "Improving our understanding of how information is encoded in vector space can\nyield valuable interpretability insights. Alongside vector dimensions, we argue\nthat it is possible for the vector norm to also carry linguistic information.\nWe develop a method to test this: an extension of the probing framework which\nallows for relative intrinsic interpretations of probing results. It relies on\nintroducing noise that ablates information encoded in embeddings, grounded in\nrandom baselines and confidence intervals. We apply the method to\nwell-established probing tasks and find evidence that confirms the existence of\nseparate information containers in English GloVe and BERT embeddings. Our\ncorrelation analysis aligns with the experimental findings that different\nencoders use the norm to encode different kinds of information: GloVe stores\nsyntactic and sentence length information in the vector norm, while BERT uses\nit to encode contextual incongruity.",
    "descriptor": "\nComments: 10 pages, 3 tables, Workshop on analyzing and interpreting neural networks for NLP\n",
    "authors": [
      "Filip Klubi\u010dka",
      "John D. Kelleher"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12206"
  },
  {
    "id": "arXiv:2210.12209",
    "title": "Motion Policy Networks",
    "abstract": "Collision-free motion generation in unknown environments is a core building\nblock for robot manipulation. Generating such motions is challenging due to\nmultiple objectives; not only should the solutions be optimal, the motion\ngenerator itself must be fast enough for real-time performance and reliable\nenough for practical deployment. A wide variety of methods have been proposed\nranging from local controllers to global planners, often being combined to\noffset their shortcomings. We present an end-to-end neural model called Motion\nPolicy Networks (M$\\pi$Nets) to generate collision-free, smooth motion from\njust a single depth camera observation. M$\\pi$Nets are trained on over 3\nmillion motion planning problems in over 500,000 environments. Our experiments\nshow that M$\\pi$Nets are significantly faster than global planners while\nexhibiting the reactivity needed to deal with dynamic scenes. They are 46%\nbetter than prior neural planners and more robust than local control policies.\nDespite being only trained in simulation, M$\\pi$Nets transfer well to the real\nrobot with noisy partial point clouds. Code and data are publicly available at\nhttps://mpinets.github.io.",
    "descriptor": "\nComments: To be published in the Conference on Robot Learning (CoRL) 2022. 10 pages with 4 figures. Appendix has 10 pages and 1 figure\n",
    "authors": [
      "Adam Fishman",
      "Adithyavairan Murali",
      "Clemens Eppner",
      "Bryan Peele",
      "Byron Boots",
      "Dieter Fox"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12209"
  },
  {
    "id": "arXiv:2210.12213",
    "title": "SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity  Representation",
    "abstract": "Named geographic entities (geo-entities for short) are the building blocks of\nmany geographic datasets. Characterizing geo-entities is integral to various\napplication domains, such as geo-intelligence and map comprehension, while a\nkey challenge is to capture the spatial-varying context of an entity. We\nhypothesize that we shall know the characteristics of a geo-entity by its\nsurrounding entities, similar to knowing word meanings by their linguistic\ncontext. Accordingly, we propose a novel spatial language model, SpaBERT, which\nprovides a general-purpose geo-entity representation based on neighboring\nentities in geospatial data. SpaBERT extends BERT to capture linearized spatial\ncontext, while incorporating a spatial coordinate embedding mechanism to\npreserve spatial relations of entities in the 2-dimensional space. SpaBERT is\npretrained with masked language modeling and masked entity prediction tasks to\nlearn spatial dependencies. We apply SpaBERT to two downstream tasks:\ngeo-entity typing and geo-entity linking. Compared with the existing language\nmodels that do not use spatial context, SpaBERT shows significant performance\nimprovement on both tasks. We also analyze the entity representation from\nSpaBERT in various settings and the effect of spatial coordinate embedding.",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Zekun Li",
      "Jina Kim",
      "Yao-Yi Chiang",
      "Muhao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12213"
  },
  {
    "id": "arXiv:2210.12214",
    "title": "Optimizing Bilingual Neural Transducer with Synthetic Code-switching  Text Generation",
    "abstract": "Code-switching describes the practice of using more than one language in the\nsame sentence. In this study, we investigate how to optimize a neural\ntransducer based bilingual automatic speech recognition (ASR) model for\ncode-switching speech. Focusing on the scenario where the ASR model is trained\nwithout supervised code-switching data, we found that semi-supervised training\nand synthetic code-switched data can improve the bilingual ASR system on\ncode-switching speech. We analyze how each of the neural transducer's encoders\ncontributes towards code-switching performance by measuring encoder-specific\nrecall values, and evaluate our English/Mandarin system on the ASCEND data set.\nOur final system achieves 25% mixed error rate (MER) on the ASCEND\nEnglish/Mandarin code-switching test set -- reducing the MER by 2.1% absolute\ncompared to the previous literature -- while maintaining good accuracy on the\nmonolingual test sets.",
    "descriptor": "\nComments: 5 pages, 1 figure, submitted to ICASSP 2023, *: equal contributions\n",
    "authors": [
      "Thien Nguyen",
      "Nathalie Tran",
      "Liuhui Deng",
      "Thiago Fraga da Silva",
      "Matthew Radzihovsky",
      "Roger Hsiao",
      "Henry Mason",
      "Stefan Braun",
      "Erik McDermott",
      "Dogan Can",
      "Pawel Swietojanski",
      "Lyan Verwimp",
      "Sibel Oyman",
      "Tresi Arvizo",
      "Honza Silovsky",
      "Arnab Ghoshal",
      "Mathieu Martel",
      "Bharat Ram Ambati",
      "Mohamed Ali"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12214"
  },
  {
    "id": "arXiv:2210.12215",
    "title": "Gui at MixMT 2022 : English-Hinglish: An MT approach for translation of  code mixed data",
    "abstract": "Code-mixed machine translation has become an important task in multilingual\ncommunities and extending the task of machine translation to code mixed data\nhas become a common task for these languages. In the shared tasks of WMT 2022,\nwe try to tackle the same for both English + Hindi to Hinglish and Hinglish to\nEnglish. The first task dealt with both Roman and Devanagari script as we had\nmonolingual data in both English and Hindi whereas the second task only had\ndata in Roman script. To our knowledge, we achieved one of the top ROUGE-L and\nWER scores for the first task of Monolingual to Code-Mixed machine translation.\nIn this paper, we discuss the use of mBART with some special pre-processing and\npost-processing (transliteration from Devanagari to Roman) for the first task\nin detail and the experiments that we performed for the second task of\ntranslating code-mixed Hinglish to monolingual English.",
    "descriptor": "",
    "authors": [
      "Akshat Gahoi",
      "Jayant Duneja",
      "Anshul Padhi",
      "Shivam Mangale",
      "Saransh Rajput",
      "Tanvi Kamble",
      "Dipti Misra Sharma",
      "Vasudeva Varma"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12215"
  },
  {
    "id": "arXiv:2210.12216",
    "title": "Feature Engineering and Classification Models for Partial Discharge in  Power Transformers",
    "abstract": "To ensure reliability, power transformers are monitored for partial discharge\n(PD) events, which are symptoms of transformer failure. Since failures can have\ncatastrophic cascading consequences, it is critical to preempt them as early as\npossible. Our goal is to classify PDs as corona, floating, particle, or void,\nto gain an understanding of the failure location. Using phase resolved PD\nsignal data, we create a small set of features, which can be used to classify\nPDs with high accuracy. This set of features consists of the total magnitude,\nthe maximum magnitude, and the length of the longest empty band. These features\nrepresent the entire signal and not just a single phase, so the feature set has\na fixed size and is easily comprehensible. With both Random Forest and SVM\nclassification methods, we attain a 99% classification accuracy, which is\nsignificantly higher than classification using phase based feature sets such as\nphase magnitude. Furthermore, we develop a stacking ensemble to combine several\nclassification models, resulting in a superior model that outperforms existing\nmethods in both accuracy and variance.",
    "descriptor": "",
    "authors": [
      "Jonathan Wang",
      "Kesheng Wu",
      "Alex Sim",
      "Seongwook Hwangbo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.12216"
  },
  {
    "id": "arXiv:2210.12217",
    "title": "Entailer: Answering Questions with Faithful and Truthful Chains of  Reasoning",
    "abstract": "Our goal is a question-answering (QA) system that can show how its answers\nare implied by its own internal beliefs via a systematic chain of reasoning.\nSuch a capability would allow better understanding of why a model produced the\nanswer it did. Our approach is to recursively combine a trained\nbackward-chaining model, capable of generating a set of premises entailing an\nanswer hypothesis, with a verifier that checks that the model itself believes\nthose premises (and the entailment itself) through self-querying. To our\nknowledge, this is the first system to generate multistep chains that are both\nfaithful (the answer follows from the reasoning) and truthful (the chain\nreflects the system's own internal beliefs). In evaluation using two different\ndatasets, users judge that a majority (70%+) of generated chains clearly show\nhow an answer follows from a set of facts - substantially better than a\nhigh-performance baseline - while preserving answer accuracy. By materializing\nmodel beliefs that systematically support an answer, new opportunities arise\nfor understanding the model's system of belief, and diagnosing and correcting\nits misunderstandings when an answer is wrong.",
    "descriptor": "\nComments: accepted at EMNLP 2022. arXiv admin note: substantial text overlap with arXiv:2204.13074\n",
    "authors": [
      "Oyvind Tafjord",
      "Bhavana Dalvi Mishra",
      "Peter Clark"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12217"
  },
  {
    "id": "arXiv:2210.12218",
    "title": "SEIFER: Scalable Edge Inference for Deep Neural Networks",
    "abstract": "Edge inference is becoming ever prevalent through its applications from\nretail to wearable technology. Clusters of networked resource-constrained edge\ndevices are becoming common, yet there is no production-ready orchestration\nsystem for deploying deep learning models over such edge networks which adopts\nthe robustness and scalability of the cloud. We present SEIFER, a framework\nutilizing a standalone Kubernetes cluster to partition a given DNN and place\nthese partitions in a distributed manner across an edge network, with the goal\nof maximizing inference throughput. The system is node fault-tolerant and\nautomatically updates deployments based on updates to the model's version. We\nprovide a preliminary evaluation of a partitioning and placement algorithm that\nworks within this framework, and show that we can improve the inference\npipeline throughput by 200% by utilizing sufficient numbers of\nresource-constrained nodes. We have implemented SEIFER in open-source software\nthat is publicly available to the research community.",
    "descriptor": "\nComments: Accepted to the \"Challenges in Deploying and Monitoring ML Systems\" Workshop of NeurIPS 2022\n",
    "authors": [
      "Arjun Parthasarathy",
      "Bhaskar Krishnamachari"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12218"
  },
  {
    "id": "arXiv:2210.12219",
    "title": "Partitioning and Placement of Deep Neural Networks on Distributed Edge  Devices to Maximize Inference Throughput",
    "abstract": "Edge inference has become more widespread, as its diverse applications range\nfrom retail to wearable technology. Clusters of networked resource-constrained\nedge devices are becoming common, yet no system exists to split a DNN across\nthese clusters while maximizing the inference throughput of the system. We\npresent an algorithm which partitions DNNs and distributes them across a set of\nedge devices with the goal of minimizing the bottleneck latency and therefore\nmaximizing inference throughput. The system scales well to systems of different\nnode memory capacities and numbers of nodes. We find that we can reduce the\nbottleneck latency by 10x over a random algorithm and 35% over a greedy joint\npartitioning-placement algorithm. Furthermore we find empirically that for the\nset of representative models we tested, the algorithm produces results within\n9.2% of the optimal bottleneck latency.",
    "descriptor": "\nComments: Accepted to the International Telecommunication Networks and Applications Conference 2022 (ITNAC 2022)\n",
    "authors": [
      "Arjun Parthasarathy",
      "Bhaskar Krishnamachari"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.12219"
  },
  {
    "id": "arXiv:2210.12220",
    "title": "Considerations for Visualizing Uncertainty in Clinical Machine Learning  Models",
    "abstract": "Clinician-facing predictive models are increasingly present in the healthcare\nsetting. Regardless of their success with respect to performance metrics, all\nmodels have uncertainty. We investigate how to visually communicate uncertainty\nin this setting in an actionable, trustworthy way. To this end, we conduct a\nqualitative study with cardiac critical care clinicians. Our results reveal\nthat clinician trust may be impacted most not by the degree of uncertainty, but\nrather by how transparent the visualization of what the sources of uncertainty\nare. Our results show a clear connection between feature interpretability and\nclinical actionability.",
    "descriptor": "\nComments: Prepared for the CHI 2021 Workshop: Realizing AI in Healthcare: Challenges Appearing in the Wild this https URL\n",
    "authors": [
      "Caitlin F. Harrigan",
      "Gabriela Morgenshtern",
      "Anna Goldenberg",
      "Fanny Chevalier"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12220"
  },
  {
    "id": "arXiv:2210.12223",
    "title": "Low-Resource Multilingual and Zero-Shot Multispeaker TTS",
    "abstract": "While neural methods for text-to-speech (TTS) have shown great advances in\nmodeling multiple speakers, even in zero-shot settings, the amount of data\nneeded for those approaches is generally not feasible for the vast majority of\nthe world's over 6,000 spoken languages. In this work, we bring together the\ntasks of zero-shot voice cloning and multilingual low-resource TTS. Using the\nlanguage agnostic meta learning (LAML) procedure and modifications to a TTS\nencoder, we show that it is possible for a system to learn speaking a new\nlanguage using just 5 minutes of training data while retaining the ability to\ninfer the voice of even unseen speakers in the newly learned language. We show\nthe success of our proposed approach in terms of intelligibility, naturalness\nand similarity to target speaker using objective metrics as well as human\nstudies and provide our code and trained models open source.",
    "descriptor": "\nComments: Accepted to AACL 2022\n",
    "authors": [
      "Florian Lux",
      "Julia Koch",
      "Ngoc Thang Vu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12223"
  },
  {
    "id": "arXiv:2210.12228",
    "title": "EDUKG: a Heterogeneous Sustainable K-12 Educational Knowledge Graph",
    "abstract": "Web and artificial intelligence technologies, especially semantic web and\nknowledge graph (KG), have recently raised significant attention in educational\nscenarios. Nevertheless, subject-specific KGs for K-12 education still lack\nsufficiency and sustainability from knowledge and data perspectives. To tackle\nthese issues, we propose EDUKG, a heterogeneous sustainable K-12 Educational\nKnowledge Graph. We first design an interdisciplinary and fine-grained ontology\nfor uniformly modeling knowledge and resource in K-12 education, where we\ndefine 635 classes, 445 object properties, and 1314 datatype properties in\ntotal. Guided by this ontology, we propose a flexible methodology for\ninteractively extracting factual knowledge from textbooks. Furthermore, we\nestablish a general mechanism based on our proposed generalized entity linking\nsystem for EDUKG's sustainable maintenance, which can dynamically index\nnumerous heterogeneous resources and data with knowledge topics in EDUKG. We\nfurther evaluate EDUKG to illustrate its sufficiency, richness, and\nvariability. We publish EDUKG with more than 252 million entities and 3.86\nbillion triplets. Our code and data repository is now available at\nhttps://github.com/THU-KEG/EDUKG.",
    "descriptor": "\nComments: 17 pages, 8 figures\n",
    "authors": [
      "Bowen Zhao",
      "Jiuding Sun",
      "Bin Xu",
      "Xingyu Lu",
      "Yuchen Li",
      "Jifan Yu",
      "Minghui Liu",
      "Tingjian Zhang",
      "Qiuyang Chen",
      "Hanming Li",
      "Lei Hou",
      "Juanzi Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12228"
  },
  {
    "id": "arXiv:2210.12229",
    "title": "Deep Reinforcement Learning for Stabilization of Large-scale  Probabilistic Boolean Networks",
    "abstract": "The ability to direct a Probabilistic Boolean Network (PBN) to a desired\nstate is important to applications such as targeted therapeutics in cancer\nbiology. Reinforcement Learning (RL) has been proposed as a framework that\nsolves a discrete-time optimal control problem cast as a Markov Decision\nProcess. We focus on an integrative framework powered by a model-free deep RL\nmethod that can address different flavours of the control problem (e.g., with\nor without control inputs; attractor state or a subset of the state space as\nthe target domain). The method is agnostic to the distribution of probabilities\nfor the next state, hence it does not use the probability transition matrix.\nThe time complexity is linear on the time steps, or interactions between the\nagent (deep RL) and the environment (PBN), during training. Indeed, we explore\nthe scalability of the deep RL approach to (set) stabilization of large-scale\nPBNs and demonstrate successful control on large networks, including a\nmetastatic melanoma PBN with 200 nodes.",
    "descriptor": "",
    "authors": [
      "Sotiris Moschoyiannis",
      "Evangelos Chatzaroulas",
      "Vytenis Sliogeris",
      "Yuhu Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12229"
  },
  {
    "id": "arXiv:2210.12230",
    "title": "Programming Bare-Metal Accelerators with Heterogeneous Threading Models:  A Case Study of Matrix-3000",
    "abstract": "As the hardware industry moves towards using specialized heterogeneous\nmany-cores to avoid the effects of the power wall, software developers are\nfinding it hard to deal with the complexity of these systems. This article\nshares our experience when developing a programming model and its supporting\ncompiler and libraries for Matrix-3000, which is designed for next-generation\nexascale supercomputers but has a complex memory hierarchy and processor\norganization. To assist its software development, we developed a software stack\nfrom scratch that includes a low-level programming interface and a high-level\nOpenCL compiler. Our low-level programming model offers native programming\nsupport for using the bare-metal accelerators of Matrix-3000, while the\nhigh-level model allows programmers to use the OpenCL programming standard. We\ndetail our design choices and highlight the lessons learned from developing\nsystems software to enable the programming of bare-metal accelerators. Our\nprogramming models have been deployed to the production environment of an\nexascale prototype system.",
    "descriptor": "",
    "authors": [
      "Jianbin Fang",
      "Peng Zhang",
      "Chun Huang",
      "Tao Tang",
      "Kai Lu",
      "Ruibo Wang",
      "Zheng Wang"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2210.12230"
  },
  {
    "id": "arXiv:2210.12231",
    "title": "Reducing Training Sample Memorization in GANs by Training with  Memorization Rejection",
    "abstract": "Generative adversarial network (GAN) continues to be a popular research\ndirection due to its high generation quality. It is observed that many\nstate-of-the-art GANs generate samples that are more similar to the training\nset than a holdout testing set from the same distribution, hinting some\ntraining samples are implicitly memorized in these models. This memorization\nbehavior is unfavorable in many applications that demand the generated samples\nto be sufficiently distinct from known samples. Nevertheless, it is unclear\nwhether it is possible to reduce memorization without compromising the\ngeneration quality. In this paper, we propose memorization rejection, a\ntraining scheme that rejects generated samples that are near-duplicates of\ntraining samples during training. Our scheme is simple, generic and can be\ndirectly applied to any GAN architecture. Experiments on multiple datasets and\nGAN models validate that memorization rejection effectively reduces training\nsample memorization, and in many cases does not sacrifice the generation\nquality. Code to reproduce the experiment results can be found at\n$\\texttt{https://github.com/jybai/MRGAN}$.",
    "descriptor": "",
    "authors": [
      "Andrew Bai",
      "Cho-Jui Hsieh",
      "Wendy Kan",
      "Hsuan-Tien Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12231"
  },
  {
    "id": "arXiv:2210.12232",
    "title": "\"If sighted people know, I should be able to know:\" Privacy Perceptions  of Bystanders with Visual Impairments around Camera-based Technology",
    "abstract": "Camera-based technology can be privacy-invasive, especially for bystanders\nwho can be captured by the cameras but do not have direct control or access to\nthe devices. The privacy threats become even more significant to bystanders\nwith visual impairments (BVI) since they cannot visually discover the use of\ncameras nearby and effectively avoid being captured. While some prior research\nhas studied visually impaired people's privacy concerns as direct users of\ncamera-based assistive technologies, no research has explored their unique\nprivacy perceptions and needs as bystanders. We conducted an in-depth interview\nstudy with 16 visually impaired participants to understand BVI's privacy\nconcerns, expectations, and needs in different camera usage scenarios. A\npreliminary survey with 90 visually impaired respondents and 96 sighted\ncontrols was conducted to compare BVI and sighted bystanders' general attitudes\ntowards cameras and elicit camera usage scenarios for the interview study. Our\nresearch revealed BVI's unique privacy challenges and perceptions around\ncameras, highlighting their needs for privacy awareness and protection. We\nsummarized design considerations for future privacy-enhancing technologies to\nfulfill BVI's privacy needs.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Yuhang Zhao",
      "Yaxing Yao",
      "Jiaru Fu",
      "Nihan Zhou"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.12232"
  },
  {
    "id": "arXiv:2210.12233",
    "title": "TCAB: A Large-Scale Text Classification Attack Benchmark",
    "abstract": "We introduce the Text Classification Attack Benchmark (TCAB), a dataset for\nanalyzing, understanding, detecting, and labeling adversarial attacks against\ntext classifiers. TCAB includes 1.5 million attack instances, generated by\ntwelve adversarial attacks targeting three classifiers trained on six source\ndatasets for sentiment analysis and abuse detection in English. Unlike standard\ntext classification, text attacks must be understood in the context of the\ntarget classifier that is being attacked, and thus features of the target\nclassifier are important as well. TCAB includes all attack instances that are\nsuccessful in flipping the predicted label; a subset of the attacks are also\nlabeled by human annotators to determine how frequently the primary semantics\nare preserved. The process of generating attacks is automated, so that TCAB can\neasily be extended to incorporate new text attacks and better classifiers as\nthey are developed. In addition to the primary tasks of detecting and labeling\nattacks, TCAB can also be used for attack localization, attack target labeling,\nand attack characterization. TCAB code and dataset are available at\nhttps://react-nlp.github.io/tcab/.",
    "descriptor": "\nComments: 32 pages, 7 figures, and 14 tables\n",
    "authors": [
      "Kalyani Asthana",
      "Zhouhang Xie",
      "Wencong You",
      "Adam Noack",
      "Jonathan Brophy",
      "Sameer Singh",
      "Daniel Lowd"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12233"
  },
  {
    "id": "arXiv:2210.12234",
    "title": "Imbalanced Classification in Medical Imaging",
    "abstract": "We propose performing imbalanced classification by regrouping majority\nclasses into small classes so that we turn the problem into balanced multiclass\nclassification. This new idea is dramatically different from popular loss\nreweighting and class resampling methods. Our preliminary result on imbalanced\nmedical image classification shows that this natural idea can substantially\nboost the classification performance as measured by average precision\n(approximately area-under-the-precision-recall-curve, or AUPRC), which is more\nappropriate for evaluating imbalanced classification than other metrics such as\nbalanced accuracy.",
    "descriptor": "",
    "authors": [
      "Le Peng",
      "Yash Travadi",
      "Rui Zhang",
      "Ying Cui",
      "Ju Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12234"
  },
  {
    "id": "arXiv:2210.12239",
    "title": "Auto-Encoder Neural Network Incorporating X-Ray Fluorescence Fundamental  Parameters with Machine Learning",
    "abstract": "We consider energy-dispersive X-ray Fluorescence (EDXRF) applications where\nthe fundamental parameters method is impractical such as when instrument\nparameters are unavailable. For example, on a mining shovel or conveyor belt,\nrocks are constantly moving (leading to varying angles of incidence and\ndistances) and there may be other factors not accounted for (like dust). Neural\nnetworks do not require instrument and fundamental parameters but training\nneural networks requires XRF spectra labelled with elemental composition, which\nis often limited because of its expense. We develop a neural network model that\nlearns from limited labelled data and learns to invert a forward model. The\nforward model uses transition energies and probabilities of all elements and\nparameterized distributions to approximate other fundamental and instrument\nparameters. We evaluate the model and baseline models on a rock dataset from a\nlithium mine and identify which elements are appropriate for this method. This\nmodel demonstrates the potential to calibrate a neural network in a noisy\nenvironment where labelled data is limited.",
    "descriptor": "\nComments: submitted to X-ray Spectrometry journal (Wiley)\n",
    "authors": [
      "Matthew Dirks",
      "David Poole"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12239"
  },
  {
    "id": "arXiv:2210.12241",
    "title": "FIND: An Unsupervised Implicit 3D Model of Articulated Human Feet",
    "abstract": "In this paper we present a high fidelity and articulated 3D human foot model.\nThe model is parameterised by a disentangled latent code in terms of shape,\ntexture and articulated pose. While high fidelity models are typically created\nwith strong supervision such as 3D keypoint correspondences or\npre-registration, we focus on the difficult case of little to no annotation. To\nthis end, we make the following contributions: (i) we develop a Foot Implicit\nNeural Deformation field model, named FIND, capable of tailoring explicit\nmeshes at any resolution i.e. for low or high powered devices; (ii) an approach\nfor training our model in various modes of weak supervision with progressively\nbetter disentanglement as more labels, such as pose categories, are provided;\n(iii) a novel unsupervised part-based loss for fitting our model to 2D images\nwhich is better than traditional photometric or silhouette losses; (iv)\nfinally, we release a new dataset of high resolution 3D human foot scans,\nFoot3D. On this dataset, we show our model outperforms a strong PCA\nimplementation trained on the same data in terms of shape quality and part\ncorrespondences, and that our novel unsupervised part-based loss improves\ninference on images.",
    "descriptor": "\nComments: BMVC 2022\n",
    "authors": [
      "Oliver Boyne",
      "James Charles",
      "Roberto Cipolla"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12241"
  },
  {
    "id": "arXiv:2210.12242",
    "title": "A Dataset for Plain Language Adaptation of Biomedical Abstracts",
    "abstract": "Though exponentially growing health-related literature has been made\navailable to a broad audience online, the language of scientific articles can\nbe difficult for the general public to understand. Therefore, adapting this\nexpert-level language into plain language versions is necessary for the public\nto reliably comprehend the vast health-related literature. Deep Learning\nalgorithms for automatic adaptation are a possible solution; however, gold\nstandard datasets are needed for proper evaluation. Proposed datasets thus far\nconsist of either pairs of comparable professional- and general public-facing\ndocuments or pairs of semantically similar sentences mined from such documents.\nThis leads to a trade-off between imperfect alignments and small test sets. To\naddress this issue, we created the Plain Language Adaptation of Biomedical\nAbstracts dataset. This dataset is the first manually adapted dataset that is\nboth document- and sentence-aligned. The dataset contains 750 adapted\nabstracts, totaling 7643 sentence pairs. Along with describing the dataset, we\nbenchmark automatic adaptation on the dataset with state-of-the-art Deep\nLearning approaches, setting baselines for future research.",
    "descriptor": "\nComments: 12 pages, 4 figures, 7 tables\n",
    "authors": [
      "Kush Attal",
      "Brian Ondov",
      "Dina Demner-Fushman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12242"
  },
  {
    "id": "arXiv:2210.12246",
    "title": "A General Architecture for Client-Agnostic Hybrid Model Editors as a  Service",
    "abstract": "In this paper, we propose a general architecture for designing language\nservers for hybrid modeling languages, that is, modeling languages that contain\nboth textual and graphical representations. The architecture consists of a\ntextual language server, a graphical language server, and a client that\ncommunicates with the two servers. The servers are implemented using the\nLanguage Server Protocol (LSP) and the Graphical Language Server Protocol\n(GLSP) and are based on a shared abstract syntax of the hybrid language. This\nmeans that only static resources need to be common between the graphical and\ntextual language servers. The servers' separation allows each to be developed\nand maintained independently, while also enabling forward-compatibility with\ntheir respective dependencies.\nWe describe a prototype implementation of our architecture in the form of a\nhybrid editor for the UML-RT language. The evaluation of the architecture via\nthis prototype gives us useful insight into further generalization of the\narchitecture and the way it is used. We then sketch a suitable extension of the\narchitecture to enable support for multiple diagram types and, thus, multiple\ngraphical views.",
    "descriptor": "",
    "authors": [
      "Liam Walsh",
      "Juergen Dingel",
      "Karim Jahed"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.12246"
  },
  {
    "id": "arXiv:2210.12247",
    "title": "Benchmarking GPU and TPU Performance with Graph Neural Networks",
    "abstract": "Many artificial intelligence (AI) devices have been developed to accelerate\nthe training and inference of neural networks models. The most common ones are\nthe Graphics Processing Unit (GPU) and Tensor Processing Unit (TPU). They are\nhighly optimized for dense data representations. However, sparse\nrepresentations such as graphs are prevalent in many domains, including\nscience. It is therefore important to characterize the performance of available\nAI accelerators on sparse data. This work analyzes and compares the GPU and TPU\nperformance training a Graph Neural Network (GNN) developed to solve a\nreal-life pattern recognition problem. Characterizing the new class of models\nacting on sparse data may prove helpful in optimizing the design of deep\nlearning libraries and future AI accelerators.",
    "descriptor": "\nComments: 8 pages, 6 figures\n",
    "authors": [
      "xiangyang Ju",
      "Yunsong Wang",
      "Daniel Murnane",
      "Nicholas Choma",
      "Steven Farrell",
      "Paolo Calafiura"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12247"
  },
  {
    "id": "arXiv:2210.12249",
    "title": "The $c$-differential spectrum of $x\\mapsto x^{\\frac{p^n+1}{2}}$ in  finite fields of odd characteristics",
    "abstract": "In the paper, we concentrate on the map $x\\mapsto x^{\\frac{p^n+1}{2}}$ on\n$\\mathbb{F}_{p^n}$ and using combinatorial and number theory techniques, we\ncompute its detailed $c$-differential spectrum for all values of $c\\neq 1$ (the\nspectrum for $c=1$ is known).",
    "descriptor": "",
    "authors": [
      "Constanza Riera",
      "Pantelimon Stanica",
      "Haode Yan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2210.12249"
  },
  {
    "id": "arXiv:2210.12250",
    "title": "TAPS: Task-Agnostic Policy Sequencing",
    "abstract": "Advances in robotic skill acquisition have made it possible to build\ngeneral-purpose libraries of primitive skills for downstream manipulation\ntasks. However, naively executing these learned primitives one after the other\nis unlikely to succeed without accounting for dependencies between actions\nprevalent in long-horizon plans. We present Task-Agnostic Policy Sequencing\n(TAPS), a scalable framework for training manipulation primitives and\ncoordinating their geometric dependencies at plan-time to efficiently solve\nlong-horizon tasks never seen by any primitive during training. Based on the\nnotion that Q-functions encode a measure of action feasibility, we formulate\nmotion planning as a maximization problem over the expected success of each\nindividual primitive in the plan, which we estimate by the product of their\nQ-values. Our experiments indicate that this objective function approximates\nground truth plan feasibility and, when used as a planning objective, reduces\nmyopic behavior and thereby promotes task success. We further demonstrate how\nTAPS can be used for task and motion planning by estimating the geometric\nfeasibility of candidate action sequences provided by a task planner. We\nevaluate our approach in simulation and on a real robot.",
    "descriptor": "",
    "authors": [
      "Christopher Agia",
      "Toki Migimatsu",
      "Jiajun Wu",
      "Jeannette Bohg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12250"
  },
  {
    "id": "arXiv:2210.12253",
    "title": "End-to-end GPU acceleration of low-order-refined preconditioning for  high-order finite element discretizations",
    "abstract": "In this paper, we present algorithms and implementations for the end-to-end\nGPU acceleration of matrix-free low-order-refined preconditioning of high-order\nfinite element problems. The methods described here allow for the construction\nof effective preconditioners for high-order problems with optimal memory usage\nand computational complexity. The preconditioners are based on the construction\nof a spectrally equivalent low-order discretization on a refined mesh, which is\nthen amenable to, for example, algebraic multigrid preconditioning. The\nconstants of equivalence are independent of mesh size and polynomial degree.\nFor vector finite element problems in $H({\\rm curl})$ and $H({\\rm div})$ (e.g.\nfor electromagnetic or radiation diffusion problems) a specially constructed\ninterpolation-histopolation basis is used to ensure fast convergence. Detailed\nperformance studies are carried out to analyze the efficiency of the GPU\nalgorithms. The kernel throughput of each of the main algorithmic components is\nmeasured, and the strong and weak parallel scalability of the methods is\ndemonstrated. The different relative weighting and significance of the\nalgorithmic components on GPUs and CPUs is discussed. Results on problems\ninvolving adaptively refined nonconforming meshes are shown, and the use of the\npreconditioners on a large-scale magnetic diffusion problem using all spaces of\nthe finite element de Rham complex is illustrated.",
    "descriptor": "\nComments: 23 pages, 13 figures\n",
    "authors": [
      "Will Pazner",
      "Tzanio Kolev",
      "Jean-Sylvain Camier"
    ],
    "subjectives": [
      "Mathematical Software (cs.MS)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12253"
  },
  {
    "id": "arXiv:2210.12254",
    "title": "Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models",
    "abstract": "Generative models based on denoising diffusion techniques have led to an\nunprecedented increase in the quality and diversity of imagery that is now\npossible to create with neural generative models. However, most contemporary\nstate-of-the-art methods are derived from a standard isotropic Gaussian\nformulation. In this work we examine the situation where non-isotropic Gaussian\ndistributions are used. We present the key mathematical derivations for\ncreating denoising diffusion models using an underlying non-isotropic Gaussian\nnoise model. We also provide initial experiments to help verify empirically\nthat this more general modelling approach can also yield high-quality samples.",
    "descriptor": "\nComments: NeurIPS 2022 Workshop ; 4 pages, 18 pages of appendix, 2 figures\n",
    "authors": [
      "Vikram Voleti",
      "Christopher Pal",
      "Adam Oberman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12254"
  },
  {
    "id": "arXiv:2210.12256",
    "title": "Uncertainty Estimates of Predictions via a General Bias-Variance  Decomposition",
    "abstract": "Reliably estimating the uncertainty of a prediction throughout the model\nlifecycle is crucial in many safety-critical applications.\nThe most common way to measure this uncertainty is via the predicted\nconfidence. While this tends to work well for in-domain samples, these\nestimates are unreliable under domain drift.\nAlternatively, a bias-variance decomposition allows to directly measure the\npredictive uncertainty across the entire input space.\nBut, such a decomposition for proper scores does not exist in current\nliterature, and for exponential families it is convoluted.\nIn this work, we introduce a general bias-variance decomposition for proper\nscores and reformulate the exponential family case, giving rise to the Bregman\nInformation as the variance term in both cases.\nThis allows us to prove that the Bregman Information for classification\nmeasures the uncertainty in the logit space. We showcase the practical\nrelevance of this decomposition on two downstream tasks.\nFirst, we show how to construct confidence intervals for predictions on the\ninstance-level based on the Bregman Information.\nSecond, we demonstrate how different approximations of the instance-level\nBregman Information allow reliable out-of-distribution detection for all\ndegrees of domain drift.",
    "descriptor": "",
    "authors": [
      "Sebastian Gruber",
      "Florian Buettner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12256"
  },
  {
    "id": "arXiv:2210.12257",
    "title": "Efficient Automatic Machine Learning via Design Graphs",
    "abstract": "Despite the success of automated machine learning (AutoML), which aims to\nfind the best design, including the architecture of deep networks and\nhyper-parameters, conventional AutoML methods are computationally expensive and\nhardly provide insights into the relations of different model design choices.\nTo tackle the challenges, we propose FALCON, an efficient sample-based method\nto search for the optimal model design. Our key insight is to model the design\nspace of possible model designs as a design graph, where the nodes represent\ndesign choices, and the edges denote design similarities. FALCON features 1) a\ntask-agnostic module, which performs message passing on the design graph via a\nGraph Neural Network (GNN), and 2) a task-specific module, which conducts label\npropagation of the known model performance information on the design graph.\nBoth modules are combined to predict the design performances in the design\nspace, navigating the search direction. We conduct extensive experiments on 27\nnode and graph classification tasks from various application domains, and an\nimage classification task on the CIFAR-10 dataset. We empirically show that\nFALCON can efficiently obtain the well-performing designs for each task using\nonly 30 explored nodes. Specifically, FALCON has a comparable time cost with\nthe one-shot approaches while achieving an average improvement of 3.3% compared\nwith the best baselines.",
    "descriptor": "\nComments: Automated Machine Learning, Sample efficiency, Design graph, Graph Neural Networks\n",
    "authors": [
      "Shirley Wu",
      "Jiaxuan You",
      "Jure Leskovec",
      "Rex Ying"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12257"
  },
  {
    "id": "arXiv:2210.12259",
    "title": "Enhancing Tabular Reasoning with Pattern Exploiting Training",
    "abstract": "Recent methods based on pre-trained language models have exhibited superior\nperformance over tabular tasks (e.g., tabular NLI), despite showing inherent\nproblems such as not using the right evidence and inconsistent predictions\nacross inputs while reasoning over the tabular data. In this work, we utilize\nPattern-Exploiting Training (PET) (i.e., strategic MLM) on pre-trained language\nmodels to strengthen these tabular reasoning models' pre-existing knowledge and\nreasoning abilities. Our upgraded model exhibits a superior understanding of\nknowledge facts and tabular reasoning compared to current baselines.\nAdditionally, we demonstrate that such models are more effective for underlying\ndownstream tasks of tabular inference on InfoTabs. Furthermore, we show our\nmodel's robustness against adversarial sets generated through various character\nand word level perturbations.",
    "descriptor": "\nComments: The 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing\n",
    "authors": [
      "Abhilash Reddy Shankarampeta",
      "Vivek Gupta",
      "Shuo Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12259"
  },
  {
    "id": "arXiv:2210.12261",
    "title": "Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination",
    "abstract": "Large-scale pretrained language models have made significant advances in\nsolving downstream language understanding tasks. However, they generally suffer\nfrom reporting bias, the phenomenon describing the lack of explicit commonsense\nknowledge in written text, e.g., ''an orange is orange''. To overcome this\nlimitation, we develop a novel approach, Z-LaVI, to endow language models with\nvisual imagination capabilities. Specifically, we leverage two complementary\ntypes of ''imaginations'': (i) recalling existing images through retrieval and\n(ii) synthesizing nonexistent images via text-to-image generation. Jointly\nexploiting the language inputs and the imagination, a pretrained\nvision-language model (e.g., CLIP) eventually composes a zero-shot solution to\nthe original language tasks. Notably, fueling language models with imagination\ncan effectively leverage visual knowledge to solve plain language tasks. In\nconsequence, Z-LaVI consistently improves the zero-shot performance of existing\nlanguage models across a diverse set of language tasks.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Yue Yang",
      "Wenlin Yao",
      "Hongming Zhang",
      "Xiaoyang Wang",
      "Dong Yu",
      "Jianshu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12261"
  },
  {
    "id": "arXiv:2210.12262",
    "title": "Group Distributionally Robust Reinforcement Learning with Hierarchical  Latent Variables",
    "abstract": "One key challenge for multi-task Reinforcement learning (RL) in practice is\nthe absence of task indicators. Robust RL has been applied to deal with task\nambiguity, but may result in over-conservative policies. To balance the\nworst-case (robustness) and average performance, we propose Group\nDistributionally Robust Markov Decision Process (GDR-MDP), a flexible\nhierarchical MDP formulation that encodes task groups via a latent mixture\nmodel. GDR-MDP identifies the optimal policy that maximizes the expected return\nunder the worst-possible qualified belief over task groups within an ambiguity\nset. We rigorously show that GDR-MDP's hierarchical structure improves\ndistributional robustness by adding regularization to the worst possible\noutcomes. We then develop deep RL algorithms for GDR-MDP for both value-based\nand policy-based RL methods. Extensive experiments on Box2D control tasks,\nMuJoCo benchmarks, and Google football platforms show that our algorithms\noutperform classic robust training algorithms across diverse environments in\nterms of robustness under belief uncertainties. Demos are available on our\nproject page (\\url{https://sites.google.com/view/gdr-rl/home}).",
    "descriptor": "\nComments: 27 pages, 10 figures\n",
    "authors": [
      "Mengdi Xu",
      "Peide Huang",
      "Yaru Niu",
      "Visak Kumar",
      "Jielin Qiu",
      "Chao Fang",
      "Kuan-Hui Lee",
      "Xuewei Qi",
      "Henry Lam",
      "Bo Li",
      "Ding Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12262"
  },
  {
    "id": "arXiv:2210.12265",
    "title": "On the Calibration of Massively Multilingual Language Models",
    "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained\npopularity due to their surprising effectiveness in cross-lingual transfer.\nWhile there has been much work in evaluating these models for their performance\non a variety of tasks and languages, little attention has been paid on how well\ncalibrated these models are with respect to the confidence in their\npredictions. We first investigate the calibration of MMLMs in the zero-shot\nsetting and observe a clear case of miscalibration in low-resource languages or\nthose which are typologically diverse from English. Next, we empirically show\nthat calibration methods like temperature scaling and label smoothing do\nreasonably well towards improving calibration in the zero-shot scenario. We\nalso find that few-shot examples in the language can further help reduce the\ncalibration errors, often substantially. Overall, our work contributes towards\nbuilding more reliable multilingual models by highlighting the issue of their\nmiscalibration, understanding what language and model specific factors\ninfluence it, and pointing out the strategies to improve the same.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Kabir Ahuja",
      "Sunayana Sitaram",
      "Sandipan Dandapat",
      "Monojit Choudhury"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12265"
  },
  {
    "id": "arXiv:2210.12268",
    "title": "An Exploration of Neural Radiance Field Scene Reconstruction: Synthetic,  Real-world and Dynamic Scenes",
    "abstract": "This project presents an exploration into 3D scene reconstruction of\nsynthetic and real-world scenes using Neural Radiance Field (NeRF) approaches.\nWe primarily take advantage of the reduction in training and rendering time of\nneural graphic primitives multi-resolution hash encoding, to reconstruct static\nvideo game scenes and real-world scenes, comparing and observing reconstruction\ndetail and limitations. Additionally, we explore dynamic scene reconstruction\nusing Neural Radiance Fields for Dynamic Scenes(D-NeRF). Finally, we extend the\nimplementation of D-NeRF, originally constrained to handle synthetic scenes to\nalso handle real-world dynamic scenes.",
    "descriptor": "",
    "authors": [
      "Benedict Quartey",
      "Tuluhan Akbulut",
      "Wasiwasi Mgonzo",
      "Zheng Xin Yong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12268"
  },
  {
    "id": "arXiv:2210.12270",
    "title": "Investigating Input Modality and Task Geometry on Precision-first 3D  Drawing in Virtual Reality",
    "abstract": "Accurately drawing non-planar 3D curves in immersive Virtual Reality (VR) is\nindispensable for many precise 3D tasks. However, due to lack of physical\nsupport, limited depth perception, and the non-planar nature of 3D curves, it\nis challenging to adjust mid-air strokes to achieve high precision. Instead of\ncreating new interaction techniques, we investigated how task geometric shapes\nand input modalities affect precision-first drawing performance in a\nwithin-subject study (n = 12) focusing on 3D target tracing in commercially\navailable VR headsets. We found that compared to using bare hands, VR\ncontrollers and pens yield nearly 30% of precision gain, and that the tasks\nwith large curvature, forward-backward or left-right orientations perform best.\nWe finally discuss opportunities for designing novel interaction techniques for\nprecise 3D drawing. We believe that our work will benefit future research\naiming to create usable toolboxes for precise 3D drawing.",
    "descriptor": "\nComments: C. Chen, M. Yarmand, Z. Xu and V. Singh, Y. Zhang and N. Weibel, \"Investigating Input Modality and Task Geometry on Precision-first 3D Drawing in Virtual Reality\", 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2022, pp. 1-10, doi: 10.1109/ISMAR55827.2022.00054\n",
    "authors": [
      "Chen Chen",
      "Matin Yarmand",
      "Zhuoqun Xu",
      "Varun Singh",
      "Yang Zhang",
      "Nadir Weibel"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.12270"
  },
  {
    "id": "arXiv:2210.12273",
    "title": "Graphemic Normalization of the Perso-Arabic Script",
    "abstract": "Since its original appearance in 1991, the Perso-Arabic script representation\nin Unicode has grown from 169 to over 440 atomic isolated characters spread\nover several code pages representing standard letters, various diacritics and\npunctuation for the original Arabic and numerous other regional orthographic\ntraditions. This paper documents the challenges that Perso-Arabic presents\nbeyond the best-documented languages, such as Arabic and Persian, building on\nearlier work by the expert community. We particularly focus on the situation in\nnatural language processing (NLP), which is affected by multiple, often\nneglected, issues such as the use of visually ambiguous yet canonically\nnonequivalent letters and the mixing of letters from different orthographies.\nAmong the contributing conflating factors are the lack of input methods, the\ninstability of modern orthographies, insufficient literacy, and loss or lack of\northographic tradition. We evaluate the effects of script normalization on\neight languages from diverse language families in the Perso-Arabic script\ndiaspora on machine translation and statistical language modeling tasks. Our\nresults indicate statistically significant improvements in performance in most\nconditions for all the languages considered when normalization is applied. We\nargue that better understanding and representation of Perso-Arabic script\nvariation within regional orthographic traditions, where those are present, is\ncrucial for further progress of modern computational NLP techniques especially\nfor languages with a paucity of resources.",
    "descriptor": "\nComments: Pre-print to appear in the Proceedings of Grapholinguistics in the 21st Century (G21C), 2022. Telecom Paris, Palaiseau, France, June 8-10, 2022. 41 pages, 38 tables, 3 figures\n",
    "authors": [
      "Raiomond Doctor",
      "Alexander Gutkin",
      "Cibu Johny",
      "Brian Roark",
      "Richard Sproat"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12273"
  },
  {
    "id": "arXiv:2210.12274",
    "title": "DeGroot-based opinion formation under a global steering mechanism",
    "abstract": "In this paper we investigate how interacting agents arrive to a consensus or\na polarized state. More specifically, we study the opinion formation process\nunder the effect of a global steering mechanism (GSM). We consider that the GSM\naggregates agents' opinions at the network level and feeds back to them a form\nof global information. We propose the GSM-DeGroot model, a new two-layer\nagent-based opinion formation model that captures the coupled dynamics between\nagent-to-agent local interactions and the GSM's steering effect. This way,\nagents are subject to the effects of a DeGroot-like local opinion propagation,\nas well as to a wide variety of possible aggregated information that can affect\ntheir opinions, such as trending news feeds, press coverage, polls, elections,\netc. The cornerstone feature of our model that, contrary to the standard\nDeGroot model, allows polarization to emerge, is the differential way in which\nagents react to the global information. We explore numerically the model\ndynamics to find regimes of qualitatively different behavior, using simulations\non synthetic data. Moreover, we challenge our model by fitting it to the\ndynamics of real topics, related to protests, social movements, and the\nescalation of a long geopolitical conflict to a war, which attracted the public\nattention and were recorded on Twitter. Our experiments show that the proposed\nmodel holds explanatory power, as it evidently captures real opinion formation\ndynamics via a relatively small set of interpretable parameters.",
    "descriptor": "\nComments: 26 pages, 8 figures, 3 tables\n",
    "authors": [
      "Ivan Conjeaud",
      "Philipp Lorenz-Spreen",
      "Argyris Kalogeratos"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.12274"
  },
  {
    "id": "arXiv:2210.12276",
    "title": "Text Editing as Imitation Game",
    "abstract": "Text editing, such as grammatical error correction, arises naturally from\nimperfect textual data. Recent works frame text editing as a multi-round\nsequence tagging task, where operations -- such as insertion and substitution\n-- are represented as a sequence of tags. While achieving good results, this\nencoding is limited in flexibility as all actions are bound to token-level\ntags. In this work, we reformulate text editing as an imitation game using\nbehavioral cloning. Specifically, we convert conventional sequence-to-sequence\ndata into state-to-action demonstrations, where the action space can be as\nflexible as needed. Instead of generating the actions one at a time, we\nintroduce a dual decoders structure to parallel the decoding while retaining\nthe dependencies between action tokens, coupled with trajectory augmentation to\nalleviate the distribution shift that imitation learning often suffers. In\nexperiments on a suite of Arithmetic Equation benchmarks, our model\nconsistently outperforms the autoregressive baselines in terms of performance,\nefficiency, and robustness. We hope our findings will shed light on future\nstudies in reinforcement learning applying sequence-level action generation to\nnatural language processing.",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022\n",
    "authors": [
      "Ning Shi",
      "Bin Tang",
      "Bo Yuan",
      "Longtao Huang",
      "Yewen Pu",
      "Jie Fu",
      "Zhouhan Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12276"
  },
  {
    "id": "arXiv:2210.12277",
    "title": "The Stochastic Proximal Distance Algorithm",
    "abstract": "Stochastic versions of proximal methods have gained much attention in\nstatistics and machine learning. These algorithms tend to admit simple,\nscalable forms, and enjoy numerical stability via implicit updates. In this\nwork, we propose and analyze a stochastic version of the recently proposed\nproximal distance algorithm, a class of iterative optimization methods that\nrecover a desired constrained estimation problem as a penalty parameter $\\rho\n\\rightarrow \\infty$. By uncovering connections to related stochastic proximal\nmethods and interpreting the penalty parameter as the learning rate, we justify\nheuristics used in practical manifestations of the proximal distance method,\nestablishing their convergence guarantees for the first time. Moreover, we\nextend recent theoretical devices to establish finite error bounds and a\ncomplete characterization of convergence rates regimes. We validate our\nanalysis via a thorough empirical study, also showing that unsurprisingly, the\nproposed method outpaces batch versions on popular learning tasks.",
    "descriptor": "",
    "authors": [
      "Haoyu Jiang",
      "Jason Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12277"
  },
  {
    "id": "arXiv:2210.12278",
    "title": "Sample Efficient Robot Learning with Structured World Models",
    "abstract": "Reinforcement learning has been demonstrated as a flexible and effective\napproach for learning a range of continuous control tasks, such as those used\nby robots to manipulate objects in their environment. But in robotics\nparticularly, real-world rollouts are costly, and sample efficiency can be a\nmajor limiting factor when learning a new skill. In game environments, the use\nof world models has been shown to improve sample efficiency while still\nachieving good performance, especially when images or other rich observations\nare provided. In this project, we explore the use of a world model in a\ndeformable robotic manipulation task, evaluating its effect on sample\nefficiency when learning to fold a cloth in simulation. We compare the use of\nRGB image observation with a feature space leveraging built-in structure\n(keypoints representing the cloth configuration), a common approach in robot\nskill learning, and compare the impact on task performance and learning\nefficiency with and without the world model. Our experiments showed that the\nusage of keypoints increased the performance of the best model on the task by\n50%, and in general, the use of a learned or constructed reduced feature space\nimproved task performance and sample efficiency. The use of a state transition\npredictor(MDN-RNN) in our world models did not have a notable effect on task\nperformance.",
    "descriptor": "",
    "authors": [
      "Tuluhan Akbulut",
      "Max Merlin",
      "Shane Parr",
      "Benedict Quartey",
      "Skye Thompson"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12278"
  },
  {
    "id": "arXiv:2210.12282",
    "title": "Bridging the Gap Between Target Networks and Functional Regularization",
    "abstract": "Bootstrapping is behind much of the successes of Deep Reinforcement Learning.\nHowever, learning the value function via bootstrapping often leads to unstable\ntraining due to fast-changing target values. Target Networks are employed to\nstabilize training by using an additional set of lagging parameters to estimate\nthe target values. Despite the popularity of Target Networks, their effect on\nthe optimization is still misunderstood. In this work, we show that they act as\nan implicit regularizer. This regularizer has disadvantages such as being\ninflexible and non convex. To overcome these issues, we propose an explicit\nFunctional Regularization that is a convex regularizer in function space and\ncan easily be tuned. We analyze the convergence of our method theoretically and\nempirically demonstrate that replacing Target Networks with the more\ntheoretically grounded Functional Regularization approach leads to better\nsample efficiency and performance improvements.",
    "descriptor": "",
    "authors": [
      "Alexandre Piche",
      "Valentin Thomas",
      "Joseph Marino",
      "Rafael Pardinas",
      "Gian Maria Marconi",
      "Christopher Pal",
      "Mohammad Emtiyaz Khan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12282"
  },
  {
    "id": "arXiv:2210.12283",
    "title": "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal  Proofs",
    "abstract": "The formalization of existing mathematical proofs is a notoriously difficult\nprocess. Despite decades of research on automation and proof assistants,\nwriting formal proofs remains arduous and only accessible to a few experts.\nWhile previous studies to automate formalization focused on powerful search\nalgorithms, no attempts were made to take advantage of available informal\nproofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method\nthat maps informal proofs to formal proof sketches, and uses the sketches to\nguide an automated prover by directing its search to easier sub-problems. We\ninvestigate two relevant setups where informal proofs are either written by\nhumans or generated by a language model. Our experiments and ablation studies\nshow that large language models are able to produce well-structured formal\nsketches that follow the same reasoning steps as the informal proofs. Guiding\nan automated prover with these sketches enhances its performance from 20.9% to\n39.3% on a collection of mathematical competition problems.",
    "descriptor": "",
    "authors": [
      "Albert Q. Jiang",
      "Sean Welleck",
      "Jin Peng Zhou",
      "Wenda Li",
      "Jiacheng Liu",
      "Mateja Jamnik",
      "Timoth\u00e9e Lacroix",
      "Yuhuai Wu",
      "Guillaume Lample"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12283"
  },
  {
    "id": "arXiv:2210.12285",
    "title": "Exploring Representation-Level Augmentation for Code Search",
    "abstract": "Code search, which aims at retrieving the most relevant code fragment for a\ngiven natural language query, is a common activity in software development\npractice. Recently, contrastive learning is widely used in code search\nresearch, where many data augmentation approaches for source code (e.g.,\nsemantic-preserving program transformation) are proposed to learn better\nrepresentations. However, these augmentations are at the raw-data level, which\nrequires additional code analysis in the preprocessing stage and additional\ntraining costs in the training stage. In this paper, we explore augmentation\nmethods that augment data (both code and query) at representation level which\ndoes not require additional data processing and training, and based on this we\npropose a general format of representation-level augmentation that unifies\nexisting methods. Then, we propose three new augmentation methods (linear\nextrapolation, binary interpolation, and Gaussian scaling) based on the general\nformat. Furthermore, we theoretically analyze the advantages of the proposed\naugmentation methods over traditional contrastive learning methods on code\nsearch. We experimentally evaluate the proposed representation-level\naugmentation methods with state-of-the-art code search models on a large-scale\npublic dataset consisting of six programming languages. The experimental\nresults show that our approach can consistently boost the performance of the\nstudied code search models. Our source code is available at\nhttps://github.com/Alex-HaochenLi/RACS.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Haochen Li",
      "Chunyan Miao",
      "Cyril Leung",
      "Yanxian Huang",
      "Yuan Huang",
      "Hongyu Zhang",
      "Yanlin Wang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12285"
  },
  {
    "id": "arXiv:2210.12288",
    "title": "Learning Ultrametric Trees for Optimal Transport Regression",
    "abstract": "Optimal transport provides a metric which quantifies the dissimilarity\nbetween probability measures. For measures supported in discrete metric spaces,\nfinding optimal transport distance has cubic time complexity in the size of the\nspace. However, measures supported on trees admit a closed-form optimal\ntransport which can be computed in linear time. In this paper, we aim to find\nan optimal tree structure for a given discrete metric space, so that the\ntree-Wasserstein distance can best approximate the optimal transport distance\nin the original space. One of our key ideas is to cast the problem in\nultrametric spaces. This helps define different tree structures and allows us\nto optimize the tree structure via projected gradient decent over space of\nultrametric matrices. During optimization, we project the parameters to the\nultrametric space via a hierarchical minimum spanning tree algorithm.\nExperimental results on real datasets show that our approach outperforms\nprevious approaches in approximating optimal transport distances. Finally,\nexperiments on synthetic data generated on ground truth trees show that our\nalgorithm can accurately uncover the underlying tree metrics.",
    "descriptor": "",
    "authors": [
      "Samantha Chen",
      "Puoya Tabaghi",
      "Yusu Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12288"
  },
  {
    "id": "arXiv:2210.12296",
    "title": "Feature selection intelligent algorithm with mutual information and  steepest ascent strategy",
    "abstract": "Remote sensing is a higher technology to produce knowledge for data mining\napplications. In principle hyperspectral images (HSIs) is a remote sensing tool\nthat provides precise classification of regions. The HSI contains more than a\nhundred of images of the ground truth (GT) map. Some images are carrying\nrelevant information, but others describe redundant information, or they are\naffected by atmospheric noise. The aim is to reduce dimensionality of HSI. Many\nstudies use mutual information (MI) or normalised forms of MI to select\nappropriate bands. In this paper we design an algorithm based also on MI, and\nwe combine MI with steepest ascent algorithm, to improve a symmetric\nuncertainty coefficient-based strategy to select relevant bands for\nclassification of HSI. This algorithm is a feature selection tool and a wrapper\nstrategy. We perform our study on HSI AVIRIS 92AV3C. This is an artificial\nintelligent system to control redundancy; we had to clear the difference of the\nresult's algorithm and the human decision, and this can be viewed as case study\nwhich human decision is perhaps different to an intelligent algorithm. Index\nTerms - Hyperspectral images, Classification, Fea-ture selection, Mutual\nInformation, Redundancy, Steepest Ascent. Artificial Intelligence",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1211.0613\n",
    "authors": [
      "Elkebir Sarhrouni",
      "Ahmed Hammouch",
      "Driss Aboutajdine"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12296"
  },
  {
    "id": "arXiv:2210.12298",
    "title": "VRContour: Bringing Contour Delineations of Medical Structures Into  Virtual Reality",
    "abstract": "Contouring is an indispensable step in Radiotherapy (RT) treatment planning.\nHowever, today's contouring software is constrained to only work with a 2D\ndisplay, which is less intuitive and requires high task loads. Virtual Reality\n(VR) has shown great potential in various specialties of healthcare and health\nsciences education due to the unique advantages of intuitive and natural\ninteractions in immersive spaces. VR-based radiation oncology integration has\nalso been advocated as a target healthcare application, allowing providers to\ndirectly interact with 3D medical structures. We present VRContour and\ninvestigate how to effectively bring contouring for radiation oncology into VR.\nThrough an autobiographical iterative design, we defined three design spaces\nfocused on contouring in VR with the support of a tracked tablet and VR stylus,\nand investigating dimensionality for information consumption and input (either\n2D or 2D + 3D). Through a within-subject study (n = 8), we found that\nvisualizations of 3D medical structures significantly increase precision, and\nreduce mental load, frustration, as well as overall contouring effort.\nParticipants also agreed with the benefits of using such metaphors for learning\npurposes.",
    "descriptor": "\nComments: C. Chen, M. Yarmand, V. Singh, M.V. Sherer, J.D. Murphy, Y. Zhang and N. Weibel, \"VRContour: Bringing Contour Delineations of Medical Structures Into Virtual Reality\", 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 2022, pp. 1-10, doi: 10.1109/ISMAR55827.2022.00020\n",
    "authors": [
      "Chen Chen",
      "Matin Yarmand",
      "Varun Singh",
      "Michael V. Sherer",
      "James D. Murphy",
      "Yang Zhang",
      "Nadir Weibel"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.12298"
  },
  {
    "id": "arXiv:2210.12300",
    "title": "BEANS: The Benchmark of Animal Sounds",
    "abstract": "The use of machine learning (ML) based techniques has become increasingly\npopular in the field of bioacoustics over the last years. Fundamental\nrequirements for the successful application of ML based techniques are curated,\nagreed upon, high-quality datasets and benchmark tasks to be learned on a given\ndataset. However, the field of bioacoustics so far lacks such public benchmarks\nwhich cover multiple tasks and species to measure the performance of ML\ntechniques in a controlled and standardized way and that allows for\nbenchmarking newly proposed techniques to existing ones. Here, we propose BEANS\n(the BEnchmark of ANimal Sounds), a collection of bioacoustics tasks and public\ndatasets, specifically designed to measure the performance of machine learning\nalgorithms in the field of bioacoustics. The benchmark proposed here consists\nof two common tasks in bioacoustics: classification and detection. It includes\n12 datasets covering various species, including birds, land and marine mammals,\nanurans, and insects. In addition to the datasets, we also present the\nperformance of a set of standard ML methods as the baseline for task\nperformance. The benchmark and baseline code is made publicly available at\n\\url{https://github.com/earthspecies/beans} in the hope of establishing a new\nstandard dataset for ML-based bioacoustic research.",
    "descriptor": "",
    "authors": [
      "Masato Hagiwara",
      "Benjamin Hoffman",
      "Jen-Yu Liu",
      "Maddie Cusimano",
      "Felix Effenberger",
      "Katie Zacarian"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12300"
  },
  {
    "id": "arXiv:2210.12301",
    "title": "Continual Reinforcement Learning with Group Symmetries",
    "abstract": "Continual reinforcement learning (RL) aims to learn a sequence of tasks while\nretaining the capability to solve seen tasks and growing a new policy to solve\nnovel tasks. Existing continual RL methods ignore that some tasks are\nequivalent under simple group operations, such as rotations or translations.\nThey thus extend a new policy for each equivalent task and train the policy\nfrom scratch, resulting in poor sample complexity and generalization\ncapability. In this work, we propose a novel continual RL framework with group\nsymmetries, which grows a policy for each group of equivalent tasks instead of\na single task. We introduce a PPO-based RL algorithm with an invariant feature\nextractor and a novel task grouping mechanism based on invariant features. We\ntest our algorithm in realistic autonomous driving scenarios, where each group\nis associated with a map configuration. We show that our algorithm assigns\ntasks to different groups with high accuracy and outperforms baselines in terms\nof generalization capability by a large margin.",
    "descriptor": "\nComments: 7 pages, 5 figures\n",
    "authors": [
      "Shiqi Liu",
      "Mengdi Xu",
      "Piede Huang",
      "Yongkang Liu",
      "Kentaro Oguchi",
      "Ding Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12301"
  },
  {
    "id": "arXiv:2210.12302",
    "title": "What do Large Language Models Learn beyond Language?",
    "abstract": "Large language models (LMs) have rapidly become a mainstay in Natural\nLanguage Processing. These models are known to acquire rich linguistic\nknowledge from training on large amounts of text. In this paper, we investigate\nif pre-training on text also confers these models with helpful `inductive\nbiases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic\ntasks involving quantitative computations, recognizing regular expressions and\nreasoning over strings. We find that pretrained models significantly outperform\ncomparable non-pretrained neural models. This remains true also in experiments\nwith training non-pretrained models with fewer parameters to account for model\nregularization effects. We further explore the effect of text domain on LMs by\npretraining models from text from different domains and provenances. Our\nexperiments surprisingly reveal that the positive effects of pre-training\npersist even when pretraining on multi-lingual text or computer code, and even\nfor text generated from synthetic languages. Our findings suggest a hitherto\nunexplored deep connection between pre-training and inductive learning\nabilities of language models.",
    "descriptor": "\nComments: Accepted at the Findings of EMNLP 2022\n",
    "authors": [
      "Avinash Madasu",
      "Shashank Srivastava"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12302"
  },
  {
    "id": "arXiv:2210.12308",
    "title": "PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based  cOnversational uNderstanding",
    "abstract": "Conversational understanding is an integral part of modern intelligent\ndevices. In a large fraction of the global traffic from customers using smart\ndigital assistants, frictions in dialogues may be attributed to incorrect\nunderstanding of the entities in a customer's query due to factors including\nambiguous mentions, mispronunciation, background noise and faulty on-device\nsignal processing. Such errors are compounded by two common deficiencies from\nintelligent devices namely, (1) the device not being tailored to individual\ncustomers, and (2) the device responses being unaware of the context in the\nconversation session. Viewing this problem via the lens of retrieval-based\nsearch engines, we build and evaluate a scalable entity correction system,\nPENTATRON. The system leverages a parametric transformer-based language model\nto learn patterns from in-session customer-device interactions coupled with a\nnon-parametric personalized entity index to compute the correct query, which\naids downstream components in reasoning about the best response. In addition to\nestablishing baselines and demonstrating the value of personalized and\ncontext-aware systems, we use multitasking to learn the domain of the correct\nentity. We also investigate the utility of language model prompts. Through\nextensive experiments, we show a significant upward movement of the key metric\n(Exact Match) by up to 500.97% (relative to the baseline).",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Niranjan Uma Naresh",
      "Ziyan Jiang",
      "Ankit",
      "Sungjin Lee",
      "Jie Hao",
      "Xing Fan",
      "Chenlei Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12308"
  },
  {
    "id": "arXiv:2210.12309",
    "title": "Learning a Grammar Inducer from Massive Uncurated Instructional Videos",
    "abstract": "Video-aided grammar induction aims to leverage video information for finding\nmore accurate syntactic grammars for accompanying text. While previous work\nfocuses on building systems for inducing grammars on text that are well-aligned\nwith video content, we investigate the scenario, in which text and video are\nonly in loose correspondence. Such data can be found in abundance online, and\nthe weak correspondence is similar to the indeterminacy problem studied in\nlanguage acquisition. Furthermore, we build a new model that can better learn\nvideo-span correlation without manually designed features adopted by previous\nwork. Experiments show that our model trained only on large-scale YouTube data\nwith no text-video alignment reports strong and robust performances across\nthree unseen datasets, despite domain shift and noisy label issues. Furthermore\nour model yields higher F1 scores than the previous state-of-the-art systems\ntrained on in-domain data.",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Songyang Zhang",
      "Linfeng Song",
      "Lifeng Jin",
      "Haitao Mi",
      "Kun Xu",
      "Dong Yu",
      "Jiebo Luo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.12309"
  },
  {
    "id": "arXiv:2210.12310",
    "title": "Tools for Extracting Spatio-Temporal Patterns in Meteorological Image  Sequences: From Feature Engineering to Attention-Based Neural Networks",
    "abstract": "Atmospheric processes involve both space and time. This is why human analysis\nof atmospheric imagery can often extract more information from animated loops\nof image sequences than from individual images. Automating such an analysis\nrequires the ability to identify spatio-temporal patterns in image sequences\nwhich is a very challenging task, because of the endless possibilities of\npatterns in both space and time. In this paper we review different concepts and\ntechniques that are useful to extract spatio-temporal context specifically for\nmeteorological applications. In this survey we first motivate the need for\nthese approaches in meteorology using two applications, solar forecasting and\ndetecting convection from satellite imagery. Then we provide an overview of\nmany different concepts and techniques that are helpful for the interpretation\nof meteorological image sequences, such as (1) feature engineering methods to\nstrengthen the desired signal in the input, using meteorological knowledge,\nclassic image processing, harmonic analysis and topological data analysis (2)\nexplain how different convolution filters (2D/3D/LSTM-convolution) can be\nutilized strategically in convolutional neural network architectures to find\npatterns in both space and time (3) discuss the powerful new concept of\n'attention' in neural networks and the powerful abilities it brings to the\ninterpretation of image sequences (4) briefly survey strategies from\nunsupervised, self-supervised and transfer learning to reduce the need for\nlarge labeled datasets. We hope that presenting an overview of these tools -\nmany of which are underutilized - will help accelerate progress in this area.",
    "descriptor": "\nComments: The paper is submitted to the EDS Journal\n",
    "authors": [
      "Akansha Singh Bansal",
      "Yoonjin Lee",
      "Kyle Hilburn",
      "Imme Ebert-Uphoff"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12310"
  },
  {
    "id": "arXiv:2210.12314",
    "title": "A Benchmark Study of Contrastive Learning for Arabic Social Meaning",
    "abstract": "Contrastive learning (CL) brought significant progress to various NLP tasks.\nDespite this progress, CL has not been applied to Arabic NLP to date. Nor is it\nclear how much benefits it could bring to particular classes of tasks such as\nthose involved in Arabic social meaning (e.g., sentiment analysis, dialect\nidentification, hate speech detection). In this work, we present a\ncomprehensive benchmark study of state-of-the-art supervised CL methods on a\nwide array of Arabic social meaning tasks. Through extensive empirical\nanalyses, we show that CL methods outperform vanilla finetuning on most tasks\nwe consider. We also show that CL can be data efficient and quantify this\nefficiency. Overall, our work allows us to demonstrate the promise of CL\nmethods, including in low-resource settings.",
    "descriptor": "",
    "authors": [
      "Md Tawkat Islam Khondaker",
      "El Moatez Billah Nagoudi",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed",
      "Laks V.S. Lakshmanan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12314"
  },
  {
    "id": "arXiv:2210.12315",
    "title": "Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion  Model",
    "abstract": "We propose a simple and novel method for generating 3D human motion from\ncomplex natural language sentences, which describe different velocity,\ndirection and composition of all kinds of actions. Different from existing\nmethods that use classical generative architecture, we apply the Denoising\nDiffusion Probabilistic Model to this task, synthesizing diverse motion results\nunder the guidance of texts. The diffusion model converts white noise into\nstructured 3D motion by a Markov process with a series of denoising steps and\nis efficiently trained by optimizing a variational lower bound. To achieve the\ngoal of text-conditioned image synthesis, we use the classifier-free guidance\nstrategy to fuse text embedding into the model during training. Our experiments\ndemonstrate that our model achieves competitive results on HumanML3D test set\nquantitatively and can generate more visually natural and diverse examples. We\nalso show with experiments that our model is capable of zero-shot generation of\nmotions for unseen text guidance.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Zhiyuan Ren",
      "Zhihong Pan",
      "Xin Zhou",
      "Le Kang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12315"
  },
  {
    "id": "arXiv:2210.12316",
    "title": "Learning Vector-Quantized Item Representation for Transferable  Sequential Recommenders",
    "abstract": "Recently, the generality of natural language text has been leveraged to\ndevelop transferable recommender systems. The basic idea is to employ\npre-trained language model (PLM) to encode item text into item representations.\nDespite the promising transferability, the binding between item text and item\nrepresentations might be too tight, leading to potential problems such as\nover-emphasizing text similarity and exaggerating domain gaps. To address this\nissue, this paper proposes VQ-Rec, a novel approach to learning\nVector-Quantized item representations for transferable sequential Recommender.\nThe major novelty of our approach lies in the new item representation scheme:\nit first maps item text into a vector of discrete indices (called item code),\nand then employs these indices to lookup the code embedding table for deriving\nitem representations. Such a scheme can be denoted as \"text -> code ->\nrepresentation\". Based on this representation scheme, we further propose an\nenhanced contrastive pre-training approach, using semi-synthetic and\nmixed-domain code representations as hard negatives. Furthermore, we design a\nnew cross-domain fine-tuning method based on a differentiable permutation-based\nnetwork. Extensive experiments conducted on six public benchmarks demonstrate\nthe effectiveness of the proposed approach, in both cross-domain and\ncross-platform settings.",
    "descriptor": "",
    "authors": [
      "Yupeng Hou",
      "Zhankui He",
      "Julian McAuley",
      "Wayne Xin Zhao"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12316"
  },
  {
    "id": "arXiv:2210.12317",
    "title": "Attitude Control of Highly Maneuverable Aircraft Using an Improved  Q-learning",
    "abstract": "Attitude control of a novel regional truss-braced wing aircraft with low\nstability characteristics is addressed in this paper using Reinforcement\nLearning (RL). In recent years, RL has been increasingly employed in\nchallenging applications, particularly, autonomous flight control. However, a\nsignificant predicament confronting discrete RL algorithms is the dimension\nlimitation of the state-action table and difficulties in defining the elements\nof the RL environment. To address these issues, in this paper, a detailed\nmathematical model of the mentioned aircraft is first developed to shape an RL\nenvironment. Subsequently, Q-learning, the most prevalent discrete RL algorithm\nwill be implemented in both the Markov Decision Process (MDP), and Partially\nObservable Markov Decision Process (POMDP) frameworks to control the\nlongitudinal mode of the air vehicle. In order to eliminate residual\nfluctuations that are a consequence of discrete action selection, and\nsimultaneously track variable pitch angles, a Fuzzy Action Assignment (FAA)\nmethod is proposed to generate continuous control commands using the trained\nQ-table. Accordingly, it will be proved that by defining an accurate reward\nfunction, along with observing all crucial states, the performance of the\nintroduced control system surpasses a well-tuned PID controller.",
    "descriptor": "\nComments: 7 pages, 7 figures\n",
    "authors": [
      "Mohsen Zahmatkesh",
      "Seyyed Ali Emami",
      "Afshin Banazadeh",
      "Paolo Castaldi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12317"
  },
  {
    "id": "arXiv:2210.12321",
    "title": "A Comprehensive Comparison of Neural Networks as Cognitive Models of  Inflection",
    "abstract": "Neural networks have long been at the center of a debate around the cognitive\nmechanism by which humans process inflectional morphology. This debate has\ngravitated into NLP by way of the question: Are neural networks a feasible\naccount for human behavior in morphological inflection? We address that\nquestion by measuring the correlation between human judgments and neural\nnetwork probabilities for unknown word inflections. We test a larger range of\narchitectures than previously studied on two important tasks for the cognitive\nprocessing debate: English past tense, and German number inflection. We find\nevidence that the Transformer may be a better account of human behavior than\nLSTMs on these datasets, and that LSTM features known to increase inflection\naccuracy do not always result in more human-like behavior.",
    "descriptor": "",
    "authors": [
      "Adam Wiemerslage",
      "Shiran Dudy",
      "Katharina Kann"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12321"
  },
  {
    "id": "arXiv:2210.12324",
    "title": "Trustworthy Human Computation: A Survey",
    "abstract": "Human computation is an approach to solving problems that prove difficult\nusing AI only, and involves the cooperation of many humans. Because human\ncomputation requires close engagement with both \"human populations as users\"\nand \"human populations as driving forces,\" establishing mutual trust between AI\nand humans is an important issue to further the development of human\ncomputation. This survey lays the groundwork for the realization of trustworthy\nhuman computation. First, the trustworthiness of human computation as computing\nsystems, that is, trust offered by humans to AI, is examined using the RAS\n(Reliability, Availability, and Serviceability) analogy, which define measures\nof trustworthiness in conventional computer systems. Next, the social\ntrustworthiness provided by human computation systems to users or participants\nis discussed from the perspective of AI ethics, including fairness, privacy,\nand transparency. Then, we consider human--AI collaboration based on two-way\ntrust, in which humans and AI build mutual trust and accomplish difficult tasks\nthrough reciprocal collaboration. Finally, future challenges and research\ndirections for realizing trustworthy human computation are discussed.",
    "descriptor": "\nComments: 35 pages, 2 figures, 9 tables\n",
    "authors": [
      "Hisashi Kashima",
      "Satoshi Oyama",
      "Hiromi Arai",
      "Junichiro Mori"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12324"
  },
  {
    "id": "arXiv:2210.12326",
    "title": "Transformer-Based Conditioned Variational Autoencoder for Dialogue  Generation",
    "abstract": "In human dialogue, a single query may elicit numerous appropriate responses.\nThe Transformer-based dialogue model produces frequently occurring sentences in\nthe corpus since it is a one-to-one mapping function. CVAE is a technique for\nreducing generic replies. In this paper, we create a new dialogue model\n(CVAE-T) based on the Transformer with CVAE structure. We use a pre-trained MLM\nmodel to rewrite some key n-grams in responses to obtain a series of negative\nexamples, and introduce a regularization term during training to explicitly\nguide the latent variable in learning the semantic differences between each\npair of positive and negative examples. Experiments suggest that the method we\ndesign is capable of producing more informative replies.",
    "descriptor": "",
    "authors": [
      "Huihui Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12326"
  },
  {
    "id": "arXiv:2210.12327",
    "title": "Design and Performance Analysis of a Chemically-Etched Flexible NFC Tag  Antenna",
    "abstract": "Near Field Communication (NFC) is a perfect example of ubiquitous computing\nthat is secured, short-ranged, low-powered contactless communication. High\ndemand is predicted for NFC, especially with wearables, and the variety of\napplications may require that this technology be fabricated onto different\nmaterials. In this research, we first designed flexible NFC antennas based on\nmathematical models. A simulation of the NFC coil inductance was performed and\nverified using the predictive models. Later, two flexible NFC antennas,\nincluding 160x80 mm2 (rectangular) and 80x80 mm2 (square) were fabricated\nthrough a chemical etching process and verified at 13.56 MHz frequency. Two\nexperiments 1) Read range detection and 2) User experience study on 22\nparticipants validate longer read range and shorter connection time of our\nfabricated flexible NFC tags.",
    "descriptor": "",
    "authors": [
      "Muhammad Enayetur Rahman",
      "Marwan Abdelatti",
      "Manbir Singh Sodhi",
      "Kunal Mankodiya"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.12327"
  },
  {
    "id": "arXiv:2210.12328",
    "title": "R$^2$F: A General Retrieval, Reading and Fusion Framework for  Document-level Natural Language Inference",
    "abstract": "Document-level natural language inference (DOCNLI) is a new challenging task\nin natural language processing, aiming at judging the entailment relationship\nbetween a pair of hypothesis and premise documents. Current datasets and\nbaselines largely follow sentence-level settings, but fail to address the\nissues raised by longer documents. In this paper, we establish a general\nsolution, named Retrieval, Reading and Fusion (R2F) framework, and a new\nsetting, by analyzing the main challenges of DOCNLI: interpretability,\nlong-range dependency, and cross-sentence inference. The basic idea of the\nframework is to simplify document-level task into a set of sentence-level\ntasks, and improve both performance and interpretability with the power of\nevidence. For each hypothesis sentence, the framework retrieves evidence\nsentences from the premise, and reads to estimate its credibility. Then the\nsentence-level results are fused to judge the relationship between the\ndocuments. For the setting, we contribute complementary evidence and entailment\nlabel annotation on hypothesis sentences, for interpretability study. Our\nexperimental results show that R2F framework can obtain state-of-the-art\nperformance and is robust for diverse evidence retrieval methods. Moreover, it\ncan give more interpretable prediction results. Our model and code are released\nat https://github.com/phoenixsecularbird/R2F.",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Hao Wang",
      "Yixin Cao",
      "Yangguang Li",
      "Zhen Huang",
      "Kun Wang",
      "Jing Shao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12328"
  },
  {
    "id": "arXiv:2210.12329",
    "title": "ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback",
    "abstract": "Recently, dataset-generation-based zero-shot learning has shown promising\nresults by training a task-specific model with a dataset synthesized from large\npre-trained language models (PLMs). The final task-specific model often\nachieves compatible or even better performance than PLMs under the zero-shot\nsetting, with orders of magnitude fewer parameters. However, synthetic datasets\nhave their drawbacks. They have long been suffering from low-quality issues\n(e.g., low informativeness and redundancy). This explains why the massive\nsynthetic data does not lead to better performance -- a scenario we would\nexpect in the human-labeled data. To improve the quality of dataset synthesis,\nwe propose a progressive zero-shot dataset generation framework, ProGen, which\nleverages the feedback from the task-specific model to guide the generation of\nnew training data via in-context examples. Extensive experiments on five text\nclassification datasets demonstrate the effectiveness of the proposed approach.\nWe also show ProGen achieves on-par or superior performance with only 1\\%\nsynthetic dataset size compared to baseline methods without in-context\nfeedback.",
    "descriptor": "\nComments: Accepted by EMNLP 2022 (Findings)\n",
    "authors": [
      "Jiacheng Ye",
      "Jiahui Gao",
      "Jiangtao Feng",
      "Zhiyong Wu",
      "Tao Yu",
      "Lingpeng Kong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12329"
  },
  {
    "id": "arXiv:2210.12330",
    "title": "Salience Allocation as Guidance for Abstractive Summarization",
    "abstract": "Abstractive summarization models typically learn to capture the salient\ninformation from scratch implicitly. Recent literature adds extractive\nsummaries as guidance for abstractive summarization models to provide hints of\nsalient content and achieves better performance. However, extractive summaries\nas guidance could be over strict, leading to information loss or noisy signals.\nFurthermore, it cannot easily adapt to documents with various abstractiveness.\nAs the number and allocation of salience content pieces vary, it is hard to\nfind a fixed threshold deciding which content should be included in the\nguidance. In this paper, we propose a novel summarization approach with a\nflexible and reliable salience guidance, namely SEASON (SaliencE Allocation as\nGuidance for Abstractive SummarizatiON). SEASON utilizes the allocation of\nsalience expectation to guide abstractive summarization and adapts well to\narticles in different abstractiveness. Automatic and human evaluations on two\nbenchmark datasets show that the proposed method is effective and reliable.\nEmpirical results on more than one million news articles demonstrate a natural\nfifteen-fifty salience split for news article sentences, providing a useful\ninsight for composing news articles.",
    "descriptor": "\nComments: Accepted by EMNLP 2022. Code and model weights are available at this https URL\n",
    "authors": [
      "Fei Wang",
      "Kaiqiang Song",
      "Hongming Zhang",
      "Lifeng Jin",
      "Sangwoo Cho",
      "Wenlin Yao",
      "Xiaoyang Wang",
      "Muhao Chen",
      "Dong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12330"
  },
  {
    "id": "arXiv:2210.12333",
    "title": "Accumulated Trivial Attention Matters in Vision Transformers on Small  Datasets",
    "abstract": "Vision Transformers has demonstrated competitive performance on computer\nvision tasks benefiting from their ability to capture long-range dependencies\nwith multi-head self-attention modules and multi-layer perceptron. However,\ncalculating global attention brings another disadvantage compared with\nconvolutional neural networks, i.e. requiring much more data and computations\nto converge, which makes it difficult to generalize well on small datasets,\nwhich is common in practical applications. Previous works are either focusing\non transferring knowledge from large datasets or adjusting the structure for\nsmall datasets. After carefully examining the self-attention modules, we\ndiscover that the number of trivial attention weights is far greater than the\nimportant ones and the accumulated trivial weights are dominating the attention\nin Vision Transformers due to their large quantity, which is not handled by the\nattention itself. This will cover useful non-trivial attention and harm the\nperformance when trivial attention includes more noise, e.g. in shallow layers\nfor some backbones. To solve this issue, we proposed to divide attention\nweights into trivial and non-trivial ones by thresholds, then Suppressing\nAccumulated Trivial Attention (SATA) weights by proposed Trivial WeIghts\nSuppression Transformation (TWIST) to reduce attention noise. Extensive\nexperiments on CIFAR-100 and Tiny-ImageNet datasets show that our suppressing\nmethod boosts the accuracy of Vision Transformers by up to 2.3%. Code is\navailable at https://github.com/xiangyu8/SATA.",
    "descriptor": "\nComments: Camera-Ready Version for WACV 2023\n",
    "authors": [
      "Xiangyu Chen",
      "Qinghao Hu",
      "Kaidong Li",
      "Cuncong Zhong",
      "Guanghui Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12333"
  },
  {
    "id": "arXiv:2210.12335",
    "title": "Guided contrastive self-supervised pre-training for automatic speech  recognition",
    "abstract": "Contrastive Predictive Coding (CPC) is a representation learning method that\nmaximizes the mutual information between intermediate latent representations\nand the output of a given model. It can be used to effectively initialize the\nencoder of an Automatic Speech Recognition (ASR) model. We present a novel\nmodification of CPC called Guided Contrastive Predictive Coding (GCPC). Our\nproposed method maximizes the mutual information between representations from a\nprior-knowledge model and the output of the model being pre-trained, allowing\nprior knowledge injection during pre-training. We validate our method on 3 ASR\ntasks: German, French and English. Our method outperforms CPC pre-training on\nall three datasets, reducing the Word Error Rate (WER) by 4.44%, 6.55% and\n15.43% relative on the German, French and English (Librispeech) tasks\nrespectively, compared to training from scratch, while CPC pre-training only\nbrings 2.96%, 1.01% and 14.39% relative WER reduction respectively.",
    "descriptor": "\nComments: To appear in SLT 2022\n",
    "authors": [
      "Aparna Khare",
      "Minhua Wu",
      "Saurabhchand Bhati",
      "Jasha Droppo",
      "Roland Maas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12335"
  },
  {
    "id": "arXiv:2210.12338",
    "title": "Open-domain Question Answering via Chain of Reasoning over Heterogeneous  Knowledge",
    "abstract": "We propose a novel open-domain question answering (ODQA) framework for\nanswering single/multi-hop questions across heterogeneous knowledge sources.\nThe key novelty of our method is the introduction of the intermediary modules\ninto the current retriever-reader pipeline. Unlike previous methods that solely\nrely on the retriever for gathering all evidence in isolation, our intermediary\nperforms a chain of reasoning over the retrieved set. Specifically, our method\nlinks the retrieved evidence with its related global context into graphs and\norganizes them into a candidate list of evidence chains. Built upon pretrained\nlanguage models, our system achieves competitive performance on two ODQA\ndatasets, OTT-QA and NQ, against tables and passages from Wikipedia. In\nparticular, our model substantially outperforms the previous state-of-the-art\non OTT-QA with an exact match score of 47.3 (45 % relative gain).",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Kaixin Ma",
      "Hao Cheng",
      "Xiaodong Liu",
      "Eric Nyberg",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12338"
  },
  {
    "id": "arXiv:2210.12339",
    "title": "P$^3$LM: Probabilistically Permuted Prophet Language Modeling for  Generative Pre-Training",
    "abstract": "Conventional autoregressive left-to-right (L2R) sequence generation faces two\nissues during decoding: limited to unidirectional target sequence modeling, and\nconstrained on strong local dependencies. To address the aforementioned\nproblem, we propose P$^3$LM, a probabilistically permuted prophet language\nmodel, which strengthens the modeling of bidirectional information and long\ntoken dependencies for sequence generation. Specifically, P$^3$LM learns to\ngenerate tokens in permuted order upon an order-aware transformer decoder, as\nwell as to generate the corresponding future $N$ tokens with a multi-stream\nattention mechanism. Extensive experiments are conducted on the GLGE benchmark,\nwhich includes four datasets for summarization, two for question generation,\none for conversational question answering, and one for dialog response\ngeneration, where P$^3$LM achieves state-of-the-art results compared with\nstrong publicly available generative pre-training methods.",
    "descriptor": "\nComments: Accepted to EMNLP(Findings) 2022\n",
    "authors": [
      "Junwei Bao",
      "Yifan Wang",
      "Jiangyong Ying",
      "Yeyun Gong",
      "Jing Zhao",
      "Youzheng Wu",
      "Xiaodong He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12339"
  },
  {
    "id": "arXiv:2210.12342",
    "title": "Detection of Risk Predictors of COVID-19 Mortality with Classifier  Machine Learning Models Operated with Routine Laboratory Biomarkers",
    "abstract": "Early evaluation of patients who require special care and high death\nexpectancy in COVID-19 and effective determination of relevant biomarkers on\nlarge sample groups are important to reduce mortality. This study aimed to\nreveal the routine blood value predictors of COVID-19 mortality and to\ndetermine the lethal risk levels of these predictors during the disease\nprocess. The dataset of the study consists of 38 routine blood values of 2597\npatients who died (n = 233) and recovered (n = 2364) from COVID-19 in\nAugust-December, 2021. In this study, histogram-based gradient boosting (HGB)\nmodel was the most successful mashine learning classifier in detecting living\nand deceased COVID-19 patients (with squared F1 metrics F1^2 = 1). The most\nefficient binary combinations with procalcitonin were obtained with D-dimer,\nESR, D.Bil and ferritin. The HGB model operated with these couples correctly\ndetected almost all of the patients who survived and died. (precision > 0.98,\nrecall > 0.98, F1^2 > 0.98). Furthermore, in the HGB model operated with a\nsingle feature, the most efficient features were Procalcitonin (F1^2 = 0.96)\nand ferritin (F1^2 = 0.91). In addition, according to the two-threshold\napproach ferritin values between 376.2 mkg/L and 396.0 mkg/L (F1^2 = 0.91) and\nprocalcitonin values between 0.2 mkg/L and 5.2 mkg/L (F1^2 = 0.95) were found\nto be fatal risk levels for COVID-19. Considering all the results, we suggest\nthat many features combined with these features, especially procalcitonin and\nferritin, operated with the HGB model, can be used to achieve very successful\nresults in the classification of those who live and die from COVID-19.Moreover,\nwe strongly recommend that clinicians consider the critical levels we have\nfound for procalcitonin and ferritin properties to reduce the lethality of\nCOVID-19 disease.",
    "descriptor": "\nComments: 34 pages, 13 figures, 6 tables\n",
    "authors": [
      "Mehmet Tahir Huyut",
      "Andrei Velichko",
      "Maksim Belyaev"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Medical Physics (physics.med-ph)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.12342"
  },
  {
    "id": "arXiv:2210.12345",
    "title": "Neural Sound Field Decomposition with Super-resolution of Sound  Direction",
    "abstract": "Sound field decomposition predicts waveforms in arbitrary directions using\nsignals from a limited number of microphones as inputs. Sound field\ndecomposition is fundamental to downstream tasks, including source\nlocalization, source separation, and spatial audio reproduction. Conventional\nsound field decomposition methods such as Ambisonics have limited spatial\ndecomposition resolution. This paper proposes a learning-based Neural Sound\nfield Decomposition (NeSD) framework to allow sound field decomposition with\nfine spatial direction resolution, using recordings from microphone capsules of\na few microphones at arbitrary positions. The inputs of a NeSD system include\nmicrophone signals, microphone positions, and queried directions. The outputs\nof a NeSD include the waveform and the presence probability of a queried\nposition. We model the NeSD systems respectively with different neural\nnetworks, including fully connected, time delay, and recurrent neural networks.\nWe show that the NeSD systems outperform conventional Ambisonics and DOANet\nmethods in sound field decomposition and source localization on speech, music,\nand sound events datasets. Demos are available at\nhttps://www.youtube.com/watch?v=0GIr6doj3BQ.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Qiuqiang Kong",
      "Shilei Liu",
      "Junjie Shi",
      "Xuzhou Ye",
      "Yin Cao",
      "Qiaoxi Zhu",
      "Yong Xu",
      "Yuxuan Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12345"
  },
  {
    "id": "arXiv:2210.12346",
    "title": "AI-based Arabic Language and Speech Tutor",
    "abstract": "In the past decade, we have observed a growing interest in using technologies\nsuch as artificial intelligence (AI), machine learning, and chatbots to provide\nassistance to language learners, especially in second language learning. By\nusing AI and natural language processing (NLP) and chatbots, we can create an\nintelligent self-learning environment that goes beyond multiple-choice\nquestions and/or fill in the blank exercises. In addition, NLP allows for\nlearning to be adaptive in that it offers more than an indication that an error\nhas occurred. It also provides a description of the error, uses linguistic\nanalysis to isolate the source of the error, and then suggests additional\ndrills to achieve optimal individualized learning outcomes. In this paper, we\npresent our approach for developing an Artificial Intelligence-based Arabic\nLanguage and Speech Tutor (AI-ALST) for teaching the Moroccan Arabic dialect.\nThe AI-ALST system is an intelligent tutor that provides analysis and\nassessment of students learning the Moroccan dialect at University of Arizona\n(UA). The AI-ALST provides a self-learned environment to practice each lesson\nfor pronunciation training. In this paper, we present our initial experimental\nevaluation of the AI-ALST that is based on MFCC (Mel frequency cepstrum\ncoefficient) feature extraction, bidirectional LSTM (Long Short-Term Memory),\nattention mechanism, and a cost-based strategy for dealing with class-imbalance\nlearning. We evaluated our tutor on the word pronunciation of lesson 1 of the\nMoroccan Arabic dialect class. The experimental results show that the AI-ALST\ncan effectively and successfully detect pronunciation errors and evaluate its\nperformance by using F_1-score, accuracy, precision, and recall.",
    "descriptor": "\nComments: Accepted to the 19th IEEE/ACS International Conference on Computer Systems and Applications\n",
    "authors": [
      "Sicong Shao",
      "Saleem Alharir",
      "Salim Hariri",
      "Pratik Satam",
      "Sonia Shiri",
      "Abdessamad Mbarki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12346"
  },
  {
    "id": "arXiv:2210.12347",
    "title": "Quantifying Complexity: An Object-Relations Approach to Complex Systems",
    "abstract": "The best way to model, understand, and quantify the information contained in\ncomplex systems is an open question in physics, mathematics, and computer\nscience. The uncertain relationship between entropy and complexity further\ncomplicates this question. With ideas drawn from the object-relations theory of\npsychology, this paper develops an object-relations model of complex systems\nwhich generalizes to systems of all types, including mathematical operations,\nmachines, biological organisms, and social structures. The resulting Complex\nInformation Entropy (CIE) equation is a robust method to quantify complexity\nacross various contexts. The paper also describes algorithms to iteratively\nupdate and improve approximate solutions to the CIE equation, to recursively\ninfer the composition of complex systems, and to discover the connections among\nobjects across different lengthscales and timescales. Applications are\ndiscussed in the fields of engineering design, atomic and molecular physics,\nchemistry, materials science, neuroscience, psychology, sociology, ecology,\neconomics, and medicine.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2210.07202\n",
    "authors": [
      "Stephen Casey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2210.12347"
  },
  {
    "id": "arXiv:2210.12348",
    "title": "A Task-aware Dual Similarity Network for Fine-grained Few-shot Learning",
    "abstract": "The goal of fine-grained few-shot learning is to recognize sub-categories\nunder the same super-category by learning few labeled samples. Most of the\nrecent approaches adopt a single similarity measure, that is, global or local\nmeasure alone. However, for fine-grained images with high intra-class variance\nand low inter-class variance, exploring global invariant features and\ndiscriminative local details is quite essential. In this paper, we propose a\nTask-aware Dual Similarity Network(TDSNet), which applies global features and\nlocal patches to achieve better performance. Specifically, a local feature\nenhancement module is adopted to activate the features with strong\ndiscriminability. Besides, task-aware attention exploits the important patches\namong the entire task. Finally, both the class prototypes obtained by global\nfeatures and discriminative local patches are employed for prediction.\nExtensive experiments on three fine-grained datasets demonstrate that the\nproposed TDSNet achieves competitive performance by comparing with other\nstate-of-the-art algorithms.",
    "descriptor": "\nComments: accepted by PRICAI2022, codes: this https URL\n",
    "authors": [
      "Yan Qi",
      "Han Sun",
      "Ningzhong Liu",
      "Huiyu Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12348"
  },
  {
    "id": "arXiv:2210.12350",
    "title": "Context-Aware Image Completion",
    "abstract": "Image completion is a task that aims to fill in the missing region of a\nmasked image with plausible contents.However, existing image completion methods\ntend to fill in the missing region with the surrounding texture instead of\nhallucinating a visual instance that is suitable in accordance with the context\nof the scene. In this work, we propose a novel image completion model, dubbed\nRefill, that hallucinates the missing instance that harmonizes well with - and\nthus preserves - the original context. Refill first adopts a transformer\narchitecture that considers the types, locations of the visible instances, and\nthe location of the missing region. Then, Refill completes the missing\nforeground and background semantic segmentation masks within the missing\nregion, providing pixel-level semantic and structural guidance to generate\nmissing contents with seamless boundaries. Finally, we condition the image\nsynthesis blocks of Refill using the completed segmentation mask to generate\nphoto-realistic contents to fill out the missing region. Experimental results\nshow the superiority of Refill over state-of-the-art image completion\napproaches on various natural images.",
    "descriptor": "",
    "authors": [
      "Jinoh Cho",
      "Minguk Kang",
      "Vibhav Vineet",
      "Jaesik Park"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12350"
  },
  {
    "id": "arXiv:2210.12352",
    "title": "NeuPhysics: Editable Neural Geometry and Physics from Monocular Videos",
    "abstract": "We present a method for learning 3D geometry and physics parameters of a\ndynamic scene from only a monocular RGB video input. To decouple the learning\nof underlying scene geometry from dynamic motion, we represent the scene as a\ntime-invariant signed distance function (SDF) which serves as a reference\nframe, along with a time-conditioned deformation field. We further bridge this\nneural geometry representation with a differentiable physics simulator by\ndesigning a two-way conversion between the neural field and its corresponding\nhexahedral mesh, enabling us to estimate physics parameters from the source\nvideo by minimizing a cycle consistency loss. Our method also allows a user to\ninteractively edit 3D objects from the source video by modifying the recovered\nhexahedral mesh, and propagating the operation back to the neural field\nrepresentation. Experiments show that our method achieves superior mesh and\nvideo reconstruction of dynamic scenes compared to competing Neural Field\napproaches, and we provide extensive examples which demonstrate its ability to\nextract useful 3D representations from videos captured with consumer-grade\ncameras.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Yi-Ling Qiao",
      "Alexander Gao",
      "Ming C. Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12352"
  },
  {
    "id": "arXiv:2210.12353",
    "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
    "abstract": "While large language models (LLMs) like GPT-3 have achieved impressive\nresults on multiple choice question answering (MCQA) tasks in the zero, one,\nand few-shot settings, they generally lag behind the MCQA state of the art\n(SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks.\nAn LLM is conditioned on a question (without the associated answer options) and\nits chosen option is the one assigned the highest probability after\nnormalization (for length, etc.). A more natural prompting approach is to\npresent the question and answer options to the LLM jointly and have it output\nthe symbol (e.g., \"A\") associated with its chosen answer option. This approach\nallows the model to explicitly compare answer options, reduces computational\ncosts, and mitigates the effects of tokenization scheme and answer option\nrepresentations on answer selection. For the natural approach to be effective\nthe LLM it is used with must be able to associate answer options with the\nsymbols that represent them. The LLM needs what we term multiple choice symbol\nbinding (MCSB) ability. This ability varies greatly by model. We show that a\nmodel with high MCSB ability performs much better with the natural approach\nthan with the traditional approach across 20 diverse datasets and largely\ncloses the gap with the SOTA, suggesting that the MCQA ability of LLMs has been\npreviously underestimated.",
    "descriptor": "",
    "authors": [
      "Joshua Robinson",
      "Christopher Michael Rytting",
      "David Wingate"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12353"
  },
  {
    "id": "arXiv:2210.12357",
    "title": "Information-Transport-based Policy for Simultaneous Translation",
    "abstract": "Simultaneous translation (ST) outputs translation while receiving the source\ninputs, and hence requires a policy to determine whether to translate a target\ntoken or wait for the next source token. The major challenge of ST is that each\ntarget token can only be translated based on the current received source\ntokens, where the received source information will directly affect the\ntranslation quality. So naturally, how much source information is received for\nthe translation of the current target token is supposed to be the pivotal\nevidence for the ST policy to decide between translating and waiting. In this\npaper, we treat the translation as information transport from source to target\nand accordingly propose an Information-Transport-based Simultaneous Translation\n(ITST). ITST quantifies the transported information weight from each source\ntoken to the current target token, and then decides whether to translate the\ntarget token according to its accumulated received information. Experiments on\nboth text-to-text ST and speech-to-text ST (a.k.a., streaming speech\ntranslation) tasks show that ITST outperforms strong baselines and achieves\nstate-of-the-art performance.",
    "descriptor": "\nComments: Accept to EMNLP 2022 main conference. 22 pages, 16 figures, 8 tables\n",
    "authors": [
      "Shaolei Zhang",
      "Yang Feng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12357"
  },
  {
    "id": "arXiv:2210.12358",
    "title": "Performance Analysis of the Full-Duplex Communicating-Radar Convergence  System",
    "abstract": "This paper aims to explore the feasibility of the spectrum sharing between\nthe communication and radar system. We investigate the full-duplex (FD) joint\nradar and communication multi-antenna system in which a node labeled ComRad\nwith a dual communication and radar capability is communicating with a downlink\nand an uplink users, as well as detecting the target of interest\nsimultaneously. Considering a full interference scenario and imperfect channel\nstate information (CSI), the fundamental performance limits of the FD JRC\nsystem are analyzed. In particular, we first obtain the downlink rate when the\nradar signals act as interference. Then, viewing the uplink channel and radar\nreturn channel as a multiple access channel, we propose an alternative\nsuccessive interference cancellation scheme, based on which the achievable\nuplink communication rate is obtained. For the radar operation, we first derive\nthe general expression of the estimation rate, which quantifies how much\ninformation is obtained about the target in terms of the direction, the range\nand the velocity. Considering the uniform linear antenna array and linear\nfrequency modulated radar signals, we further obtain the exact closed-form\nestimation rate. Numerical simulations reveal that the joint manner of the\ncommunication and radar operations achieves larger rate regions compared to\nthat of working independently.",
    "descriptor": "\nComments: 31 pages, 9 figures, Accepted by China Communication\n",
    "authors": [
      "Yinghong Guo",
      "Cheng Li",
      "Chaoxian Zhang",
      "Yao Yao",
      "Bin Xia"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12358"
  },
  {
    "id": "arXiv:2210.12359",
    "title": "A Discipline of Programming with Quantities",
    "abstract": "In scientific and engineering applications, physical quantities embodied as\nunits of measurement (UoM) are frequently used. The loss of the Mars climate\norbiter, attributed to a confusion between the metric and imperial unit\nsystems, popularised the disastrous consequences of incorrectly handling\nmeasurement values. Dimensional analysis can be used to ensure expressions\ncontaining annotated values are evaluated correctly. This has led to the\ndevelopment of a large number of libraries, languages and validators to ensure\ndevelopers can specify and verify UoM information in their designs and codes.\nMany tools can also automatically convert values between commensurable UoM,\nsuch as yards and metres. However these systems do not differentiate between\nquantities and dimensions. For instance torque and work, which share the same\nUoM, can not be interchanged because they do not represent the same entity. We\npresent a named quantity layer that complements dimensional analysis by\nensuring that values of different quantities are safely managed. Our technique\nis a mixture of analysis and discipline, where expressions involving\nmultiplications are relegated to functions, in order to ensure that named\nquantities are handled soundly.",
    "descriptor": "\nComments: This paper extends previous work to include a notion of safe KOQ arithmetic and a demonstration of how this discipline deals with information loss, along with a streamlined functional presentation of the checking algorithm\n",
    "authors": [
      "Steve McKeever"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.12359"
  },
  {
    "id": "arXiv:2210.12360",
    "title": "Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual  Understanding With Multilingual Language Models",
    "abstract": "Pre-trained multilingual language models show significant performance gains\nfor zero-shot cross-lingual model transfer on a wide range of natural language\nunderstanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,\npre-trained models are only fine-tuned on English data and tested on a variety\nof target languages. In this paper, we do cross-lingual evaluation on various\nNLU tasks (sentence classification, sequence labeling, question answering)\nusing prompt-tuning and compare it with fine-tuning. The results show that\nprompt tuning achieves much better cross-lingual transfer than fine-tuning\nacross datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we\ndemonstrate through the analysis that prompt tuning can have better\ncross-lingual transferability of representations on downstream tasks with\nbetter aligned decision boundaries.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Lifu Tu",
      "Caiming Xiong",
      "Yingbo Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12360"
  },
  {
    "id": "arXiv:2210.12362",
    "title": "EnDex: Evaluation of Dialogue Engagingness at Scale",
    "abstract": "We propose EnDex, the first human-reaction based model to evaluate dialogue\nengagingness. EnDex is trained on 80k Reddit-based Engagement Dataset (RED)\ncurated using a novel distant-supervision framework. Engagingness is a key\nmeasure that captures high-level quality of AI dialogue systems and closely\nreflects actual user experience. However, data shortage, plus the abstract and\nextensive definition of engagingness makes it challenging to develop an\nautomatic metric. Our work departs from mainstream approaches that use\nsynthetic negative examples to train binary classifiers, and instead, proposes\na solution using distant-supervision from human-reaction feedback. To support\nthe soundness of our EnDex metric, we offer a theoretical foundation for\nengagement, an extensive ablation study, and empirical evidence of high\ncorrelation on five engagingness related datasets. We will release code,\noff-the-shelf EnDex model, and a large-scale dataset upon paper publication to\nfacilitate future research.",
    "descriptor": "",
    "authors": [
      "Guangxuan Xu",
      "Ruibo Liu",
      "Fabrice Harel-Canada",
      "Nischal Reddy Chandra",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12362"
  },
  {
    "id": "arXiv:2210.12364",
    "title": "FCGEC: Fine-Grained Corpus for Chinese Grammatical Error Correction",
    "abstract": "Grammatical Error Correction (GEC) has been broadly applied in automatic\ncorrection and proofreading system recently. However, it is still immature in\nChinese GEC due to limited high-quality data from native speakers in terms of\ncategory and scale. In this paper, we present FCGEC, a fine-grained corpus to\ndetect, identify and correct the grammatical errors. FCGEC is a human-annotated\ncorpus with multiple references, consisting of 41,340 sentences collected\nmainly from multi-choice questions in public school Chinese examinations.\nFurthermore, we propose a Switch-Tagger-Generator (STG) baseline model to\ncorrect the grammatical errors in low-resource settings. Compared to other GEC\nbenchmark models, experimental results illustrate that STG outperforms them on\nour FCGEC. However, there exists a significant gap between benchmark models and\nhumans that encourages future models to bridge it.",
    "descriptor": "\nComments: Long paper, accepted at the Findings of EMNLP 2022\n",
    "authors": [
      "Lvxiaowei Xu",
      "Jianwang Wu",
      "Jiawei Peng",
      "Jiayu Fu",
      "Ming Cai"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12364"
  },
  {
    "id": "arXiv:2210.12365",
    "title": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer  Data Augmentation",
    "abstract": "While counterfactual data augmentation offers a promising step towards robust\ngeneralization in natural language processing, producing a set of\ncounterfactuals that offer valuable inductive bias for models remains a\nchallenge. Most existing approaches for producing counterfactuals, manual or\nautomated, rely on small perturbations via minimal edits, resulting in\nsimplistic changes. We introduce NeuroCounterfactuals, designed as loose\ncounterfactuals, allowing for larger edits which result in naturalistic\ngenerations containing linguistic diversity, while still bearing similarity to\nthe original document. Our novel generative approach bridges the benefits of\nconstrained decoding, with those of language model adaptation for sentiment\nsteering. Training data augmentation with our generations results in both\nin-domain and out-of-domain improvements for sentiment classification,\noutperforming even manually curated counterfactuals, under select settings. We\nfurther present detailed analyses to show the advantages of\nNeuroCounterfactuals over approaches involving simple, minimal edits.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Phillip Howard",
      "Gadi Singer",
      "Vasudev Lal",
      "Yejin Choi",
      "Swabha Swayamdipta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12365"
  },
  {
    "id": "arXiv:2210.12367",
    "title": "Precisely the Point: Adversarial Augmentations for Faithful and  Informative Text Generation",
    "abstract": "Though model robustness has been extensively studied in language\nunderstanding, the robustness of Seq2Seq generation remains understudied. In\nthis paper, we conduct the first quantitative analysis on the robustness of\npre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq\nmodel (BART) is still vulnerable, which leads to significant degeneration in\nfaithfulness and informativeness for text generation tasks. This motivated us\nto further propose a novel adversarial augmentation framework, namely AdvSeq,\nfor generally improving faithfulness and informativeness of Seq2Seq models via\nenhancing their robustness. AdvSeq automatically constructs two types of\nadversarial augmentations during training, including implicit adversarial\nsamples by perturbing word representations and explicit adversarial samples by\nword swapping, both of which effectively improve Seq2Seq robustness. Extensive\nexperiments on three popular text generation tasks demonstrate that AdvSeq\nsignificantly improves both the faithfulness and informativeness of Seq2Seq\ngeneration under both automatic and human evaluation settings.",
    "descriptor": "\nComments: EMNLP 2022 Main\n",
    "authors": [
      "Wenhao Wu",
      "Wei Li",
      "Jiachen Liu",
      "Xinyan Xiao",
      "Sujian Li",
      "Yajuan Lyu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12367"
  },
  {
    "id": "arXiv:2210.12368",
    "title": "Counterfactual Generation Under Confounding",
    "abstract": "A machine learning model, under the influence of observed or unobserved\nconfounders in the training data, can learn spurious correlations and fail to\ngeneralize when deployed. For image classifiers, augmenting a training dataset\nusing counterfactual examples has been empirically shown to break spurious\ncorrelations. However, the counterfactual generation task itself becomes more\ndifficult as the level of confounding increases. Existing methods for\ncounterfactual generation under confounding consider a fixed set of\ninterventions (e.g., texture, rotation) and are not flexible enough to capture\ndiverse data-generating processes. Given a causal generative process, we\nformally characterize the adverse effects of confounding on any downstream\ntasks and show that the correlation between generative factors (attributes) can\nbe used to quantitatively measure confounding between generative factors. To\nminimize such correlation, we propose a counterfactual generation method that\nlearns to modify the value of any attribute in an image and generate new images\ngiven a set of observed attributes, even when the dataset is highly confounded.\nThese counterfactual images are then used to regularize the downstream\nclassifier such that the learned representations are the same across various\ngenerative factors conditioned on the class label. Our method is\ncomputationally efficient, simple to implement, and works well for any number\nof generative factors and confounding variables. Our experimental results on\nboth synthetic (MNIST variants) and real-world (CelebA) datasets show the\nusefulness of our approach.",
    "descriptor": "",
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Saloni Dash",
      "Amit Sharma",
      "Vineeth N Balasubramanian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12368"
  },
  {
    "id": "arXiv:2210.12369",
    "title": "Explanation Shift: Detecting distribution shifts on tabular data via the  explanation space",
    "abstract": "As input data distributions evolve, the predictive performance of machine\nlearning models tends to deteriorate. In the past, predictive performance was\nconsidered the key indicator to monitor. However, explanation aspects have come\nto attention within the last years. In this work, we investigate how model\npredictive performance and model explanation characteristics are affected under\ndistribution shifts and how these key indicators are related to each other for\ntabular data. We find that the modeling of explanation shifts can be a better\nindicator for the detection of predictive performance changes than\nstate-of-the-art techniques based on representations of distribution shifts. We\nprovide a mathematical analysis of different types of distribution shifts as\nwell as synthetic experimental examples.",
    "descriptor": "\nComments: Neural Information Processing Systems (NeurIPS 2022). Workshop on Distribution Shifts: Connecting Methods and Applications\n",
    "authors": [
      "Carlos Mougan",
      "Klaus Broelemann",
      "Gjergji Kasneci",
      "Thanassis Tiropanis",
      "Steffen Staab"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12369"
  },
  {
    "id": "arXiv:2210.12373",
    "title": "Strategic Decisions Survey, Taxonomy, and Future Directions from  Artificial Intelligence Perspective",
    "abstract": "Strategic Decision-Making is always challenging because it is inherently\nuncertain, ambiguous, risky, and complex. It is the art of possibility. We\ndevelop a systematic taxonomy of decision-making frames that consists of 6\nbases, 18 categorical, and 54 frames. We aim to lay out the computational\nfoundation that is possible to capture a comprehensive landscape view of a\nstrategic problem. Compared with traditional models, it covers irrational,\nnon-rational and rational frames c dealing with certainty, uncertainty,\ncomplexity, ambiguity, chaos, and ignorance.",
    "descriptor": "",
    "authors": [
      "Caesar Wu",
      "Kotagiri Ramamohanarao",
      "Rui Zhang",
      "Pascal Bouvry"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12373"
  },
  {
    "id": "arXiv:2210.12374",
    "title": "ReasTAP: Injecting Table Reasoning Skills During Pre-training via  Synthetic Reasoning Examples",
    "abstract": "Reasoning over tabular data requires both table structure understanding and a\nbroad set of table reasoning skills. Current models with table-specific\narchitectures and pre-training methods perform well on understanding table\nstructures, but they still struggle with tasks that require various table\nreasoning skills. In this work, we develop ReasTAP to show that high-level\ntable reasoning skills can be injected into models during pre-training without\na complex table-specific architecture design. We define 7 table reasoning\nskills, such as numerical operation, temporal comparison, and conjunction. Each\nreasoning skill is associated with one example generator, which synthesizes\nquestions over semi-structured tables according to the sampled templates. We\nmodel the table pre-training task as a sequence generation task and pre-train\nReasTAP to generate precise answers to the synthetic examples. ReasTAP is\nevaluated on four benchmarks covering three downstream tasks including: 1)\nWikiSQL and WTQ for Table Question Answering; 2) TabFact for Table Fact\nVerification; and 3) LogicNLG for Faithful Table-to-Text Generation.\nExperimental results demonstrate that ReasTAP achieves new state-of-the-art\nperformance on all benchmarks and delivers a significant improvement on\nlow-resource setting. Our code is publicly available at\nhttps://github.com/Yale-LILY/ReasTAP.",
    "descriptor": "\nComments: accepted by EMNLP 2022\n",
    "authors": [
      "Yilun Zhao",
      "Linyong Nan",
      "Zhenting Qi",
      "Rui Zhang",
      "Dragomir Radev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12374"
  },
  {
    "id": "arXiv:2210.12375",
    "title": "torchode: A Parallel ODE Solver for PyTorch",
    "abstract": "We introduce an ODE solver for the PyTorch ecosystem that can solve multiple\nODEs in parallel independently from each other while achieving significant\nperformance gains. Our implementation tracks each ODE's progress separately and\nis carefully optimized for GPUs and compatibility with PyTorch's JIT compiler.\nIts design lets researchers easily augment any aspect of the solver and collect\nand analyze internal solver statistics. In our experiments, our implementation\nis up to 4.3 times faster per step than other ODE solvers and it is robust\nagainst within-batch interactions that lead other solvers to take up to 4 times\nas many steps.",
    "descriptor": "\nComments: Accepted at The Symbiosis of Deep Learning and Differential Equations Workshop, NeurIPS, 2022\n",
    "authors": [
      "Marten Lienen",
      "Stephan G\u00fcnnemann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12375"
  },
  {
    "id": "arXiv:2210.12378",
    "title": "Correcting Diverse Factual Errors in Abstractive Summarization via  Post-Editing and Language Model Infilling",
    "abstract": "Abstractive summarization models often generate inconsistent summaries\ncontaining factual errors or hallucinated content. Recent works focus on\ncorrecting factual errors in generated summaries via post-editing. Such\ncorrection models are trained using adversarial non-factual summaries\nconstructed using heuristic rules for injecting errors. However, generating\nnon-factual summaries using heuristics often does not generalize well to actual\nmodel errors. In this work, we propose to generate hard, representative\nsynthetic examples of non-factual summaries through infilling language models.\nWith this data, we train a more robust fact-correction model to post-edit the\nsummaries to improve factual consistency. Through quantitative and qualitative\nexperiments on two popular summarization datasets -- CNN/DM and XSum -- we show\nthat our approach vastly outperforms prior methods in correcting erroneous\nsummaries. Our model -- FactEdit -- improves factuality scores by over ~11\npoints on CNN/DM and over ~31 points on XSum on average across multiple\nsummarization models, producing more factual summaries while maintaining\ncompetitive summarization quality.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Vidhisha Balachandran",
      "Hannaneh Hajishirzi",
      "William Cohen",
      "Yulia Tsvetkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12378"
  },
  {
    "id": "arXiv:2210.12381",
    "title": "S2WAT: Image Style Transfer via Hierarchical Vision Transformer using  Strips Window Attention",
    "abstract": "This paper presents a new hierarchical vision Transformer for image style\ntransfer, called Strips Window Attention Transformer (S2WAT), which serves as\nan encoder of encoder-transfer-decoder architecture. With hierarchical\nfeatures, S2WAT can leverage proven techniques in other fields of computer\nvision, such as feature pyramid networks (FPN) or U-Net, to image style\ntransfer in future works. However, the existing window-based Transformers will\ncause a problem that the stylized images will be grid-like when introducing\nthem into image style transfer directly. To solve this problem, we propose\nS2WAT whose representation is computed with Strips Window Attention (SpW\nAttention). The SpW Attention can integrate both local information and\nlong-range dependencies in horizontal and vertical directions by a novel\nfeature fusion scheme named Attn Merge. Moreover, previous window-based\nTransformers require that the resolution of features needs to be divisible by\nwindow size which limits the inputs of arbitrary size. In this paper, we take\nadvantages of padding & un-padding operations to make S2WAT support inputs of\narbitrary size. Qualitative and quantitative experiments demonstrate that S2WAT\nachieves comparable performance of state-of-the-art CNN-based, Flow-based and\nTransformer-based approaches.",
    "descriptor": "",
    "authors": [
      "Chiyu Zhang",
      "Jun Yang",
      "Lei Wang",
      "Zaiyan Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12381"
  },
  {
    "id": "arXiv:2210.12383",
    "title": "Stance Detection and Open Research Avenues",
    "abstract": "This tutorial aims to cover the state-of-the-art on stance detection and\naddress open research avenues for interested researchers and practitioners.\nStance detection is a recent research topic where the stance towards a given\ntarget or target set is determined based on the given content and there are\nsignificant application opportunities of stance detection in various domains.\nThe tutorial comprises two parts where the first part outlines the fundamental\nconcepts, problems, approaches, and resources of stance detection, while the\nsecond part covers open research avenues and application areas of stance\ndetection. The tutorial will be a useful guide for researchers and\npractitioners of stance detection, social media analysis, information\nretrieval, and natural language processing.",
    "descriptor": "",
    "authors": [
      "Dilek K\u00fc\u00e7\u00fck",
      "Fazli Can"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12383"
  },
  {
    "id": "arXiv:2210.12384",
    "title": "The Devil is in the Conflict: Disentangled Information Graph Neural  Networks for Fraud Detection",
    "abstract": "Graph-based fraud detection has heretofore received considerable attention.\nOwning to the great success of Graph Neural Networks (GNNs), many approaches\nadopting GNNs for fraud detection has been gaining momentum. However, most\nexisting methods are based on the strong inductive bias of homophily, which\nindicates that the context neighbors tend to have same labels or similar\nfeatures. In real scenarios, fraudsters often engage in camouflage behaviors in\norder to avoid detection system. Therefore, the homophilic assumption no longer\nholds, which is known as the inconsistency problem. In this paper, we argue\nthat the performance degradation is mainly attributed to the inconsistency\nbetween topology and attribute. To address this problem, we propose to\ndisentangle the fraud network into two views, each corresponding to topology\nand attribute respectively. Then we propose a simple and effective method that\nuses the attention mechanism to adaptively fuse two views which captures\ndata-specific preference. In addition, we further improve it by introducing\nmutual information constraints for topology and attribute. To this end, we\npropose a Disentangled Information Graph Neural Network (DIGNN) model, which\nutilizes variational bounds to find an approximate solution to our proposed\noptimization objective function. Extensive experiments demonstrate that our\nmodel can significantly outperform stateof-the-art baselines on real-world\nfraud detection datasets.",
    "descriptor": "\nComments: 10 pages, 8 figures, IEEE International Conference on Data Mining (ICDM)\n",
    "authors": [
      "Zhixun Li",
      "Dingshuo Chen",
      "Qiang Liu",
      "Shu Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.12384"
  },
  {
    "id": "arXiv:2210.12386",
    "title": "Learning Feasibility of Factored Nonlinear Programs in Robotic  Manipulation Planning",
    "abstract": "A factored Nonlinear Program (Factored-NLP) explicitly models the\ndependencies between a set of continuous variables and nonlinear constraints,\nproviding an expressive formulation for relevant robotics problems such as\nmanipulation planning or simultaneous localization and mapping. When the\nproblem is over-constrained or infeasible, a fundamental issue is to detect a\nminimal subset of variables and constraints that are infeasible.Previous\napproaches require solving several nonlinear programs, incrementally adding and\nremoving constraints, and are thus computationally expensive. In this paper, we\npropose a graph neural architecture that predicts which variables and\nconstraints are jointly infeasible. The model is trained with a dataset of\nlabeled subgraphs of Factored-NLPs, and importantly, can make useful\npredictions on larger factored nonlinear programs than the ones seen during\ntraining. We evaluate our approach in robotic manipulation planning, where our\nmodel is able to generalize to longer manipulation sequences involving more\nobjects and robots, and different geometric environments. The experiments show\nthat the learned model accelerates general algorithms for conflict extraction\n(by a factor of 50) and heuristic algorithms that exploit expert knowledge (by\na factor of 4).",
    "descriptor": "\nComments: Submitted to ICRA 2023\n",
    "authors": [
      "Joaquim Ortiz-Haro",
      "Jung-Su Ha",
      "Danny Driess",
      "Erez Karpas",
      "Marc Toussaint"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12386"
  },
  {
    "id": "arXiv:2210.12387",
    "title": "Whisker-Inspired Tactile Sensing for Contact Localization on Robot  Manipulators",
    "abstract": "Perceiving the environment through touch is important for robots to reach in\ncluttered environments, but devising a way to sense without disturbing objects\nis challenging. This work presents the design and modelling of whisker-inspired\nsensors that attach to the surface of a robot manipulator to sense its\nsurrounding through light contacts. We obtain a sensor model using a\ncalibration process that applies to straight and curved whiskers. We then\npropose a sensing algorithm using Bayesian filtering to localize contact\npoints. The algorithm combines the accurate proprioceptive sensing of the robot\nand sensor readings from the deflections of the whiskers. Our results show that\nour algorithm is able to track contact points with sub-millimeter accuracy,\noutperforming a baseline method. Finally, we demonstrate our sensor and\nperception method in a real-world system where a robot moves in between\nfree-standing objects and uses the whisker sensors to track contacts tracing\nobject contours.",
    "descriptor": "\nComments: 8 pages, 7 figures, conference\n",
    "authors": [
      "Michael A. Lin",
      "Emilio Reyes",
      "Jeannette Bohg",
      "Mark R. Cutkosky"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12387"
  },
  {
    "id": "arXiv:2210.12389",
    "title": "Neural Distortion Fields for Spatial Calibration of Wide Field-of-View  Near-Eye Displays",
    "abstract": "We propose a spatial calibration method for wide Field-of-View (FoV) Near-Eye\nDisplays (NEDs) with complex image distortions. Image distortions in NEDs can\ndestroy the reality of the virtual object and cause sickness. To achieve\ndistortion-free images in NEDs, it is necessary to establish a pixel-by-pixel\ncorrespondence between the viewpoint and the displayed image. Designing compact\nand wide-FoV NEDs requires complex optical designs. In such designs, the\ndisplayed images are subject to gaze-contingent, non-linear geometric\ndistortions, which explicit geometric models can be difficult to represent or\ncomputationally intensive to optimize.\nTo solve these problems, we propose Neural Distortion Field (NDF), a\nfully-connected deep neural network that implicitly represents display surfaces\ncomplexly distorted in spaces. NDF takes spatial position and gaze direction as\ninput and outputs the display pixel coordinate and its intensity as perceived\nin the input gaze direction. We synthesize the distortion map from a novel\nviewpoint by querying points on the ray from the viewpoint and computing a\nweighted sum to project output display coordinates into an image. Experiments\nshowed that NDF calibrates an augmented reality NED with 90$^{\\circ}$ FoV with\nabout 3.23 pixel (5.8 arcmin) median error using only 8 training viewpoints.\nAdditionally, we confirmed that NDF calibrates more accurately than the\nnon-linear polynomial fitting, especially around the center of the FoV.",
    "descriptor": "\nComments: 17 pages. This is a preprint of a publication at OSA Optics Express 30(22) pp.40628-40644, 2022\n",
    "authors": [
      "Yuichi Hiroi",
      "Kiyosato Someya",
      "Yuta Itoh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.12389"
  },
  {
    "id": "arXiv:2210.12391",
    "title": "MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity  Recognition",
    "abstract": "African languages are spoken by over a billion people, but are\nunderrepresented in NLP research and development. The challenges impeding\nprogress include the limited availability of annotated datasets, as well as a\nlack of understanding of the settings where current methods are effective. In\nthis paper, we make progress towards solutions for these challenges, focusing\non the task of named entity recognition (NER). We create the largest\nhuman-annotated NER dataset for 20 African languages, and we study the behavior\nof state-of-the-art cross-lingual transfer methods in an Africa-centric\nsetting, demonstrating that the choice of source language significantly affects\nperformance. We show that choosing the best transfer language improves\nzero-shot F1 scores by an average of 14 points across 20 languages compared to\nusing English. Our results highlight the need for benchmark datasets and models\nthat cover typologically-diverse African languages.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "David Ifeoluwa Adelani",
      "Graham Neubig",
      "Sebastian Ruder",
      "Shruti Rijhwani",
      "Michael Beukman",
      "Chester Palen-Michel",
      "Constantine Lignos",
      "Jesujoba O. Alabi",
      "Shamsuddeen H. Muhammad",
      "Peter Nabende",
      "Cheikh M. Bamba Dione",
      "Andiswa Bukula",
      "Rooweither Mabuya",
      "Bonaventure F. P. Dossou",
      "Blessing Sibanda",
      "Happy Buzaaba",
      "Jonathan Mukiibi",
      "Godson Kalipe",
      "Derguene Mbaye",
      "Amelia Taylor",
      "Fatoumata Kabore",
      "Chris Chinenye Emezue",
      "Anuoluwapo Aremu",
      "Perez Ogayo",
      "Catherine Gitau",
      "Edwin Munkoh-Buabeng",
      "Victoire M. Koagne",
      "Allahsera Auguste Tapo",
      "Tebogo Macucwa",
      "Vukosi Marivate",
      "Elvis Mboning",
      "Tajuddeen Gwadabe",
      "Tosin Adewumi",
      "Orevaoghene Ahia",
      "Joyce Nakatumba-Nabende",
      "Neo L. Mokono",
      "Ignatius Ezeani",
      "Chiamaka Chukwuneke",
      "Mofetoluwa Adeyemi",
      "Gilles Q. Hacheme",
      "Idris Abdulmumin",
      "Odunayo Ogundepo",
      "Oreen Yousuf",
      "Tatiana Moteu Ngoli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12391"
  },
  {
    "id": "arXiv:2210.12396",
    "title": "ADDMU: Detection of Far-Boundary Adversarial Examples with Data and  Model Uncertainty Estimation",
    "abstract": "Adversarial Examples Detection (AED) is a crucial defense technique against\nadversarial attacks and has drawn increasing attention from the Natural\nLanguage Processing (NLP) community. Despite the surge of new AED methods, our\nstudies show that existing methods heavily rely on a shortcut to achieve good\nperformance. In other words, current search-based adversarial attacks in NLP\nstop once model predictions change, and thus most adversarial examples\ngenerated by those attacks are located near model decision boundaries. To\nsurpass this shortcut and fairly evaluate AED methods, we propose to test AED\nmethods with \\textbf{F}ar \\textbf{B}oundary (\\textbf{FB}) adversarial examples.\nExisting methods show worse than random guess performance under this scenario.\nTo overcome this limitation, we propose a new technique, \\textbf{ADDMU},\n\\textbf{a}dversary \\textbf{d}etection with \\textbf{d}ata and \\textbf{m}odel\n\\textbf{u}ncertainty, which combines two types of uncertainty estimation for\nboth regular and FB adversarial example detection. Our new method outperforms\nprevious methods by 3.6 and 6.0 \\emph{AUC} points under each scenario. Finally,\nour analysis shows that the two types of uncertainty provided by \\textbf{ADDMU}\ncan be leveraged to characterize adversarial examples and identify the ones\nthat contribute most to model's robustness in adversarial training.",
    "descriptor": "\nComments: 18 pages, EMNLP 2022, main conference, long paper\n",
    "authors": [
      "Fan Yin",
      "Yao Li",
      "Cho-Jui Hsieh",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12396"
  },
  {
    "id": "arXiv:2210.12397",
    "title": "MetaASSIST: Robust Dialogue State Tracking with Meta Learning",
    "abstract": "Existing dialogue datasets contain lots of noise in their state annotations.\nSuch noise can hurt model training and ultimately lead to poor generalization\nperformance. A general framework named ASSIST has recently been proposed to\ntrain robust dialogue state tracking (DST) models. It introduces an auxiliary\nmodel to generate pseudo labels for the noisy training set. These pseudo labels\nare combined with vanilla labels by a common fixed weighting parameter to train\nthe primary DST model. Notwithstanding the improvements of ASSIST on DST,\ntuning the weighting parameter is challenging. Moreover, a single parameter\nshared by all slots and all instances may be suboptimal. To overcome these\nlimitations, we propose a meta learning-based framework MetaASSIST to\nadaptively learn the weighting parameter. Specifically, we propose three\nschemes with varying degrees of flexibility, ranging from slot-wise to both\nslot-wise and instance-wise, to convert the weighting parameter into learnable\nfunctions. These functions are trained in a meta-learning manner by taking the\nvalidation set as meta data. Experimental results demonstrate that all three\nschemes can achieve competitive performance. Most impressively, we achieve a\nstate-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.",
    "descriptor": "\nComments: To appear at EMNLP 2022, 13 pages\n",
    "authors": [
      "Fanghua Ye",
      "Xi Wang",
      "Jie Huang",
      "Shenghui Li",
      "Samuel Stern",
      "Emine Yilmaz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12397"
  },
  {
    "id": "arXiv:2210.12398",
    "title": "NeARportation: A Remote Real-time Neural Rendering Framework",
    "abstract": "While the presentation of photo-realistic appearance plays a major role in\nimmersion in an augmented virtuality environment, displaying the\nphoto-realistic appearance of real objects remains a challenging problem.\nRecent developments in photogrammetry have facilitated the incorporation of\nreal objects into virtual space. However, photo-realistic photogrammetry\nrequires a dedicated measurement environment, and there is a trade-off between\nmeasurement cost and quality. Furthermore, even with photo-realistic appearance\nmeasurements, there is a trade-off between rendering quality and framerate.\nThere is no framework that could resolve these trade-offs and easily provide a\nphoto-realistic appearance in real-time. Our NeARportation framework combines\nserver-client bidirectional communication and neural rendering to resolve these\ntrade-offs. Neural rendering on the server receives the client's head posture\nand generates a novel-view image with realistic appearance reproduction, which\nis streamed onto the client's display. By applying our framework to a\nstereoscopic display, we confirmed that it could display a high-fidelity\nappearance on full-HD stereo videos at 35-40 frames-per-second (fps), according\nto the user's head motion.",
    "descriptor": "\nComments: 5 pages. This is a preprint of a paper accepted at VRST'22 conference. project URL: this https URL\n",
    "authors": [
      "Yuichi Hiroi",
      "Yuta Itoh",
      "Jun Rekimoto"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.12398"
  },
  {
    "id": "arXiv:2210.12400",
    "title": "Varifocal Question Generation for Fact-checking",
    "abstract": "Fact-checking requires retrieving evidence related to a claim under\ninvestigation. The task can be formulated as question generation based on a\nclaim, followed by question answering. However, recent question generation\napproaches assume that the answer is known and typically contained in a passage\ngiven as input, whereas such passages are what is being sought when verifying a\nclaim. In this paper, we present {\\it Varifocal}, a method that generates\nquestions based on different focal points within a given claim, i.e.\\ different\nspans of the claim and its metadata, such as its source and date. Our method\noutperforms previous work on a fact-checking question generation dataset on a\nwide range of automatic evaluation metrics. These results are corroborated by\nour manual evaluation, which indicates that our method generates more relevant\nand informative questions. We further demonstrate the potential of focal points\nin generating sets of clarification questions for product descriptions.",
    "descriptor": "\nComments: Accepted at EMNLP 2022, 13 pages\n",
    "authors": [
      "Nedjma Ousidhoum",
      "Zhangdie Yuan",
      "Andreas Vlachos"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12400"
  },
  {
    "id": "arXiv:2210.12401",
    "title": "PcMSP: A Dataset for Scientific Action Graphs Extraction from  Polycrystalline Materials Synthesis Procedure Text",
    "abstract": "Scientific action graphs extraction from materials synthesis procedures is\nimportant for reproducible research, machine automation, and material\nprediction. But the lack of annotated data has hindered progress in this field.\nWe demonstrate an effort to annotate Polycrystalline Materials Synthesis\nProcedures (PcMSP) from 305 open access scientific articles for the\nconstruction of synthesis action graphs. This is a new dataset for material\nscience information extraction that simultaneously contains the synthesis\nsentences extracted from the experimental paragraphs, as well as the entity\nmentions and intra-sentence relations. A two-step human annotation and\ninter-annotator agreement study guarantee the high quality of the PcMSP corpus.\nWe introduce four natural language processing tasks: sentence classification,\nnamed entity recognition, relation classification, and joint extraction of\nentities and relations. Comprehensive experiments validate the effectiveness of\nseveral state-of-the-art models for these challenges while leaving large space\nfor improvement. We also perform the error analysis and point out some unique\nchallenges that require further investigation. We will release our annotation\nscheme, the corpus, and codes to the research community to alleviate the\nscarcity of labeled data in this domain.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Xianjun Yang",
      "Ya Zhuo",
      "Julia Zuo",
      "Xinlu Zhang",
      "Stephen Wilson",
      "Linda Petzold"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12401"
  },
  {
    "id": "arXiv:2210.12402",
    "title": "DIGMN: Dynamic Intent Guided Meta Network for Differentiated User  Engagement Forecasting in Online Professional Social Platforms",
    "abstract": "User engagement prediction plays a critical role for designing interaction\nstrategies to grow user engagement and increase revenue in online social\nplatforms. Through the in-depth analysis of the real-world data from the\nworld's largest professional social platforms, i.e., LinkedIn, we find that\nusers expose diverse engagement patterns, and a major reason for the\ndifferences in user engagement patterns is that users have different intents.\nThat is, people have different intents when using LinkedIn, e.g., applying for\njobs, building connections, or checking notifications, which shows quite\ndifferent engagement patterns. Meanwhile, user intents and the corresponding\nengagement patterns may change over time. Although such pattern differences and\ndynamics are essential for user engagement prediction, differentiating user\nengagement patterns based on user dynamic intents for better user engagement\nforecasting has not received enough attention in previous works. In this paper,\nwe proposed a Dynamic Intent Guided Meta Network (DIGMN), which can explicitly\nmodel user intent varying with time and perform differentiated user engagement\nforecasting. Specifically, we derive some interpretable basic user intents as\nprior knowledge from data mining and introduce prior intents in explicitly\nmodeling dynamic user intent. Furthermore, based on the dynamic user intent\nrepresentations, we propose a meta predictor to perform differentiated user\nengagement forecasting. Through a comprehensive evaluation on LinkedIn\nanonymous user data, our method outperforms state-of-the-art baselines\nsignificantly, i.e., 2.96% and 3.48% absolute error reduction, on\ncoarse-grained and fine-grained user engagement prediction tasks, respectively,\ndemonstrating the effectiveness of our method.",
    "descriptor": "\nComments: 10 pages, Accepted by WSDM'23\n",
    "authors": [
      "Feifan Li",
      "Lun Du",
      "Qiang Fu",
      "Shi Han",
      "Yushu Du",
      "Guangming Lu",
      "Zi Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12402"
  },
  {
    "id": "arXiv:2210.12403",
    "title": "PATS: Sensitivity-aware Noisy Learning for Pretrained Language Models",
    "abstract": "A wide range of NLP tasks benefit from the fine-tuning of pretrained language\nmodels (PLMs). However, a number of redundant parameters which contribute less\nto the downstream task are observed in a directly fine-tuned model. We consider\nthe gap between pretraining and downstream tasks hinders the training of these\nredundant parameters, and results in a suboptimal performance of the overall\nmodel. In this paper, we present PATS (Perturbation According To Sensitivity),\na noisy training mechanism which considers each parameter's importance in the\ndownstream task to help fine-tune PLMs. The main idea of PATS is to add bigger\nnoise to parameters with lower sensitivity and vice versa, in order to activate\nmore parameters' contributions to downstream tasks without affecting the\nsensitive ones much. Extensive experiments conducted on different tasks of the\nGLUE benchmark show PATS can consistently empower the fine-tuning of different\nsizes of PLMs, and the parameters in the well-performing models always have\nmore concentrated distributions of sensitivities, which experimentally proves\nthe effectiveness of our method.",
    "descriptor": "\nComments: Accepted by EMNLP 2022 main conference\n",
    "authors": [
      "Yupeng Zhang",
      "Hongzhi Zhang",
      "Sirui Wang",
      "Wei Wu",
      "Zhoujun Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12403"
  },
  {
    "id": "arXiv:2210.12407",
    "title": "Two new families of fourth-order explicit exponential Runge-Kutta  methods with four stages for stiff or highly oscillatory systems",
    "abstract": "In this paper, two new families of fourth-order explicit exponential\nRunge-Kutta methods with four stages are studied for stiff or highly\noscillatory systems $y'(t)+My(t)=f(y(t))$.We analyze modified and simplified\nversions of fourth-order explicit exponential Runge-Kutta methods,\nrespectively, which are different from standard exponential Runge-Kutta\nmethods. Using the Taylor series of numerical and exact solutions, we obtain\nthe order conditions of these new explicit exponential methods, which reduce to\nthose of the standard Runge-Kutta methods when $M \\rightarrow 0$. We show the\nconvergence of these new exponential methods in detail. Numerical experiments\nare carried out, and the numerical results demonstrate the accuracy and\nefficiency of these new exponential methods when applied to the stiff systems\nor highly oscillatory problems.",
    "descriptor": "",
    "authors": [
      "Xianfa Hu",
      "Yonglei Fang",
      "Bin Wang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12407"
  },
  {
    "id": "arXiv:2210.12408",
    "title": "Fast Abstracts and Student Forum Proceedings, 18th European Dependable  Computing Conference -- EDCC 2022",
    "abstract": "Collection of manuscripts accepted for presentation at the Student Forum and\nFast Abstracts tracks of the 18th European Dependable Computing Conference\n(EDCC 2022).",
    "descriptor": "\nComments: Editors: Ib\\'eria Medeiros and Geert Deconinck\n",
    "authors": [
      "Ib\u00e9ria Medeiros",
      "Geert Deconinck"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12408"
  },
  {
    "id": "arXiv:2210.12409",
    "title": "Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in  Transformer-Based Variational AutoEncoder for Diverse Text Generation",
    "abstract": "Variational Auto-Encoder (VAE) has been widely adopted in text generation.\nAmong many variants, recurrent VAE learns token-wise latent variables with each\nconditioned on the preceding ones, which captures sequential variability better\nin the era of RNN. However, it is unclear how to incorporate such recurrent\ndynamics into the recently dominant Transformer due to its parallelism. In this\nwork, we propose DELLA, a Transformer-based recurrent VAE structure. DELLA\nimposes recurrence on segment-wise latent variables with arbitrarily separated\ntext segments and constructs the posterior distribution with residual\nparameterization. Besides, we design an acceleration method by approximating\nidempotent matrices, which allows parallelism while maintaining the conditional\ndependence of latent variables. We demonstrate that DELLA could enhance the\nentanglement of each segment and preceding latent variables and deduce a\nnon-zero lower bound of the KL term, providing a theoretical guarantee of\ngeneration diversity. Experiments on two unconditional and one conditional\ngeneration tasks show that DELLA achieves significantly improved diversity\nwhile maintaining satisfactory generation quality.",
    "descriptor": "\nComments: EMNLP 2022 Findings\n",
    "authors": [
      "Jinyi Hu",
      "Xiaoyuan Yi",
      "Wenhao Li",
      "Maosong Sun",
      "Xing Xie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12409"
  },
  {
    "id": "arXiv:2210.12410",
    "title": "The Shared Task on Gender Rewriting",
    "abstract": "In this paper, we present the results and findings of the Shared Task on\nGender Rewriting, which was organized as part of the Seventh Arabic Natural\nLanguage Processing Workshop. The task of gender rewriting refers to generating\nalternatives of a given sentence to match different target user gender contexts\n(e.g., female speaker with a male listener, a male speaker with a male\nlistener, etc.). This requires changing the grammatical gender (masculine or\nfeminine) of certain words referring to the users. In this task, we focus on\nArabic, a gender-marking morphologically rich language. A total of five teams\nfrom four countries participated in the shared task.",
    "descriptor": "",
    "authors": [
      "Bashar Alhafni",
      "Nizar Habash",
      "Houda Bouamor",
      "Ossama Obeid",
      "Sultan Alrowili",
      "Daliyah Alzeer",
      "Khawlah M. Alshanqiti",
      "Ahmed ElBakry",
      "Muhammad ElNokrashy",
      "Mohamed Gabr",
      "Abderrahmane Issam",
      "Abdelrahim Qaddoumi",
      "K. Vijay-Shanker",
      "Mahmoud Zyate"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12410"
  },
  {
    "id": "arXiv:2210.12415",
    "title": "ALT: Breaking the Wall between Graph and Operator Level Optimizations  for Deep Learning Compilation",
    "abstract": "Deep learning models rely on highly optimized tensor libraries for efficient\ninference on heterogeneous hardware. Current deep compilers typically\npredetermine layouts of tensors and then optimize loops of operators. However,\nsuch unidirectional and one-off workflow strictly separates graph-level\noptimization and operator-level optimization into different system layers,\nmissing opportunities for unified tuning. This paper proposes ALT, a compiler\nthat performs joint graph- and operator-level optimizations for deep models.\nJOG provides a generic transformation module to manipulate layouts and loops\nwith easy-to-use primitive functions. JOG further integrates an auto-tuning\nmodule that jointly optimizes graph-level data layouts and operator-level loops\nwhile guaranteeing efficiency. Experimental results show that JOG significantly\noutperforms state-of-the-art compilers (e.g., Ansor) in terms of both single\noperator performance (e.g., 1.5x speedup on average) and end-to-end inference\nperformance (e.g., 1.4x speedup on average).",
    "descriptor": "",
    "authors": [
      "Zhiying Xu",
      "Jiafan Xu",
      "Hongding Peng",
      "Wei Wang",
      "Xiaoliang Wang",
      "Haoran Wan",
      "Haipeng Dai",
      "Yixu Xu",
      "Hao Cheng",
      "Kun Wang",
      "Guihai Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12415"
  },
  {
    "id": "arXiv:2210.12417",
    "title": "SLAM: Semantic Learning based Activation Map for Weakly Supervised  Semantic Segmentation",
    "abstract": "Recent mainstream weakly-supervised semantic segmentation (WSSS) approaches\nbased on image-level annotations mainly relies on binary image-level\nclassification with limited representation capacity. In this paper, we propose\na novel semantic learning based framework for WSSS, named SLAM (Semantic\nLearning based Activation Map). We firstly design a semantic encoder to learn\nsemantics of each object category and extract category-specific semantic\nembeddings from an input image. The semantic embeddings of foreground and\nbackground are then integrated to a segmentation network to learn the\nactivation map. Four loss functions, i.e, category-foreground,\ncategory-background, activation regularization, and consistency loss are\nproposed to ensure the correctness, completeness, compactness and consistency\nof the activation map. Experimental results show that our semantic learning\nbased SLAM achieves much better performance than binary image-level\nclassification based approaches, i.e., around 3\\% mIoU higher than OC-CSE\n\\cite{occse}, CPN \\cite{cpn} on PASCAL VOC dataset. Our SLAM also surpasses AMN\n\\cite{amn} trained with strong per-pixel constraint and CLIMS \\cite{clims}\nutilizing extra multi-modal knowledge. Code will be made available.",
    "descriptor": "",
    "authors": [
      "Junliang Chen",
      "Xiaodong Zhao",
      "Minmin Liu",
      "Linlin Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12417"
  },
  {
    "id": "arXiv:2210.12418",
    "title": "MILD: Multimodal Interactive Latent Dynamics for Learning Human-Robot  Interaction",
    "abstract": "Modeling interaction dynamics to generate robot trajectories that enable a\nrobot to adapt and react to a human's actions and intentions is critical for\nefficient and effective collaborative Human-Robot Interactions (HRI). Learning\nfrom Demonstration (LfD) methods from Human-Human Interactions (HHI) have shown\npromising results, especially when coupled with representation learning\ntechniques. However, such methods for learning HRI either do not scale well to\nhigh dimensional data or cannot accurately adapt to changing via-poses of the\ninteracting partner. We propose Multimodal Interactive Latent Dynamics (MILD),\na method that couples deep representation learning and probabilistic machine\nlearning to address the problem of two-party physical HRIs. We learn the\ninteraction dynamics from demonstrations, using Hidden Semi-Markov Models\n(HSMMs) to model the joint distribution of the interacting agents in the latent\nspace of a Variational Autoencoder (VAE). Our experimental evaluations for\nlearning HRI from HHI demonstrations show that MILD effectively captures the\nmultimodality in the latent representations of HRI tasks, allowing us to decode\nthe varying dynamics occurring in such tasks. Compared to related work, MILD\ngenerates more accurate trajectories for the controlled agent (robot) when\nconditioned on the observed agent's (human) trajectory. Notably, MILD can learn\ndirectly from camera-based pose estimations to generate trajectories, which we\nthen map to a humanoid robot without the need for any additional training.",
    "descriptor": "\nComments: Accepted at the IEEE-RAS International Conference on Humanoid Robots (Humanoids) 2022\n",
    "authors": [
      "Vignesh Prasad",
      "Dorothea Koert",
      "Ruth Stock-Homburg",
      "Jan Peters",
      "Georgia Chalvatzaki"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12418"
  },
  {
    "id": "arXiv:2210.12427",
    "title": "Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and  Reliable Language Model",
    "abstract": "In knowledge distillation, a student model is trained with supervisions from\nboth knowledge from a teacher and observations drawn from a training data\ndistribution. Knowledge of a teacher is considered a subject that holds\ninter-class relations which send a meaningful supervision to a student; hence,\nmuch effort has been put to find such knowledge to be distilled. In this paper,\nwe explore a question that has been given little attention: \"when to distill\nsuch knowledge.\" The question is answered in our work with the concept of model\ncalibration; we view a teacher model not only as a source of knowledge but also\nas a gauge to detect miscalibration of a student. This simple and yet novel\nview leads to a hard gate knowledge distillation scheme that switches between\nlearning from a teacher model and training data. We verify the gating mechanism\nin the context of natural language generation at both the token-level and the\nsentence-level. Empirical comparisons with strong baselines show that hard gate\nknowledge distillation not only improves model generalization, but also\nsignificantly lowers model calibration error.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Dongkyu Lee",
      "Zhiliang Tian",
      "Yingxiu Zhao",
      "Ka Chun Cheung",
      "Nevin L. Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12427"
  },
  {
    "id": "arXiv:2210.12429",
    "title": "Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in  Dialog Systems",
    "abstract": "Dialog systems are often designed or trained to output human-like responses.\nHowever, some responses may be impossible for a machine to truthfully say (e.g.\n\"that movie made me cry\"). Highly anthropomorphic responses might make users\nuncomfortable or implicitly deceive them into thinking they are interacting\nwith a human. We collect human ratings on the feasibility of approximately 900\ntwo-turn dialogs sampled from 9 diverse data sources. Ratings are for two\nhypothetical machine embodiments: a futuristic humanoid robot and a digital\nassistant. We find that for some data-sources commonly used to train dialog\nsystems, 20-30% of utterances are not viewed as possible for a machine. Rating\nis marginally affected by machine embodiment. We explore qualitative and\nquantitative reasons for these ratings. Finally, we build classifiers and\nexplore how modeling configuration might affect output permissibly, and discuss\nimplications for building less falsely anthropomorphic dialog systems.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "David Gros",
      "Yu Li",
      "Zhou Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12429"
  },
  {
    "id": "arXiv:2210.12430",
    "title": "Speech Emotion Recognition via an Attentive Time-Frequency Neural  Network",
    "abstract": "Spectrogram is commonly used as the input feature of deep neural networks to\nlearn the high(er)-level time-frequency pattern of speech signal for speech\nemotion recognition (SER). \\textcolor{black}{Generally, different emotions\ncorrespond to specific energy activations both within frequency bands and time\nframes on spectrogram, which indicates the frequency and time domains are both\nessential to represent the emotion for SER. However, recent spectrogram-based\nworks mainly focus on modeling the long-term dependency in time domain, leading\nto these methods encountering the following two issues: (1) neglecting to model\nthe emotion-related correlations within frequency domain during the\ntime-frequency joint learning; (2) ignoring to capture the specific frequency\nbands associated with emotions.} To cope with the issues, we propose an\nattentive time-frequency neural network (ATFNN) for SER, including a\ntime-frequency neural network (TFNN) and time-frequency attention.\nSpecifically, aiming at the first issue, we design a TFNN with a\nfrequency-domain encoder (F-Encoder) based on the Transformer encoder and a\ntime-domain encoder (T-Encoder) based on the Bidirectional Long Short-Term\nMemory (Bi-LSTM). The F-Encoder and T-Encoder model the correlations within\nfrequency bands and time frames, respectively, and they are embedded into a\ntime-frequency joint learning strategy to obtain the time-frequency patterns\nfor speech emotions. Moreover, to handle the second issue, we also adopt\ntime-frequency attention with a frequency-attention network (F-Attention) and a\ntime-attention network (T-Attention) to focus on the emotion-related frequency\nband ranges and time frame ranges, which can enhance the discriminability of\nspeech emotion features.",
    "descriptor": "\nComments: This paper has been accepted as a regular paper on IEEE Transactions on Computational Social Systems\n",
    "authors": [
      "Cheng Lu",
      "Wenming Zheng",
      "Hailun Lian",
      "Yuan Zong",
      "Chuangao Tang",
      "Sunan Li",
      "Yan Zhao"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12430"
  },
  {
    "id": "arXiv:2210.12432",
    "title": "Structure-Unified M-Tree Coding Solver for MathWord Problem",
    "abstract": "As one of the challenging NLP tasks, designing math word problem (MWP)\nsolvers has attracted increasing research attention for the past few years. In\nprevious work, models designed by taking into account the properties of the\nbinary tree structure of mathematical expressions at the output side have\nachieved better performance. However, the expressions corresponding to a MWP\nare often diverse (e.g., $n_1+n_2 \\times n_3-n_4$, $n_3\\times n_2-n_4+n_1$,\netc.), and so are the corresponding binary trees, which creates difficulties in\nmodel learning due to the non-deterministic output space. In this paper, we\npropose the Structure-Unified M-Tree Coding Solver (SUMC-Solver), which applies\na tree with any M branches (M-tree) to unify the output structures. To learn\nthe M-tree, we use a mapping to convert the M-tree into the M-tree codes, where\ncodes store the information of the paths from tree root to leaf nodes and the\ninformation of leaf nodes themselves, and then devise a Sequence-to-Code\n(seq2code) model to generate the codes. Experimental results on the widely used\nMAWPS and Math23K datasets have demonstrated that SUMC-Solver not only\noutperforms several state-of-the-art models under similar experimental settings\nbut also performs much better under low-resource conditions.",
    "descriptor": "\nComments: Accepted by EMNLP2022\n",
    "authors": [
      "Bin Wang",
      "Jiangzhou Ju",
      "Yang Fan",
      "Xin-Yu Dai",
      "Shujian Huang",
      "Jiajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12432"
  },
  {
    "id": "arXiv:2210.12435",
    "title": "Generative Prompt Tuning for Relation Classification",
    "abstract": "Using prompts to explore the knowledge contained within pre-trained language\nmodels for downstream tasks has now become an active topic. Current prompt\ntuning methods mostly convert the downstream tasks to masked language modeling\nproblems by adding cloze-style phrases and mapping all labels to verbalizations\nwith fixed length, which has proven effective for tasks with simple label\nspaces. However, when applied to relation classification exhibiting complex\nlabel spaces, vanilla prompt tuning methods may struggle with label\nverbalizations with arbitrary lengths due to rigid prompt restrictions.\nInspired by the text infilling task for pre-training generative models that can\nflexibly predict missing spans, we propose a novel generative prompt tuning\nmethod to reformulate relation classification as an infilling problem, which\nfrees our approach from limitations of current prompt based approaches and thus\nfully exploits rich semantics of entity and relation types. In addition, we\ndesign entity-guided decoding and discriminative relation scoring to generate\nand align relations effectively and efficiently during inference. Extensive\nexperiments under fully supervised settings and low-resource settings\ndemonstrate the effectiveness of our approach.",
    "descriptor": "\nComments: Accepted by Findings of EMNLP 2022\n",
    "authors": [
      "Jiale Han",
      "Shuai Zhao",
      "Bo Cheng",
      "Shengkun Ma",
      "Wei Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12435"
  },
  {
    "id": "arXiv:2210.12437",
    "title": "Extractive Summarization of Legal Decisions using Multi-task Learning  and Maximal Marginal Relevance",
    "abstract": "Summarizing legal decisions requires the expertise of law practitioners,\nwhich is both time- and cost-intensive. This paper presents techniques for\nextractive summarization of legal decisions in a low-resource setting using\nlimited expert annotated data. We test a set of models that locate relevant\ncontent using a sequential model and tackle redundancy by leveraging maximal\nmarginal relevance to compose summaries. We also demonstrate an implicit\napproach to help train our proposed models generate more informative summaries.\nOur multi-task learning model variant leverages rhetorical role identification\nas an auxiliary task to further improve the summarizer. We perform extensive\nexperiments on datasets containing legal decisions from the US Board of\nVeterans' Appeals and conduct quantitative and expert-ranked evaluations of our\nmodels. Our results show that the proposed approaches can achieve ROUGE scores\nvis-\\`a-vis expert extracted summaries that match those achieved by\ninter-annotator comparison.",
    "descriptor": "\nComments: Accepted at Findings of EMNLP 2022\n",
    "authors": [
      "Abhishek Agarwal",
      "Shanshan Xu",
      "Matthias Grabmair"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12437"
  },
  {
    "id": "arXiv:2210.12438",
    "title": "Algorithms with Prediction Portfolios",
    "abstract": "The research area of algorithms with predictions has seen recent success\nshowing how to incorporate machine learning into algorithm design to improve\nperformance when the predictions are correct, while retaining worst-case\nguarantees when they are not. Most previous work has assumed that the algorithm\nhas access to a single predictor. However, in practice, there are many machine\nlearning methods available, often with incomparable generalization guarantees,\nmaking it hard to pick a best method a priori. In this work we consider\nscenarios where multiple predictors are available to the algorithm and the\nquestion is how to best utilize them.\nIdeally, we would like the algorithm's performance to depend on the quality\nof the best predictor. However, utilizing more predictions comes with a cost,\nsince we now have to identify which prediction is the best. We study the use of\nmultiple predictors for a number of fundamental problems, including matching,\nload balancing, and non-clairvoyant scheduling, which have been well-studied in\nthe single predictor setting. For each of these problems we introduce new\nalgorithms that take advantage of multiple predictors, and prove bounds on the\nresulting performance.",
    "descriptor": "\nComments: 24 pages. Appears at NeurIPS 2022\n",
    "authors": [
      "Michael Dinitz",
      "Sungjin Im",
      "Thomas Lavastida",
      "Benjamin Moseley",
      "Sergei Vassilvitskii"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.12438"
  },
  {
    "id": "arXiv:2210.12440",
    "title": "Spectrum-BERT: Pre-training of Deep Bidirectional Transformers for  Spectral Classification of Chinese Liquors",
    "abstract": "Spectral detection technology, as a non-invasive method for rapid detection\nof substances, combined with deep learning algorithms, has been widely used in\nfood detection. However, in real scenarios, acquiring and labeling spectral\ndata is an extremely labor-intensive task, which makes it impossible to provide\nenough high-quality data for training efficient supervised deep learning\nmodels. To better leverage limited samples, we apply pre-training & fine-tuning\nparadigm to the field of spectral detection for the first time and propose a\npre-training method of deep bidirectional transformers for spectral\nclassification of Chinese liquors, abbreviated as Spectrum-BERT. Specifically,\nfirst, to retain the model's sensitivity to the characteristic peak position\nand local information of the spectral curve, we innovatively partition the\ncurve into multiple blocks and obtain the embeddings of different blocks, as\nthe feature input for the next calculation. Second, in the pre-training stage,\nwe elaborately design two pre-training tasks, Next Curve Prediction (NCP) and\nMasked Curve Model (MCM), so that the model can effectively utilize unlabeled\nsamples to capture the potential knowledge of spectral data, breaking the\nrestrictions of the insufficient labeled samples, and improving the\napplicability and performance of the model in practical scenarios. Finally, we\nconduct a large number of experiments on the real liquor spectral dataset. In\nthe comparative experiments, the proposed Spectrum-BERT significantly\noutperforms the baselines in multiple metrics and this advantage is more\nsignificant on the imbalanced dataset. Moreover, in the parameter sensitivity\nexperiment, we also analyze the model performance under different parameter\nsettings, to provide a reference for subsequent research.",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Yansong Wang",
      "Yundong Sun",
      "Yansheng Fu",
      "Dongjie Zhu",
      "Zhaoshuo Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12440"
  },
  {
    "id": "arXiv:2210.12444",
    "title": "Weakly-Supervised Temporal Article Grounding",
    "abstract": "Given a long untrimmed video and natural language queries, video grounding\n(VG) aims to temporally localize the semantically-aligned video segments.\nAlmost all existing VG work holds two simple but unrealistic assumptions: 1)\nAll query sentences can be grounded in the corresponding video. 2) All query\nsentences for the same video are always at the same semantic scale.\nUnfortunately, both assumptions make today's VG models fail to work in\npractice. For example, in real-world multimodal assets (eg, news articles),\nmost of the sentences in the article can not be grounded in their affiliated\nvideos, and they typically have rich hierarchical relations (ie, at different\nsemantic scales). To this end, we propose a new challenging grounding task:\nWeakly-Supervised temporal Article Grounding (WSAG). Specifically, given an\narticle and a relevant video, WSAG aims to localize all ``groundable''\nsentences to the video, and these sentences are possibly at different semantic\nscales. Accordingly, we collect the first WSAG dataset to facilitate this task:\nYouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow\narticles and plentiful YouTube videos. In addition, we propose a simple but\neffective method DualMIL for WSAG, which consists of a two-level MIL loss and a\nsingle-/cross- sentence constraint loss. These training objectives are\ncarefully designed for these relaxed assumptions. Extensive ablations have\nverified the effectiveness of DualMIL.",
    "descriptor": "\nComments: EMNLP 2022, this https URL\n",
    "authors": [
      "Long Chen",
      "Yulei Niu",
      "Brian Chen",
      "Xudong Lin",
      "Guangxing Han",
      "Christopher Thomas",
      "Hammad Ayyubi",
      "Heng Ji",
      "Shih-Fu Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.12444"
  },
  {
    "id": "arXiv:2210.12445",
    "title": "Cross-domain Generalization for AMR Parsing",
    "abstract": "Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph\nfrom textual input. Recently, there has been notable growth in AMR parsing\nperformance. However, most existing work focuses on improving the performance\nin the specific domain, ignoring the potential domain dependence of AMR parsing\nsystems. To address this, we extensively evaluate five representative AMR\nparsers on five domains and analyze challenges to cross-domain AMR parsing. We\nobserve that challenges to cross-domain AMR parsing mainly arise from the\ndistribution shift of words and AMR concepts. Based on our observation, we\ninvestigate two approaches to reduce the domain distribution divergence of text\nand AMR features, respectively. Experimental results on two out-of-domain test\nsets show the superiority of our method.",
    "descriptor": "\nComments: Accepted to EMNLP2022 main conference\n",
    "authors": [
      "Xuefeng Bai",
      "Seng Yang",
      "Leyang Cui",
      "Linfeng Song",
      "Yue Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12445"
  },
  {
    "id": "arXiv:2210.12446",
    "title": "Learning Classifiers for Imbalanced and Overlapping Data",
    "abstract": "This study is about inducing classifiers using data that is imbalanced, with\na minority class being under-represented in relation to the majority classes.\nThe first section of this research focuses on the main characteristics of data\nthat generate this problem. Following a study of previous, relevant research, a\nvariety of artificial, imbalanced data sets influenced by important elements\nwere created. These data sets were used to create decision trees and rule-based\nclassifiers. The second section of this research looks into how to improve\nclassifiers by pre-processing data with resampling approaches. The results of\nthe following trials are compared to the performance of distinct pre-processing\nre-sampling methods: two variants of random over-sampling and focused\nunder-sampling NCR. This paper further optimises class imbalance with a new\nmethod called Sparsity. The data is made more sparse from its class centers,\nhence making it more homogenous.",
    "descriptor": "",
    "authors": [
      "Shivaditya Shivganesh",
      "Nitin Narayanan N",
      "Pranav Murali",
      "Ajaykumar M"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12446"
  },
  {
    "id": "arXiv:2210.12448",
    "title": "Probing Transfer in Deep Reinforcement Learning without Task Engineering",
    "abstract": "We evaluate the use of original game curricula supported by the Atari 2600\nconsole as a heterogeneous transfer benchmark for deep reinforcement learning\nagents. Game designers created curricula using combinations of several discrete\nmodifications to the basic versions of games such as Space Invaders, Breakout\nand Freeway, making them progressively more challenging for human players. By\nformally organising these modifications into several factors of variation, we\nare able to show that Analyses of Variance (ANOVA) are a potent tool for\nstudying the effects of human-relevant domain changes on the learning and\ntransfer performance of a deep reinforcement learning agent. Since no manual\ntask engineering is needed on our part, leveraging the original multi-factorial\ndesign avoids the pitfalls of unintentionally biasing the experimental setup.\nWe find that game design factors have a large and statistically significant\nimpact on an agent's ability to learn, and so do their combinatorial\ninteractions. Furthermore, we show that zero-shot transfer from the basic games\nto their respective variations is possible, but the variance in performance is\nalso largely explained by interactions between factors. As such, we argue that\nAtari game curricula offer a challenging benchmark for transfer learning in RL,\nthat can help the community better understand the generalisation capabilities\nof RL agents along dimensions which meaningfully impact human generalisation\nperformance. As a start, we report that value-function finetuning of regularly\ntrained agents achieves positive transfer in a majority of cases, but\nsignificant headroom for algorithmic innovation remains. We conclude with the\nobservation that selective transfer from multiple variants could further\nimprove performance.",
    "descriptor": "",
    "authors": [
      "Andrei A. Rusu",
      "Sebastian Flennerhag",
      "Dushyant Rao",
      "Razvan Pascanu",
      "Raia Hadsell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12448"
  },
  {
    "id": "arXiv:2210.12453",
    "title": "NeuroPrim: An Attention-based Model for Solving NP-hard Spanning Tree  Problems",
    "abstract": "Spanning tree problems with special constraints are widely applied in\nreal-life scenarios, such as water supply, transportation and\ntelecommunications, which often require complex algorithm design and\nexponential time to solve. In recent years, there has been a surge of interest\nin end-to-end Deep Neural Networks (DNNs) to solve routing problems. However,\nas the output of such methods is a sequence of vertices, it is difficult to\napply them to combinatorial optimization problems where the solution set\nconsists of a edges sets, such as various spanning tree problems. In this\npaper, we propose NeuroPrim, a novel framework combining neural networks and\nthe Prim algorithm, which is trained by REINFORCE with the POMO baseline to\nlearn metrics for selecting edges for different spanning tree problems. We\napply it to three difficult problems on Euclidean spaces, namely\nDegree-constrained Minimum Spanning Tree Problem (DCMSTP), Minimum Routing Cost\nSpanning Tree Problem (MRCSTP) and Steiner Tree Problem in Graphs (STPG).\nExperimental results show that our model is able to outperform some of the\nheuristics and obtain extremely small gaps of less than $0.1\\%$ for simple\nproblems such as DCMST with degree constraint $3$ and special cases of STPG up\nto 100 vertices. In addition, we find no significant degradation on problem\ninstances as large as 1000, which demonstrates its strong generalization\nability.",
    "descriptor": "",
    "authors": [
      "Yuchen Shi",
      "Congying Han",
      "Tiande Guo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12453"
  },
  {
    "id": "arXiv:2210.12456",
    "title": "Abstract Interpretation-Based Feature Importance for SVMs",
    "abstract": "We propose a symbolic representation for support vector machines (SVMs) by\nmeans of abstract interpretation, a well-known and successful technique for\ndesigning and implementing static program analyses. We leverage this\nabstraction in two ways: (1) to enhance the interpretability of SVMs by\nderiving a novel feature importance measure, called abstract feature importance\n(AFI), that does not depend in any way on a given dataset of the accuracy of\nthe SVM and is very fast to compute, and (2) for verifying stability, notably\nindividual fairness, of SVMs and producing concrete counterexamples when the\nverification fails. We implemented our approach and we empirically demonstrated\nits effectiveness on SVMs based on linear and non-linear (polynomial and radial\nbasis function) kernels. Our experimental results show that, independently of\nthe accuracy of the SVM, our AFI measure correlates much more strongly with the\nstability of the SVM to feature perturbations than feature importance measures\nwidely available in machine learning software such as permutation feature\nimportance. It thus gives better insight into the trustworthiness of SVMs.",
    "descriptor": "",
    "authors": [
      "Abhinandan Pal",
      "Francesco Ranzato",
      "Caterina Urban",
      "Marco Zanella"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12456"
  },
  {
    "id": "arXiv:2210.12459",
    "title": "There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with  Adversarial Activated Multi-Reference Learning",
    "abstract": "Knowledge-grounded conversation (KGC) shows excellent potential to deliver an\nengaging and informative response. However, existing approaches emphasize\nselecting one golden knowledge given a particular dialogue context, overlooking\nthe one-to-many phenomenon in dialogue. As a result, the existing paradigm\nlimits the diversity of knowledge selection and generation. To this end, we\nestablish a multi-reference KGC dataset and propose a series of metrics to\nsystematically assess the one-to-many efficacy of existing KGC models.\nFurthermore, to extend the hypothesis space of knowledge selection to enhance\nthe mapping relationship between multiple knowledge and multiple responses, we\ndevise a span-based variational model and optimize the model in a wake-sleep\nstyle with an ameliorated evidence lower bound objective to learn the\none-to-many generalization. Both automatic and human evaluations demonstrate\nthe efficacy of our approach.",
    "descriptor": "\nComments: To appear at EMNLP 2022 main conference. The first two authors contributed equally\n",
    "authors": [
      "Xueliang Zhao",
      "Tingchen Fu",
      "Chongyang Tao",
      "Rui Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12459"
  },
  {
    "id": "arXiv:2210.12460",
    "title": "Collaborative Reasoning on Multi-Modal Semantic Graphs for  Video-Grounded Dialogue Generation",
    "abstract": "We study video-grounded dialogue generation, where a response is generated\nbased on the dialogue context and the associated video. The primary challenges\nof this task lie in (1) the difficulty of integrating video data into\npre-trained language models (PLMs) which presents obstacles to exploiting the\npower of large-scale pre-training; and (2) the necessity of taking into account\nthe complementarity of various modalities throughout the reasoning process.\nAlthough having made remarkable progress in video-grounded dialogue generation,\nexisting methods still fall short when it comes to integrating with PLMs in a\nway that allows information from different modalities to complement each other.\nTo alleviate these issues, we first propose extracting pertinent information\nfrom videos and turning it into reasoning paths that are acceptable to PLMs.\nAdditionally, we propose a multi-agent reinforcement learning method to\ncollaboratively perform reasoning on different modalities (i.e., video and\ndialogue context). Empirical experiment results on two public datasets indicate\nthat the proposed model can significantly outperform state-of-the-art models by\nlarge margins on both automatic and human evaluations.",
    "descriptor": "\nComments: To appear at EMNLP 2022 findings\n",
    "authors": [
      "Xueliang Zhao",
      "Yuxuan Wang",
      "Chongyang Tao",
      "Chenshuo Wang",
      "Dongyan Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12460"
  },
  {
    "id": "arXiv:2210.12461",
    "title": "Towards Efficient Dialogue Pre-training with Transferable and  Interpretable Latent Structure",
    "abstract": "With the availability of massive general-domain dialogue data, pre-trained\ndialogue generation appears to be super appealing to transfer knowledge from\nthe general domain to downstream applications. In most existing work, such\ntransferable ability is mainly obtained by fitting a large model with hundreds\nof millions of parameters on massive data in an exhaustive way, leading to\ninefficient running and poor interpretability. This paper proposes a novel\ndialogue generation model with a latent structure that is easily transferable\nfrom the general domain to downstream tasks in a lightweight and transparent\nway. Experiments on two benchmarks validate the effectiveness of the proposed\nmodel. Thanks to the transferable latent structure, our model is able to yield\nbetter dialogue responses than four strong baselines in terms of both automatic\nand human evaluations, and our model with about 22% parameters particularly\ndelivers a 5x speedup in running time compared with the strongest baseline.\nMoreover, the proposed model is explainable by interpreting the discrete latent\nvariables.",
    "descriptor": "\nComments: To appear at EMNLP 2022 main conference\n",
    "authors": [
      "Xueliang Zhao",
      "Lemao Liu",
      "Tingchen Fu",
      "Shuming Shi",
      "Dongyan Zhao",
      "Rui Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12461"
  },
  {
    "id": "arXiv:2210.12463",
    "title": "EtriCA: Event-Triggered Context-Aware Story Generation Augmented by  Cross Attention",
    "abstract": "One of the key challenges of automatic story generation is how to generate a\nlong narrative that can maintain fluency, relevance, and coherence. Despite\nrecent progress, current story generation systems still face the challenge of\nhow to effectively capture contextual and event features, which has a profound\nimpact on a model's generation performance. To address these challenges, we\npresent EtriCA, a novel neural generation model, which improves the relevance\nand coherence of the generated stories through residually mapping context\nfeatures to event sequences with a cross-attention mechanism. Such a feature\ncapturing mechanism allows our model to better exploit the logical relatedness\nbetween events when generating stories. Extensive experiments based on both\nautomatic and human evaluations show that our model significantly outperforms\nstate-of-the-art baselines, demonstrating the effectiveness of our model in\nleveraging context and event features.",
    "descriptor": "\nComments: Accepted by EMNLP 2022 Findings\n",
    "authors": [
      "Chen Tang",
      "Chenghua Lin",
      "Henglin Huang",
      "Frank Guerin",
      "Zhihao Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12463"
  },
  {
    "id": "arXiv:2210.12464",
    "title": "Volatility forecasting using Deep Learning and sentiment analysis",
    "abstract": "Several studies have shown that deep learning models can provide more\naccurate volatility forecasts than the traditional methods used within this\ndomain. This paper presents a composite model that merges a deep learning\napproach with sentiment analysis for predicting market volatility. To classify\npublic sentiment, we use a Convolutional Neural Network, which obtained data\nfrom Reddit global news headlines. We then describe a composite forecasting\nmodel, a Long-Short-Term-Memory Neural Network method, to use historical\nsentiment and the previous day's volatility to make forecasts. We employed this\nmethod on the past volatility of the S\\&P500 and the major BRICS indices to\ncorroborate its effectiveness. Our results demonstrate that including sentiment\ncan improve deep learning volatility forecasting models. However, in contrast\nto return forecasting, the performance benefits of including sentiment appear\nfor volatility forecasting appears to be market specific.",
    "descriptor": "\nComments: 13 pages, 4 figures, accepted for publication at the SACAIR 2022 conference\n",
    "authors": [
      "V Ncume",
      "T. L van Zyl",
      "A Paskaramoorthy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12464"
  },
  {
    "id": "arXiv:2210.12467",
    "title": "ECTSum: A New Benchmark Dataset For Bullet Point Summarization of Long  Earnings Call Transcripts",
    "abstract": "Despite tremendous progress in automatic summarization, state-of-the-art\nmethods are predominantly trained to excel in summarizing short newswire\narticles, or documents with strong layout biases such as scientific articles or\ngovernment reports. Efficient techniques to summarize financial documents,\nincluding facts and figures, have largely been unexplored, majorly due to the\nunavailability of suitable datasets. In this work, we present ECTSum, a new\ndataset with transcripts of earnings calls (ECTs), hosted by publicly traded\ncompanies, as documents, and short experts-written telegram-style bullet point\nsummaries derived from corresponding Reuters articles. ECTs are long\nunstructured documents without any prescribed length limit or format. We\nbenchmark our dataset with state-of-the-art summarizers across various metrics\nevaluating the content quality and factual consistency of the generated\nsummaries. Finally, we present a simple-yet-effective approach, ECT-BPS, to\ngenerate a set of bullet points that precisely capture the important facts\ndiscussed in the calls.",
    "descriptor": "\nComments: 14 pages; Accepted as a Long Paper in EMNLP 2022 (Main Conference); Codes: this https URL\n",
    "authors": [
      "Rajdeep Mukherjee",
      "Abhinav Bohra",
      "Akash Banerjee",
      "Soumya Sharma",
      "Manjunath Hegde",
      "Afreen Shaikh",
      "Shivani Shrivastava",
      "Koustuv Dasgupta",
      "Niloy Ganguly",
      "Saptarshi Ghosh",
      "Pawan Goyal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12467"
  },
  {
    "id": "arXiv:2210.12468",
    "title": "Discrepancy Minimization in Input-Sparsity Time",
    "abstract": "A recent work of Larsen [Lar23] gave a faster combinatorial alternative to\nBansal's SDP algorithm for finding a coloring $x\\in\\{-1,1\\}^n$ that\napproximately minimizes the discrepancy $\\mathrm{disc}(A,x) : = \\| A x\n\\|_{\\infty}$ of a general real-valued $m\\times n$ matrix $A$. Larsen's\nalgorithm runs in $\\widetilde{O}(mn^2)$ time compared to Bansal's\n$\\widetilde{O}(mn^{4.5})$-time algorithm, at the price of a slightly weaker\nlogarithmic approximation ratio in terms of the hereditary discrepancy of $A$\n[Ban10].\nIn this work we present a combinatorial $\\widetilde{O}(\\mathrm{nnz}(A) +\nn^3)$ time algorithm with the same approximation guarantee as Larsen, which is\noptimal for tall matrices $m=\\mathrm{poly}(n)$. Using a more intricate analysis\nand fast matrix-multiplication, we achieve $\\widetilde{O}(\\mathrm{nnz}(A) +\nn^{2.53})$ time, which breaks cubic runtime for square matrices, and bypasses\nthe barrier of linear-programming approaches [ES14] for which input-sparsity\ntime is currently out of reach.\nOur algorithm relies on two main ideas: (i) A new sketching technique for\nfinding a projection matrix with short $\\ell_2$-basis using implicit\nleverage-score sampling; (ii) A data structure for faster implementation of the\niterative Edge-Walk partial-coloring algorithm of Lovett-Meka, using an\nalternative analysis that enables ``lazy\" batch-updates with low-rank\ncorrections. Our result nearly closes the computational gap between real-valued\nand binary matrices (set-systems), for which input-sparsity time coloring was\nvery recently obtained [JSS23].",
    "descriptor": "",
    "authors": [
      "Yichuan Deng",
      "Zhao Song",
      "Omri Weinstein"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.12468"
  },
  {
    "id": "arXiv:2210.12470",
    "title": "Learning Correlated Stackelberg Equilibrium in General-Sum  Multi-Leader-Single-Follower Games",
    "abstract": "Many real-world strategic games involve interactions between multiple\nplayers. We study a hierarchical multi-player game structure, where players\nwith asymmetric roles can be separated into leaders and followers, a setting\noften referred to as Stackelberg game or leader-follower game. In particular,\nwe focus on a Stackelberg game scenario where there are multiple leaders and a\nsingle follower, called the Multi-Leader-Single-Follower (MLSF) game. We\npropose a novel asymmetric equilibrium concept for the MLSF game called\nCorrelated Stackelberg Equilibrium (CSE). We design online learning algorithms\nthat enable the players to interact in a distributed manner, and prove that it\ncan achieve no-external Stackelberg-regret learning. This further translates to\nthe convergence to approximate CSE via a reduction from no-external regret to\nno-swap regret. At the core of our works, we solve the intricate problem of how\nto learn equilibrium in leader-follower games with noisy bandit feedback by\nbalancing exploration and exploitation in different learning structures.",
    "descriptor": "",
    "authors": [
      "Yaolong Yu",
      "Haifeng Xu",
      "Haipeng Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12470"
  },
  {
    "id": "arXiv:2210.12476",
    "title": "A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for  Mobile Devices",
    "abstract": "Real-time object pose estimation and tracking is challenging but essential\nfor emerging augmented reality (AR) applications. In general, state-of-the-art\nmethods address this problem using deep neural networks which indeed yield\nsatisfactory results. Nevertheless, the high computational cost of these\nmethods makes them unsuitable for mobile devices where real-world applications\nusually take place. In addition, head-mounted displays such as AR glasses\nrequire at least 90~FPS to avoid motion sickness, which further complicates the\nproblem. We propose a flexible-frame-rate object pose estimation and tracking\nsystem for mobile devices. It is a monocular visual-inertial-based system with\na client-server architecture. Inertial measurement unit (IMU) pose propagation\nis performed on the client side for high speed tracking, and RGB image-based 3D\npose estimation is performed on the server side to obtain accurate poses, after\nwhich the pose is sent to the client side for visual-inertial fusion, where we\npropose a bias self-correction mechanism to reduce drift. We also propose a\npose inspection algorithm to detect tracking failures and incorrect pose\nestimation. Connected by high-speed networking, our system supports flexible\nframe rates up to 120 FPS and guarantees high precision and real-time tracking\non low-end devices. Both simulations and real world experiments show that our\nmethod achieves accurate and robust object tracking.",
    "descriptor": "",
    "authors": [
      "Yo-Chung Lau",
      "Kuan-Wei Tseng",
      "I-Ju Hsieh",
      "Hsiao-Ching Tseng",
      "Yi-Ping Hung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12476"
  },
  {
    "id": "arXiv:2210.12478",
    "title": "DiscoSense: Commonsense Reasoning with Discourse Connectives",
    "abstract": "We present DiscoSense, a benchmark for commonsense reasoning via\nunderstanding a wide variety of discourse connectives. We generate compelling\ndistractors in DiscoSense using Conditional Adversarial Filtering, an extension\nof Adversarial Filtering that employs conditional generation. We show that\nstate-of-the-art pre-trained language models struggle to perform well on\nDiscoSense, which makes this dataset ideal for evaluating next-generation\ncommonsense reasoning systems.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Prajjwal Bhargava",
      "Vincent Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12478"
  },
  {
    "id": "arXiv:2210.12484",
    "title": "SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored  GEC-Oriented Parser",
    "abstract": "This work proposes a syntax-enhanced grammatical error correction (GEC)\napproach named SynGEC that effectively incorporates dependency syntactic\ninformation into the encoder part of GEC models. The key challenge for this\nidea is that off-the-shelf parsers are unreliable when processing ungrammatical\nsentences. To confront this challenge, we propose to build a tailored\nGEC-oriented parser (GOPar) using parallel GEC training data as a pivot. First,\nwe design an extended syntax representation scheme that allows us to represent\nboth grammatical errors and syntax in a unified tree structure. Then, we obtain\nparse trees of the source incorrect sentences by projecting trees of the target\ncorrect sentences. Finally, we train GOPar with such projected trees. For GEC,\nwe employ the graph convolution network to encode source-side syntactic\ninformation produced by GOPar, and fuse them with the outputs of the\nTransformer encoder. Experiments on mainstream English and Chinese GEC datasets\nshow that our proposed SynGEC approach consistently and substantially\noutperforms strong baselines and achieves competitive performance. Our code and\ndata are all publicly available at https://github.com/HillZhang1999/SynGEC.",
    "descriptor": "\nComments: Accepted by EMNLP2022 (main conference)\n",
    "authors": [
      "Yue Zhang",
      "Bo Zhang",
      "Zhenghua Li",
      "Zuyi Bao",
      "Chen Li",
      "Min Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12484"
  },
  {
    "id": "arXiv:2210.12485",
    "title": "DANLI: Deliberative Agent for Following Natural Language Instructions",
    "abstract": "Recent years have seen an increasing amount of work on embodied AI agents\nthat can perform tasks by following human language instructions. However, most\nof these agents are reactive, meaning that they simply learn and imitate\nbehaviors encountered in the training data. These reactive agents are\ninsufficient for long-horizon complex tasks. To address this limitation, we\npropose a neuro-symbolic deliberative agent that, while following language\ninstructions, proactively applies reasoning and planning based on its neural\nand symbolic representations acquired from past experience (e.g., natural\nlanguage and egocentric vision). We show that our deliberative agent achieves\ngreater than 70% improvement over reactive baselines on the challenging TEACh\nbenchmark. Moreover, the underlying reasoning and planning processes, together\nwith our modular framework, offer impressive transparency and explainability to\nthe behaviors of the agent. This enables an in-depth understanding of the\nagent's capabilities, which shed light on challenges and opportunities for\nfuture embodied agents for instruction following. The code is available at\nhttps://github.com/sled-group/DANLI.",
    "descriptor": "\nComments: Accepted in EMNLP 2022\n",
    "authors": [
      "Yichi Zhang",
      "Jianing Yang",
      "Jiayi Pan",
      "Shane Storks",
      "Nikhil Devraj",
      "Ziqiao Ma",
      "Keunwoo Peter Yu",
      "Yuwei Bao",
      "Joyce Chai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12485"
  },
  {
    "id": "arXiv:2210.12487",
    "title": "MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure",
    "abstract": "In this paper, we propose a comprehensive benchmark to investigate models'\nlogical reasoning capabilities in complex real-life scenarios. Current\nexplanation datasets often employ synthetic data with simple reasoning\nstructures. Therefore, it cannot express more complex reasoning processes, such\nas the rebuttal to a reasoning step and the degree of certainty of the\nevidence. To this end, we propose a comprehensive logical reasoning explanation\nform. Based on the multi-hop chain of reasoning, the explanation form includes\nthree main components: (1) The condition of rebuttal that the reasoning node\ncan be challenged; (2) Logical formulae that uncover the internal texture of\nreasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The\nfine-grained structure conforms to the real logical reasoning scenario, better\nfitting the human cognitive process but, simultaneously, is more challenging\nfor the current models. We evaluate the current best models' performance on\nthis new explanation form. The experimental results show that generating\nreasoning graphs remains a challenging task for current models, even with the\nhelp of giant pre-trained language models.",
    "descriptor": "\nComments: To appear at the main conference of EMNLP 2022\n",
    "authors": [
      "Yinya Huang",
      "Hongming Zhang",
      "Ruixin Hong",
      "Xiaodan Liang",
      "Changshui Zhang",
      "Dong Yu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.12487"
  },
  {
    "id": "arXiv:2210.12491",
    "title": "Estimating oil and gas recovery factors via machine learning:  Database-dependent accuracy and reliability",
    "abstract": "With recent advances in artificial intelligence, machine learning (ML)\napproaches have become an attractive tool in petroleum engineering,\nparticularly for reservoir characterizations. A key reservoir property is\nhydrocarbon recovery factor (RF) whose accurate estimation would provide\ndecisive insights to drilling and production strategies. Therefore, this study\naims to estimate the hydrocarbon RF for exploration from various reservoir\ncharacteristics, such as porosity, permeability, pressure, and water saturation\nvia the ML. We applied three regression-based models including the extreme\ngradient boosting (XGBoost), support vector machine (SVM), and stepwise\nmultiple linear regression (MLR) and various combinations of three databases to\nconstruct ML models and estimate the oil and/or gas RF. Using two databases and\nthe cross-validation method, we evaluated the performance of the ML models. In\neach iteration 90 and 10% of the data were respectively used to train and test\nthe models. The third independent database was then used to further assess the\nconstructed models. For both oil and gas RFs, we found that the XGBoost model\nestimated the RF for the train and test datasets more accurately than the SVM\nand MLR models. However, the performance of all the models were unsatisfactory\nfor the independent databases. Results demonstrated that the ML algorithms were\nhighly dependent and sensitive to the databases based on which they were\ntrained. Statistical tests revealed that such unsatisfactory performances were\nbecause the distributions of input features and target variables in the train\ndatasets were significantly different from those in the independent databases\n(p-value < 0.05).",
    "descriptor": "",
    "authors": [
      "Alireza Roustazadeh",
      "Behzad Ghanbarian",
      "Mohammad B. Shadmand",
      "Vahid Taslimitehrani",
      "Larry W. Lake"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12491"
  },
  {
    "id": "arXiv:2210.12492",
    "title": "NeuroMapper: In-browser Visualizer for Neural Network Training",
    "abstract": "We present our ongoing work NeuroMapper, an in-browser visualization tool\nthat helps machine learning (ML) developers interpret the evolution of a model\nduring training, providing a new way to monitor the training process and\nvisually discover reasons for suboptimal training. While most existing deep\nneural networks (DNNs) interpretation tools are designed for already-trained\nmodel, NeuroMapper scalably visualizes the evolution of the embeddings of a\nmodel's blocks across training epochs, enabling real-time visualization of\n40,000 embedded points. To promote the embedding visualizations' spatial\ncoherence across epochs, NeuroMapper adapts AlignedUMAP, a recent nonlinear\ndimensionality reduction technique to align the embeddings. With NeuroMapper,\nusers can explore the training dynamics of a Resnet-50 model, and adjust the\nembedding visualizations' parameters in real time. NeuroMapper is open-sourced\nat https://github.com/poloclub/NeuroMapper and runs in all modern web browsers.\nA demo of the tool in action is available at:\nhttps://poloclub.github.io/NeuroMapper/.",
    "descriptor": "\nComments: IEEE VIS 2022\n",
    "authors": [
      "Zhiyan Zhou",
      "Kevin Li",
      "Haekyu Park",
      "Megan Dass",
      "Austin Wright",
      "Nilaksh Das",
      "Duen Horng Chau"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12492"
  },
  {
    "id": "arXiv:2210.12493",
    "title": "A Design Space for Human Sensor and Actuator Focused In-Vehicle  Interaction Based on a Systematic Literature Review",
    "abstract": "Automotive user interfaces constantly change due to increasing automation,\nnovel features, additional applications, and user demands. While in-vehicle\ninteraction can utilize numerous promising modalities, no existing overview\nincludes an extensive set of human sensors and actuators and interaction\nlocations throughout the vehicle interior. We conducted a systematic literature\nreview of 327 publications leading to a design space for in-vehicle interaction\nthat outlines existing and lack of work regarding input and output modalities,\nlocations, and multimodal interaction. To investigate user acceptance of\npossible modalities and locations inferred from existing work and gaps unveiled\nin our design space, we conducted an online study (N=48). The study revealed\nusers' general acceptance of novel modalities (e.g., brain or thermal activity)\nand interaction with locations other than the front (e.g., seat or table). Our\nwork helps practitioners evaluate key design decisions, exploit trends, and\nexplore new areas in the domain of in-vehicle interaction.",
    "descriptor": "\nComments: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies\n",
    "authors": [
      "Pascal Jansen",
      "Mark Colley",
      "Enrico Rukzio"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.12493"
  },
  {
    "id": "arXiv:2210.12494",
    "title": "Generalized Likelihood Ratio Test With One-Class Classifiers",
    "abstract": "One-class classification (OCC) is the problem of deciding whether an observed\nsample belongs to a target class or not. We consider the problem of learning an\nOCC model when the dataset available at the learning stage contains only\nsamples from the target class. We aim at obtaining a classifier that performs\nas the generalized likelihood ratio test (GLRT), which is a well-known and\nprovably optimal (under specific assumptions) classifier when the statistic of\nthe target class is available. To this end, we consider both the multilayer\nperceptron neural network (NN) and the support vector machine (SVM) models.\nThey are trained as two-class classifiers using an artificial dataset for the\nalternative class, obtained by generating random samples, uniformly over the\ndomain of the target-class dataset. We prove that, under suitable assumptions,\nthe models converge (with a large dataset) to the GLRT. Moreover, we show that\nthe one-class least squares SVM (OCLSSVM) at convergence performs as the GLRT,\nwith a suitable transformation function. Lastly, we compare the obtained\nsolutions with the autoencoder (AE) classifier, which does not in general\nprovide the GLRT",
    "descriptor": "\nComments: 10 pages, 4 figure, submitted to IEEE Transactions on Neural Networks and Learning Systems\n",
    "authors": [
      "Francesco Ardizzon",
      "Stefano Tomasin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12494"
  },
  {
    "id": "arXiv:2210.12495",
    "title": "Quartic Samples Suffice for Fourier Interpolation",
    "abstract": "We study the classic problem of interpolating a Fourier-sparse signal in the\ntime duration $[0, T]$ from noisy samples in the same range, where the ground\ntruth signal can be any $k$-Fourier-sparse signal with band-limit $[-F, F]$.\nOur main result is an efficient Fourier Interpolation algorithm that improves\nthe previous best algorithm by [Chen, Kane, Price and Song, FOCS 2016] in the\nfollowing three aspects:\n$\\bullet$ The sample complexity is improved from $\\widetilde{O}(k^{51})$ to\n$\\widetilde{O}(k^{4})$.\n$\\bullet$ The time complexity is improved from $\n\\widetilde{O}(k^{10\\omega+40})$ to $\\widetilde{O}(k^{4 \\omega})$.\n$\\bullet$ The output sparsity is improved from $\\widetilde{O}(k^{10})$ to\n$\\widetilde{O}(k^{4})$.\nHere, $\\omega$ denotes the exponent of fast matrix multiplication. The\nstate-of-the-art sample complexity of this problem is $\\widetilde{O}(k)$, but\ncan only be achieved by an *exponential-time* algorithm. Our algorithm uses\nslightly more samples ($\\sim k^4$) in exchange for small polynomial runtime,\nlaying the groundwork for a practical Fourier Interpolation algorithm.\nThe centerpiece of our algorithm is a new sufficient condition for the\nfrequency estimation task -- a high signal-to-noise (SNR) band condition --\nwhich allows for efficient and accurate signal reconstruction. Based on this\ncondition together with a new structural decomposition of Fourier signals\n(Signal Equivalent Method), we design a cheap algorithm for estimating each\n\"significant\" frequency within a narrow range, which is then combined with a\nnew high-accuracy signal estimation algorithm to reconstruct the ground-truth\nsignal.",
    "descriptor": "",
    "authors": [
      "Zhao Song",
      "Baocheng Sun",
      "Omri Weinstein",
      "Ruizhe Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.12495"
  },
  {
    "id": "arXiv:2210.12496",
    "title": "Bayesian Optimization with Conformal Coverage Guarantees",
    "abstract": "Bayesian optimization is a coherent, ubiquitous approach to decision-making\nunder uncertainty, with applications including multi-arm bandits, active\nlearning, and black-box optimization. Bayesian optimization selects decisions\n(i.e. objective function queries) with maximal expected utility with respect to\nthe posterior distribution of a Bayesian model, which quantifies reducible,\nepistemic uncertainty about query outcomes. In practice, subjectively\nimplausible outcomes can occur regularly for two reasons: 1) model\nmisspecification and 2) covariate shift. Conformal prediction is an uncertainty\nquantification method with coverage guarantees even for misspecified models and\na simple mechanism to correct for covariate shift. We propose conformal\nBayesian optimization, which directs queries towards regions of search space\nwhere the model predictions have guaranteed validity, and investigate its\nbehavior on a suite of black-box optimization tasks and tabular ranking tasks.\nIn many cases we find that query coverage can be significantly improved without\nharming sample-efficiency.",
    "descriptor": "\nComments: For code, see this https URL\n",
    "authors": [
      "Samuel Stanton",
      "Wesley Maddox",
      "Andrew Gordon Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12496"
  },
  {
    "id": "arXiv:2210.12499",
    "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and  Cross-lingual NLU",
    "abstract": "Curriculum Learning (CL) is a technique of training models via ranking\nexamples in a typically increasing difficulty trend with the aim of\naccelerating convergence and improving generalisability. Current approaches for\nNatural Language Understanding (NLU) tasks use CL to improve in-distribution\ndata performance often via heuristic-oriented or task-agnostic difficulties. In\nthis work, instead, we employ CL for NLU by taking advantage of training\ndynamics as difficulty metrics, i.e., statistics that measure the behavior of\nthe model at hand on specific task-data instances during training and propose\nmodifications of existing CL schedulers based on these statistics. Differently\nfrom existing works, we focus on evaluating models on in-distribution (ID),\nout-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer\ndatasets. We show across several NLU tasks that CL with training dynamics can\nresult in better performance mostly on zero-shot cross-lingual transfer and OOD\nsettings with improvements up by 8.5% in certain cases. Overall, experiments\nindicate that training dynamics can lead to better performing models with\nsmoother training compared to other difficulty metrics while being 20% faster\non average. In addition, through analysis we shed light on the correlations of\ntask-specific versus task-agnostic metrics.",
    "descriptor": "\nComments: 17 pages, 4 figures, 6 tables. To appear in EMNLP 2022\n",
    "authors": [
      "Fenia Christopoulou",
      "Gerasimos Lampouras",
      "Ignacio Iacobacci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12499"
  },
  {
    "id": "arXiv:2210.12504",
    "title": "Generative Modeling of High-resolution Global Precipitation Forecasts",
    "abstract": "Forecasting global precipitation patterns and, in particular, extreme\nprecipitation events is of critical importance to preparing for and adapting to\nclimate change. Making accurate high-resolution precipitation forecasts using\ntraditional physical models remains a major challenge in operational weather\nforecasting as they incur substantial computational costs and struggle to\nachieve sufficient forecast skill. Recently, deep-learning-based models have\nshown great promise in closing the gap with numerical weather prediction (NWP)\nmodels in terms of precipitation forecast skill, opening up exciting new\navenues for precipitation modeling. However, it is challenging for these deep\nlearning models to fully resolve the fine-scale structures of precipitation\nphenomena and adequately characterize the extremes of the long-tailed\nprecipitation distribution. In this work, we present several improvements to\nthe architecture and training process of a current state-of-the art deep\nlearning precipitation model (FourCastNet) using a novel generative adversarial\nnetwork (GAN) to better capture fine scales and extremes. Our improvements\nachieve superior performance in capturing the extreme percentiles of global\nprecipitation, while comparable to state-of-the-art NWP models in terms of\nforecast skill at 1--2 day lead times. Together, these improvements set a new\nstate-of-the-art in global precipitation forecasting.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022 Tackling Climate Change with Machine Learning Workshop\n",
    "authors": [
      "James Duncan",
      "Shashank Subramanian",
      "Peter Harrington"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.12504"
  },
  {
    "id": "arXiv:2210.12506",
    "title": "Self-supervised Graph-based Point-of-interest Recommendation",
    "abstract": "The exponential growth of Location-based Social Networks (LBSNs) has greatly\nstimulated the demand for precise location-based recommendation services. Next\nPoint-of-Interest (POI) recommendation, which aims to provide personalised POI\nsuggestions for users based on their visiting histories, has become a prominent\ncomponent in location-based e-commerce. Recent POI recommenders mainly employ\nself-attention mechanism or graph neural networks to model complex high-order\nPOI-wise interactions. However, most of them are merely trained on the\nhistorical check-in data in a standard supervised learning manner, which fail\nto fully explore each user's multi-faceted preferences, and suffer from data\nscarcity and long-tailed POI distribution, resulting in sub-optimal\nperformance. To this end, we propose a Self-s}upervised Graph-enhanced POI\nRecommender (S2GRec) for next POI recommendation. In particular, we devise a\nnovel Graph-enhanced Self-attentive layer to incorporate the collaborative\nsignals from both global transition graph and local trajectory graphs to\nuncover the transitional dependencies among POIs and capture a user's temporal\ninterests. In order to counteract the scarcity and incompleteness of POI\ncheck-ins, we propose a novel self-supervised learning paradigm in \\ssgrec,\nwhere the trajectory representations are contrastively learned from two\naugmented views on geolocations and temporal transitions. Extensive experiments\nare conducted on three real-world LBSN datasets, demonstrating the\neffectiveness of our model against state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Yang Li",
      "Tong Chen",
      "Peng-Fei Zhang",
      "Zi Huang",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12506"
  },
  {
    "id": "arXiv:2210.12509",
    "title": "Cut-and-Approximate: 3D Shape Reconstruction from Planar Cross-sections  with Deep Reinforcement Learning",
    "abstract": "Current methods for 3D object reconstruction from a set of planar\ncross-sections still struggle to capture detailed topology or require a\nconsiderable number of cross-sections. In this paper, we present, to the best\nof our knowledge the first 3D shape reconstruction network to solve this task\nwhich additionally uses orthographic projections of the shape. Our method is\nbased on applying a Reinforcement Learning algorithm to learn how to\neffectively parse the shape using a trial-and-error scheme relying on scalar\nrewards. This method cuts a part of a 3D shape in each step which is then\napproximated as a polygon mesh. The agent aims to maximize the reward that\ndepends on the accuracy of surface reconstruction for the approximated parts.\nWe also consider pre-training of the network for faster learning using\ndemonstrations generated by a heuristic approach. Experiments show that our\ntraining algorithm which benefits from both imitation learning and also self\nexploration, learns efficient policies faster, which results the agent to\nproduce visually compelling results.",
    "descriptor": "\nComments: 13 pages; NeurIPS paper\n",
    "authors": [
      "Azimkhon Ostonov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12509"
  },
  {
    "id": "arXiv:2210.12511",
    "title": "DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in  Interactive Autonomous Driving Agents",
    "abstract": "In the real world, autonomous driving agents navigate in highly dynamic\nenvironments full of unexpected situations where pre-trained models are\nunreliable. In these situations, what is immediately available to vehicles is\noften only human operators. Empowering autonomous driving agents with the\nability to navigate in a continuous and dynamic environment and to communicate\nwith humans through sensorimotor-grounded dialogue becomes critical. To this\nend, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a\nnovel interactive simulation platform that enables the creation of unexpected\nsituations on the fly to support empirical studies on situated communication\nwith autonomous driving agents. Based on this platform, we created the Situated\nDialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of\n8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed\naudio. SDN is developed to evaluate the agent's ability to predict dialogue\nmoves from humans as well as generate its own dialogue moves and physical\nnavigation actions. We further developed a transformer-based baseline model for\nthese SDN tasks. Our empirical results indicate that language guided-navigation\nin a highly dynamic environment is an extremely difficult task for end-to-end\nmodels. These results will provide insight towards future work on robust\nautonomous driving agents. The DOROTHIE platform, SDN benchmark, and code for\nthe baseline model are available at https://github.com/sled-group/DOROTHIE.",
    "descriptor": "\nComments: Findings of EMNLP, 2022\n",
    "authors": [
      "Ziqiao Ma",
      "Ben VanDerPloeg",
      "Cristian-Paul Bara",
      "Huang Yidong",
      "Eui-In Kim",
      "Felix Gervits",
      "Matthew Marge",
      "Joyce Chai"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12511"
  },
  {
    "id": "arXiv:2210.12513",
    "title": "HAM: Hierarchical Attention Model with High Performance for 3D Visual  Grounding",
    "abstract": "This paper tackles an emerging and challenging vision-language task, 3D\nvisual grounding on point clouds. Many recent works benefit from Transformer\nwith the well-known attention mechanism, leading to a tremendous breakthrough\nfor this task. However, we find that they realize the achievement by using\nvarious pre-training or multi-stage processing. To simplify the pipeline, we\ncarefully investigate 3D visual grounding and propose three fundamental\nquestions about how to develop an end-to-end model with high performance for\nthis task. To address these problems, we especially introduce a novel\nHierarchical Attention Model (HAM), offering multi-granularity representation\nand efficient augmentation for both given texts and multi-modal visual inputs.\nMore importantly, HAM ranks first on the large-scale ScanRefer challenge, which\noutperforms all the existing methods by a significant margin. Codes will be\nreleased after acceptance.",
    "descriptor": "\nComments: Champion on ECCV 2022 ScanRefer Challenge\n",
    "authors": [
      "Jiaming Chen",
      "Weixin Luo",
      "Xiaolin Wei",
      "Lin Ma",
      "Wei Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12513"
  },
  {
    "id": "arXiv:2210.12514",
    "title": "Asymptotically compatible energy of variable-step fractional BDF2  formula for time-fractional Cahn-Hilliard model",
    "abstract": "A new discrete energy dissipation law of the variable-step fractional BDF2\n(second-order backward differentiation formula) scheme is established for\ntime-fractional Cahn-Hilliard model with the Caputo's fractional derivative of\norder $\\alpha\\in(0,1)$, under a weak step-ratio constraint $0.4753\\le\n\\tau_k/\\tau_{k-1}<r^*(\\alpha)$, where $\\tau_k$ is the $k$-th time-step size and\n$r^*(\\alpha)\\ge4.660$ for $\\alpha\\in(0,1)$.We propose a novel discrete gradient\nstructure by a local-nonlocal splitting technique, that is, the fractional BDF2\nformula is split into a local part analogue to the two-step backward\ndifferentiation formula of the first derivative and a nonlocal part analogue to\nthe L1-type formula of the Caputo's derivative. More interestingly, in the\nsense of the limit $\\alpha\\rightarrow1^-$, the discrete energy and the\ncorresponding energy dissipation law are asymptotically compatible with the\nassociated discrete energy and the energy dissipation law of the variable-step\nBDF2 method for the classical Cahn-Hilliard equation, respectively. Numerical\nexamples with an adaptive stepping procedure are provided to demonstrate the\naccuracy and the effectiveness of our proposed method.",
    "descriptor": "\nComments: 21 pages, 22 figures\n",
    "authors": [
      "Hong-lin Liao",
      "Nan Liu",
      "Xuan Zhao"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12514"
  },
  {
    "id": "arXiv:2210.12515",
    "title": "SpectraNet: Multivariate Forecasting and Imputation under Distribution  Shifts and Missing Data",
    "abstract": "In this work, we tackle two widespread challenges in real applications for\ntime-series forecasting that have been largely understudied: distribution\nshifts and missing data. We propose SpectraNet, a novel multivariate\ntime-series forecasting model that dynamically infers a latent space spectral\ndecomposition to capture current temporal dynamics and correlations on the\nrecent observed history. A Convolution Neural Network maps the learned\nrepresentation by sequentially mixing its components and refining the output.\nOur proposed approach can simultaneously produce forecasts and interpolate past\nobservations and can, therefore, greatly simplify production systems by\nunifying imputation and forecasting tasks into a single model. SpectraNet\nachieves SoTA performance simultaneously on both tasks on five benchmark\ndatasets, compared to forecasting and imputation models, with up to 92% fewer\nparameters and comparable training times. On settings with up to 80% missing\ndata, SpectraNet has average performance improvements of almost 50% over the\nsecond-best alternative. Our code is available at\nhttps://github.com/cchallu/spectranet.",
    "descriptor": "",
    "authors": [
      "Cristian Challu",
      "Peihong Jiang",
      "Ying Nian Wu",
      "Laurent Callot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12515"
  },
  {
    "id": "arXiv:2210.12517",
    "title": "Exploring The Landscape of Distributional Robustness for Question  Answering Models",
    "abstract": "We conduct a large empirical evaluation to investigate the landscape of\ndistributional robustness in question answering. Our investigation spans over\n350 models and 16 question answering datasets, including a diverse set of\narchitectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter\ntuning, in-context learning, etc.). We find that, in many cases, model\nvariations do not affect robustness and in-distribution performance alone\ndetermines out-of-distribution performance. Moreover, our findings indicate\nthat i) zero-shot and in-context learning methods are more robust to\ndistribution shifts than fully fine-tuned models; ii) few-shot prompt\nfine-tuned models exhibit better robustness than few-shot fine-tuned span\nprediction models; iii) parameter-efficient and robustness enhancing training\nmethods provide no significant robustness improvements. In addition, we\npublicly release all evaluations to encourage researchers to further analyze\nrobustness trends for question answering models.",
    "descriptor": "\nComments: EMNLP Findings 2022\n",
    "authors": [
      "Anas Awadalla",
      "Mitchell Wortsman",
      "Gabriel Ilharco",
      "Sewon Min",
      "Ian Magnusson",
      "Hannaneh Hajishirzi",
      "Ludwig Schmidt"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12517"
  },
  {
    "id": "arXiv:2210.12521",
    "title": "H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding  Object Articulations from Interactions",
    "abstract": "The world is filled with articulated objects that are difficult to determine\nhow to use from vision alone, e.g., a door might open inwards or outwards.\nHumans handle these objects with strategic trial-and-error: first pushing a\ndoor then pulling if that doesn't work. We enable these capabilities in\nautonomous agents by proposing \"Hypothesize, Simulate, Act, Update, and Repeat\"\n(H-SAUR), a probabilistic generative framework that simultaneously generates a\ndistribution of hypotheses about how objects articulate given input\nobservations, captures certainty over hypotheses over time, and infer plausible\nactions for exploration and goal-conditioned manipulation. We compare our model\nwith existing work in manipulating objects after a handful of exploration\nactions, on the PartNet-Mobility dataset. We further propose a novel\nPuzzleBoxes benchmark that contains locked boxes that require multiple steps to\nsolve. We show that the proposed model significantly outperforms the current\nstate-of-the-art articulated object manipulation framework, despite using zero\ntraining data. We further improve the test-time efficiency of H-SAUR by\nintegrating a learned prior from learning-based vision models.",
    "descriptor": "",
    "authors": [
      "Kei Ota",
      "Hsiao-Yu Tung",
      "Kevin A. Smith",
      "Anoop Cherian",
      "Tim K. Marks",
      "Alan Sullivan",
      "Asako Kanezaki",
      "Joshua B. Tenenbaum"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12521"
  },
  {
    "id": "arXiv:2210.12523",
    "title": "How Real is Real: Evaluating the Robustness of Real-World Super  Resolution",
    "abstract": "Image super-resolution (SR) is a field in computer vision that focuses on\nreconstructing high-resolution images from the respective low-resolution image.\nHowever, super-resolution is a well-known ill-posed problem as most methods\nrely on the downsampling method performed on the high-resolution image to form\nthe low-resolution image to be known. Unfortunately, this is not something that\nis available in real-life super-resolution applications such as increasing the\nquality of a photo taken on a mobile phone. In this paper we will evaluate\nmultiple state-of-the-art super-resolution methods and gauge their performance\nwhen presented with various types of real-life images and discuss the benefits\nand drawbacks of each method. We also introduce a novel dataset, WideRealSR,\ncontaining real images from a wide variety of sources. Finally, through careful\nexperimentation and evaluation, we will present a potential solution to\nalleviate the generalization problem which is imminent in most state-of-the-art\nsuper-resolution models.",
    "descriptor": "\nComments: Machine Learning Practical Final Report, The University of Edinburgh\n",
    "authors": [
      "Athiya Deviyani",
      "Efe Sinan Hoplamaz",
      "Alan Savio Paul"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.12523"
  },
  {
    "id": "arXiv:2210.12524",
    "title": "Efficient Hair Style Transfer with Generative Adversarial Networks",
    "abstract": "Despite the recent success of image generation and style transfer with\nGenerative Adversarial Networks (GANs), hair synthesis and style transfer\nremain challenging due to the shape and style variability of human hair in\nin-the-wild conditions. The current state-of-the-art hair synthesis approaches\nstruggle to maintain global composition of the target style and cannot be used\nin real-time applications due to their high running costs on high-resolution\nportrait images. Therefore, We propose a novel hairstyle transfer method,\ncalled EHGAN, which reduces computational costs to enable real-time processing\nwhile improving the transfer of hairstyle with better global structure compared\nto the other state-of-the-art hair synthesis methods. To achieve this goal, we\ntrain an encoder and a low-resolution generator to transfer hairstyle and then,\nincrease the resolution of results with a pre-trained super-resolution model.\nWe utilize Adaptive Instance Normalization (AdaIN) and design our novel Hair\nBlending Block (HBB) to obtain the best performance of the generator. EHGAN\nneeds around 2.7 times and over 10,000 times less time consumption than the\nstate-of-the-art MichiGAN and LOHO methods respectively while obtaining better\nphotorealism and structural similarity to the desired style than its\ncompetitors.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2010.16417 by other authors\n",
    "authors": [
      "Muhammed Pektas",
      "Baris Gecer",
      "Aybars Ugur"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.12524"
  },
  {
    "id": "arXiv:2210.12526",
    "title": "Federated Calibration and Evaluation of Binary Classifiers",
    "abstract": "We address two major obstacles to practical use of supervised classifiers on\ndistributed private data. Whether a classifier was trained by a federation of\ncooperating clients or trained centrally out of distribution, (1) the output\nscores must be calibrated, and (2) performance metrics must be evaluated -- all\nwithout assembling labels in one place. In particular, we show how to perform\ncalibration and compute precision, recall, accuracy and ROC-AUC in the\nfederated setting under three privacy models (i) secure aggregation, (ii)\ndistributed differential privacy, (iii) local differential privacy. Our\ntheorems and experiments clarify tradeoffs between privacy, accuracy, and data\nefficiency. They also help decide whether a given application has sufficient\ndata to support federated calibration and evaluation.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Graham Cormode",
      "Igor Markov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12526"
  },
  {
    "id": "arXiv:2210.12527",
    "title": "Baby Physical Safety Monitoring in Smart Home Using Action Recognition  System",
    "abstract": "Humans are able to intuitively deduce actions that took place between two\nstates in observations via deductive reasoning. This is because the brain\noperates on a bidirectional communication model, which has radically improved\nthe accuracy of recognition and prediction based on features connected to\nprevious experiences. During the past decade, deep learning models for action\nrecognition have significantly improved. However, deep neural networks struggle\nwith these tasks on a smaller dataset for specific Action Recognition (AR)\ntasks. As with most action recognition tasks, the ambiguity of accurately\ndescribing activities in spatial-temporal data is a drawback that can be\novercome by curating suitable datasets, including careful annotations and\npreprocessing of video data for analyzing various recognition tasks. In this\nstudy, we present a novel lightweight framework combining transfer learning\ntechniques with a Conv2D LSTM layer to extract features from the pre-trained\nI3D model on the Kinetics dataset for a new AR task (Smart Baby Care) that\nrequires a smaller dataset and less computational resources. Furthermore, we\ndeveloped a benchmark dataset and an automated model that uses LSTM convolution\nwith I3D (ConvLSTM-I3D) for recognizing and predicting baby activities in a\nsmart baby room. Finally, we implemented video augmentation to improve model\nperformance on the smart baby care task. Compared to other benchmark models,\nour experimental framework achieved better performance with less computational\nresources.",
    "descriptor": "",
    "authors": [
      "Victor Adewopo",
      "Nelly Elsayed",
      "Kelly Anderson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12527"
  },
  {
    "id": "arXiv:2210.12529",
    "title": "On-Demand Sampling: Learning Optimally from Multiple Distributions",
    "abstract": "Social and real-world considerations such as robustness, fairness, social\nwelfare and multi-agent tradeoffs have given rise to multi-distribution\nlearning paradigms, such as collaborative, group distributionally robust, and\nfair federated learning. In each of these settings, a learner seeks to minimize\nits worst-case loss over a set of $n$ predefined distributions, while using as\nfew samples as possible. In this paper, we establish the optimal sample\ncomplexity of these learning paradigms and give algorithms that meet this\nsample complexity. Importantly, our sample complexity bounds exceed that of the\nsample complexity of learning a single distribution only by an additive factor\nof $n \\log(n) / \\epsilon^2$. These improve upon the best known sample\ncomplexity of agnostic federated learning by Mohri et al. by a multiplicative\nfactor of $n$, the sample complexity of collaborative learning by Nguyen and\nZakynthinou by a multiplicative factor $\\log n / \\epsilon^3$, and give the\nfirst sample complexity bounds for the group DRO objective of Sagawa et al. To\nachieve optimal sample complexity, our algorithms learn to sample and learn\nfrom distributions on demand. Our algorithm design and analysis is enabled by\nour extensions of stochastic optimization techniques for solving stochastic\nzero-sum games. In particular, we contribute variants of Stochastic Mirror\nDescent that can trade off between players' access to cheap one-off samples or\nmore expensive reusable ones.",
    "descriptor": "\nComments: 39 pages, 1 figure. Authors are ordered alphabetically. Appearing at the Thirty-sixth Conference on Neural Information Processing Systems (Neurips 2022)\n",
    "authors": [
      "Nika Haghtalab",
      "Michael I. Jordan",
      "Eric Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.12529"
  },
  {
    "id": "arXiv:2210.12530",
    "title": "LMPriors: Pre-Trained Language Models as Task-Specific Priors",
    "abstract": "Particularly in low-data regimes, an outstanding challenge in machine\nlearning is developing principled techniques for augmenting our models with\nsuitable priors. This is to encourage them to learn in ways that are compatible\nwith our understanding of the world. But in contrast to generic priors such as\nshrinkage or sparsity, we draw inspiration from the recent successes of\nlarge-scale language models (LMs) to construct task-specific priors distilled\nfrom the rich knowledge of LMs. Our method, Language Model Priors (LMPriors),\nincorporates auxiliary natural language metadata about the task -- such as\nvariable names and descriptions -- to encourage downstream model outputs to be\nconsistent with the LM's common-sense reasoning based on the metadata.\nEmpirically, we demonstrate that LMPriors improve model performance in settings\nwhere such natural language descriptions are available, and perform well on\nseveral tasks that benefit from such prior knowledge, such as feature\nselection, causal inference, and safe reinforcement learning.",
    "descriptor": "\nComments: First two authors contributed equally\n",
    "authors": [
      "Kristy Choi",
      "Chris Cundy",
      "Sanjari Srivastava",
      "Stefano Ermon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12530"
  },
  {
    "id": "arXiv:2210.12531",
    "title": "Why Do You Feel This Way? Summarizing Triggers of Emotions in Social  Media Posts",
    "abstract": "Crises such as the COVID-19 pandemic continuously threaten our world and\nemotionally affect billions of people worldwide in distinct ways. Understanding\nthe triggers leading to people's emotions is of crucial importance. Social\nmedia posts can be a good source of such analysis, yet these texts tend to be\ncharged with multiple emotions, with triggers scattering across multiple\nsentences. This paper takes a novel angle, namely, emotion detection and\ntrigger summarization, aiming to both detect perceived emotions in text, and\nsummarize events and their appraisals that trigger each emotion. To support\nthis goal, we introduce CovidET (Emotions and their Triggers during Covid-19),\na dataset of ~1,900 English Reddit posts related to COVID-19, which contains\nmanual annotations of perceived emotions and abstractive summaries of their\ntriggers described in the post. We develop strong baselines to jointly detect\nemotions and summarize emotion triggers. Our analyses show that CovidET\npresents new challenges in emotion-specific summarization, as well as\nmulti-emotion detection in long social media posts.",
    "descriptor": "\nComments: EMNLP 2022 Camera Ready Version\n",
    "authors": [
      "Hongli Zhan",
      "Tiberiu Sosea",
      "Cornelia Caragea",
      "Junyi Jessy Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.12531"
  },
  {
    "id": "arXiv:2210.12538",
    "title": "Compressing multidimensional weather and climate data into neural  networks",
    "abstract": "Weather and climate simulations produce petabytes of high-resolution data\nthat are later analyzed by researchers in order to understand climate change or\nsevere weather. We propose a new method of compressing this multidimensional\nweather and climate data: a coordinate-based neural network is trained to\noverfit the data, and the resulting parameters are taken as a compact\nrepresentation of the original grid-based data. While compression ratios range\nfrom 300x to more than 3,000x, our method outperforms the state-of-the-art\ncompressor SZ3 in terms of weighted RMSE, MAE. It can faithfully preserve\nimportant large scale atmosphere structures and does not introduce artifacts.\nWhen using the resulting neural network as a 790x compressed dataloader to\ntrain the WeatherBench forecasting model, its RMSE increases by less than 2%.\nThe three orders of magnitude compression democratizes access to\nhigh-resolution climate data and enables numerous new research directions.",
    "descriptor": "",
    "authors": [
      "Langwen Huang",
      "Torsten Hoefler"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.12538"
  },
  {
    "id": "arXiv:2210.12539",
    "title": "ACP+: An Age Control Protocol for the Internet",
    "abstract": "We present ACP+, an age control protocol, which is a transport layer protocol\nthat regulates the rate at which update packets from a source are sent over the\nInternet to a monitor. The source would like to keep the average age of sensed\ninformation at the monitor to a minimum, given the network conditions.\nExtensive experimentation helps us shed light on age control over the current\nInternet and its implications for sources sending updates over a shared\nwireless access to monitors in the cloud. We also show that many congestion\ncontrol algorithms proposed over the years for the Transmission Control\nProtocol (TCP), including hybrid approaches that achieve higher throughputs at\nlower delays than traditional loss-based congestion control, are unsuitable for\nage control.",
    "descriptor": "\nComments: Under submission. arXiv admin note: text overlap with arXiv:2103.07797, arXiv:1811.03353\n",
    "authors": [
      "Tanya Shreedhar",
      "Sanjit K. Kaul",
      "Roy D. Yates"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.12539"
  },
  {
    "id": "arXiv:2210.12540",
    "title": "EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric  Code Switching",
    "abstract": "Accurate alignment between languages is fundamental for improving\ncross-lingual pre-trained language models (XLMs). Motivated by the natural\nphenomenon of code-switching (CS) in multilingual speakers, CS has been used as\nan effective data augmentation method that offers language alignment at word-\nor phrase-level, in contrast to sentence-level via parallel instances. Existing\napproaches either use dictionaries or parallel sentences with word-alignment to\ngenerate CS data by randomly switching words in a sentence. However, such\nmethods can be suboptimal as dictionaries disregard semantics, and syntax might\nbecome invalid after random word switching. In this work, we propose EntityCS,\na method that focuses on Entity-level Code-Switching to capture fine-grained\ncross-lingual semantics without corrupting syntax. We use Wikidata and the\nEnglish Wikipedia to construct an entity-centric CS corpus by switching\nentities to their counterparts in other languages. We further propose\nentity-oriented masking strategies during intermediate model training on the\nEntityCS corpus for improving entity prediction. Evaluation of the trained\nmodels on four entity-centric downstream tasks shows consistent improvements\nover the baseline with a notable increase of 10% in Fact Retrieval. We release\nthe corpus and models to assist research on code-switching and enriching XLMs\nwith external knowledge.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Chenxi Whitehouse",
      "Fenia Christopoulou",
      "Ignacio Iacobacci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12540"
  },
  {
    "id": "arXiv:2210.12541",
    "title": "GCT: Gated Contextual Transformer for Sequential Audio Tagging",
    "abstract": "Audio tagging aims to assign predefined tags to audio clips to indicate the\nclass information of audio events. Sequential audio tagging (SAT) means\ndetecting both the class information of audio events, and the order in which\nthey occur within the audio clip. Most existing methods for SAT are based on\nconnectionist temporal classification (CTC). However, CTC cannot effectively\ncapture connections between events due to the conditional independence\nassumption between outputs at different times. The contextual Transformer\n(cTransformer) addresses this issue by exploiting contextual information in\nSAT. Nevertheless, cTransformer is also limited in exploiting contextual\ninformation as it only uses forward information in inference. This paper\nproposes a gated contextual Transformer (GCT) with forward-backward inference\n(FBI). In addition, a gated contextual multi-layer perceptron (GCMLP) block is\nproposed in GCT to improve the performance of cTransformer structurally.\nExperiments on two real-life audio datasets show that the proposed GCT with\nGCMLP and FBI performs better than the CTC-based methods and cTransformer. To\npromote research on SAT, the manually annotated sequential labels for the two\ndatasets are released.",
    "descriptor": "",
    "authors": [
      "Yuanbo Hou",
      "Yun Wang",
      "Wenwu Wang",
      "Dick Botteldooren"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12541"
  },
  {
    "id": "arXiv:2210.12542",
    "title": "Integral Equation Methods for the Morse-Ingard Equations",
    "abstract": "We present two (a decoupled and a coupled) integral-equation-based methods\nfor the Morse-Ingard equations subject to Neumann boundary conditions on the\nexterior domain. Both methods are based on second-kind integral equation (SKIE)\nformulations. The coupled method is well-conditioned and can achieve high\naccuracy. The decoupled method has lower computational cost and more\nflexibility in dealing with the boundary layer; however, it is prone to the\nill-conditioning of the decoupling transform and cannot achieve as high\naccuracy as the coupled method. We show numerical examples using a Nystr\\\"om\nmethod based on quadrature-by-expansion (QBX) with fast-multipole acceleration.\nWe demonstrate the accuracy and efficiency of the solvers in both two and three\ndimensions with complex geometry.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Wei",
      "Andreas Kl\u00f6ckner",
      "Robert C. Kirby"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12542"
  },
  {
    "id": "arXiv:2210.12543",
    "title": "Edge-weighted Online Stochastic Matching: Beating $1-\\frac1e$",
    "abstract": "We study the edge-weighted online stochastic matching problem. Since Feldman,\nMehta, Mirrokni, and Muthukrishnan proposed the $(1-\\frac1e)$-competitive\nSuggested Matching algorithm, there has been no improvement for the general\nedge-weighted online stochastic matching problem. In this paper, we introduce\nthe first algorithm beating the $1-\\frac1e$ bound in this setting, achieving a\ncompetitive ratio of $0.645$. Under the LP proposed by Jaillet and Lu, we\ndesign an algorithmic preprocessing, dividing all edges into two classes. Then\nbased on the Suggested Matching algorithm, we adjust the matching strategy to\nimprove the performance on one class in the early stage and on another class in\nthe late stage, while keeping the matching events of different edges highly\nindependent. By balancing them, we guarantee the matching probability of every\nsingle edge.",
    "descriptor": "",
    "authors": [
      "Shuyi Yan"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.12543"
  },
  {
    "id": "arXiv:2210.12546",
    "title": "Policy Optimization with Advantage Regularization for Long-Term Fairness  in Decision Systems",
    "abstract": "Long-term fairness is an important factor of consideration in designing and\ndeploying learning-based decision systems in high-stake decision-making\ncontexts. Recent work has proposed the use of Markov Decision Processes (MDPs)\nto formulate decision-making with long-term fairness requirements in\ndynamically changing environments, and demonstrated major challenges in\ndirectly deploying heuristic and rule-based policies that worked well in static\nenvironments. We show that policy optimization methods from deep reinforcement\nlearning can be used to find strictly better decision policies that can often\nachieve both higher overall utility and less violation of the fairness\nrequirements, compared to previously-known strategies. In particular, we\npropose new methods for imposing fairness requirements in policy optimization\nby regularizing the advantage evaluation of different actions. Our proposed\nmethods make it easy to impose fairness constraints without reward engineering\nor sacrificing training efficiency. We perform detailed analyses in three\nestablished case studies, including attention allocation in incident\nmonitoring, bank loan approval, and vaccine distribution in population\nnetworks.",
    "descriptor": "\nComments: 14 pages, published in NeurIPS 2022\n",
    "authors": [
      "Eric Yang Yu",
      "Zhizhen Qin",
      "Min Kyung Lee",
      "Sicun Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.12546"
  },
  {
    "id": "arXiv:2210.12547",
    "title": "SurCo: Learning Linear Surrogates For Combinatorial Nonlinear  Optimization Problems",
    "abstract": "Optimization problems with expensive nonlinear cost functions and\ncombinatorial constraints appear in many real-world applications, but remain\nchallenging to solve efficiently. Existing combinatorial solvers like Mixed\nInteger Linear Programming can be fast in practice but cannot readily optimize\nnonlinear cost functions, while general nonlinear optimizers like gradient\ndescent often do not handle complex combinatorial structures, may require many\nqueries of the cost function, and are prone to local optima. To bridge this\ngap, we propose SurCo that learns linear Surrogate costs which can be used by\nexisting Combinatorial solvers to output good solutions to the original\nnonlinear combinatorial optimization problem, combining the flexibility of\ngradient-based methods with the structure of linear combinatorial optimization.\nWe learn these linear surrogates end-to-end with the nonlinear loss by\ndifferentiating through the linear surrogate solver. Three variants of SurCo\nare proposed: SurCo-zero operates on individual nonlinear problems, SurCo-prior\ntrains a linear surrogate predictor on distributions of problems, and\nSurCo-hybrid uses a model trained offline to warm start online solving for\nSurCo-zero. We analyze our method theoretically and empirically, showing smooth\nconvergence and improved performance. Experiments show that compared to\nstate-of-the-art approaches and expert-designed heuristics, SurCo obtains lower\ncost solutions with comparable or faster solve time for two realworld\nindustry-level applications: embedding table sharding and inverse photonic\ndesign.",
    "descriptor": "",
    "authors": [
      "Aaron Ferber",
      "Taoan Huang",
      "Daochen Zha",
      "Martin Schubert",
      "Benoit Steiner",
      "Bistra Dilkina",
      "Yuandong Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12547"
  },
  {
    "id": "arXiv:2210.12551",
    "title": "The Schwarz alternating method for the seamless coupling of nonlinear  reduced order models and full order models",
    "abstract": "Projection-based model order reduction allows for the parsimonious\nrepresentation of full order models (FOMs), typically obtained through the\ndiscretization of certain partial differential equations (PDEs) using\nconventional techniques where the discretization may contain a very large\nnumber of degrees of freedom. As a result of this more compact representation,\nthe resulting projection-based reduced order models (ROMs) can achieve\nconsiderable computational speedups, which are especially useful in real-time\nor multi-query analyses. One known deficiency of projection-based ROMs is that\nthey can suffer from a lack of robustness, stability and accuracy, especially\nin the predictive regime, which ultimately limits their useful application.\nAnother research gap that has prevented the widespread adoption of ROMs within\nthe modeling and simulation community is the lack of theoretical and\nalgorithmic foundations necessary for the \"plug-and-play\" integration of these\nmodels into existing multi-scale and multi-physics frameworks. This paper\ndescribes a new methodology that has the potential to address both of the\naforementioned deficiencies by coupling projection-based ROMs with each other\nas well as with conventional FOMs by means of the Schwarz alternating method.\nLeveraging recent work that adapted the Schwarz alternating method to enable\nconsistent and concurrent multi-scale coupling of finite element FOMs in solid\nmechanics, we present a new extension of the Schwarz formulation that enables\nROM-FOM and ROM-ROM coupling in nonlinear solid mechanics. In order to maintain\nefficiency, we employ hyper-reduction via the Energy-Conserving Sampling and\nWeighting approach. We evaluate the proposed coupling approach in the\nreproductive as well as in the predictive regime on a canonical test case that\ninvolves the dynamic propagation of a traveling wave in a nonlinear\nhyper-elastic material.",
    "descriptor": "",
    "authors": [
      "Joshua Barnett",
      "Irina Tezaur",
      "Alejandro Mota"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Mathematical Physics (math-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.12551"
  },
  {
    "id": "arXiv:2210.12553",
    "title": "Understanding Domain Learning in Language Models Through Subpopulation  Analysis",
    "abstract": "We investigate how different domains are encoded in modern neural network\narchitectures. We analyze the relationship between natural language domains,\nmodel size, and the amount of training data used. The primary analysis tool we\ndevelop is based on subpopulation analysis with Singular Vector Canonical\nCorrelation Analysis (SVCCA), which we apply to Transformer-based language\nmodels (LMs). We compare the latent representations of such a language model at\nits different layers from a pair of models: a model trained on multiple domains\n(an experimental model) and a model trained on a single domain (a control\nmodel). Through our method, we find that increasing the model capacity impacts\nhow domain information is stored in upper and lower layers differently. In\naddition, we show that larger experimental models simultaneously embed\ndomain-specific information as if they were conjoined control models. These\nfindings are confirmed qualitatively, demonstrating the validity of our method.",
    "descriptor": "\nComments: Accepted to BlackboxNLP 2022\n",
    "authors": [
      "Zheng Zhao",
      "Yftah Ziser",
      "Shay B. Cohen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12553"
  },
  {
    "id": "arXiv:2210.12556",
    "title": "B$^3$RTDP: A Belief Branch and Bound Real-Time Dynamic Programming  Approach to Solving POMDPs",
    "abstract": "Partially Observable Markov Decision Processes (POMDPs) offer a promising\nworld representation for autonomous agents, as they can model both transitional\nand perceptual uncertainties. Calculating the optimal solution to POMDP\nproblems can be computationally expensive as they require reasoning over the\n(possibly infinite) space of beliefs. Several approaches have been proposed to\novercome this difficulty, such as discretizing the belief space, point-based\nbelief sampling, and Monte Carlo tree search. The Real-Time Dynamic Programming\napproach of the RTDP-Bel algorithm approximates the value function by storing\nit in a hashtable with discretized belief keys. We propose an extension to the\nRTDP-Bel algorithm which we call Belief Branch and Bound RTDP (B$^3$RTDP). Our\nalgorithm uses a bounded value function representation and takes advantage of\nthis in two novel ways: a search-bounding technique based on action selection\nconvergence probabilities, and a method for leveraging early action convergence\ncalled the \\textit{Convergence Frontier}. Lastly, we empirically demonstrate\nthat B$^3$RTDP can achieve greater returns in less time than the\nstate-of-the-art SARSOP solver on known POMDP problems.",
    "descriptor": "\nComments: Originally authored in 2014-2015\n",
    "authors": [
      "Sigurdur Orn Adalgeirsson",
      "Cynthia Breazeal"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12556"
  },
  {
    "id": "arXiv:2210.12560",
    "title": "PHEE: A Dataset for Pharmacovigilance Event Extraction from Text",
    "abstract": "The primary goal of drug safety researchers and regulators is to promptly\nidentify adverse drug reactions. Doing so may in turn prevent or reduce the\nharm to patients and ultimately improve public health. Evaluating and\nmonitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever\ngrowing collection of spontaneous reports from health professionals,\nphysicians, and pharmacists, and information voluntarily submitted by patients.\nIn this scenario, facilitating analysis of such reports via automation has the\npotential to rapidly identify safety signals. Unfortunately, public resources\nfor developing natural language models for this task are scant. We present\nPHEE, a novel dataset for pharmacovigilance comprising over 5000 annotated\nevents from medical case reports and biomedical literature, making it the\nlargest such public dataset to date. We describe the hierarchical event schema\ndesigned to provide coarse and fine-grained information about patients'\ndemographics, treatments and (side) effects. Along with the discussion of the\ndataset, we present a thorough experimental evaluation of current\nstate-of-the-art approaches for biomedical event extraction, point out their\nlimitations, and highlight open challenges to foster future research in this\narea.",
    "descriptor": "\nComments: 17 pages, 3 figures, EMNLP2022 accepted\n",
    "authors": [
      "Zhaoyue Sun",
      "Jiazheng Li",
      "Gabriele Pergola",
      "Byron C. Wallace",
      "Bino John",
      "Nigel Greene",
      "Joseph Kim",
      "Yulan He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12560"
  },
  {
    "id": "arXiv:2210.12562",
    "title": "Greedy Modality Selection via Approximate Submodular Maximization",
    "abstract": "Multimodal learning considers learning from multi-modality data, aiming to\nfuse heterogeneous sources of information. However, it is not always feasible\nto leverage all available modalities due to memory constraints. Further,\ntraining on all the modalities may be inefficient when redundant information\nexists within data, such as different subsets of modalities providing similar\nperformance. In light of these challenges, we study modality selection,\nintending to efficiently select the most informative and complementary\nmodalities under certain computational constraints. We formulate a theoretical\nframework for optimizing modality selection in multimodal learning and\nintroduce a utility measure to quantify the benefit of selecting a modality.\nFor this optimization problem, we present efficient algorithms when the utility\nmeasure exhibits monotonicity and approximate submodularity. We also connect\nthe utility measure with existing Shapley-value-based feature importance\nscores. Last, we demonstrate the efficacy of our algorithm on synthetic\n(Patch-MNIST) and two real-world (PEMS-SF, CMU-MOSI) datasets.",
    "descriptor": "\nComments: Uncertainty in Artificial Intelligence (UAI) 2022\n",
    "authors": [
      "Runxiang Cheng",
      "Gargi Balasubramaniam",
      "Yifei He",
      "Yao-Hung Hubert Tsai",
      "Han Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12562"
  },
  {
    "id": "arXiv:2210.12563",
    "title": "On the Limitations of Reference-Free Evaluations of Generated Text",
    "abstract": "There is significant interest in developing evaluation metrics which\naccurately estimate the quality of generated text without the aid of a\nhuman-written reference text, which can be time consuming and expensive to\ncollect or entirely unavailable in online applications. However, in this work,\nwe demonstrate that these reference-free metrics are inherently biased and\nlimited in their ability to evaluate generated text, and we argue that they\nshould not be used to measure progress on tasks like machine translation or\nsummarization. We show how reference-free metrics are equivalent to using one\ngeneration model to evaluate another, which has several limitations: (1) the\nmetrics can be optimized at test time to find the approximate best-possible\noutput, (2) they are inherently biased toward models which are more similar to\ntheir own, and (3) they can be biased against higher-quality outputs, including\nthose written by humans. Therefore, we recommend that reference-free metrics\nshould be used as diagnostic tools for analyzing and understanding model\nbehavior instead of measures of how well models perform a task, in which the\ngoal is to achieve as high of a score as possible.",
    "descriptor": "",
    "authors": [
      "Daniel Deutsch",
      "Rotem Dror",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12563"
  },
  {
    "id": "arXiv:2210.12564",
    "title": "HuPR: A Benchmark for Human Pose Estimation Using Millimeter Wave Radar",
    "abstract": "This paper introduces a novel human pose estimation benchmark, Human Pose\nwith Millimeter Wave Radar (HuPR), that includes synchronized vision and radio\nsignal components. This dataset is created using cross-calibrated mmWave radar\nsensors and a monocular RGB camera for cross-modality training of radar-based\nhuman pose estimation. There are two advantages of using mmWave radar to\nperform human pose estimation. First, it is robust to dark and low-light\nconditions. Second, it is not visually perceivable by humans and thus, can be\nwidely applied to applications with privacy concerns, e.g., surveillance\nsystems in patient rooms. In addition to the benchmark, we propose a\ncross-modality training framework that leverages the ground-truth 2D keypoints\nrepresenting human body joints for training, which are systematically generated\nfrom the pre-trained 2D pose estimation network based on a monocular camera\ninput image, avoiding laborious manual label annotation efforts. The framework\nconsists of a new radar pre-processing method that better extracts the velocity\ninformation from radar data, Cross- and Self-Attention Module (CSAM), to fuse\nmulti-scale radar features, and Pose Refinement Graph Convolutional Networks\n(PRGCN), to refine the predicted keypoint confidence heatmaps. Our intensive\nexperiments on the HuPR benchmark show that the proposed scheme achieves better\nhuman pose estimation performance with only radar data, as compared to\ntraditional pre-processing solutions and previous radio-frequency-based\nmethods.",
    "descriptor": "",
    "authors": [
      "Shih-Po Lee",
      "Niraj Prakash Kini",
      "Wen-Hsiao Peng",
      "Ching-Wen Ma",
      "Jenq-Neng Hwang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12564"
  },
  {
    "id": "arXiv:2210.12565",
    "title": "A Visual Tour Of Current Challenges In Multimodal Language Models",
    "abstract": "Transformer models trained on massive text corpora have become the de facto\nmodels for a wide range of natural language processing tasks. However, learning\neffective word representations for function words remains challenging.\nMultimodal learning, which visually grounds transformer models in imagery, can\novercome the challenges to some extent; however, there is still much work to be\ndone. In this study, we explore the extent to which visual grounding\nfacilitates the acquisition of function words using stable diffusion models\nthat employ multimodal models for text-to-image generation. Out of seven\ncategories of function words, along with numerous subcategories, we find that\nstable diffusion models effectively model only a small fraction of function\nwords -- a few pronoun subcategories and relatives. We hope that our findings\nwill stimulate the development of new datasets and approaches that enable\nmultimodal models to learn better representations of function words.",
    "descriptor": "",
    "authors": [
      "Shashank Sonkar",
      "Naiming Liu",
      "Richard G. Baraniuk"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12565"
  },
  {
    "id": "arXiv:2210.12566",
    "title": "Solving Continuous Control via Q-learning",
    "abstract": "While there has been substantial success in applying actor-critic methods to\ncontinuous control, simpler critic-only methods such as Q-learning often remain\nintractable in the associated high-dimensional action spaces. However, most\nactor-critic methods come at the cost of added complexity: heuristics for\nstabilization, compute requirements as well as wider hyperparameter search\nspaces. We show that these issues can be largely alleviated via Q-learning by\ncombining action discretization with value decomposition, framing single-agent\ncontrol as cooperative multi-agent reinforcement learning (MARL). With\nbang-bang actions, performance of this critic-only approach matches\nstate-of-the-art continuous actor-critic methods when learning from features or\npixels. We extend classical bandit examples from cooperative MARL to provide\nintuition for how decoupled critics leverage state information to coordinate\njoint optimization, and demonstrate surprisingly strong performance across a\nwide variety of continuous control tasks.",
    "descriptor": "",
    "authors": [
      "Tim Seyde",
      "Peter Werner",
      "Wilko Schwarting",
      "Igor Gilitschenski",
      "Martin Riedmiller",
      "Daniela Rus",
      "Markus Wulfmeier"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12566"
  },
  {
    "id": "arXiv:2210.12571",
    "title": "A Temporal Type-2 Fuzzy System for Time-dependent Explainable Artificial  Intelligence",
    "abstract": "Explainable Artificial Intelligence (XAI) is a paradigm that delivers\ntransparent models and decisions, which are easy to understand, analyze, and\naugment by a non-technical audience. Fuzzy Logic Systems (FLS) based XAI can\nprovide an explainable framework, while also modeling uncertainties present in\nreal-world environments, which renders it suitable for applications where\nexplainability is a requirement. However, most real-life processes are not\ncharacterized by high levels of uncertainties alone; they are inherently\ntime-dependent as well, i.e., the processes change with time. In this work, we\npresent novel Temporal Type-2 FLS Based Approach for time-dependent XAI (TXAI)\nsystems, which can account for the likelihood of a measurement's occurrence in\nthe time domain using (the measurement's) frequency of occurrence. In Temporal\nType-2 Fuzzy Sets (TT2FSs), a four-dimensional (4D) time-dependent membership\nfunction is developed where relations are used to construct the inter-relations\nbetween the elements of the universe of discourse and its frequency of\noccurrence. The TXAI system manifested better classification prowess, with\n10-fold test datasets, with a mean recall of 95.40\\% than a standard XAI system\n(based on non-temporal general type-2 (GT2) fuzzy sets) that had a mean recall\nof 87.04\\%. TXAI also performed significantly better than most non-explainable\nAI systems between 3.95\\%, to 19.04\\% improvement gain in mean recall. In\naddition, TXAI can also outline the most likely time-dependent trajectories\nusing the frequency of occurrence values embedded in the TXAI model; viz. given\na rule at a determined time interval, what will be the next most likely rule at\na subsequent time interval. In this regard, the proposed TXAI system can have\nprofound implications for delineating the evolution of real-life time-dependent\nprocesses, such as behavioural or biological processes.",
    "descriptor": "",
    "authors": [
      "Mehrin Kiani",
      "Javier Andreu-Perez",
      "Hani Hagras"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12571"
  },
  {
    "id": "arXiv:2210.12573",
    "title": "An Efficient Nonlinear Acceleration method that Exploits Symmetry of the  Hessian",
    "abstract": "Nonlinear acceleration methods are powerful techniques to speed up\nfixed-point iterations. However, many acceleration methods require storing a\nlarge number of previous iterates and this can become impractical if\ncomputational resources are limited. In this paper, we propose a nonlinear\nTruncated Generalized Conjugate Residual method (nlTGCR) whose goal is to\nexploit the symmetry of the Hessian to reduce memory usage. The proposed method\ncan be interpreted as either an inexact Newton or a quasi-Newton method. We\nshow that, with the help of global strategies like residual check techniques,\nnlTGCR can converge globally for general nonlinear problems and that under mild\nconditions, nlTGCR is able to achieve superlinear convergence. We further\nanalyze the convergence of nlTGCR in a stochastic setting. Numerical results\ndemonstrate the superiority of nlTGCR when compared with several other\ncompetitive baseline approaches on a few problems. Our code will be available\nin the future.",
    "descriptor": "\nComments: Optimization, Short-term recurrence method by exploiting Hessian, Numerical Analysis, Iterative Method, Quasi-Newton, Anderson Acceleration, 31 pages\n",
    "authors": [
      "Huan He",
      "Shifan Zhao",
      "Ziyuan Tang",
      "Joyce C Ho",
      "Yousef Saad",
      "Yuanzhe Xi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12573"
  },
  {
    "id": "arXiv:2210.12574",
    "title": "The Curious Case of Absolute Position Embeddings",
    "abstract": "Transformer language models encode the notion of word order using positional\ninformation. Most commonly, this positional information is represented by\nabsolute position embeddings (APEs), that are learned from the pretraining\ndata. However, in natural language, it is not absolute position that matters,\nbut relative position, and the extent to which APEs can capture this type of\ninformation has not been investigated. In this work, we observe that models\ntrained with APE over-rely on positional information to the point that they\nbreak-down when subjected to sentences with shifted position information.\nSpecifically, when models are subjected to sentences starting from a non-zero\nposition (excluding the effect of priming), they exhibit noticeably degraded\nperformance on zero to full-shot tasks, across a range of model families and\nmodel sizes. Our findings raise questions about the efficacy of APEs to model\nthe relativity of position information, and invite further introspection on the\nsentence and word order processing strategies employed by these models.",
    "descriptor": "\nComments: Accepted at EMNLP 2022 Findings; 5 pages and 15 pages Appendix\n",
    "authors": [
      "Koustuv Sinha",
      "Amirhossein Kazemnejad",
      "Siva Reddy",
      "Joelle Pineau",
      "Dieuwke Hupkes",
      "Adina Williams"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12574"
  },
  {
    "id": "arXiv:2210.12575",
    "title": "Outsourcing Training without Uploading Data via Efficient Collaborative  Open-Source Sampling",
    "abstract": "As deep learning blooms with growing demand for computation and data\nresources, outsourcing model training to a powerful cloud server becomes an\nattractive alternative to training at a low-power and cost-effective end\ndevice. Traditional outsourcing requires uploading device data to the cloud\nserver, which can be infeasible in many real-world applications due to the\noften sensitive nature of the collected data and the limited communication\nbandwidth. To tackle these challenges, we propose to leverage widely available\nopen-source data, which is a massive dataset collected from public and\nheterogeneous sources (e.g., Internet images). We develop a novel strategy\ncalled Efficient Collaborative Open-source Sampling (ECOS) to construct a\nproximal proxy dataset from open-source data for cloud training, in lieu of\nclient data. ECOS probes open-source data on the cloud server to sense the\ndistribution of client data via a communication- and computation-efficient\nsampling process, which only communicates a few compressed public features and\nclient scalar responses. Extensive empirical studies show that the proposed\nECOS improves the quality of automated client labeling, model compression, and\nlabel outsourcing when applied in various learning scenarios.",
    "descriptor": "\nComments: Accepted to NeurIPS'22\n",
    "authors": [
      "Junyuan Hong",
      "Lingjuan Lyu",
      "Jiayu Zhou",
      "Michael Spranger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12575"
  },
  {
    "id": "arXiv:2210.12579",
    "title": "Efficient Nearest Neighbor Search for Cross-Encoder Models using Matrix  Factorization",
    "abstract": "Efficient k-nearest neighbor search is a fundamental task, foundational for\nmany problems in NLP. When the similarity is measured by dot-product between\ndual-encoder vectors or $\\ell_2$-distance, there already exist many scalable\nand efficient search methods. But not so when similarity is measured by more\naccurate and expensive black-box neural similarity models, such as\ncross-encoders, which jointly encode the query and candidate neighbor. The\ncross-encoders' high computational cost typically limits their use to reranking\ncandidates retrieved by a cheaper model, such as dual encoder or TF-IDF.\nHowever, the accuracy of such a two-stage approach is upper-bounded by the\nrecall of the initial candidate set, and potentially requires additional\ntraining to align the auxiliary retrieval model with the cross-encoder model.\nIn this paper, we present an approach that avoids the use of a dual-encoder for\nretrieval, relying solely on the cross-encoder. Retrieval is made efficient\nwith CUR decomposition, a matrix decomposition approach that approximates all\npairwise cross-encoder distances from a small subset of rows and columns of the\ndistance matrix. Indexing items using our approach is computationally cheaper\nthan training an auxiliary dual-encoder model through distillation.\nEmpirically, for k > 10, our approach provides test-time\nrecall-vs-computational cost trade-offs superior to the current widely-used\nmethods that re-rank items retrieved using a dual-encoder or TF-IDF.",
    "descriptor": "\nComments: EMNLP 2022. Code for all experiments and model checkpoints are available at this https URL\n",
    "authors": [
      "Nishant Yadav",
      "Nicholas Monath",
      "Rico Angell",
      "Manzil Zaheer",
      "Andrew McCallum"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12579"
  },
  {
    "id": "arXiv:2210.12581",
    "title": "A First Look at CI/CD Adoptions in Open-Source Android Apps",
    "abstract": "Continuous Integration (CI) and Continuous Delivery (CD) have been\ndemonstrated to be effective in facilitating software building, testing, and\ndeployment. Many research studies have investigated and subsequently improved\ntheir working processes. Unfortunately, such research efforts have largely not\ntouched on the usage of CI/CD in the development of Android apps. We fill this\ngap by conducting an exploratory study of CI/CD adoption in open-source Android\napps. We start by collecting a set of 84,475 open-source Android apps from the\nmost popular three online code hosting sites, namely Github, GitLab, and\nBitbucket. We then look into those apps and find that (1) only around 10\\% of\napps have leveraged CI/CD services, i.e., the majority of open-source Android\napps are developed without accessing CI/CD services, (2) a small number of apps\n(291) has even adopted multiple CI/CD services, (3) nearly half of the apps\nadopted CI/CD services have not really used them, and (4) CI/CD services are\nuseful to improve the popularity of projects.",
    "descriptor": "",
    "authors": [
      "Pei Liu",
      "Xiaoyu Sun",
      "Yanjie Zhao",
      "Yonghui Liu",
      "John Grundy",
      "Li Li"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.12581"
  },
  {
    "id": "arXiv:2210.12582",
    "title": "Language Model Pre-Training with Sparse Latent Typing",
    "abstract": "Modern large-scale Pre-trained Language Models (PLMs) have achieved\ntremendous success on a wide range of downstream tasks. However, most of the LM\npre-training objectives only focus on text reconstruction, but have not sought\nto learn latent-level interpretable representations of sentences. In this\npaper, we manage to push the language models to obtain a deeper understanding\nof sentences by proposing a new pre-training objective, Sparse Latent Typing,\nwhich enables the model to sparsely extract sentence-level keywords with\ndiverse latent types. Experimental results show that our model is able to learn\ninterpretable latent type categories in a self-supervised manner without using\nany external knowledge. Besides, the language model pre-trained with such an\nobjective also significantly improves Information Extraction related downstream\ntasks in both supervised and few-shot settings. Our code is publicly available\nat: https://github.com/renll/SparseLT.",
    "descriptor": "",
    "authors": [
      "Liliang Ren",
      "Zixuan Zhang",
      "Han Wang",
      "Clare R. Voss",
      "Chengxiang Zhai",
      "Heng Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12582"
  },
  {
    "id": "arXiv:2210.12583",
    "title": "Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model  Predictive Control",
    "abstract": "Model-based control requires an accurate model of the system dynamics for\nprecisely and safely controlling the robot in complex and dynamic environments.\nMoreover, in presence of variations in the operating conditions, the model\nshould be continuously refined to compensate for dynamics changes. In this\npaper, we propose a self-supervised learning approach to actively model robot\ndiscrete-time dynamics. We combine offline learning from past experience and\nonline learning from present robot interaction with the unknown environment.\nThese two ingredients enable highly sample-efficient and adaptive learning for\naccurate inference of the model dynamics in real-time even in operating regimes\nsignificantly different from the training distribution. Moreover, we design an\nuncertainty-aware model predictive controller that is conditioned to the\naleatoric (data) uncertainty of the learned dynamics. The controller actively\nselects the optimal control actions that (i) optimize the control performance\nand (ii) boost the online learning sample efficiency. We apply the proposed\nmethod to a quadrotor system in multiple challenging real-world experiments.\nOur approach exhibits high flexibility and generalization capabilities by\nconsistently adapting to unseen flight conditions, while it significantly\noutperforms classical and adaptive control baselines.",
    "descriptor": "",
    "authors": [
      "Alessandro Saviolo",
      "Jonathan Frey",
      "Abhishek Rathod",
      "Moritz Diehl",
      "Giuseppe Loianno"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12583"
  },
  {
    "id": "arXiv:2210.12584",
    "title": "MR-Based Electrical Property Reconstruction Using Physics-Informed  Neural Networks",
    "abstract": "Electrical properties (EP), namely permittivity and electric conductivity,\ndictate the interactions between electromagnetic waves and biological tissue.\nEP can be potential biomarkers for pathology characterization, such as cancer,\nand improve therapeutic modalities, such radiofrequency hyperthermia and\nablation. MR-based electrical properties tomography (MR-EPT) uses MR\nmeasurements to reconstruct the EP maps. Using the homogeneous Helmholtz\nequation, EP can be directly computed through calculations of second order\nspatial derivatives of the measured magnetic transmit or receive fields\n$(B_{1}^{+}, B_{1}^{-})$. However, the numerical approximation of derivatives\nleads to noise amplifications in the measurements and thus erroneous\nreconstructions. Recently, a noise-robust supervised learning-based method\n(DL-EPT) was introduced for EP reconstruction. However, the pattern-matching\nnature of such network does not allow it to generalize for new samples since\nthe network's training is done on a limited number of simulated data. In this\nwork, we leverage recent developments on physics-informed deep learning to\nsolve the Helmholtz equation for the EP reconstruction. We develop deep neural\nnetwork (NN) algorithms that are constrained by the Helmholtz equation to\neffectively de-noise the $B_{1}^{+}$ measurements and reconstruct EP directly\nat an arbitrarily high spatial resolution without requiring any known\n$B_{1}^{+}$ and EP distribution pairs.",
    "descriptor": "\nComments: 2 pages, 4 figures, accepted by QMR Lucca workshop on MR Phase, Magnetic Susceptibility and Electrical Properties Mapping\n",
    "authors": [
      "Xinling Yu",
      "Jos\u00e9 E. C. Serrall\u00e9s",
      "Ilias I. Giannakopoulos",
      "Ziyue Liu",
      "Luca Daniel",
      "Riccardo Lattanzi",
      "Zheng Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Biological Physics (physics.bio-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.12584"
  },
  {
    "id": "arXiv:2210.12587",
    "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge  transfer method for few-shot prompt tuning",
    "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a\ndownstream task conditioning on frozen pre-trained models, have attracted\ngrowing interest due to its parameter efficiency. With large language models\nand sufficient training data, prompt tuning performs comparably to full-model\ntuning. However, with limited training samples in few-shot settings, prompt\ntuning fails to match the performance of full-model fine-tuning. In this work,\nwe focus on improving the few-shot performance of prompt tuning by transferring\nknowledge from soft prompts of source tasks. Recognizing the good\ngeneralization capabilities of ensemble methods in low-data regime, we first\nexperiment and show that a simple ensemble of model predictions based on\ndifferent source prompts, outperforms existing multi-prompt knowledge transfer\napproaches such as source prompt fusion in the few-shot setting. Motivated by\nthis observation, we further investigate model ensembles and propose\nSample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the\ncontribution of each source model for each target sample separately when\nensembling source model outputs. Through this way, SESoM inherits the superior\ngeneralization of model ensemble approaches and simultaneously captures the\nsample-specific competence of each source prompt. We conduct experiments across\na diverse set of eight NLP tasks using models of different scales (T5-{base,\nlarge, XL}) and find that SESoM consistently outperforms the existing models of\nthe same as well as larger parametric scale by a large margin.",
    "descriptor": "",
    "authors": [
      "Xiangyu Peng",
      "Chen Xing",
      "Prafulla Kumar Choubey",
      "Chien-Sheng Wu",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12587"
  },
  {
    "id": "arXiv:2210.12590",
    "title": "MetaEMS: A Meta Reinforcement Learning-based Control Framework for  Building Energy Management System",
    "abstract": "The building sector has been recognized as one of the primary sectors for\nworldwide energy consumption. Improving the energy efficiency of the building\nsector can help reduce the operation cost and reduce the greenhouse gas\nemission. The energy management system (EMS) can monitor and control the\noperations of built-in appliances in buildings, so an efficient EMS is of\ncrucial importance to improve the building operation efficiency and maintain\nsafe operations. With the growing penetration of renewable energy and\nelectrical appliances, increasing attention has been paid to the development of\nintelligent building EMS. Recently, reinforcement learning (RL) has been\napplied for building EMS and has shown promising potential. However, most of\nthe current RL-based EMS solutions would need a large amount of data to learn a\nreliable control policy, which limits the applicability of these solutions in\nthe real world. In this work, we propose MetaEMS, which can help achieve better\nenergy management performance with the benefits of RL and meta-learning.\nExperiment results showcase that our proposed MetaEMS can adapt faster to\nenvironment changes and perform better in most situations compared with other\nbaselines.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1909.10165 by other authors\n",
    "authors": [
      "Huiliang Zhang",
      "Di Wu",
      "Benoit Boulet"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12590"
  },
  {
    "id": "arXiv:2210.12591",
    "title": "The Future of Work: Agile in a Hybrid World",
    "abstract": "An agile organization adapts what they are building to match their customer's\nevolving needs. Agile teams also adapt to changes in their organization's work\nenvironment. The latest change is the evolving environment of \"hybrid\" work - a\nmix of in-person and virtual staff. Team members might sometimes work together\nin the office, work from home, or work in other locations, and they may\nstruggle to sustain a high level of collaboration and innovation. It isn't just\npandemic social distancing - many of us want to work from home to eliminate our\ncommute and spend more time with family. Are there learnings and best practices\nthat organizations can use to become and stay effective in a hybrid world? An\nXP 2022 panel organized by Steven Fraser (Innoxec) discussed these questions in\nJune 2022. The panel was facilitated by Hendrik Esser (Ericsson) and featured\nAlistair Cockburn (Heart of Agile), Sandy Mamoli (Nomad8), Nils Brede Moe\n(SINTEF), Jaana Nyfjord (Spotify), and Darja Smite (Blekinge Institute of\nTechnology).",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "Dennis Mancl",
      "Steven D. Fraser"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.12591"
  },
  {
    "id": "arXiv:2210.12593",
    "title": "Single Image Super-Resolution via a Dual Interactive Implicit Neural  Network",
    "abstract": "In this paper, we introduce a novel implicit neural network for the task of\nsingle image super-resolution at arbitrary scale factors. To do this, we\nrepresent an image as a decoding function that maps locations in the image\nalong with their associated features to their reciprocal pixel attributes.\nSince the pixel locations are continuous in this representation, our method can\nrefer to any location in an image of varying resolution. To retrieve an image\nof a particular resolution, we apply a decoding function to a grid of locations\neach of which refers to the center of a pixel in the output image. In contrast\nto other techniques, our dual interactive neural network decouples content and\npositional features. As a result, we obtain a fully implicit representation of\nthe image that solves the super-resolution problem at (real-valued) elective\nscales using a single model. We demonstrate the efficacy and flexibility of our\napproach against the state of the art on publicly available benchmark datasets.",
    "descriptor": "\nComments: To be published in the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n",
    "authors": [
      "Quan H. Nguyen",
      "William J. Beksi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12593"
  },
  {
    "id": "arXiv:2210.12595",
    "title": "Online Probabilistic Model Identification using Adaptive Recursive MCMC",
    "abstract": "The Bayesian paradigm provides a rigorous framework for estimating the whole\nprobability distribution over unknown parameters, but due to high computational\ncosts, its online application can be difficult. We propose the Adaptive\nRecursive Markov Chain Monte Carlo (ARMCMC) method, which calculates the\ncomplete probability density function of model parameters while alleviating the\ndrawbacks of traditional online methods. These flaws include being limited to\nGaussian noise, being solely applicable to linear in the parameters (LIP)\nsystems, and having persisting excitation requirements (PE). A variable jump\ndistribution based on a temporal forgetting factor (TFF) is proposed in ARMCMC.\nThe TFF can be utilized in many dynamical systems as an effective way to\nadaptively present the forgetting factor instead of a constant hyperparameter.\nThe particular jump distribution has tailored towards hybrid/multi-modal\nsystems that enables inferences among modes by providing a trade-off between\nexploitation and exploration. These trade-off are adjusted based on parameter\nevolution rate. In comparison to traditional MCMC techniques, we show that\nARMCMC requires fewer samples to obtain the same accuracy and reliability. We\nshow our method on two challenging benchmarks: parameter estimation in a soft\nbending actuator and the Hunt-Crossley dynamic model. We also compare our\nmethod with recursive least squares and the particle filter, and show that our\ntechnique has significantly more accurate point estimates as well as a decrease\nin tracking error of the value of interest.",
    "descriptor": "\nComments: 9 pages, 7 figures, 3 tables\n",
    "authors": [
      "Pedram Agand",
      "Mo Chen",
      "Hamid D. Taghirad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.12595"
  },
  {
    "id": "arXiv:2210.12596",
    "title": "DMODE: Differential Monocular Object Distance Estimation Module without  Class Specific Information",
    "abstract": "Using a single camera to estimate the distances of objects reduces costs\ncompared to stereo-vision and LiDAR. Although monocular distance estimation has\nbeen studied in the literature, previous methods mostly rely on knowing an\nobject's class in some way. This can result in deteriorated performance for\ndataset with multi-class objects and objects with an undefined class. In this\npaper, we aim to overcome the potential downsides of class-specific approaches,\nand provide an alternative technique called DMODE that does not require any\ninformation relating to its class. Using differential approaches, we combine\nthe changes in an object's size over time together with the camera's motion to\nestimate the object's distance. Since DMODE is class agnostic method, it is\neasily adaptable to new environments. Therefore, it is able to maintain\nperformance across different object detectors, and be easily adapted to new\nobject classes. We tested our model across different scenarios of training and\ntesting on the KITTI MOTS dataset's ground-truth bounding box annotations, and\nbounding box outputs of TrackRCNN and EagerMOT. The instantaneous change of\nbounding box sizes and camera position are then used to obtain an object's\nposition in 3D without measuring its detection source or class properties. Our\nresults show that we are able to outperform traditional alternatives methods\ne.g. IPM \\cite{TuohyIPM}, SVR \\cite{svr}, and \\cite{zhu2019learning} in test\nenvironments with multi-class object distance detections.",
    "descriptor": "\nComments: 9 pages, 3 figures, 3 tables\n",
    "authors": [
      "Pedram Agand",
      "Michael Chang",
      "Mo Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12596"
  },
  {
    "id": "arXiv:2210.12598",
    "title": "GANI: Global Attacks on Graph Neural Networks via Imperceptible Node  Injections",
    "abstract": "Graph neural networks (GNNs) have found successful applications in various\ngraph-related tasks. However, recent studies have shown that many GNNs are\nvulnerable to adversarial attacks. In a vast majority of existing studies,\nadversarial attacks on GNNs are launched via direct modification of the\noriginal graph such as adding/removing links, which may not be applicable in\npractice. In this paper, we focus on a realistic attack operation via injecting\nfake nodes. The proposed Global Attack strategy via Node Injection (GANI) is\ndesigned under the comprehensive consideration of an unnoticeable perturbation\nsetting from both structure and feature domains. Specifically, to make the node\ninjections as imperceptible and effective as possible, we propose a sampling\noperation to determine the degree of the newly injected nodes, and then\ngenerate features and select neighbors for these injected nodes based on the\nstatistical information of features and evolutionary perturbations obtained\nfrom a genetic algorithm, respectively. In particular, the proposed feature\ngeneration mechanism is suitable for both binary and continuous node features.\nExtensive experimental results on benchmark datasets against both general and\ndefended GNNs show strong attack performance of GANI. Moreover, the\nimperceptibility analyses also demonstrate that GANI achieves a relatively\nunnoticeable injection on benchmark datasets.",
    "descriptor": "",
    "authors": [
      "Junyuan Fang",
      "Haixian Wen",
      "Jiajing Wu",
      "Qi Xuan",
      "Zibin Zheng",
      "Chi K. Tse"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12598"
  },
  {
    "id": "arXiv:2210.12599",
    "title": "Transformers For Recognition In Overhead Imagery: A Reality Check",
    "abstract": "There is evidence that transformers offer state-of-the-art recognition\nperformance on tasks involving overhead imagery (e.g., satellite imagery).\nHowever, it is difficult to make unbiased empirical comparisons between\ncompeting deep learning models, making it unclear whether, and to what extent,\ntransformer-based models are beneficial. In this paper we systematically\ncompare the impact of adding transformer structures into state-of-the-art\nsegmentation models for overhead imagery. Each model is given a similar budget\nof free parameters, and their hyperparameters are optimized using Bayesian\nOptimization with a fixed quantity of data and computation time. We conduct our\nexperiments with a large and diverse dataset comprising two large public\nbenchmarks: Inria and DeepGlobe. We perform additional ablation studies to\nexplore the impact of specific transformer-based modeling choices. Our results\nsuggest that transformers provide consistent, but modest, performance\nimprovements. We only observe this advantage however in hybrid models that\ncombine convolutional and transformer-based structures, while fully\ntransformer-based models achieve relatively poor performance.",
    "descriptor": "\nComments: \\c{opyright} 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works\n",
    "authors": [
      "Francesco Luzi",
      "Aneesh Gupta",
      "Leslie Collins",
      "Kyle Bradbury",
      "Jordan Malof"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12599"
  },
  {
    "id": "arXiv:2210.12601",
    "title": "Sublinear-Time Algorithms for Max Cut, Max E2Lin$(q)$, and Unique Label  Cover on Expanders",
    "abstract": "We show sublinear-time algorithms for Max Cut and Max E2Lin$(q)$ on expanders\nin the adjacency list model that distinguishes instances with the optimal value\nmore than $1-\\varepsilon$ from those with the optimal value less than $1-\\rho$\nfor $\\rho \\gg \\varepsilon$. The time complexities for Max Cut and Max\n$2$Lin$(q)$ are $\\widetilde{O}(\\frac{1}{\\phi^2\\rho} \\cdot\nm^{1/2+O(\\varepsilon/(\\phi^2\\rho))})$ and\n$\\widetilde{O}(\\mathrm{poly}(\\frac{q}{\\phi\\rho})\\cdot\n{(mq)}^{1/2+O(q^6\\varepsilon/\\phi^2\\rho^2)})$, respectively, where $m$ is the\nnumber of edges in the underlying graph and $\\phi$ is its conductance. Then, we\nshow a sublinear-time algorithm for Unique Label Cover on expanders with $\\phi\n\\gg \\epsilon$ in the bounded-degree model. The time complexity of our algorithm\nis $\\widetilde{O}_d(2^{q^{O(1)}\\cdot\\phi^{1/q}\\cdot \\varepsilon^{-1/2}}\\cdot\nn^{1/2+q^{O(q)}\\cdot \\varepsilon^{4^{1.5-q}}\\cdot \\phi^{-2}})$, where $n$ is\nthe number of variables. We complement these algorithmic results by showing\nthat testing $3$-colorability requires $\\Omega(n)$ queries even on expanders.",
    "descriptor": "\nComments: To appear in SODA'23\n",
    "authors": [
      "Pan Peng",
      "Yuichi Yoshida"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.12601"
  },
  {
    "id": "arXiv:2210.12605",
    "title": "Keep CALM and CRDT On",
    "abstract": "Despite decades of research and practical experience, developers have few\ntools for programming reliable distributed applications without resorting to\nexpensive coordination techniques. Conflict-free replicated datatypes (CRDTs)\nare a promising line of work that enable coordination-free replication and\noffer certain eventual consistency guarantees in a relatively simple\nobject-oriented API. Yet CRDT guarantees extend only to data updates;\nobservations of CRDT state are unconstrained and unsafe. We propose an agenda\nthat embraces the simplicity of CRDTs, but provides richer, more uniform\nguarantees. We extend CRDTs with a query model that reasons about which queries\nare safe without coordination by applying monotonicity results from the CALM\nTheorem, and lay out a larger agenda for developing CRDT data stores that let\ndevelopers safely and efficiently interact with replicated application state.",
    "descriptor": "",
    "authors": [
      "Shadaj Laddad",
      "Conor Power",
      "Mae Milano",
      "Alvin Cheung",
      "Natacha Crooks",
      "Joseph M. Hellerstein"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.12605"
  },
  {
    "id": "arXiv:2210.12606",
    "title": "Nash Equilibria and Pitfalls of Adversarial Training in Adversarial  Robustness Games",
    "abstract": "Adversarial training is a standard technique for training adversarially\nrobust models. In this paper, we study adversarial training as an alternating\nbest-response strategy in a 2-player zero-sum game. We prove that even in a\nsimple scenario of a linear classifier and a statistical model that abstracts\nrobust vs. non-robust features, the alternating best response strategy of such\ngame may not converge. On the other hand, a unique pure Nash equilibrium of the\ngame exists and is provably robust. We support our theoretical results with\nexperiments, showing the non-convergence of adversarial training and the\nrobustness of Nash equilibrium.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Maria-Florina Balcan",
      "Rattana Pukdee",
      "Pradeep Ravikumar",
      "Hongyang Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.12606"
  },
  {
    "id": "arXiv:2210.12607",
    "title": "Learning to Perform Complex Tasks through Compositional Fine-Tuning of  Language Models",
    "abstract": "How to usefully encode compositional task structure has long been a core\nchallenge in AI. Recent work in chain of thought prompting has shown that for\nvery large neural language models (LMs), explicitly demonstrating the\ninferential steps involved in a target task may improve performance over\nend-to-end learning that focuses on the target task alone. However, chain of\nthought prompting has significant limitations due to its dependency on huge\npretrained LMs. In this work, we present compositional fine-tuning (CFT): an\napproach based on explicitly decomposing a target task into component tasks,\nand then fine-tuning smaller LMs on a curriculum of such component tasks. We\napply CFT to recommendation tasks in two domains, world travel and local\ndining, as well as a previously studied inferential task (sports\nunderstanding). We show that CFT outperforms end-to-end learning even with\nequal amounts of data, and gets consistently better as more component tasks are\nmodeled via fine-tuning. Compared with chain of thought prompting, CFT performs\nat least as well using LMs only 7.4% of the size, and is moreover applicable to\ntask domains for which data are not available during pretraining.",
    "descriptor": "\nComments: Accepted to findings of EMNLP 2022. Data and code available at this https URL\n",
    "authors": [
      "Victor S. Bursztyn",
      "David Demeter",
      "Doug Downey",
      "Larry Birnbaum"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12607"
  },
  {
    "id": "arXiv:2210.12609",
    "title": "Blockchain and Machine Learning for Fraud Detection: A  Privacy-Preserving and Adaptive Incentive Based Approach",
    "abstract": "Financial fraud cases are on the rise even with the current technological\nadvancements. Due to the lack of inter-organization synergy and because of\nprivacy concerns, authentic financial transaction data is rarely available. On\nthe other hand, data-driven technologies like machine learning need authentic\ndata to perform precisely in real-world systems. This study proposes a\nblockchain and smart contract-based approach to achieve robust Machine Learning\n(ML) algorithm for e-commerce fraud detection by facilitating\ninter-organizational collaboration. The proposed method uses blockchain to\nsecure the privacy of the data. Smart contract deployed inside the network\nfully automates the system. An ML model is incrementally upgraded from\ncollaborative data provided by the organizations connected to the blockchain.\nTo incentivize the organizations, we have introduced an incentive mechanism\nthat is adaptive to the difficulty level in updating a model. The organizations\nreceive incentives based on the difficulty faced in updating the ML model. A\nmining criterion has been proposed to mine the block efficiently. And finally,\nthe blockchain network istested under different difficulty levels and under\ndifferent volumes of data to test its efficiency. The model achieved 98.93%\ntesting accuracy and 98.22% Fbeta score (recall-biased f measure) over eight\nincremental updates. Our experiment shows that both data volume and difficulty\nlevel of blockchain impacts the mining time. For difficulty level less than\nfive, mining time and difficulty level has a positive correlation. For\ndifficulty level two and three, less than a second is required to mine a block\nin our system. Difficulty level five poses much more difficulties to mine the\nblocks.",
    "descriptor": "",
    "authors": [
      "Tahmid Hasan Pranto",
      "Kazi Tamzid Akhter Md Hasib",
      "Tahsinur Rahman",
      "AKM Bahalul Haque",
      "A.K.M. Najmul Islam",
      "Rashedur M. Rahman"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12609"
  },
  {
    "id": "arXiv:2210.12610",
    "title": "Partially Trusting the Service Mesh Control Plane",
    "abstract": "Zero Trust is a novel cybersecurity model that focuses on continually\nevaluating trust to prevent the initiation and horizontal spreading of attacks.\nA cloud-native Service Mesh is an example of Zero Trust Architecture that can\nfilter out external threats. However, the Service Mesh does not shield the\nApplication Owner from internal threats, such as a rogue administrator of the\ncluster where their application is deployed. In this work, we are enhancing the\nService Mesh to allow the definition and reinforcement of a Verifiable\nConfiguration that is defined and signed off by the Application Owner. Backed\nby automated digital signing solutions and confidential computing technologies,\nthe Verifiable Configuration allows changing the trust model of the Service\nMesh, from the data plane fully trusting the control plane to partially\ntrusting it. This lets the application benefit from all the functions provided\nby the Service Mesh (resource discovery, traffic management, mutual\nauthentication, access control, observability), while ensuring that the Cluster\nAdministrator cannot change the state of the application in a way that was not\nintended by the Application Owner.",
    "descriptor": "",
    "authors": [
      "Constantin Adam",
      "Abdulhamid Adebayo",
      "Hubertus Franke",
      "Edward Snible",
      "Tobin Feldman-Fitzthum",
      "James Cadden",
      "Nerla Jean-Louis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12610"
  },
  {
    "id": "arXiv:2210.12612",
    "title": "Pufferfish Privacy: An Information-Theoretic Study",
    "abstract": "Pufferfish privacy (PP) is a generalization of differential privacy (DP),\nthat offers flexibility in specifying sensitive information and integrates\ndomain knowledge into the privacy definition. Inspired by the illuminating\nequivalent formulation of DP in terms of mutual information due to Cuff and Yu,\nthis work explores PP through the lens of information theory. We provide an\ninformation-theoretic formulation of PP, termed mutual information PP (MI-PP),\nin terms of the conditional mutual information between the mechanism and the\nsecret, given the public information. We show that MI-PP is implied by the\nregular PP and characterize conditions under which the reverse implication is\nalso true, recovering the DP information-theoretic equivalence result as a\nspecial case. We establish convexity, composability, and post-processing\nproperties for MI-PP mechanisms and derive noise levels for the Gaussian and\nLaplace mechanisms. The obtained mechanisms are applicable under relaxed\nassumptions and provide improved noise levels in some regimes, compared to\nclassic, sensitivity-based approaches. Lastly, applications of MI-PP to\nauditing privacy frameworks, statistical inference tasks, and algorithm\nstability are explored.",
    "descriptor": "",
    "authors": [
      "Theshani Nuradha",
      "Ziv Goldfeld"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12612"
  },
  {
    "id": "arXiv:2210.12613",
    "title": "Towards Energy-Efficient, Low-Latency and Accurate Spiking LSTMs",
    "abstract": "Spiking Neural Networks (SNNs) have emerged as an attractive spatio-temporal\ncomputing paradigm for complex vision tasks. However, most existing works yield\nmodels that require many time steps and do not leverage the inherent temporal\ndynamics of spiking neural networks, even for sequential tasks. Motivated by\nthis observation, we propose an \\rev{optimized spiking long short-term memory\nnetworks (LSTM) training framework that involves a novel ANN-to-SNN conversion\nframework, followed by SNN training}. In particular, we propose novel\nactivation functions in the source LSTM architecture and judiciously select a\nsubset of them for conversion to integrate-and-fire (IF) activations with\noptimal bias shifts. Additionally, we derive the leaky-integrate-and-fire (LIF)\nactivation functions converted from their non-spiking LSTM counterparts which\njustifies the need to jointly optimize the weights, threshold, and leak\nparameter. We also propose a pipelined parallel processing scheme which hides\nthe SNN time steps, significantly improving system latency, especially for long\nsequences. The resulting SNNs have high activation sparsity and require only\naccumulate operations (AC), in contrast to expensive multiply-and-accumulates\n(MAC) needed for ANNs, except for the input layer when using direct encoding,\nyielding significant improvements in energy efficiency. We evaluate our\nframework on sequential learning tasks including temporal MNIST, Google Speech\nCommands (GSC), and UCI Smartphone datasets on different LSTM architectures. We\nobtain test accuracy of 94.75% with only 2 time steps with direct encoding on\nthe GSC dataset with 4.1x lower energy than an iso-architecture standard LSTM.",
    "descriptor": "",
    "authors": [
      "Gourav Datta",
      "Haoqin Deng",
      "Robert Aviles",
      "Peter A. Beerel"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.12613"
  },
  {
    "id": "arXiv:2210.12614",
    "title": "A Solution to Slosh-free Robot Trajectory Optimization",
    "abstract": "This paper is about fast slosh free fluid transportation. Existing approaches\nare either computationally heavy or only suitable for specific robots and\ncontainer shapes. We model the end effector as a point mass suspended by a\nspherical pendulum and study the requirements for slosh free motion and the\nvalidity of the point mass model. In this approach, slosh free trajectories are\ngenerated by controlling the pendulum's pivot and simulating the motion of the\npoint mass. We cast the trajectory optimization problem as a quadratic program;\nthis strategy can be used to obtain valid control inputs. Through simulations\nand experiments on a 7 DoF Franka Emika Panda robot we validate the\neffectiveness of the proposed approach.",
    "descriptor": "\nComments: IEEE/RSJ IROS 2022\n",
    "authors": [
      "Rafael I. Cabral Muchacho",
      "Riddhiman Laha",
      "Luis F.C. Figueredo",
      "Sami Haddadin"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12614"
  },
  {
    "id": "arXiv:2210.12617",
    "title": "Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval",
    "abstract": "Video corpus moment retrieval (VCMR) is the task to retrieve the most\nrelevant video moment from a large video corpus using a natural language query.\nFor narrative videos, e.g., dramas or movies, the holistic understanding of\ntemporal dynamics and multimodal reasoning is crucial. Previous works have\nshown promising results; however, they relied on the expensive query\nannotations for VCMR, i.e., the corresponding moment intervals. To overcome\nthis problem, we propose a self-supervised learning framework: Modal-specific\nPseudo Query Generation Network (MPGN). First, MPGN selects candidate temporal\nmoments via subtitle-based moment sampling. Then, it generates pseudo queries\nexploiting both visual and textual information from the selected temporal\nmoments. Through the multimodal information in the pseudo queries, we show that\nMPGN successfully learns to localize the video corpus moment without any\nexplicit annotation. We validate the effectiveness of MPGN on the TVR dataset,\nshowing competitive results compared with both supervised models and\nunsupervised setting models.",
    "descriptor": "\nComments: Accepted by EMNLP 2022 main conference\n",
    "authors": [
      "Minjoon Jung",
      "Seongho Choi",
      "Joochan Kim",
      "Jin-Hwa Kim",
      "Byoung-Tak Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12617"
  },
  {
    "id": "arXiv:2210.12619",
    "title": "Conformal Predictor for Improving Zero-shot Text Classification  Efficiency",
    "abstract": "Pre-trained language models (PLMs) have been shown effective for zero-shot\n(0shot) text classification. 0shot models based on natural language inference\n(NLI) and next sentence prediction (NSP) employ cross-encoder architecture and\ninfer by making a forward pass through the model for each label-text pair\nseparately. This increases the computational cost to make inferences linearly\nin the number of labels. In this work, we improve the efficiency of such\ncross-encoder-based 0shot models by restricting the number of likely labels\nusing another fast base classifier-based conformal predictor (CP) calibrated on\nsamples labeled by the 0shot model. Since a CP generates prediction sets with\ncoverage guarantees, it reduces the number of target labels without excluding\nthe most probable label based on the 0shot model. We experiment with three\nintent and two topic classification datasets. With a suitable CP for each\ndataset, we reduce the average inference time for NLI- and NSP-based models by\n25.6% and 22.2% respectively, without dropping performance below the predefined\nerror rate of 1%.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Prafulla Kumar Choubey",
      "Yu Bai",
      "Chien-Sheng Wu",
      "Wenhao Liu",
      "Nazneen Rajani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12619"
  },
  {
    "id": "arXiv:2210.12621",
    "title": "Development of a Hybrid Simulation and Experiment Test Platform for  Dynamic Positioning Vessels",
    "abstract": "The harsh ocean environment and complex operating condition require high\ndynamic positioning (DP) capability of offshore vessel. The design, development\nand performance evaluation of DP system are generally carried out by numerical\nsimulations or scale model experiments. Compared with the time-consuming and\nlaborious experiment, the simulation is convenient and low cost, but its\nresults lack practical reference due to oversimplification of the model.\nTherefore, this paper presents a hybrid simulation and experiment test platform\nfor DP vessels. Its characteristics are: the realistic calculation of\nenvironmental loads and motion response, the consistency of algorithms and\nparameters for simulation and experiment greatly shortening the time of\nexperiment adjusting, switchable and online renewable controller facilitating\nalgorithm testing. The test platform can test the performance of DP system and\ndetermine the operational time window. In the hydrodynamic simulation, the six\ndegree-of-freedom model is used to describe the dynamic response of the DP\nvessel, considering the fluid memory effect and frequency-dependent\nhydrodynamic parameters. In the experiment, the similarity theory based on the\nsame Froude number is used to ensure the consistency of control parameters with\nsimulation. Finally, a case study of DP shuttle tanker is used to verify the\ncredibility of the test platform.",
    "descriptor": "",
    "authors": [
      "Changjun Hu",
      "Quan Shi",
      "Xin Li",
      "Xiaoxian Guo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12621"
  },
  {
    "id": "arXiv:2210.12622",
    "title": "Facial De-occlusion Network for Virtual Telepresence Systems",
    "abstract": "To see what is not in the image is one of the broader missions of computer\nvision. Technology to inpaint images has made significant progress with the\ncoming of deep learning. This paper proposes a method to tackle occlusion\nspecific to human faces. Virtual presence is a promising direction in\ncommunication and recreation for the future. However, Virtual Reality (VR)\nheadsets occlude a significant portion of the face, hindering the\nphoto-realistic appearance of the face in the virtual world. State-of-the-art\nimage inpainting methods for de-occluding the eye region does not give usable\nresults. To this end, we propose a working solution that gives usable results\nto tackle this problem enabling the use of the real-time photo-realistic\nde-occluded face of the user in VR settings.",
    "descriptor": "\nComments: This workshop paper is presented in CVPR Workshop on Computer Vision for Augmented and Virtual Reality, New Orleans, LA, 2022. Link: this https URL\n",
    "authors": [
      "Surabhi Gupta",
      "Ashwath Shetty",
      "Avinash Sharma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12622"
  },
  {
    "id": "arXiv:2210.12623",
    "title": "Model and Data Transfer for Cross-Lingual Sequence Labelling in  Zero-Resource Settings",
    "abstract": "Zero-resource cross-lingual transfer approaches aim to apply supervised\nmodels from a source language to unlabelled target languages. In this paper we\nperform an in-depth study of the two main techniques employed so far for\ncross-lingual zero-resource sequence labelling, based either on data or model\ntransfer. Although previous research has proposed translation and annotation\nprojection (data-based cross-lingual transfer) as an effective technique for\ncross-lingual sequence labelling, in this paper we experimentally demonstrate\nthat high capacity multilingual language models applied in a zero-shot\n(model-based cross-lingual transfer) setting consistently outperform data-based\ncross-lingual transfer approaches. A detailed analysis of our results suggests\nthat this might be due to important differences in language use. More\nspecifically, machine translation often generates a textual signal which is\ndifferent to what the models are exposed to when using gold standard data,\nwhich affects both the fine-tuning and evaluation processes. Our results also\nindicate that data-based cross-lingual transfer approaches remain a competitive\noption when high-capacity multilingual language models are not available.",
    "descriptor": "\nComments: Findings of the EMNLP 2022\n",
    "authors": [
      "Iker Garc\u00eda-Ferrero",
      "Rodrigo Agerri",
      "German Rigau"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12623"
  },
  {
    "id": "arXiv:2210.12624",
    "title": "Mitigating Gradient Bias in Multi-objective Learning: A Provably  Convergent Stochastic Approach",
    "abstract": "Machine learning problems with multiple objective functions appear either in\nlearning with multiple criteria where learning has to make a trade-off between\nmultiple performance metrics such as fairness, safety and accuracy; or, in\nmulti-task learning where multiple tasks are optimized jointly, sharing\ninductive bias between them. This problems are often tackled by the\nmulti-objective optimization framework. However, existing stochastic\nmulti-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad,\netc.) all adopt a biased noisy gradient direction, which leads to degraded\nempirical performance. To this end, we develop a stochastic Multi-objective\ngradient Correction (MoCo) method for multi-objective optimization. The unique\nfeature of our method is that it can guarantee convergence without increasing\nthe batch size even in the non-convex setting. Simulations on multi-task\nsupervised and reinforcement learning demonstrate the effectiveness of our\nmethod relative to state-of-the-art methods.",
    "descriptor": "",
    "authors": [
      "Heshan Fernando",
      "Han Shen",
      "Miao Liu",
      "Subhajit Chaudhury",
      "Keerthiram Murugesan",
      "Tianyi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12624"
  },
  {
    "id": "arXiv:2210.12625",
    "title": "Fast Beam Alignment via Pure Exploration in Multi-armed Bandits",
    "abstract": "The beam alignment (BA) problem consists in accurately aligning the\ntransmitter and receiver beams to establish a reliable communication link in\nwireless communication systems. Existing BA methods search the entire beam\nspace to identify the optimal transmit-receive beam pair. This incurs a\nsignificant latency when the number of antennas is large. In this work, we\ndevelop a bandit-based fast BA algorithm to reduce BA latency for\nmillimeter-wave (mmWave) communications. Our algorithm is named Two-Phase\nHeteroscedastic Track-and-Stop (2PHT\\&S). We first formulate the BA problem as\na pure exploration problem in multi-armed bandits in which the objective is to\nminimize the required number of time steps given a certain fixed confidence\nlevel. By taking advantage of the correlation structure among beams that the\ninformation from nearby beams is similar and the heteroscedastic property that\nthe variance of the reward of an arm (beam) is related to its mean, the\nproposed algorithm groups all beams into several beam sets such that the\noptimal beam set is first selected and the optimal beam is identified in this\nset after that. Theoretical analysis and simulation results on synthetic and\nsemi-practical channel data demonstrate the clear superiority of the proposed\nalgorithm vis-\\`a-vis other baseline competitors.",
    "descriptor": "\nComments: 16 pages, 9 figures; Accepted to the IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Yi Wei",
      "Zixin Zhong",
      "Vincent Y. F. Tan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12625"
  },
  {
    "id": "arXiv:2210.12628",
    "title": "Spending Thinking Time Wisely: Accelerating MCTS with Virtual Expansions",
    "abstract": "One of the most important AI research questions is to trade off computation\nversus performance since ``perfect rationality\" exists in theory but is\nimpossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has\nattracted considerable attention due to the significant performance improvement\nin various challenging domains. However, the expensive time cost during search\nseverely restricts its scope for applications. This paper proposes the Virtual\nMCTS (V-MCTS), a variant of MCTS that spends more search time on harder states\nand less search time on simpler states adaptively. We give theoretical bounds\nof the proposed method and evaluate the performance and computations on $9\n\\times 9$ Go board games and Atari games. Experiments show that our method can\nachieve comparable performances to the original search algorithm while\nrequiring less than $50\\%$ search time on average. We believe that this\napproach is a viable alternative for tasks under limited time and resources.\nThe code is available at \\url{https://github.com/YeWR/V-MCTS.git}.",
    "descriptor": "",
    "authors": [
      "Weirui Ye",
      "Pieter Abbeel",
      "Yang Gao"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12628"
  },
  {
    "id": "arXiv:2210.12631",
    "title": "Guided Skill Learning and Abstraction for Long-Horizon Manipulation",
    "abstract": "To assist with everyday human activities, robots must solve complex\nlong-horizon tasks and generalize to new settings. Recent deep reinforcement\nlearning (RL) methods show promises in fully autonomous learning, but they\nstruggle to reach long-term goals in large environments. On the other hand,\nTask and Motion Planning (TAMP) approaches excel at solving and generalizing\nacross long-horizon tasks, thanks to their powerful state and action\nabstractions. But they assume predefined skill sets, which limits their\nreal-world applications. In this work, we combine the benefits of these two\nparadigms and propose an integrated task planning and skill learning framework\nnamed LEAGUE (Learning and Abstraction with Guidance). LEAGUE leverages\nsymbolic interface of a task planner to guide RL-based skill learning and\ncreates abstract state space to enable skill reuse. More importantly, LEAGUE\nlearns manipulation skills in-situ of the task planning system, continuously\ngrowing its capability and the set of tasks that it can solve. We demonstrate\nLEAGUE on three challenging simulated task domains and show that LEAGUE\noutperforms baselines by a large margin, and that the learned skills can be\nreused to accelerate learning in new tasks and domains. Additional resource is\navailable at https://bit.ly/3eUOx4N.",
    "descriptor": "",
    "authors": [
      "Shuo Cheng",
      "Danfei Xu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12631"
  },
  {
    "id": "arXiv:2210.12633",
    "title": "Integrated Access and Backhaul in Cell-free Massive MIMO Systems",
    "abstract": "One of the major challenges with cell-free (CF) massive multiple-input\nmultiple-output (MIMO) networks is providing backhaul links for a large number\nof distributed access points (APs). In general, providing fiber optics backhaul\nfor these APs is not cost-effective and also reduces network scalability.\nWireless backhauling can be a promising solution that can be integrated with\nwireless access links to increase spectrum efficiency. In this paper, the\napplication of integrated access and backhaul (IAB) technique in\nmillimeter-wave (mmWave) CF massive MIMO systems is investigated. The access\nand backhaul links share a frequency spectrum in the mmWave bands, and in both,\nhybrid beamforming techniques are adopted for signal transmission. The\nbandwidth allocation (division) parameter between the two link types as well as\nthe beamforming matrices are optimized to maximize the end-to-end data-rate.\nThis leads to a non-convex optimization problem for which an efficient solution\nmethod is proposed. The simulation results show the effectiveness of the IAB\ntechnique and our proposed scheme in CF massive MIMO systems. These simulations\nalso compare the proposed hybrid beamforming method with a fully digital\nsolution in terms of the number of radio frequency (RF) chains and the volume\nof backhaul traffic. Finally, the effect of increasing the number of APs on the\nusers data rates in terms of wireless access and backhaul links constraints is\nalso examined.",
    "descriptor": "",
    "authors": [
      "Ali Hosseinalipour Jazi",
      "S. Mohammad Razavizadeh",
      "Tommy Svensson"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.12633"
  },
  {
    "id": "arXiv:2210.12634",
    "title": "RSVG: Exploring Data and Models for Visual Grounding on Remote Sensing  Data",
    "abstract": "In this paper, we introduce the task of visual grounding for remote sensing\ndata (RSVG). RSVG aims to localize the referred objects in remote sensing (RS)\nimages with the guidance of natural language. To retrieve rich information from\nRS imagery using natural language, many research tasks, like RS image visual\nquestion answering, RS image captioning, and RS image-text retrieval have been\ninvestigated a lot. However, the object-level visual grounding on RS images is\nstill under-explored. Thus, in this work, we propose to construct the dataset\nand explore deep learning models for the RSVG task. Specifically, our\ncontributions can be summarized as follows. 1) We build the new large-scale\nbenchmark dataset of RSVG, termed RSVGD, to fully advance the research of RSVG.\nThis new dataset includes image/expression/box triplets for training and\nevaluating visual grounding models. 2) We benchmark extensive state-of-the-art\n(SOTA) natural image visual grounding methods on the constructed RSVGD dataset,\nand some insightful analyses are provided based on the results. 3) A novel\ntransformer-based Multi-Level Cross-Modal feature learning (MLCM) module is\nproposed. Remotely-sensed images are usually with large scale variations and\ncluttered backgrounds. To deal with the scale-variation problem, the MLCM\nmodule takes advantage of multi-scale visual features and multi-granularity\ntextual embeddings to learn more discriminative representations. To cope with\nthe cluttered background problem, MLCM adaptively filters irrelevant noise and\nenhances salient features. In this way, our proposed model can incorporate more\neffective multi-level and multi-modal features to boost performance.\nFurthermore, this work also provides useful insights for developing better RSVG\nmodels. The dataset and code will be publicly available at\nhttps://github.com/ZhanYang-nwpu/RSVG-pytorch.",
    "descriptor": "\nComments: 12 pages, 10 figures\n",
    "authors": [
      "Yang Zhan",
      "Zhitong Xiong",
      "Yuan Yuan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12634"
  },
  {
    "id": "arXiv:2210.12635",
    "title": "Quantitative Evidence on Overlooked Aspects of Enrollment Speaker  Embeddings for Target Speaker Separation",
    "abstract": "Single channel target speaker separation (TSS) aims at extracting a speaker's\nvoice from a mixture of multiple talkers given an enrollment utterance of that\nspeaker. A typical deep learning TSS framework consists of an upstream model\nthat obtains enrollment speaker embeddings and a downstream model that performs\nthe separation conditioned on the embeddings. In this paper, we look into\nseveral important but overlooked aspects of the enrollment embeddings,\nincluding the suitability of the widely used speaker identification embeddings,\nthe introduction of the log-mel filterbank and self-supervised embeddings, and\nthe embeddings' cross-dataset generalization capability. Our results show that\nthe speaker identification embeddings could lose relevant information due to a\nsub-optimal metric, training objective, or common pre-processing. In contrast,\nboth the filterbank and the self-supervised embeddings preserve the integrity\nof the speaker information, but the former consistently outperforms the latter\nin a cross-dataset evaluation. The competitive separation and generalization\nperformance of the previously overlooked filterbank embedding is consistent\nacross our study, which calls for future research on better upstream features.",
    "descriptor": "",
    "authors": [
      "Xiaoyu Liu",
      "Xu Li",
      "Joan Serr\u00e0"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12635"
  },
  {
    "id": "arXiv:2210.12637",
    "title": "Neural Eigenfunctions Are Structured Representation Learners",
    "abstract": "In this paper, we introduce a scalable method for learning structured,\nadaptive-length deep representations. Our approach is to train neural networks\nsuch that they approximate the principal eigenfunctions of a kernel. We show\nthat, when the kernel is derived from positive relations in a contrastive\nlearning setup, our method outperforms a number of competitive baselines in\nvisual representation learning and transfer learning benchmarks, and\nimportantly, produces structured representations where the order of features\nindicates degrees of importance. We demonstrate using such representations as\nadaptive-length codes in image retrieval systems. By truncation according to\nfeature importance, our method requires up to 16$\\times$ shorter representation\nlength than leading self-supervised learning methods to achieve similar\nretrieval performance. We further apply our method to graph data and report\nstrong results on a node representation learning benchmark with more than one\nmillion nodes.",
    "descriptor": "",
    "authors": [
      "Zhijie Deng",
      "Jiaxin Shi",
      "Hao Zhang",
      "Peng Cui",
      "Cewu Lu",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12637"
  },
  {
    "id": "arXiv:2210.12638",
    "title": "Tucker-O-Minus Decomposition for Multi-view Tensor Subspace Clustering",
    "abstract": "With powerful ability to exploit latent structure of self-representation\ninformation, different tensor decompositions have been employed into low rank\nmulti-view clustering (LRMVC) models for achieving significant performance.\nHowever, current approaches suffer from a series of problems related to those\ntensor decomposition, such as the unbalanced matricization scheme, rotation\nsensitivity, deficient correlations capture and so forth. All these will lead\nto LRMVC having insufficient access to global information, which is contrary to\nthe target of multi-view clustering. To alleviate these problems, we propose a\nnew tensor decomposition called Tucker-O-Minus Decomposition (TOMD) for\nmulti-view clustering. Specifically, based on the Tucker format, we\nadditionally employ the O-minus structure, which consists of a circle with an\nefficient bridge linking two weekly correlated factors. In this way, the core\ntensor in Tucker format is replaced by the O-minus architecture with a more\nbalanced structure, and the enhanced capacity of capturing the global low rank\ninformation will be achieved. The proposed TOMD also provides more compact and\npowerful representation abilities for the self-representation tensor,\nsimultaneously. The alternating direction method of multipliers is used to\nsolve the proposed model TOMD-MVC. Numerical experiments on six benchmark data\nsets demonstrate the superiority of our proposed method in terms of F-score,\nprecision, recall, normalized mutual information, adjusted rand index, and\naccuracy.",
    "descriptor": "",
    "authors": [
      "Yingcong Lu",
      "Yipeng Liu",
      "Zhen Long",
      "Zhangxin Chen",
      "Ce Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12638"
  },
  {
    "id": "arXiv:2210.12640",
    "title": "PoKE: Prior Knowledge Enhanced Emotional Support Conversation with  Latent Variable",
    "abstract": "Emotional support conversation (ESC) task can utilize various support\nstrategies to help people relieve emotional distress and overcome the problem\nthey face, which have attracted much attention in these years. The emotional\nsupport is a critical communication skill that should be trained into dialogue\nsystems. Most existing studies predict support strategy according to current\ncontext and provide corresponding emotional support in response. However, these\nworks ignore two significant characteristics of ESC. (a) Abundant prior\nknowledge exists in historical conversations, such as the responses to similar\ncases and the general order of support strategies, which has a great reference\nvalue for current conversation. (b) There is a one-to-many mapping relationship\nbetween context and support strategy, i.e.multiple strategies are reasonable\nfor a single context. It lays a better foundation for the diversity of\ngenerations. To take into account these two key factors, we Prior Knowledge\nEnhanced emotional support conversation with latent variable model, PoKE. The\nproposed model fully taps the potential of prior knowledge in terms of\nexemplars and strategy sequence and then utilizes a latent variable to model\nthe one-to-many relationship of support strategy. Furthermore, we introduce a\nmemory schema to effectively incorporate encoded knowledge into decoder.\nExperiment results on benchmark dataset~(i.e., ESConv) show that our PoKE\noutperforms existing baselines on both automatic evaluation and human\nevaluation. Further experiments prove that abundant prior knowledge is\nconducive to high-quality emotional support, and a well-learned latent variable\nis critical to the diversity of generations.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Xiaohan Xu",
      "Xuying Meng",
      "Yequan Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12640"
  },
  {
    "id": "arXiv:2210.12642",
    "title": "Accelerated Linearized Laplace Approximation for Bayesian Deep Learning",
    "abstract": "Laplace approximation (LA) and its linearized variant (LLA) enable effortless\nadaptation of pretrained deep neural networks to Bayesian neural networks. The\ngeneralized Gauss-Newton (GGN) approximation is typically introduced to improve\ntheir tractability. However, LA and LLA are still confronted with non-trivial\ninefficiency issues and should rely on Kronecker-factored, diagonal, or even\nlast-layer approximate GGN matrices in practical use. These approximations are\nlikely to harm the fidelity of learning outcomes. To tackle this issue,\ninspired by the connections between LLA and neural tangent kernels (NTKs), we\ndevelop a Nystrom approximation to NTKs to accelerate LLA. Our method benefits\nfrom the capability of popular deep learning libraries for forward mode\nautomatic differentiation, and enjoys reassuring theoretical guarantees.\nExtensive studies reflect the merits of the proposed method in aspects of both\nscalability and performance. Our method can even scale up to architectures like\nvision transformers. We also offer valuable ablation studies to diagnose our\nmethod. Code is available at \\url{https://github.com/thudzj/ELLA}.",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Zhijie Deng",
      "Feng Zhou",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12642"
  },
  {
    "id": "arXiv:2210.12647",
    "title": "Binary sequences with a low correlation via cyclotomic function fields  with odd characteristics",
    "abstract": "Sequences with a low correlation have very important applications in\ncommunications, cryptography, and compressed sensing. In the literature, many\nefforts have been made to construct good sequences with various lengths where\nbinary sequences attracts great attention. As a result, various constructions\nof good binary sequences have been proposed. However, most of the known\nconstructions made use of the multiplicative cyclic group structure of finite\nfield $\\mathbb{F}_{p^n}$ for a prime $p$ and a positive integer $n$. In fact,\nall $p^n+1$ rational places including the place at infinity of the rational\nfunction field over $\\mathbb{F}_{p^n}$ form a cyclic structure under an\nautomorphism of order $p^n+1$. In this paper, we make use of this cyclic\nstructure to provide an explicit construction of binary sequences with a low\ncorrelation of length $p^n+1$ via cyclotomic function fields over\n$\\mathbb{F}_{p^n}$ for any odd prime $p$. Each family of binary sequences has\nsize $p^n-2$ and its correlation is upper bounded by $4+\\lfloor 2\\cdot\np^{n/2}\\rfloor$. To the best of our knowledge, this is the first construction\nof binary sequences with a low correlation of length $p^n+1$ for odd prime $p$.\nMoreover, our sequences can be constructed explicitly and have competitive\nparameters.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2107.11766\n",
    "authors": [
      "Lingfei Jin",
      "Liming Ma",
      "Chaoping Xing"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12647"
  },
  {
    "id": "arXiv:2210.12648",
    "title": "Clarinet: A Music Retrieval System",
    "abstract": "A MIDI based approach for music recognition is proposed and implemented in\nthis paper. Our Clarinet music retrieval system is designed to search piano\nMIDI files with high recall and speed. We design a novel melody extraction\nalgorithm that improves recall results by more than 10%. We also implement 3\nalgorithms for retrieval-two self designed (RSA Note and RSA Time), and a\nmodified version of the Mongeau Sankoff Algorithm. Algorithms to achieve tempo\nand scale invariance are also discussed in this paper. The paper also contains\ndetailed experimentation and benchmarks with four different metrics. Clarinet\nachieves recall scores of more than 94%.",
    "descriptor": "",
    "authors": [
      "Kshitij Alwadhi",
      "Rohan Sharma",
      "Siddhant Sharma"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12648"
  },
  {
    "id": "arXiv:2210.12649",
    "title": "Anticipative Feature Fusion Transformer for Multi-Modal Action  Anticipation",
    "abstract": "Although human action anticipation is a task which is inherently multi-modal,\nstate-of-the-art methods on well known action anticipation datasets leverage\nthis data by applying ensemble methods and averaging scores of unimodal\nanticipation networks. In this work we introduce transformer based modality\nfusion techniques, which unify multi-modal data at an early stage. Our\nAnticipative Feature Fusion Transformer (AFFT) proves to be superior to popular\nscore fusion approaches and presents state-of-the-art results outperforming\nprevious methods on EpicKitchens-100 and EGTEA Gaze+. Our model is easily\nextensible and allows for adding new modalities without architectural changes.\nConsequently, we extracted audio features on EpicKitchens-100 which we add to\nthe set of commonly used features in the community.",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Zeyun Zhong",
      "David Schneider",
      "Michael Voit",
      "Rainer Stiefelhagen",
      "J\u00fcrgen Beyerer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12649"
  },
  {
    "id": "arXiv:2210.12651",
    "title": "Unsupervised Non-transferable Text Classification",
    "abstract": "Training a good deep learning model requires substantial data and computing\nresources, which makes the resulting neural model a valuable intellectual\nproperty. To prevent the neural network from being undesirably exploited,\nnon-transferable learning has been proposed to reduce the model generalization\nability in specific target domains. However, existing approaches require\nlabeled data for the target domain which can be difficult to obtain.\nFurthermore, they do not have the mechanism to still recover the model's\nability to access the target domain. In this paper, we propose a novel\nunsupervised non-transferable learning method for the text classification task\nthat does not require annotated target domain data. We further introduce a\nsecret key component in our approach for recovering the access to the target\ndomain, where we design both an explicit and an implicit method for doing so.\nExtensive experiments demonstrate the effectiveness of our approach.",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Guangtao Zeng",
      "Wei Lu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12651"
  },
  {
    "id": "arXiv:2210.12653",
    "title": "SAT: Improving Semi-Supervised Text Classification with Simple  Instance-Adaptive Self-Training",
    "abstract": "Self-training methods have been explored in recent years and have exhibited\ngreat performance in improving semi-supervised learning. This work presents a\nSimple instance-Adaptive self-Training method (SAT) for semi-supervised text\nclassification. SAT first generates two augmented views for each unlabeled data\nand then trains a meta-learner to automatically identify the relative strength\nof augmentations based on the similarity between the original view and the\naugmented views. The weakly-augmented view is fed to the model to produce a\npseudo-label and the strongly-augmented view is used to train the model to\npredict the same pseudo-label. We conducted extensive experiments and analyses\non three text classification datasets and found that with varying sizes of\nlabeled training data, SAT consistently shows competitive performance compared\nto existing semi-supervised learning methods. Our code can be found at\n\\url{https://github.com/declare-lab/SAT.git}.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 Findings\n",
    "authors": [
      "Hui Chen",
      "Wei Han",
      "Soujanya Poria"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12653"
  },
  {
    "id": "arXiv:2210.12654",
    "title": "Cross-document Event Coreference Search: Task, Dataset and Modeling",
    "abstract": "The task of Cross-document Coreference Resolution has been traditionally\nformulated as requiring to identify all coreference links across a given set of\ndocuments. We propose an appealing, and often more applicable, complementary\nset up for the task - Cross-document Coreference Search, focusing in this paper\non event coreference. Concretely, given a mention in context of an event of\ninterest, considered as a query, the task is to find all coreferring mentions\nfor the query event in a large document collection. To support research on this\ntask, we create a corresponding dataset, which is derived from Wikipedia while\nleveraging annotations in the available Wikipedia Event Coreference dataset\n(WEC-Eng). Observing that the coreference search setup is largely analogous to\nthe setting of Open Domain Question Answering, we adapt the prominent Deep\nPassage Retrieval (DPR) model to our setting, as an appealing baseline.\nFinally, we present a novel model that integrates a powerful coreference\nscoring scheme into the DPR architecture, yielding improved performance.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Alon Eirew",
      "Avi Caciularu",
      "Ido Dagan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12654"
  },
  {
    "id": "arXiv:2210.12655",
    "title": "A Trustless Architecture of Blockchain-enabled Metaverse",
    "abstract": "Metaverse has rekindled human beings' desire to further break space-time\nbarriers by fusing the virtual and real worlds. However, security and privacy\nthreats hinder us from building a utopia. A metaverse embraces various\ntechniques, while at the same time inheriting their pitfalls and thus exposing\nlarge attack surfaces. Blockchain, proposed in 2008, was regarded as a key\nbuilding block of metaverses. it enables transparent and trusted computing\nenvironments using tamper-resistant decentralized ledgers. Currently,\nblockchain supports Decentralized Finance (DeFi) and Non-fungible Tokens (NFT)\nfor metaverses. However, the power of a blockchain has not been sufficiently\nexploited. In this article, we propose a novel trustless architecture of\nblockchain-enabled metaverse, aiming to provide efficient resource integration\nand allocation by consolidating hardware and software components. To realize\nour design objectives, we provide an On-Demand Trusted Computing Environment\n(OTCE) technique based on local trust evaluation. Specifically, the\narchitecture adopts a hypergraph to represent a metaverse, in which each\nhyperedge links a group of users with certain relationship. Then the trust\nlevel of each user group can be evaluated based on graph analytics techniques.\nBased on the trust value, each group can determine its security plan on demand,\nfree from interference by irrelevant nodes. Besides, OTCEs enable large-scale\nand flexible application environments (sandboxes) while preserving a strong\nsecurity guarantee.",
    "descriptor": "\nComments: 7 pages, 4 figures\n",
    "authors": [
      "Minghui Xu",
      "Yihao Guo",
      "Qin Hu",
      "Zehui Xiong",
      "Dongxiao Yu",
      "Xiuzhen Cheng"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12655"
  },
  {
    "id": "arXiv:2210.12657",
    "title": "Deciphering Contact Interactions and Exploration Strategies Underlying  Tactile Perception of Material Softness",
    "abstract": "Our sense of touch is essential and permeates in interactions involving\nnatural explorations and affective communications. For instance, we routinely\njudge the ripeness of fruit at the grocery store, caress the arm of a spouse to\noffer comfort, and stroke textiles to gauge their softness. Meanwhile,\ninteractive displays that provide tactile feedback are becoming normal and\nubiquitous in our daily lives, and are extending rich and immersive\ninteractions into augmented and virtual reality. To replicate touch sensation\nand make virtual objects feel tangible, such feedback will need to relay a\nsense of compliance, or softness, one of the key dimensions underlying haptic\nperception. As our understanding of softness perception remains incomplete,\nthis study seeks to understand exploratory strategies and perceptual cues that\nmay optimally encode material softness. Specifically, we employ methods of\ncomputational finite element modeling, biomechanical experimentation,\npsychophysical evaluation, and data-driven analysis. Overall, this work may aid\nin engineering the next-generation wearable haptic displays, which must be more\ntangible, compatible, and perceptually naturalistic.",
    "descriptor": "\nComments: PHD Dissertation (Doctor of Philosophy), Systems Engineering, University of Virginia\n",
    "authors": [
      "Chang Xu"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2210.12657"
  },
  {
    "id": "arXiv:2210.12658",
    "title": "Extending Phrase Grounding with Pronouns in Visual Dialogues",
    "abstract": "Conventional phrase grounding aims to localize noun phrases mentioned in a\ngiven caption to their corresponding image regions, which has achieved great\nsuccess recently. Apparently, sole noun phrase grounding is not enough for\ncross-modal visual language understanding. Here we extend the task by\nconsidering pronouns as well. First, we construct a dataset of phrase grounding\nwith both noun phrases and pronouns to image regions. Based on the dataset, we\ntest the performance of phrase grounding by using a state-of-the-art literature\nmodel of this line. Then, we enhance the baseline grounding model with\ncoreference information which should help our task potentially, modeling the\ncoreference structures with graph convolutional networks. Experiments on our\ndataset, interestingly, show that pronouns are easier to ground than noun\nphrases, where the possible reason might be that these pronouns are much less\nambiguous. Additionally, our final model with coreference information can\nsignificantly boost the grounding performance of both noun phrases and\npronouns.",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Panzhong Lu",
      "Xin Zhang",
      "Meishan Zhang",
      "Min Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12658"
  },
  {
    "id": "arXiv:2210.12659",
    "title": "Mapping Process for the Task: Wikidata Statements to Text as Wikipedia  Sentences",
    "abstract": "Acknowledged as one of the most successful online cooperative projects in\nhuman society, Wikipedia has obtained rapid growth in recent years and desires\ncontinuously to expand content and disseminate knowledge values for everyone\nglobally. The shortage of volunteers brings to Wikipedia many issues, including\ndeveloping content for over 300 languages at the present. Therefore, the\nbenefit that machines can automatically generate content to reduce human\nefforts on Wikipedia language projects could be considerable. In this paper, we\npropose our mapping process for the task of converting Wikidata statements to\nnatural language text (WS2T) for Wikipedia projects at the sentence level. The\nmain step is to organize statements, represented as a group of quadruples and\ntriples, and then to map them to corresponding sentences in English Wikipedia.\nWe evaluate the output corpus in various aspects: sentence structure analysis,\nnoise filtering, and relationships between sentence components based on word\nembedding models. The results are helpful not only for the data-to-text\ngeneration task but also for other relevant works in the field.",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Hoang Thang Ta",
      "Alexander Gelbukha",
      "Grigori Sidorov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12659"
  },
  {
    "id": "arXiv:2210.12662",
    "title": "Improving Chinese Named Entity Recognition by Search Engine Augmentation",
    "abstract": "Compared with English, Chinese suffers from more grammatical ambiguities,\nlike fuzzy word boundaries and polysemous words. In this case, contextual\ninformation is not sufficient to support Chinese named entity recognition\n(NER), especially for rare and emerging named entities. Semantic augmentation\nusing external knowledge is a potential way to alleviate this problem, while\nhow to obtain and leverage external knowledge for the NER task remains a\nchallenge. In this paper, we propose a neural-based approach to perform\nsemantic augmentation using external knowledge from search engine for Chinese\nNER. In particular, a multi-channel semantic fusion model is adopted to\ngenerate the augmented input representations, which aggregates external related\ntexts retrieved from the search engine. Experiments have shown the superiority\nof our model across 4 NER datasets, including formal and social media language\ncontexts, which further prove the effectiveness of our approach.",
    "descriptor": "",
    "authors": [
      "Qinghua Mao",
      "Jiatong Li",
      "Kui Meng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12662"
  },
  {
    "id": "arXiv:2210.12663",
    "title": "No-Regret Learning in Two-Echelon Supply Chain with Unknown Demand  Distribution",
    "abstract": "Supply chain management (SCM) has been recognized as an important discipline\nwith applications to many industries, where the two-echelon stochastic\ninventory model, involving one downstream retailer and one upstream supplier,\nplays a fundamental role for developing firms' SCM strategies. In this work, we\naim at designing online learning algorithms for this problem with an unknown\ndemand distribution, which brings distinct features as compared to classic\nonline optimization problems. Specifically, we consider the two-echelon supply\nchain model introduced in [Cachon and Zipkin, 1999] under two different\nsettings: the centralized setting, where a planner decides both agents'\nstrategy simultaneously, and the decentralized setting, where two agents decide\ntheir strategy independently and selfishly. We design algorithms that achieve\nfavorable guarantees for both regret and convergence to the optimal inventory\ndecision in both settings, and additionally for individual regret in the\ndecentralized setting. Our algorithms are based on Online Gradient Descent and\nOnline Newton Step, together with several new ingredients specifically designed\nfor our problem. We also implement our algorithms and show their empirical\neffectiveness.",
    "descriptor": "",
    "authors": [
      "Mengxiao Zhang",
      "Shi Chen",
      "Haipeng Luo",
      "Yingfei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12663"
  },
  {
    "id": "arXiv:2210.12669",
    "title": "Meta Learning of Interface Conditions for Multi-Domain Physics-Informed  Neural Networks",
    "abstract": "Physics-informed neural networks (PINNs) are emerging as popular mesh-free\nsolvers for partial differential equations (PDEs). Recent extensions decompose\nthe domain, applying different PINNs to solve the equation in each subdomain\nand aligning the solution at the interface of the subdomains. Hence, they can\nfurther alleviate the problem complexity, reduce the computational cost, and\nallow parallelization. However, the performance of the multi-domain PINNs is\nsensitive to the choice of the interface conditions for solution alignment.\nWhile quite a few conditions have been proposed, there is no suggestion about\nhow to select the conditions according to specific problems. To address this\ngap, we propose META Learning of Interface Conditions (METALIC), a simple,\nefficient yet powerful approach to dynamically determine the optimal interface\nconditions for solving a family of parametric PDEs. Specifically, we develop\ntwo contextual multi-arm bandit models. The first one applies to the entire\ntraining procedure, and online updates a Gaussian process (GP) reward surrogate\nthat given the PDE parameters and interface conditions predicts the solution\nerror. The second one partitions the training into two stages, one is the\nstochastic phase and the other deterministic phase; we update a GP surrogate\nfor each phase to enable different condition selections at the two stages so as\nto further bolster the flexibility and performance. We have shown the advantage\nof METALIC on four bench-mark PDE families.",
    "descriptor": "",
    "authors": [
      "Shibo Li",
      "Michael Penwarden",
      "Robert M. Kirby",
      "Shandian Zhe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.12669"
  },
  {
    "id": "arXiv:2210.12673",
    "title": "Lexical Generalization Improves with Larger Models and Longer Training",
    "abstract": "While fine-tuned language models perform well on many tasks, they were also\nshown to rely on superficial surface features such as lexical overlap.\nExcessive utilization of such heuristics can lead to failure on challenging\ninputs. We analyze the use of lexical overlap heuristics in natural language\ninference, paraphrase detection, and reading comprehension (using a novel\ncontrastive dataset), and find that larger models are much less susceptible to\nadopting lexical overlap heuristics. We also find that longer training leads\nmodels to abandon lexical overlap heuristics. Finally, we provide evidence that\nthe disparity between models size has its source in the pre-trained model",
    "descriptor": "\nComments: Accepted to EMNLP 2022 as Findings Paper\n",
    "authors": [
      "Elron Bandel",
      "Yoav Goldberg.",
      "Yanai Elazar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12673"
  },
  {
    "id": "arXiv:2210.12674",
    "title": "Towards Generalizable and Robust Text-to-SQL Parsing",
    "abstract": "Text-to-SQL parsing tackles the problem of mapping natural language questions\nto executable SQL queries. In practice, text-to-SQL parsers often encounter\nvarious challenging scenarios, requiring them to be generalizable and robust.\nWhile most existing work addresses a particular generalization or robustness\nchallenge, we aim to study it in a more comprehensive manner. In specific, we\nbelieve that text-to-SQL parsers should be (1) generalizable at three levels of\ngeneralization, namely i.i.d., zero-shot, and compositional, and (2) robust\nagainst input perturbations. To enhance these capabilities of the parser, we\npropose a novel TKK framework consisting of Task decomposition, Knowledge\nacquisition, and Knowledge composition to learn text-to-SQL parsing in stages.\nBy dividing the learning process into multiple stages, our framework improves\nthe parser's ability to acquire general SQL knowledge instead of capturing\nspurious patterns, making it more generalizable and robust. Experimental\nresults under various generalization and robustness settings show that our\nframework is effective in all scenarios and achieves state-of-the-art\nperformance on the Spider, SParC, and CoSQL datasets. Code can be found at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/tkk.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Chang Gao",
      "Bowen Li",
      "Wenxuan Zhang",
      "Wai Lam",
      "Binhua Li",
      "Fei Huang",
      "Luo Si",
      "Yongbin Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12674"
  },
  {
    "id": "arXiv:2210.12678",
    "title": "ComFact: A Benchmark for Linking Contextual Commonsense Knowledge",
    "abstract": "Understanding rich narratives, such as dialogues and stories, often requires\nnatural language processing systems to access relevant knowledge from\ncommonsense knowledge graphs. However, these systems typically retrieve facts\nfrom KGs using simple heuristics that disregard the complex challenges of\nidentifying situationally-relevant commonsense knowledge (e.g.,\ncontextualization, implicitness, ambiguity).\nIn this work, we propose the new task of commonsense fact linking, where\nmodels are given contexts and trained to identify situationally-relevant\ncommonsense knowledge from KGs. Our novel benchmark, ComFact, contains ~293k\nin-context relevance annotations for commonsense triplets across four\nstylistically diverse dialogue and storytelling datasets. Experimental results\nconfirm that heuristic fact linking approaches are imprecise knowledge\nextractors. Learned fact linking models demonstrate across-the-board\nperformance improvements (~34.6% F1) over these heuristics. Furthermore,\nimproved knowledge retrieval yielded average downstream improvements of 9.8%\nfor a dialogue response generation task. However, fact linking models still\nsignificantly underperform humans, suggesting our benchmark is a promising\ntestbed for research in commonsense augmentation of NLP systems.",
    "descriptor": "\nComments: Findings of EMNLP 2022, long paper\n",
    "authors": [
      "Silin Gao",
      "Jena D. Hwang",
      "Saya Kanno",
      "Hiromi Wakaki",
      "Yuki Mitsufuji",
      "Antoine Bosselut"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12678"
  },
  {
    "id": "arXiv:2210.12681",
    "title": "Rethinking Rotation in Self-Supervised Contrastive Learning: Adaptive  Positive or Negative Data Augmentation",
    "abstract": "Rotation is frequently listed as a candidate for data augmentation in\ncontrastive learning but seldom provides satisfactory improvements. We argue\nthat this is because the rotated image is always treated as either positive or\nnegative. The semantics of an image can be rotation-invariant or\nrotation-variant, so whether the rotated image is treated as positive or\nnegative should be determined based on the content of the image. Therefore, we\npropose a novel augmentation strategy, adaptive Positive or Negative Data\nAugmentation (PNDA),\nin which an original and its rotated image are a positive pair if they are\nsemantically close and a negative pair if they are semantically different. To\nachieve PNDA, we first determine whether rotation is positive or negative on an\nimage-by-image basis in an unsupervised way. Then, we apply PNDA to contrastive\nlearning frameworks. Our experiments showed that PNDA improves the performance\nof contrastive learning. The code is available at \\url{\nhttps://github.com/AtsuMiyai/rethinking_rotation}.",
    "descriptor": "\nComments: Accepted to Winter Conference on Applications of Computer Vision (WACV), 2023\n",
    "authors": [
      "Atsuyuki Miyai",
      "Qing Yu",
      "Daiki Ikami",
      "Go Irie",
      "Kiyoharu Aizawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12681"
  },
  {
    "id": "arXiv:2210.12682",
    "title": "Photo-realistic Neural Domain Randomization",
    "abstract": "Synthetic data is a scalable alternative to manual supervision, but it\nrequires overcoming the sim-to-real domain gap. This discrepancy between\nvirtual and real worlds is addressed by two seemingly opposed approaches:\nimproving the realism of simulation or foregoing realism entirely via domain\nrandomization. In this paper, we show that the recent progress in neural\nrendering enables a new unified approach we call Photo-realistic Neural Domain\nRandomization (PNDR). We propose to learn a composition of neural networks that\nacts as a physics-based ray tracer generating high-quality renderings from\nscene geometry alone. Our approach is modular, composed of different neural\nnetworks for materials, lighting, and rendering, thus enabling randomization of\ndifferent key image generation components in a differentiable pipeline. Once\ntrained, our method can be combined with other methods and used to generate\nphoto-realistic image augmentations online and significantly more efficiently\nthan via traditional ray-tracing. We demonstrate the usefulness of PNDR through\ntwo downstream tasks: 6D object detection and monocular depth estimation. Our\nexperiments show that training with PNDR enables generalization to novel scenes\nand significantly outperforms the state of the art in terms of real-world\ntransfer.",
    "descriptor": "\nComments: Accepted to European Conference on Computer Vision (ECCV), 2022\n",
    "authors": [
      "Sergey Zakharov",
      "Rares Ambrus",
      "Vitor Guizilini",
      "Wadim Kehl",
      "Adrien Gaidon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12682"
  },
  {
    "id": "arXiv:2210.12683",
    "title": "GAN-based Facial Attribute Manipulation",
    "abstract": "Facial Attribute Manipulation (FAM) aims to aesthetically modify a given face\nimage to render desired attributes, which has received significant attention\ndue to its broad practical applications ranging from digital entertainment to\nbiometric forensics. In the last decade, with the remarkable success of\nGenerative Adversarial Networks (GANs) in synthesizing realistic images,\nnumerous GAN-based models have been proposed to solve FAM with various problem\nformulation approaches and guiding information representations. This paper\npresents a comprehensive survey of GAN-based FAM methods with a focus on\nsummarizing their principal motivations and technical details. The main\ncontents of this survey include: (i) an introduction to the research background\nand basic concepts related to FAM, (ii) a systematic review of GAN-based FAM\nmethods in three main categories, and (iii) an in-depth discussion of important\nproperties of FAM methods, open issues, and future research directions. This\nsurvey not only builds a good starting point for researchers new to this field\nbut also serves as a reference for the vision community.",
    "descriptor": "",
    "authors": [
      "Yunfan Liu",
      "Qi Li",
      "Qiyao Deng",
      "Zhenan Sun",
      "Ming-Hsuan Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12683"
  },
  {
    "id": "arXiv:2210.12685",
    "title": "Less Emphasis on Difficult Layer Regions: Curriculum Learning for  Singularly Perturbed Convection-Diffusion-Reaction Problems",
    "abstract": "Although Physics-Informed Neural Networks (PINNs) have been successfully\napplied to various differential equations, accurately solving perturbed\nconvection-diffusion-reaction problems is still extremely challenging for\nPINNs. This paper investigates the source of the learning difficulties and\nfinds that the rapid transition of potential solution in the layer region\ncauses the failure of convergence. Based on this finding, we present a\ncurriculum learning method that encourages neural networks to ``prioritize the\nlearning on easier non-layer regions''. The method helps PINNs to dynamically\nadjust the training data weights, speed up the learning procedure, and\nultimately significantly improve the accuracy of the network approximation.\nExtensive evaluation on multiple typical model equations shows that the\nproposed approach accurately captures the resolution of the layer regions, and\nachieves multiple orders of magnitude lower root-mean-squared error than\nordinary PINNs. We provide our PyTorch code at\nhttps://github.com/WYu-Feng/CLPINN",
    "descriptor": "\nComments: 21 pages\n",
    "authors": [
      "Yufeng Wang",
      "Cong Xu",
      "Min Yang",
      "Jin Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12685"
  },
  {
    "id": "arXiv:2210.12686",
    "title": "Holistic Interaction Transformer Network for Action Detection",
    "abstract": "Actions are about how we interact with the environment, including other\npeople, objects, and ourselves. In this paper, we propose a novel multi-modal\nHolistic Interaction Transformer Network (HIT) that leverages the largely\nignored, but critical hand and pose information essential to most human\nactions. The proposed \"HIT\" network is a comprehensive bi-modal framework that\ncomprises an RGB stream and a pose stream. Each of them separately models\nperson, object, and hand interactions. Within each sub-network, an\nIntra-Modality Aggregation module (IMA) is introduced that selectively merges\nindividual interaction units. The resulting features from each modality are\nthen glued using an Attentive Fusion Mechanism (AFM). Finally, we extract cues\nfrom the temporal context to better classify the occurring actions using cached\nmemory. Our method significantly outperforms previous approaches on the J-HMDB,\nUCF101-24, and MultiSports datasets. We also achieve competitive results on\nAVA. The code will be available at https://github.com/joslefaure/HIT.",
    "descriptor": "\nComments: To appear at WACV 2023\n",
    "authors": [
      "Gueter Josmy Faure",
      "Min-Hung Chen",
      "Shang-Hong Lai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12686"
  },
  {
    "id": "arXiv:2210.12687",
    "title": "BotsTalk: Machine-sourced Framework for Automatic Curation of  Large-scale Multi-skill Dialogue Datasets",
    "abstract": "To build open-domain chatbots that are able to use diverse communicative\nskills, we propose a novel framework BotsTalk, where multiple agents grounded\nto the specific target skills participate in a conversation to automatically\nannotate multi-skill dialogues. We further present Blended Skill BotsTalk\n(BSBT), a large-scale multi-skill dialogue dataset comprising 300K\nconversations. Through extensive experiments, we demonstrate that our dataset\ncan be effective for multi-skill dialogue systems which require an\nunderstanding of skill blending as well as skill grounding. Our code and data\nare available at https://github.com/convei-lab/BotsTalk.",
    "descriptor": "\nComments: Accepted to EMNLP2022. Code and data are available at this https URL\n",
    "authors": [
      "Minju Kim",
      "Chaehyeong Kim",
      "Yongho Song",
      "Seung-won Hwang",
      "Jinyoung Yeo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12687"
  },
  {
    "id": "arXiv:2210.12688",
    "title": "How \"Multi\" is Multi-Document Summarization?",
    "abstract": "The task of multi-document summarization (MDS) aims at models that, given\nmultiple documents as input, are able to generate a summary that combines\ndisperse information, originally spread across these documents. Accordingly, it\nis expected that both reference summaries in MDS datasets, as well as system\nsummaries, would indeed be based on such dispersed information. In this paper,\nwe argue for quantifying and assessing this expectation. To that end, we\npropose an automated measure for evaluating the degree to which a summary is\n``disperse'', in the sense of the number of source documents needed to cover\nits content. We apply our measure to empirically analyze several popular MDS\ndatasets, with respect to their reference summaries, as well as the output of\nstate-of-the-art systems. Our results show that certain MDS datasets barely\nrequire combining information from multiple documents, where a single document\noften covers the full summary content. Overall, we advocate using our metric\nfor assessing and improving the degree to which summarization datasets require\ncombining multi-document information, and similarly how summarization models\nactually meet this challenge. Our code is available in\nhttps://github.com/ariecattan/multi_mds.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ruben Wolhandler",
      "Arie Cattan",
      "Ori Ernst",
      "Ido Dagan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12688"
  },
  {
    "id": "arXiv:2210.12689",
    "title": "Face Emotion Recognization Using Dataset Augmentation Based on Neural  Network",
    "abstract": "Facial expression is one of the most external indications of a person's\nfeelings and emotions. In daily conversation, according to the psychologist,\nonly 7\\% and 38\\% of information is communicated through words and sounds\nrespective, while up to 55\\% is through facial expression. It plays an\nimportant role in coordinating interpersonal relationships. Ekman and Friesen\nrecognized six essential emotions in the nineteenth century depending on a\ncross-cultural study, which indicated that people feel each basic emotion in\nthe same fashion despite culture. As a branch of the field of analyzing\nsentiment, facial expression recognition offers broad application prospects in\na variety of domains, including the interaction between humans and computers,\nhealthcare, and behavior monitoring. Therefore, many researchers have devoted\nthemselves to facial expression recognition. In this paper, an effective hybrid\ndata augmentation method is used. This approach is operated on two public\ndatasets, and four benchmark models see some remarkable results.",
    "descriptor": "\nComments: 5 pages, 8 figures, 3 tables\n",
    "authors": [
      "Mengyu Rao",
      "Ruiyi Bao",
      "Liangshun Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12689"
  },
  {
    "id": "arXiv:2210.12690",
    "title": "DyCSC: Modeling the Evolutionary Process of Dynamic Networks Based on  Cluster Structure",
    "abstract": "Temporal networks are an important type of network whose topological\nstructure changes over time. Compared with methods on static networks, temporal\nnetwork embedding (TNE) methods are facing three challenges: 1) it cannot\ndescribe the temporal dependence across network snapshots; 2) the node\nembedding in the latent space fails to indicate changes in the network\ntopology; and 3) it cannot avoid a lot of redundant computation via parameter\ninheritance on a series of snapshots. To this end, we propose a novel temporal\nnetwork embedding method named Dynamic Cluster Structure Constraint model\n(DyCSC), whose core idea is to capture the evolution of temporal networks by\nimposing a temporal constraint on the tendency of the nodes in the network to a\ngiven number of clusters. It not only generates low-dimensional embedding\nvectors for nodes but also preserves the dynamic nonlinear features of temporal\nnetworks. Experimental results on multiple realworld datasets have demonstrated\nthe superiority of DyCSC for temporal graph embedding, as it consistently\noutperforms competing methods by significant margins in multiple temporal link\nprediction tasks. Moreover, the ablation study further validates the\neffectiveness of the proposed temporal constraint.",
    "descriptor": "\nComments: 8 pages, 4 figures. arXiv admin note: text overlap with arXiv:2107.03767, arXiv:1908.09710 by other authors\n",
    "authors": [
      "Shanfan Zhang",
      "Zhan Bu"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12690"
  },
  {
    "id": "arXiv:2210.12691",
    "title": "Practical Implementation of Sequence Selection for Nonlinear  Probabilistic Shaping",
    "abstract": "We propose two novel techniques to implement sequence selection (SS) for\nfiber nonlinearity mitigation, demonstrating a nonlinear shaping gain of 0.24\nbits/s/Hz, just 0.1 bits/s/Hz below the SS capacity lower bound.",
    "descriptor": "\nComments: Submitted for presentation to the OFC conference 2023\n",
    "authors": [
      "Stella Civelli",
      "Enrico Forestieri",
      "Marco Secondini"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.12691"
  },
  {
    "id": "arXiv:2210.12692",
    "title": "Focus Is All You Need For Chinese Grammatical Error Correction",
    "abstract": "Chinese Grammatical Error Correction (CGEC) aims to automatically detect and\ncorrect grammatical errors contained in Chinese text. In the long term,\nresearchers regard CGEC as a task with a certain degree of uncertainty, that\nis, an ungrammatical sentence may often have multiple references. However, we\nargue that even though this is a very reasonable hypothesis, it is too harsh\nfor the intelligence of the mainstream models in this era. In this paper, we\nfirst discover that multiple references do not actually bring positive gains to\nmodel training. On the contrary, it is beneficial to the CGEC model if the\nmodel can pay attention to small but essential data during the training\nprocess. Furthermore, we propose a simple yet effective training strategy\ncalled OneTarget to improve the focus ability of the CGEC models and thus\nimprove the CGEC performance. Extensive experiments and detailed analyses\ndemonstrate the correctness of our discovery and the effectiveness of our\nproposed method.",
    "descriptor": "\nComments: Submitted to ICASSP2023 (currently under review)\n",
    "authors": [
      "Jingheng Ye",
      "Yinghui Li",
      "Shirong Ma",
      "Rui Xie",
      "Wei Wu",
      "Hai-Tao Zheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12692"
  },
  {
    "id": "arXiv:2210.12693",
    "title": "Coupling User Preference with External Rewards to Enable Driver-centered  and Resource-aware EV Charging Recommendation",
    "abstract": "Electric Vehicle (EV) charging recommendation that both accommodates user\npreference and adapts to the ever-changing external environment arises as a\ncost-effective strategy to alleviate the range anxiety of private EV drivers.\nPrevious studies focus on centralized strategies to achieve optimized resource\nallocation, particularly useful for privacy-indifferent taxi fleets and\nfixed-route public transits. However, private EV driver seeks a more\npersonalized and resource-aware charging recommendation that is tailor-made to\naccommodate the user preference (when and where to charge) yet sufficiently\nadaptive to the spatiotemporal mismatch between charging supply and demand.\nHere we propose a novel Regularized Actor-Critic (RAC) charging recommendation\napproach that would allow each EV driver to strike an optimal balance between\nthe user preference (historical charging pattern) and the external reward\n(driving distance and wait time). Experimental results on two real-world\ndatasets demonstrate the unique features and superior performance of our\napproach to the competing methods.",
    "descriptor": "\nComments: 16 pages, 5 figures. To appear in the Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2022)\n",
    "authors": [
      "Chengyin Li",
      "Zheng Dong",
      "Nathan Fisher",
      "Dongxiao Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.12693"
  },
  {
    "id": "arXiv:2210.12694",
    "title": "Do Language Models Understand Measurements?",
    "abstract": "Recent success of pre-trained language models (PLMs) has stimulated interest\nin their ability to understand and work with numbers. Yet, the numerical\nreasoning over measurements has not been formally studied despite their\nimportance. In this study, we show that PLMs lack the capability required for\nreasoning over measurements. Furthermore, we find that a language model trained\non a measurement-rich corpus shows better performance on understanding\nmeasurements. We propose a simple embedding strategy to better distinguish\nbetween numbers and units, which leads to a significant improvement in the\nprobing tasks.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Sungjin Park",
      "Seungwoo Ryu",
      "Edward Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12694"
  },
  {
    "id": "arXiv:2210.12696",
    "title": "On the Transformation of Latent Space in Fine-Tuned NLP Models",
    "abstract": "We study the evolution of latent space in fine-tuned NLP models. Different\nfrom the commonly used probing-framework, we opt for an unsupervised method to\nanalyze representations. More specifically, we discover latent concepts in the\nrepresentational space using hierarchical clustering. We then use an alignment\nfunction to gauge the similarity between the latent space of a pre-trained\nmodel and its fine-tuned version. We use traditional linguistic concepts to\nfacilitate our understanding and also study how the model space transforms\ntowards task-specific information. We perform a thorough analysis, comparing\npre-trained and fine-tuned models across three models and three downstream\ntasks. The notable findings of our work are: i) the latent space of the higher\nlayers evolve towards task-specific concepts, ii) whereas the lower layers\nretain generic concepts acquired in the pre-trained model, iii) we discovered\nthat some concepts in the higher layers acquire polarity towards the output\nclass, and iv) that these concepts can be used for generating adversarial\ntriggers.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Nadir Durrani",
      "Hassan Sajjad",
      "Fahim Dalvi",
      "Firoj Alam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12696"
  },
  {
    "id": "arXiv:2210.12704",
    "title": "Batch Multi-Fidelity Active Learning with Budget Constraints",
    "abstract": "Learning functions with high-dimensional outputs is critical in many\napplications, such as physical simulation and engineering design. However,\ncollecting training examples for these applications is often costly, e.g. by\nrunning numerical solvers. The recent work (Li et al., 2022) proposes the first\nmulti-fidelity active learning approach for high-dimensional outputs, which can\nacquire examples at different fidelities to reduce the cost while improving the\nlearning performance. However, this method only queries at one pair of fidelity\nand input at a time, and hence has a risk to bring in strongly correlated\nexamples to reduce the learning efficiency. In this paper, we propose Batch\nMulti-Fidelity Active Learning with Budget Constraints (BMFAL-BC), which can\npromote the diversity of training examples to improve the benefit-cost ratio,\nwhile respecting a given budget constraint for batch queries. Hence, our method\ncan be more practically useful. Specifically, we propose a novel batch\nacquisition function that measures the mutual information between a batch of\nmulti-fidelity queries and the target function, so as to penalize highly\ncorrelated queries and encourages diversity. The optimization of the batch\nacquisition function is challenging in that it involves a combinatorial search\nover many fidelities while subject to the budget constraint. To address this\nchallenge, we develop a weighted greedy algorithm that can sequentially\nidentify each (fidelity, input) pair, while achieving a near $(1 -\n1/e)$-approximation of the optimum. We show the advantage of our method in\nseveral computational physics and engineering applications.",
    "descriptor": "",
    "authors": [
      "Shibo Li",
      "Jeff M. Phillips",
      "Xin Yu",
      "Robert M. Kirby",
      "Shandian Zhe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12704"
  },
  {
    "id": "arXiv:2210.12705",
    "title": "Few-Shot Meta Learning for Recognizing Facial Phenotypes of Genetic  Disorders",
    "abstract": "Computer vision-based methods have valuable use cases in precision medicine,\nand recognizing facial phenotypes of genetic disorders is one of them. Many\ngenetic disorders are known to affect faces' visual appearance and geometry.\nAutomated classification and similarity retrieval aid physicians in\ndecision-making to diagnose possible genetic conditions as early as possible.\nPrevious work has addressed the problem as a classification problem and used\ndeep learning methods. The challenging issue in practice is the sparse label\ndistribution and huge class imbalances across categories. Furthermore, most\ndisorders have few labeled samples in training sets, making representation\nlearning and generalization essential to acquiring a reliable feature\ndescriptor. In this study, we used a facial recognition model trained on a\nlarge corpus of healthy individuals as a pre-task and transferred it to facial\nphenotype recognition. Furthermore, we created simple baselines of few-shot\nmeta-learning methods to improve our base feature descriptor. Our quantitative\nresults on GestaltMatcher Database show that our CNN baseline surpasses\nprevious works, including GestaltMatcher, and few-shot meta-learning strategies\nimprove retrieval performance in frequent and rare classes.",
    "descriptor": "",
    "authors": [
      "\u00d6mer S\u00fcmer",
      "Fabio Hellmann",
      "Alexander Hustinx",
      "Tzung-Chien Hsieh",
      "Elisabeth Andr\u00e9",
      "Peter Krawitz"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.12705"
  },
  {
    "id": "arXiv:2210.12706",
    "title": "Robust Adaptive Prescribed-Time Control for Parameter-Varying Nonlinear  Systems",
    "abstract": "It is an interesting open problem to achieve adaptive prescribed-time control\nfor strict-feedback systems with unknown and fast or even abrupt time-varying\nparameters. In this paper we present a solution with the aid of several design\nand analysis innovations. First, by using a spatiotemporal transformation, we\nconvert the original system operational over finite time interval into one\noperational over infinite time interval, allowing for Lyapunov asymptotic\ndesign and recasting prescribed-time stabilization on finite time domain into\nasymptotic stabilization on infinite time domain. Second, to deal with\ntime-varying parameters with unknown variation boundaries, we use congelation\nof variables method and establish three separate adaptive laws for parameter\nestimation (two for the unknown parameters in the feedback path and one for the\nunknown parameter in the input path), in doing so we utilize two tuning\nfunctions to eliminate over-parametrization. Third, to achieve asymptotic\nconvergence for the transformed system, we make use of nonlinear damping design\nand non-regressor-based design to cope with time-varying perturbations, and\nfinally, we derive the prescribed-time control scheme from the asymptotic\ncontroller via inverse temporal-scale transformation. The boundedness of all\nclosed-loop signals and control input is proved rigorously through Lyapunov\nanalysis, squeeze theorem, and two novel lemmas built upon the method of\nvariation of constants. Numerical simulation verifies the effectiveness of the\nproposed method.",
    "descriptor": "",
    "authors": [
      "Hefu Ye",
      "Yongduan Song"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12706"
  },
  {
    "id": "arXiv:2210.12712",
    "title": "Prescribed-Time Control and Its Latest Developments",
    "abstract": "Prescribed-time (PT) control, originated from \\textit{Song et al.}, has\ngained increasing attention among control community. The salient feature of PT\ncontrol lies in its ability to achieve system stability within a finite\nsettling time user-assignable in advance irrespective of initial conditions. It\nis such a unique feature that has enticed many follow-up studies on this\ntechnically important area, motivating numerous research advancements. In this\narticle, we provide a comprehensive survey on the recent developments in PT\ncontrol. Through a concise introduction to the concept of PT control, and a\nunique taxonomy covering: 1) from robust PT control to adaptive PT control; 2)\nfrom PT control for single-input-single-output (SISO) systems to\nmulti-input-multi-output (MIMO) systems; and 3) from PT control for single\nsystems to multi-agent systems, we present an accessible review of this\ninteresting topic. We highlight key techniques, fundamental assumptions adopted\nin various developments as well as some new design ideas. We also discuss\nseveral possibles future research directions towards PT control.",
    "descriptor": "",
    "authors": [
      "Hefu Ye",
      "Yongduan Song",
      "Frank L. Lewis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12712"
  },
  {
    "id": "arXiv:2210.12714",
    "title": "Generative Knowledge Graph Construction: A Review",
    "abstract": "Generative Knowledge Graph Construction (KGC) refers to those methods that\nleverage the sequence-to-sequence framework for building knowledge graphs,\nwhich is flexible and can be adapted to widespread tasks. In this study, we\nsummarize the recent compelling progress in generative knowledge graph\nconstruction. We present the advantages and weaknesses of each paradigm in\nterms of different generation targets and provide theoretical insight and\nempirical analysis. Based on the review, we suggest promising research\ndirections for the future. Our contributions are threefold: (1) We present a\ndetailed, complete taxonomy for the generative KGC methods; (2) We provide a\ntheoretical and empirical analysis of the generative KGC methods; (3) We\npropose several research directions that can be developed in the future.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 and a public repository is available in this https URL\n",
    "authors": [
      "Hongbin Ye",
      "Ningyu Zhang",
      "Hui Chen",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12714"
  },
  {
    "id": "arXiv:2210.12715",
    "title": "Adaptive Control with Global Exponential Stability for Parameter-Varying  Nonlinear Systems under Unknown Control Gains",
    "abstract": "It is nontrivial to achieve exponential stability even for time-invariant\nnonlinear systems with matched uncertainties and persistent excitation (PE)\ncondition. In this paper, without the need for PE condition, we address the\nproblem of global exponential stabilization of strict-feedback systems with\nmismatched uncertainties and unknown yet time-varying control gains. The\nresultant control, embedded with time-varying feedback gains, is capable of\nensuring global exponential stability of parametric-strict-feedback systems in\nthe absence of persistence of excitation. By using the enhanced Nussbaum\nfunction, the previous results are extended to more general nonlinear systems\nwhere the sign and magnitude of the time-varying control gain are unknown. In\nparticular, the argument of the Nussbaum function is guaranteed to be always\npositive with the aid of nonlinear damping design, which is critical to perform\na straightforward technical analysis of the boundedness of the Nussbaum\nfunction. Finally, the global exponential stability of parameter-varying\nstrict-feedback systems, the boundedness of the control input and the update\nrate, and the asymptotic constancy of the parameter estimate are established.\nNumerical simulations are carried out to verify the effectiveness and benefits\nof the proposed methods.",
    "descriptor": "",
    "authors": [
      "Hefu Ye",
      "Haijia Wu",
      "Kai Zhao",
      "Yongduan Song"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12715"
  },
  {
    "id": "arXiv:2210.12719",
    "title": "Learning General World Models in a Handful of Reward-Free Deployments",
    "abstract": "Building generally capable agents is a grand challenge for deep reinforcement\nlearning (RL). To approach this challenge practically, we outline two key\ndesiderata: 1) to facilitate generalization, exploration should be task\nagnostic; 2) to facilitate scalability, exploration policies should collect\nlarge quantities of data without costly centralized retraining. Combining these\ntwo properties, we introduce the reward-free deployment efficiency setting, a\nnew paradigm for RL research. We then present CASCADE, a novel approach for\nself-supervised exploration in this new setting. CASCADE seeks to learn a world\nmodel by collecting data with a population of agents, using an information\ntheoretic objective inspired by Bayesian Active Learning. CASCADE achieves this\nby specifically maximizing the diversity of trajectories sampled by the\npopulation through a novel cascading objective. We provide theoretical\nintuition for CASCADE which we show in a tabular setting improves upon na\\\"ive\napproaches that do not account for population diversity. We then demonstrate\nthat CASCADE collects diverse task-agnostic datasets and learns agents that\ngeneralize zero-shot to novel, unseen downstream tasks on Atari, MiniGrid,\nCrafter and the DM Control Suite. Code and videos are available at\nhttps://ycxuyingchen.github.io/cascade/",
    "descriptor": "\nComments: To be published at NeurIPS 2022. Code and videos available at this https URL\n",
    "authors": [
      "Yingchen Xu",
      "Jack Parker-Holder",
      "Aldo Pacchiano",
      "Philip J. Ball",
      "Oleh Rybkin",
      "Stephen J. Roberts",
      "Tim Rockt\u00e4schel",
      "Edward Grefenstette"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12719"
  },
  {
    "id": "arXiv:2210.12720",
    "title": "Span-based joint entity and relation extraction augmented with sequence  tagging mechanism",
    "abstract": "Span-based joint extraction simultaneously conducts named entity recognition\n(NER) and relation extraction (RE) in text span form. However, since previous\nspan-based models rely on span-level classifications, they cannot benefit from\ntoken-level label information, which has been proven advantageous for the task.\nIn this paper, we propose a Sequence Tagging augmented Span-based Network\n(STSN), a span-based joint model that can make use of token-level label\ninformation. In STSN, we construct a core neural architecture by deep stacking\nmultiple attention layers, each of which consists of three basic attention\nunits. On the one hand, the core architecture enables our model to learn\ntoken-level label information via the sequence tagging mechanism and then uses\nthe information in the span-based joint extraction; on the other hand, it\nestablishes a bi-directional information interaction between NER and RE.\nExperimental results on three benchmark datasets show that STSN consistently\noutperforms the strongest baselines in terms of F1, creating new\nstate-of-the-art results.",
    "descriptor": "\nComments: Accept by Science China-Information Sciences\n",
    "authors": [
      "Bin Ji",
      "Shasha Li",
      "Hao Xu",
      "Jie Yu",
      "Jun Ma",
      "Huijun Liu",
      "Jing Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12720"
  },
  {
    "id": "arXiv:2210.12727",
    "title": "Additive Interventions Yield Robust Multi-Domain Machine Translation  Models",
    "abstract": "Additive interventions are a recently-proposed mechanism for controlling\ntarget-side attributes in neural machine translation. In contrast to tag-based\napproaches which manipulate the raw source sequence, interventions work by\ndirectly modulating the encoder representation of all tokens in the sequence.\nWe examine the role of additive interventions in a large-scale multi-domain\nmachine translation setting and compare its performance in various inference\nscenarios. We find that while the performance difference is small between\nintervention-based systems and tag-based systems when the domain label matches\nthe test domain, intervention-based systems are robust to label error, making\nthem an attractive choice under label uncertainty. Further, we find that the\nsuperiority of single-domain fine-tuning comes under question when training\ndata size is scaled, contradicting previous findings.",
    "descriptor": "\nComments: 7 pages, 7 figures, WMT22 (Research Track)\n",
    "authors": [
      "Elijah Rippeth",
      "Matt Post"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12727"
  },
  {
    "id": "arXiv:2210.12733",
    "title": "Self-supervised Amodal Video Object Segmentation",
    "abstract": "Amodal perception requires inferring the full shape of an object that is\npartially occluded. This task is particularly challenging on two levels: (1) it\nrequires more information than what is contained in the instant retina or\nimaging sensor, (2) it is difficult to obtain enough well-annotated amodal\nlabels for supervision. To this end, this paper develops a new framework of\nSelf-supervised amodal Video object segmentation (SaVos). Our method\nefficiently leverages the visual information of video temporal sequences to\ninfer the amodal mask of objects. The key intuition is that the occluded part\nof an object can be explained away if that part is visible in other frames,\npossibly deformed as long as the deformation can be reasonably learned.\nAccordingly, we derive a novel self-supervised learning paradigm that\nefficiently utilizes the visible object parts as the supervision to guide the\ntraining on videos. In addition to learning type prior to complete masks for\nknown types, SaVos also learns the spatiotemporal prior, which is also useful\nfor the amodal task and could generalize to unseen types. The proposed\nframework achieves the state-of-the-art performance on the synthetic amodal\nsegmentation benchmark FISHBOWL and the real world benchmark KINS-Video-Car.\nFurther, it lends itself well to being transferred to novel distributions using\ntest-time adaptation, outperforming existing models even after the transfer to\na new distribution.",
    "descriptor": "\nComments: accepted in Neurips2022\n",
    "authors": [
      "Jian Yao",
      "Yuxin Hong",
      "Chiyu Wang",
      "Tianjun Xiao",
      "Tong He",
      "Francesco Locatello",
      "David Wipf",
      "Yanwei Fu",
      "Zheng Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12733"
  },
  {
    "id": "arXiv:2210.12736",
    "title": "Achievable Error Exponents for Almost Fixed-Length Hypothesis Testing  and Classification",
    "abstract": "We revisit multiple hypothesis testing and propose a two-phase test, where\neach phase is a fixed-length test and the second-phase proceeds only if a\nreject option is decided in the first phase. We derive achievable error\nexponents of error probabilities under each hypothesis and show that our\ntwo-phase test bridges over fixed-length and sequential tests in the similar\nspirit of Lalitha and Javidi (ISIT, 2016) for binary hypothesis testing.\nSpecifically, our test could achieve the performance close to a sequential test\nwith the asymptotic complexity of a fixed-length test and such test is named\nthe almost fixed-length test. Motivated by practical applications where the\ngenerating distribution under each hypothesis is \\emph{unknown}, we generalize\nour results to the statistical classification framework of Gutman (TIT, 1989).\nWe first consider binary classification and then generalize our results to\n$M$-ary classification. For both cases, we propose a two-phase test, derive\nachievable error exponents and demonstrate that our two-phase test bridges over\nfixed-length and sequential tests. In particular, for $M$-ary classification,\nno final reject option is required to achieve the same exponent as the\nsequential test of Haghifam, Tan, and Khisti (TIT, 2021). Our results\ngeneralize the design and analysis of the almost fixed-length test for binary\nhypothesis testing to broader and more practical families of $M$-ary hypothesis\ntesting and statistical classification.",
    "descriptor": "\nComments: 18 pages, 3 figures\n",
    "authors": [
      "Lin Zhou",
      "Jun Diao",
      "Lin Bai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.12736"
  },
  {
    "id": "arXiv:2210.12737",
    "title": "Granger Causality for Predictability in Dynamic Mode Decomposition",
    "abstract": "The dynamic mode decomposition (DMD) technique extracts the dominant modes\ncharacterizing the innate dynamical behavior of the system within the\nmeasurement data. For appropriate identification of dominant modes from the\nmeasurement data, the DMD algorithm necessitates ensuring the quality of the\ninput measurement data sequences. On that account, for validating the usability\nof the dataset for the DMD algorithm, the paper proposed two conditions:\nPersistence of excitation (PE) and the Granger Causality Test (GCT). The\nvirtual data sequences are designed with the hankel matrix representation such\nthat the dimensions of the subspace spanning the essential system modes are\nincreased with the addition of new state variables. The PE condition provides\nthe lower bound for the trajectory length, and the GCT provides the order of\nthe model. Satisfying the PE condition enables estimating an approximate linear\nmodel, but the predictability with the identified model is only assured with\nthe temporal causation among data searched with GCT. The proposed methodology\nis validated with the application for coherency identification (CI) in a\nmulti-machine power system (MMPS), an essential phenomenon in transient\nstability analysis. The significance of PE condition and GCT is demonstrated\nthrough various case studies implemented on 22 bus six generator system.",
    "descriptor": "",
    "authors": [
      "G. Revati",
      "Syed Shadab",
      "K. Sonam",
      "S. R. Wagh",
      "N. M. Singh"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12737"
  },
  {
    "id": "arXiv:2210.12739",
    "title": "Functional Indirection Neural Estimator for Better Out-of-distribution  Generalization",
    "abstract": "The capacity to achieve out-of-distribution (OOD) generalization is a\nhallmark of human intelligence and yet remains out of reach for machines. This\nremarkable capability has been attributed to our abilities to make conceptual\nabstraction and analogy, and to a mechanism known as indirection, which binds\ntwo representations and uses one representation to refer to the other. Inspired\nby these mechanisms, we hypothesize that OOD generalization may be achieved by\nperforming analogy-making and indirection in the functional space instead of\nthe data space as in current methods. To realize this, we design FINE\n(Functional Indirection Neural Estimator), a neural framework that learns to\ncompose functions that map data input to output on-the-fly. FINE consists of a\nbackbone network and a trainable semantic memory of basis weight matrices. Upon\nseeing a new input-output data pair, FINE dynamically constructs the backbone\nweights by mixing the basis weights. The mixing coefficients are indirectly\ncomputed through querying a separate corresponding semantic memory using the\ndata pair. We demonstrate empirically that FINE can strongly improve\nout-of-distribution generalization on IQ tasks that involve geometric\ntransformations. In particular, we train FINE and competing models on IQ tasks\nusing images from the MNIST, Omniglot and CIFAR100 datasets and test on tasks\nwith unseen image classes from one or different datasets and unseen\ntransformation rules. FINE not only achieves the best performance on all tasks\nbut also is able to adapt to small-scale data scenarios.",
    "descriptor": "\nComments: Accepted paper at NeurIPS 2022\n",
    "authors": [
      "Kha Pham",
      "Hung Le",
      "Man Ngo",
      "Truyen Tran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12739"
  },
  {
    "id": "arXiv:2210.12746",
    "title": "Principal Component Classification",
    "abstract": "We propose to directly compute classification estimates by learning features\nencoded with their class scores. Our resulting model has a encoder-decoder\nstructure suitable for supervised learning, it is computationally efficient and\nperforms well for classification on several datasets.",
    "descriptor": "\nComments: 5 pages; 5 figures; 1 table\n",
    "authors": [
      "Rozenn Dahyot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12746"
  },
  {
    "id": "arXiv:2210.12748",
    "title": "SC-wLS: Towards Interpretable Feed-forward Camera Re-localization",
    "abstract": "Visual re-localization aims to recover camera poses in a known environment,\nwhich is vital for applications like robotics or augmented reality.\nFeed-forward absolute camera pose regression methods directly output poses by a\nnetwork, but suffer from low accuracy. Meanwhile, scene coordinate based\nmethods are accurate, but need iterative RANSAC post-processing, which brings\nchallenges to efficient end-to-end training and inference. In order to have the\nbest of both worlds, we propose a feed-forward method termed SC-wLS that\nexploits all scene coordinate estimates for weighted least squares pose\nregression. This differentiable formulation exploits a weight network imposed\non 2D-3D correspondences, and requires pose supervision only. Qualitative\nresults demonstrate the interpretability of learned weights. Evaluations on\n7Scenes and Cambridge datasets show significantly promoted performance when\ncompared with former feed-forward counterparts. Moreover, our SC-wLS method\nenables a new capability: self-supervised test-time adaptation on the weight\nnetwork. Codes and models are publicly available.",
    "descriptor": "\nComments: ECCV 2022\n",
    "authors": [
      "Xin Wu",
      "Hao Zhao",
      "Shunkai Li",
      "Yingdian Cao",
      "Hongbin Zha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12748"
  },
  {
    "id": "arXiv:2210.12752",
    "title": "UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision  Transformer for Face Forgery Detection",
    "abstract": "Intra-frame inconsistency has been proved to be effective for the\ngeneralization of face forgery detection. However, learning to focus on these\ninconsistency requires extra pixel-level forged location annotations. Acquiring\nsuch annotations is non-trivial. Some existing methods generate large-scale\nsynthesized data with location annotations, which is only composed of real\nimages and cannot capture the properties of forgery regions. Others generate\nforgery location labels by subtracting paired real and fake images, yet such\npaired data is difficult to collected and the generated label is usually\ndiscontinuous. To overcome these limitations, we propose a novel Unsupervised\nInconsistency-Aware method based on Vision Transformer, called UIA-ViT, which\nonly makes use of video-level labels and can learn inconsistency-aware feature\nwithout pixel-level annotations. Due to the self-attention mechanism, the\nattention map among patch embeddings naturally represents the consistency\nrelation, making the vision Transformer suitable for the consistency\nrepresentation learning. Based on vision Transformer, we propose two key\ncomponents: Unsupervised Patch Consistency Learning (UPCL) and Progressive\nConsistency Weighted Assemble (PCWA). UPCL is designed for learning the\nconsistency-related representation with progressive optimized pseudo\nannotations. PCWA enhances the final classification embedding with previous\npatch embeddings optimized by UPCL to further improve the detection\nperformance. Extensive experiments demonstrate the effectiveness of the\nproposed method.",
    "descriptor": "\nComments: accepted by ECCV 2022 (oral)\n",
    "authors": [
      "Wanyi Zhuang",
      "Qi Chu",
      "Zhentao Tan",
      "Qiankun Liu",
      "Haojie Yuan",
      "Changtao Miao",
      "Zixiang Luo",
      "Nenghai Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12752"
  },
  {
    "id": "arXiv:2210.12755",
    "title": "LCPFormer: Towards Effective 3D Point Cloud Analysis via Local Context  Propagation in Transformers",
    "abstract": "Transformer with its underlying attention mechanism and the ability to\ncapture long-range dependencies makes it become a natural choice for unordered\npoint cloud data. However, separated local regions from the general sampling\narchitecture corrupt the structural information of the instances, and the\ninherent relationships between adjacent local regions lack exploration, while\nlocal structural information is crucial in a transformer-based 3D point cloud\nmodel. Therefore, in this paper, we propose a novel module named Local Context\nPropagation (LCP) to exploit the message passing between neighboring local\nregions and make their representations more informative and discriminative.\nMore specifically, we use the overlap points of adjacent local regions (which\nstatistically show to be prevalent) as intermediaries, then re-weight the\nfeatures of these shared points from different local regions before passing\nthem to the next layers. Inserting the LCP module between two transformer\nlayers results in a significant improvement in network expressiveness. Finally,\nwe design a flexible LCPFormer architecture equipped with the LCP module. The\nproposed method is applicable to different tasks and outperforms various\ntransformer-based methods in benchmarks including 3D shape classification and\ndense prediction tasks such as 3D object detection and semantic segmentation.\nCode will be released for reproduction.",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Zhuoxu Huang",
      "Zhiyou Zhao",
      "Banghuai Li",
      "Jungong Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12755"
  },
  {
    "id": "arXiv:2210.12756",
    "title": "VP-SLAM: A Monocular Real-time Visual SLAM with Points, Lines and  Vanishing Points",
    "abstract": "Traditional monocular Visual Simultaneous Localization and Mapping (vSLAM)\nsystems can be divided into three categories: those that use features, those\nthat rely on the image itself, and hybrid models. In the case of feature-based\nmethods, new research has evolved to incorporate more information from their\nenvironment using geometric primitives beyond points, such as lines and planes.\nThis is because in many environments, which are man-made environments,\ncharacterized as Manhattan world, geometric primitives such as lines and planes\noccupy most of the space in the environment. The exploitation of these schemes\ncan lead to the introduction of algorithms capable of optimizing the trajectory\nof a Visual SLAM system and also helping to construct an exuberant map. Thus,\nwe present a real-time monocular Visual SLAM system that incorporates real-time\nmethods for line and VP extraction, as well as two strategies that exploit\nvanishing points to estimate the robot's translation and improve its\nrotation.Particularly, we build on ORB-SLAM2, which is considered the current\nstate-of-the-art solution in terms of both accuracy and efficiency, and extend\nits formulation to handle lines and VPs to create two strategies the first\noptimize the rotation and the second refine the translation part from the known\nrotation. First, we extract VPs using a real-time method and use them for a\nglobal rotation optimization strategy. Second, we present a translation\nestimation method that takes advantage of last-stage rotation optimization to\nmodel a linear system. Finally, we evaluate our system on the TUM RGB-D\nbenchmark and demonstrate that the proposed system achieves state-of-the-art\nresults and runs in real time, and its performance remains close to the\noriginal ORB-SLAM2 system",
    "descriptor": "",
    "authors": [
      "Andreas Georgis",
      "Panagiotis Mermigkas",
      "Petros Maragos"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12756"
  },
  {
    "id": "arXiv:2210.12758",
    "title": "Beta R-CNN: Looking into Pedestrian Detection from Another Perspective",
    "abstract": "Recently significant progress has been made in pedestrian detection, but it\nremains challenging to achieve high performance in occluded and crowded scenes.\nIt could be attributed mostly to the widely used representation of pedestrians,\ni.e., 2D axis-aligned bounding box, which just describes the approximate\nlocation and size of the object. Bounding box models the object as a uniform\ndistribution within the boundary, making pedestrians indistinguishable in\noccluded and crowded scenes due to much noise. To eliminate the problem, we\npropose a novel representation based on 2D beta distribution, named Beta\nRepresentation. It pictures a pedestrian by explicitly constructing the\nrelationship between full-body and visible boxes, and emphasizes the center of\nvisual mass by assigning different probability values to pixels. As a result,\nBeta Representation is much better for distinguishing highly-overlapped\ninstances in crowded scenes with a new NMS strategy named BetaNMS. What's more,\nto fully exploit Beta Representation, a novel pipeline Beta R-CNN equipped with\nBetaHead and BetaMask is proposed, leading to high detection performance in\noccluded and crowded scenes.",
    "descriptor": "\nComments: NeurIPS 2020\n",
    "authors": [
      "Zixuan Xu",
      "Banghuai Li",
      "Ye Yuan",
      "Anhong Dang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12758"
  },
  {
    "id": "arXiv:2210.12760",
    "title": "A study of uncertainty quantification in overparametrized  high-dimensional models",
    "abstract": "Uncertainty quantification is a central challenge in reliable and trustworthy\nmachine learning. Naive measures such as last-layer scores are well-known to\nyield overconfident estimates in the context of overparametrized neural\nnetworks. Several methods, ranging from temperature scaling to different\nBayesian treatments of neural networks, have been proposed to mitigate\noverconfidence, most often supported by the numerical observation that they\nyield better calibrated uncertainty measures. In this work, we provide a sharp\ncomparison between popular uncertainty measures for binary classification in a\nmathematically tractable model for overparametrized neural networks: the random\nfeatures model. We discuss a trade-off between classification accuracy and\ncalibration, unveiling a double descent like behavior in the calibration curve\nof optimally regularized estimators as a function of overparametrization. This\nis in contrast with the empirical Bayes method, which we show to be well\ncalibrated in our setting despite the higher generalization error and\noverparametrization.",
    "descriptor": "",
    "authors": [
      "Lucas Clart\u00e9",
      "Bruno Loureiro",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12760"
  },
  {
    "id": "arXiv:2210.12763",
    "title": "Discriminative Language Model as Semantic Consistency Scorer for  Prompt-based Few-Shot Text Classification",
    "abstract": "This paper proposes a novel prompt-based finetuning method (called DLM-SCS)\nfor few-shot text classification by utilizing the discriminative language model\nELECTRA that is pretrained to distinguish whether a token is original or\ngenerated. The underlying idea is that the prompt instantiated with the true\nlabel should have higher semantic consistency score than other prompts with\nfalse labels. Since a prompt usually consists of several components (or parts),\nits semantic consistency can be decomposed accordingly. The semantic\nconsistency of each component is then computed by making use of the pretrained\nELECTRA model, without introducing extra parameters. Extensive experiments have\nshown that our model outperforms several state-of-the-art prompt-based few-shot\nmethods.",
    "descriptor": "\nComments: 21 pages, 3 figures\n",
    "authors": [
      "Zhipeng Xie",
      "Yahe Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12763"
  },
  {
    "id": "arXiv:2210.12765",
    "title": "Multi-Objective GFlowNets",
    "abstract": "In many applications of machine learning, like drug discovery and material\ndesign, the goal is to generate candidates that simultaneously maximize a set\nof objectives. As these objectives are often conflicting, there is no single\ncandidate that simultaneously maximizes all objectives, but rather a set of\nPareto-optimal candidates where one objective cannot be improved without\nworsening another. Moreover, in practice, these objectives are often\nunder-specified, making the diversity of candidates a key consideration. The\nexisting multi-objective optimization methods focus predominantly on covering\nthe Pareto front, failing to capture diversity in the space of candidates.\nMotivated by the success of GFlowNets for generation of diverse candidates in a\nsingle objective setting, in this paper we consider Multi-Objective GFlowNets\n(MOGFNs). MOGFNs consist of a novel Conditional GFlowNet which models a family\nof single-objective sub-problems derived by decomposing the multi-objective\noptimization problem. Our work is the first to empirically demonstrate\nconditional GFlowNets. Through a series of experiments on synthetic and\nbenchmark tasks, we empirically demonstrate that MOGFNs outperform existing\nmethods in terms of Hypervolume, R2-distance and candidate diversity. We also\ndemonstrate the effectiveness of MOGFNs over existing methods in active\nlearning settings. Finally, we supplement our empirical results with a careful\nanalysis of each component of MOGFNs.",
    "descriptor": "\nComments: 25 pages, 8 figures\n",
    "authors": [
      "Moksh Jain",
      "Sharath Chandra Raparthy",
      "Alex Hernandez-Garcia",
      "Jarrid Rector-Brooks",
      "Yoshua Bengio",
      "Santiago Miret",
      "Emmanuel Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12765"
  },
  {
    "id": "arXiv:2210.12770",
    "title": "On Cross-Domain Pre-Trained Language Models for Clinical Text Mining:  How Do They Perform on Data-Constrained Fine-Tuning?",
    "abstract": "Pre-trained language models (PLMs) have been deployed in many natural\nlanguage processing (NLP) tasks and in various domains. Language model\npre-training from general or mixed domain rich data plus fine-tuning using\nsmall amounts of available data in a low resource domain demonstrated\nbeneficial results by researchers. In this work, we question this statement and\nverify if BERT-based PLMs from the biomedical domain can perform well in\nclinical text mining tasks via fine-tuning. We test the state-of-the-art\nmodels, i.e. Bioformer which is pre-trained on a large amount of biomedical\ndata from PubMed corpus. We use a historical n2c2 clinical NLP challenge\ndataset for fine-tuning its task-adapted version (BioformerApt), and show that\ntheir performances are actually very low. We also present our own end-to-end\nmodel, TransformerCRF, which is developed using Transformer and conditional\nrandom fields (CRFs) as encoder and decoder. We further create a new variation\nmodel by adding a CRF layer on top of PLM Bioformer (BioformerCRF). We\ninvestigate the performances of TransformerCRF on clinical text mining tasks by\ntraining from scratch using a limited amount of data, as well as the model\nBioformerCRF. Experimental evaluation shows that, in a \\textit{constrained\nsetting}, all tested models are \\textit{far from ideal} regarding extreme\nlow-frequency special token recognition, even though they can achieve\nrelatively higher accuracy on overall text tagging. Our models including source\ncodes will be hosted at \\url{https://github.com/poethan/TransformerCRF}.",
    "descriptor": "",
    "authors": [
      "Yuping Wu",
      "Lifeng Han",
      "Valerio Antonini",
      "Goran Nenadic"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12770"
  },
  {
    "id": "arXiv:2210.12775",
    "title": "McQueen: a Benchmark for Multimodal Conversational Query Rewrite",
    "abstract": "The task of query rewrite aims to convert an in-context query to its\nfully-specified version where ellipsis and coreference are completed and\nreferred-back according to the history context. Although much progress has been\nmade, less efforts have been paid to real scenario conversations that involve\ndrawing information from more than one modalities. In this paper, we propose\nthe task of multimodal conversational query rewrite (McQR), which performs\nquery rewrite under the multimodal visual conversation setting. We collect a\nlarge-scale dataset named McQueen based on manual annotation, which contains\n15k visual conversations and over 80k queries where each one is associated with\na fully-specified rewrite version. In addition, for entities appearing in the\nrewrite, we provide the corresponding image box annotation. We then use the\nMcQueen dataset to benchmark a state-of-the-art method for effectively tackling\nthe McQR task, which is based on a multimodal pre-trained model with pointer\ngenerator. Extensive experiments are performed to demonstrate the effectiveness\nof our model on this task\\footnote{The dataset and code of this paper are both\navailable in \\url{https://github.com/yfyuan01/MQR}",
    "descriptor": "\nComments: Accepted by EMNLP22\n",
    "authors": [
      "Yifei Yuan",
      "Chen Shi",
      "Runze Wang",
      "Liyi Chen",
      "Feijun Jiang",
      "Yuan You",
      "Wai Lam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12775"
  },
  {
    "id": "arXiv:2210.12776",
    "title": "To Signal or Not to Signal? Layering Traffic Analysis Resistance on  Secure Instant Messaging",
    "abstract": "Traffic analysis for instant messaging (IM) applications continues to pose an\nimportant privacy challenge. In particular, transport-level data can leak\nunintentional information about IM -- such as who communicates with whom.\nExisting tools for metadata privacy have adoption obstacles, including the\nrisks of being scrutinized for having a particular app installed, and\nperformance overheads incompatible with mobile devices. We posit that\nresilience to traffic analysis must be directly supported by major IM services\nthemselves, and must be done in a low-latency manner without breaking existing\nfeatures. As a first step in this direction, we propose a hybrid model that\ncombines regular network traffic and deniable messages. We present a novel\nprotocol for deniable instant messaging that we call DenIM that is a variant of\nthe Signal protocol. DenIM is built on the principle that deniable messages can\nbe incorporated as part of padding in regular traffic. By padding traffic,\nDenIM achieves bandwidth overhead that scales with the volume of regular\ntraffic, as opposed to scaling with time or the number of users. To show the\neffectiveness of DenIM, we construct a formal model and prove that DenIM's\ndeniability guarantees hold against strong adversaries such as internet service\nproviders, and implement and empirically evaluate a proof-of-concept version of\nDenIM.",
    "descriptor": "",
    "authors": [
      "Boel Nelson",
      "Elena Pagnin",
      "Aslan Askarov"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12776"
  },
  {
    "id": "arXiv:2210.12777",
    "title": "Retrieve, Reason, and Refine: Generating Accurate and Faithful Patient  Instructions",
    "abstract": "The \"Patient Instruction\" (PI), which contains critical instructional\ninformation provided both to carers and to the patient at the time of\ndischarge, is essential for the patient to manage their condition outside\nhospital. An accurate and easy-to-follow PI can improve the self-management of\npatients which can in turn reduce hospital readmission rates. However, writing\nan appropriate PI can be extremely time-consuming for physicians, and is\nsubject to being incomplete or error-prone for (potentially overworked)\nphysicians. Therefore, we propose a new task that can provide an objective\nmeans of avoiding incompleteness, while reducing clinical workload: the\nautomatic generation of the PI, which is imagined as being a document that the\nclinician can review, modify, and approve as necessary (rather than taking the\nhuman \"out of the loop\"). We build a benchmark clinical dataset and propose the\nRe3Writer, which imitates the working patterns of physicians to first retrieve\nrelated working experience from historical PIs written by physicians, then\nreason related medical knowledge. Finally, it refines the retrieved working\nexperience and reasoned medical knowledge to extract useful information, which\nis used to generate the PI for previously-unseen patient according to their\nhealth records during hospitalization. Our experiments show that, using our\nmethod, the performance of five different models can be substantially boosted\nacross all metrics, with up to 20%, 11%, and 19% relative improvements in\nBLEU-4, ROUGE-L, and METEOR, respectively. Meanwhile, we show results from\nhuman evaluations to measure the effectiveness in terms of its usefulness for\nclinical practice. The code is available at\nhttps://github.com/AI-in-Hospitals/Patient-Instructions",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. (Thirty-sixth Conference on Neural Information Processing Systems)\n",
    "authors": [
      "Fenglin Liu",
      "Bang Yang",
      "Chenyu You",
      "Xian Wu",
      "Shen Ge",
      "Zhangdaihong Liu",
      "Xu Sun",
      "Yang Yang",
      "David A. Clifton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12777"
  },
  {
    "id": "arXiv:2210.12778",
    "title": "Local and Global Structure Preservation Based Spectral Clustering",
    "abstract": "Spectral Clustering (SC) is widely used for clustering data on a nonlinear\nmanifold. SC aims to cluster data by considering the preservation of the local\nneighborhood structure on the manifold data. This paper extends Spectral\nClustering to Local and Global Structure Preservation Based Spectral Clustering\n(LGPSC) that incorporates both global structure and local neighborhood\nstructure simultaneously. For this extension, LGPSC proposes two models to\nextend local structures preservation to local and global structures\npreservation: Spectral clustering guided Principal component analysis model and\nMultilevel model. Finally, we compare the experimental results of the\nstate-of-the-art methods with our two models of LGPSC on various data sets such\nthat the experimental results confirm the effectiveness of our LGPSC models to\ncluster nonlinear data.",
    "descriptor": "\nComments: 19pages, 2 figures, 5 tables\n",
    "authors": [
      "Kajal Eybpoosh",
      "Mansoor Rezghi",
      "Abbas Heydari"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12778"
  },
  {
    "id": "arXiv:2210.12782",
    "title": "Compressing Explicit Voxel Grid Representations: fast NeRFs become also  small",
    "abstract": "NeRFs have revolutionized the world of per-scene radiance field\nreconstruction because of their intrinsic compactness. One of the main\nlimitations of NeRFs is their slow rendering speed, both at training and\ninference time. Recent research focuses on the optimization of an explicit\nvoxel grid (EVG) that represents the scene, which can be paired with neural\nnetworks to learn radiance fields. This approach significantly enhances the\nspeed both at train and inference time, but at the cost of large memory\noccupation. In this work we propose Re:NeRF, an approach that specifically\ntargets EVG-NeRFs compressibility, aiming to reduce memory storage of NeRF\nmodels while maintaining comparable performance. We benchmark our approach with\nthree different EVG-NeRF architectures on four popular benchmarks, showing\nRe:NeRF's broad usability and effectiveness.",
    "descriptor": "",
    "authors": [
      "Chenxi Lola Deng",
      "Enzo Tartaglione"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12782"
  },
  {
    "id": "arXiv:2210.12785",
    "title": "An Improved RaftStereo Trained with A Mixed Dataset for the Robust  Vision Challenge 2022",
    "abstract": "Stereo-matching is a fundamental problem in computer vision. Despite recent\nprogress by deep learning, improving the robustness is ineluctable when\ndeploying stereo-matching models to real-world applications. Different from the\ncommon practices, i.e., developing an elaborate model to achieve robustness, we\nargue that collecting multiple available datasets for training is a cheaper way\nto increase generalization ability. Specifically, this report presents an\nimproved RaftStereo trained with a mixed dataset of seven public datasets for\nthe robust vision challenge (denoted as iRaftStereo_RVC). When evaluated on the\ntraining sets of Middlebury, KITTI-2015, and ETH3D, the model outperforms its\ncounterparts trained with only one dataset, such as the popular Sceneflow.\nAfter fine-tuning the pre-trained model on the three datasets of the challenge,\nit ranks at 2nd place on the stereo leaderboard, demonstrating the benefits of\nmixed dataset pre-training.",
    "descriptor": "\nComments: Technical report; Ranking at 2nd on the stereo track of Robust Vision Challenge 2022\n",
    "authors": [
      "Hualie Jiang",
      "Rui Xu",
      "Wenjie Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12785"
  },
  {
    "id": "arXiv:2210.12786",
    "title": "When Can Transformers Ground and Compose: Insights from Compositional  Generalization Benchmarks",
    "abstract": "Humans can reason compositionally whilst grounding language utterances to the\nreal world. Recent benchmarks like ReaSCAN use navigation tasks grounded in a\ngrid world to assess whether neural models exhibit similar capabilities. In\nthis work, we present a simple transformer-based model that outperforms\nspecialized architectures on ReaSCAN and a modified version of gSCAN. On\nanalyzing the task, we find that identifying the target location in the grid\nworld is the main challenge for the models. Furthermore, we show that a\nparticular split in ReaSCAN, which tests depth generalization, is unfair. On an\namended version of this split, we show that transformers can generalize to\ndeeper input structures. Finally, we design a simpler grounded compositional\ngeneralization task, RefEx, to investigate how transformers reason\ncompositionally. We show that a single self-attention layer with a single head\ngeneralizes to novel combinations of object attributes. Moreover, we derive a\nprecise mathematical construction of the transformer's computations from the\nlearned network. Overall, we provide valuable insights about the grounded\ncompositional generalization task and the behaviour of transformers on it,\nwhich would be useful for researchers working in this area.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ankur Sikarwar",
      "Arkil Patel",
      "Navin Goyal"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12786"
  },
  {
    "id": "arXiv:2210.12787",
    "title": "Respecting Transfer Gap in Knowledge Distillation",
    "abstract": "Knowledge distillation (KD) is essentially a process of transferring a\nteacher model's behavior, e.g., network response, to a student model. The\nnetwork response serves as additional supervision to formulate the machine\ndomain, which uses the data collected from the human domain as a transfer set.\nTraditional KD methods hold an underlying assumption that the data collected in\nboth human domain and machine domain are both independent and identically\ndistributed (IID). We point out that this naive assumption is unrealistic and\nthere is indeed a transfer gap between the two domains. Although the gap offers\nthe student model external knowledge from the machine domain, the imbalanced\nteacher knowledge would make us incorrectly estimate how much to transfer from\nteacher to student per sample on the non-IID transfer set. To tackle this\nchallenge, we propose Inverse Probability Weighting Distillation (IPWD) that\nestimates the propensity score of a training sample belonging to the machine\ndomain, and assigns its inverse amount to compensate for under-represented\nsamples. Experiments on CIFAR-100 and ImageNet demonstrate the effectiveness of\nIPWD for both two-stage distillation and one-stage self-distillation.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Yulei Niu",
      "Long Chen",
      "Chang Zhou",
      "Hanwang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12787"
  },
  {
    "id": "arXiv:2210.12789",
    "title": "Clustering-based Tile Embedding (CTE): A General Representation for  Level Design with Skewed Tile Distributions",
    "abstract": "There has been significant research interest in Procedural Level Generation\nvia Machine Learning (PLGML), applying ML techniques to automated level\ngeneration. One recent trend is in the direction of learning representations\nfor level design via embeddings, such as tile embeddings. Tile Embeddings are\ncontinuous vector representations of game levels unifying their visual,\ncontextual and behavioural information. However, the original tile embedding\nstruggled to generate levels with skewed tile distributions. For instance,\nSuper Mario Bros. (SMB) wherein a majority of tiles represent the background.\nTo remedy this, we present a modified tile embedding representation referred to\nas Clustering-based Tile Embedding (CTE). Further, we employ clustering to\ndiscretize the continuous CTE representation and present a novel two-step level\ngeneration to leverage both these representations. We evaluate the performance\nof our approach in generating levels for seen and unseen games with skewed tile\ndistributions and outperform the original tile embeddings.",
    "descriptor": "\nComments: 11 pages, 6 figures, 4 tables\n",
    "authors": [
      "Mrunal Jadhav",
      "Matthew Guzdial"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12789"
  },
  {
    "id": "arXiv:2210.12795",
    "title": "Realistic Data Augmentation Framework for Enhancing Tabular Reasoning",
    "abstract": "Existing approaches to constructing training data for Natural Language\nInference (NLI) tasks, such as for semi-structured table reasoning, are either\nvia crowdsourcing or fully automatic methods. However, the former is expensive\nand time-consuming and thus limits scale, and the latter often produces naive\nexamples that may lack complex reasoning. This paper develops a realistic\nsemi-automated framework for data augmentation for tabular inference. Instead\nof manually generating a hypothesis for each table, our methodology generates\nhypothesis templates transferable to similar tables. In addition, our framework\nentails the creation of rational counterfactual tables based on human written\nlogical constraints and premise paraphrasing. For our case study, we use the\nInfoTabs, which is an entity-centric tabular inference dataset. We observed\nthat our framework could generate human-like tabular inference examples, which\ncould benefit training data augmentation, especially in the scenario with\nlimited supervision.",
    "descriptor": "\nComments: The 2022 Conference on Empirical Methods in Natural Language Processing\n",
    "authors": [
      "Dibyakanti Kumar",
      "Vivek Gupta",
      "Soumya Sharma",
      "Shuo Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12795"
  },
  {
    "id": "arXiv:2210.12798",
    "title": "MM-Align: Learning Optimal Transport-based Alignment Dynamics for Fast  and Accurate Inference on Missing Modality Sequences",
    "abstract": "Existing multimodal tasks mostly target at the complete input modality\nsetting, i.e., each modality is either complete or completely missing in both\ntraining and test sets. However, the randomly missing situations have still\nbeen underexplored. In this paper, we present a novel approach named MM-Align\nto address the missing-modality inference problem. Concretely, we propose 1) an\nalignment dynamics learning module based on the theory of optimal transport\n(OT) for indirect missing data imputation; 2) a denoising training algorithm to\nsimultaneously enhance the imputation results and backbone network performance.\nCompared with previous methods which devote to reconstructing the missing\ninputs, MM-Align learns to capture and imitate the alignment dynamics between\nmodality sequences. Results of comprehensive experiments on three datasets\ncovering two multimodal tasks empirically demonstrate that our method can\nperform more accurate and faster inference and relieve overfitting under\nvarious missing conditions.",
    "descriptor": "\nComments: Accepted as a long paper at EMNLP 2022\n",
    "authors": [
      "Wei Han",
      "Hui Chen",
      "Min-Yen Kan",
      "Soujanya Poria"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12798"
  },
  {
    "id": "arXiv:2210.12802",
    "title": "Translation Word-Level Auto-Completion: What can we achieve out of the  box?",
    "abstract": "Research on Machine Translation (MT) has achieved important breakthroughs in\nseveral areas. While there is much more to be done in order to build on this\nsuccess, we believe that the language industry needs better ways to take full\nadvantage of current achievements. Due to a combination of factors, including\ntime, resources, and skills, businesses tend to apply pragmatism into their AI\nworkflows. Hence, they concentrate more on outcomes, e.g. delivery, shipping,\nreleases, and features, and adopt high-level working production solutions,\nwhere possible. Among the features thought to be helpful for translators are\nsentence-level and word-level translation auto-suggestion and auto-completion.\nSuggesting alternatives can inspire translators and limit their need to refer\nto external resources, which hopefully boosts their productivity. This work\ndescribes our submissions to WMT's shared task on word-level auto-completion,\nfor the Chinese-to-English, English-to-Chinese, German-to-English, and\nEnglish-to-German language directions. We investigate the possibility of using\npre-trained models and out-of-the-box features from available libraries. We\nemploy random sampling to generate diverse alternatives, which reveals good\nresults. Furthermore, we introduce our open-source API, based on CTranslate2,\nto serve translations, auto-suggestions, and auto-completions.",
    "descriptor": "\nComments: WMT 2022\n",
    "authors": [
      "Yasmin Moslem",
      "Rejwanul Haque",
      "Andy Way"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.12802"
  },
  {
    "id": "arXiv:2210.12803",
    "title": "lqgnet: hybrid model-based and data-driven linear quadratic stochastic  control",
    "abstract": "Stochastic control deals with finding an optimal control signal for a\ndynamical system in a setting with uncertainty, playing a key role in numerous\napplications. The linear quadratic Gaussian (LQG) is a widely-used setting,\nwhere the system dynamics is represented as a linear Gaussian statespace (SS)\nmodel, and the objective function is quadratic. For this setting, the optimal\ncontroller is obtained in closed form by the separation principle. However, in\npractice, the underlying system dynamics often cannot be faithfully captured by\na fully known linear Gaussian SS model, limiting its performance. Here, we\npresent LQGNet, a stochastic controller that leverages data to operate under\npartially known dynamics. LQGNet augments the state tracking module of\nseparation-based control with a dedicated trainable algorithm. The resulting\nsystem preserves the operation of classic LQG control while learning to cope\nwith partially known SS models without having to fully identify the dynamics.\nWe empirically show that LQGNet outperforms classic stochastic control by\novercoming mismatched SS models.",
    "descriptor": "\nComments: Submitted to ICASSP23\n",
    "authors": [
      "Solomon Goldgraber Casspi",
      "Oliver Husser",
      "Guy Revach",
      "Nir Shlezinger"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12803"
  },
  {
    "id": "arXiv:2210.12805",
    "title": "Optimal Design of Volt/VAR Control Rules for Inverter-Interfaced  Distributed Energy Resources",
    "abstract": "The IEEE 1547 Standard for the interconnection of distributed energy\nresources (DERs) to distribution grids provisions that smart inverters could be\nimplementing Volt/VAR control rules among other options. Such rules enable DERs\nto respond autonomously in response to time-varying grid loading conditions.\nThe rules comprise affine droop control augmented with a deadband and\nsaturation regions. Nonetheless, selecting the shape of these rules is not an\nobvious task, and the default options may not be optimal or dynamically stable.\nTo this end, this work develops a novel methodology for customizing Volt/VAR\nrules on a per-bus basis for a single-phase feeder. The rules are adjusted by\nthe utility every few hours depending on anticipated demand and solar\nscenarios. Using a projected gradient descent-based algorithm, rules are\ndesigned to improve the feeder's voltage profile, comply with IEEE 1547\nconstraints, and guarantee stability of the underlying nonlinear grid dynamics.\nThe stability region is inner approximated by a polytope and the rules are\njudiciously parameterized so their feasible set is convex. Numerical tests\nusing real-world data on the IEEE 141-bus feeder corroborate the scalability of\nthe methodology and its explore the trade-offs of Volt/VAR control with\nalternatives.",
    "descriptor": "",
    "authors": [
      "Ilgiz Murzakhanov",
      "Sarthak Gupta",
      "Spyros Chatzivasileiadis",
      "Vassilis Kekatos"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12805"
  },
  {
    "id": "arXiv:2210.12806",
    "title": "Active Exploration for Robotic Manipulation",
    "abstract": "Robotic manipulation stands as a largely unsolved problem despite significant\nadvances in robotics and machine learning in recent years. One of the key\nchallenges in manipulation is the exploration of the dynamics of the\nenvironment when there is continuous contact between the objects being\nmanipulated. This paper proposes a model-based active exploration approach that\nenables efficient learning in sparse-reward robotic manipulation tasks. The\nproposed method estimates an information gain objective using an ensemble of\nprobabilistic models and deploys model predictive control (MPC) to plan actions\nonline that maximize the expected reward while also performing directed\nexploration. We evaluate our proposed algorithm in simulation and on a real\nrobot, trained from scratch with our method, on a challenging ball pushing task\non tilted tables, where the target ball position is not known to the agent\na-priori. Our real-world robot experiment serves as a fundamental application\nof active exploration in model-based reinforcement learning of complex robotic\nmanipulation tasks.",
    "descriptor": "\nComments: Published without appendix at \"International Conference on Intelligent Robots and Systems (IROS)\" 2022\n",
    "authors": [
      "Tim Schneider",
      "Boris Belousov",
      "Georgia Chalvatzaki",
      "Diego Romeres",
      "Devesh K. Jha",
      "Jan Peters"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12806"
  },
  {
    "id": "arXiv:2210.12808",
    "title": "Flow-Level Packet Loss Detection via Sketch Decomposition and Matrix  Optimization",
    "abstract": "For cloud service providers, fine-grained packet loss detection across data\ncenters is crucial in improving their service level and increasing business\nincome. However, the inability to obtain sufficient measurements makes it\ndifficult owing to the fundamental limit that the wide-area network links\nresponsible for communication are not under their management. Moreover,\nmillisecond-level delay jitter and clock synchronization errors in the WAN\ndisable many tools that perform well in data center networks on this issue.\nTherefore, there is an urgent need to develop a new tool or method. In this\nwork, we propose SketchDecomp, a novel loss detection method, from a\nmathematical perspective that has never been considered before. Its key is to\ndecompose sketches upstream and downstream into several sub-sketches and builds\na low-rank matrix optimization model to solve them. Extensive experiments on\nthe test bed demonstrate its superiority.",
    "descriptor": "",
    "authors": [
      "Zhenyu Ming",
      "Wei Zhang",
      "Yanwei Xu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.12808"
  },
  {
    "id": "arXiv:2210.12809",
    "title": "Automated Essay Scoring using Transformers",
    "abstract": "Despite being investigated for over five decades, the task of automated essay\nscoring continues to draw a lot of attention in the NLP community, in part\nbecause of its commercial and educational values as well as the associated\nresearch challenges. Large pre-trained models have made remarkable progress in\nNLP. Data augmentation techniques have also helped build state-of-the-art\nmodels for automated essay scoring. Many works in the past have attempted to\nsolve this problem by using RNNs, LSTMs, etc. This work examines the\ntransformer models like BERT, RoBERTa, etc. We empirically demonstrate the\neffectiveness of transformer models and data augmentation for automated essay\ngrading across many topics using a single model.",
    "descriptor": "",
    "authors": [
      "Kshitij Gupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12809"
  },
  {
    "id": "arXiv:2210.12810",
    "title": "Code4Struct: Code Generation for Few-Shot Structured Prediction from  Natural Language",
    "abstract": "Large Language Model (LLM) trained on the mixture of text and code has\ndemonstrated impressive capability in translating natural language (NL) into\nstructured code. In this work, we propose Code4Struct to leverage such\ntext-to-structure translation capability to tackle structured prediction tasks\nin NLP. For example, Event Argument Extraction (EAE) aims to convert text into\nevent-argument structures that can be represented as a class object using code.\nThis alignment between structures and code enables us to take advantage of\nProgramming Language (PL) features such as inheritance and type annotation to\nintroduce external knowledge or add constraints with ease. We exploit the\nanalogy between PL and NLP problems, and, as a case study, we use Code4Struct\nto tackle the EAE task using code generation. We ask a LLM to generate code to\ninstantiate an event class with predicted arguments given a NL sentence.\nDespite only using 50 training instances for each event type, Code4Struct is\ncomparable to fully-supervised models trained on 4,202 event instances and,\nwhen given the same 50-shot data, outperforms current state-of-the-art (SOTA)\nby 20.8% absolute F1. When prompted with hierarchical event types implemented\nusing inheritance, Code4Struct can predict arguments for low-resource event\ntypes using 10-shot training instances from its sibling event type and\noutperforms zero-shot baseline by 12% absolute F1.",
    "descriptor": "",
    "authors": [
      "Xingyao Wang",
      "Sha Li",
      "Heng Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12810"
  },
  {
    "id": "arXiv:2210.12813",
    "title": "TAPE: Assessing Few-shot Russian Language Understanding",
    "abstract": "Recent advances in zero-shot and few-shot learning have shown promise for a\nscope of research and practical purposes. However, this fast-growing area lacks\nstandardized evaluation suites for non-English languages, hindering progress\noutside the Anglo-centric paradigm. To address this line of research, we\npropose TAPE (Text Attack and Perturbation Evaluation), a novel benchmark that\nincludes six more complex NLU tasks for Russian, covering multi-hop reasoning,\nethical concepts, logic and commonsense knowledge. The TAPE's design focuses on\nsystematic zero-shot and few-shot NLU evaluation: (i) linguistic-oriented\nadversarial attacks and perturbations for analyzing robustness, and (ii)\nsubpopulations for nuanced interpretation. The detailed analysis of testing the\nautoregressive baselines indicates that simple spelling-based perturbations\naffect the performance the most, while paraphrasing the input has a more\nnegligible effect. At the same time, the results demonstrate a significant gap\nbetween the neural and human baselines for most tasks. We publicly release TAPE\n(tape-benchmark.com) to foster research on robust LMs that can generalize to\nnew tasks when little to no supervision is available.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 Findings\n",
    "authors": [
      "Ekaterina Taktasheva",
      "Tatiana Shavrina",
      "Alena Fenogenova",
      "Denis Shevelev",
      "Nadezhda Katricheva",
      "Maria Tikhonova",
      "Albina Akhmetgareeva",
      "Oleg Zinkevich",
      "Anastasiia Bashmakova",
      "Svetlana Iordanskaia",
      "Alena Spiridonova",
      "Valentina Kurenshchikova",
      "Ekaterina Artemova",
      "Vladislav Mikhailov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12813"
  },
  {
    "id": "arXiv:2210.12814",
    "title": "RuCoLA: Russian Corpus of Linguistic Acceptability",
    "abstract": "Linguistic acceptability (LA) attracts the attention of the research\ncommunity due to its many uses, such as testing the grammatical knowledge of\nlanguage models and filtering implausible texts with acceptability classifiers.\nHowever, the application scope of LA in languages other than English is limited\ndue to the lack of high-quality resources. To this end, we introduce the\nRussian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up\nunder the well-established binary LA approach. RuCoLA consists of $9.8$k\nin-domain sentences from linguistic publications and $3.6$k out-of-domain\nsentences produced by generative models. The out-of-domain set is created to\nfacilitate the practical use of acceptability for improving language\ngeneration. Our paper describes the data collection protocol and presents a\nfine-grained analysis of acceptability classification experiments with a range\nof baseline approaches. In particular, we demonstrate that the most widely used\nlanguage models still fall behind humans by a large margin, especially when\ndetecting morphological and semantic errors. We release RuCoLA, the code of\nexperiments, and a public leaderboard (rucola-benchmark.com) to assess the\nlinguistic competence of language models for Russian.",
    "descriptor": "\nComments: Accepted to the EMNLP 2022 main conference\n",
    "authors": [
      "Vladislav Mikhailov",
      "Tatiana Shamardina",
      "Max Ryabinin",
      "Alena Pestova",
      "Ivan Smurov",
      "Ekaterina Artemova"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12814"
  },
  {
    "id": "arXiv:2210.12816",
    "title": "Simple Alternating Minimization Provably Solves Complete Dictionary  Learning",
    "abstract": "This paper focuses on complete dictionary learning problem, where the goal is\nto reparametrize a set of given signals as linear combinations of atoms from a\nlearned dictionary. There are two main challenges faced by theoretical and\npractical studies of dictionary learning: the lack of theoretical guarantees\nfor practically-used heuristic algorithms, and their poor scalability when\ndealing with huge-scale datasets. Towards addressing these issues, we show that\nwhen the dictionary to be learned is orthogonal, that an alternating\nminimization method directly applied to the nonconvex and discrete formulation\nof the problem exactly recovers the ground truth. For the huge-scale,\npotentially online setting, we propose a minibatch version of our algorithm,\nwhich can provably learn a complete dictionary from a huge-scale dataset with\nminimal sample complexity, linear sparsity level, and linear convergence rate,\nthereby negating the need for any convex relaxation for the problem. Our\nnumerical experiments showcase the superiority of our method compared with the\nexisting techniques when applied to tasks on real data.",
    "descriptor": "",
    "authors": [
      "Geyu Liang",
      "Gavin Zhang",
      "Salar Fattahi",
      "Richard Y. Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12816"
  },
  {
    "id": "arXiv:2210.12817",
    "title": "Complexity of the Boundary-Guarding Art Gallery Problem",
    "abstract": "We resolve the complexity of the boundary-guarding variant of the art gallery\nproblem, showing that it is $\\exists\\mathbb{R}$-complete, meaning that it is\nequivalent under polynomial time reductions to deciding whether a polynomial\nsystem of equations has a real solution. Introduced by Victor Klee in 1973, the\nart gallery problem concerns finding configurations of \\emph{guards} which\ntogether can see every point inside of an \\emph{art gallery} shaped like a\nsimple polygon. The original version of this problem has previously been shown\nto $\\exists\\mathbb{R}$-hard, but until now the complexity of the variant where\nguards only need to guard the walls of the art gallery was an open problem. Our\nresults can also be used to provide a simpler proof of the\n$\\exists\\mathbb{R}$-hardness of the standard art gallery problem. In\nparticular, we show how the algebraic constraints describing a polynomial\nsystem of equations can somewhat naturally occur in an art gallery setting.",
    "descriptor": "\nComments: 24 pages, 27 figures\n",
    "authors": [
      "Jack Stade"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.12817"
  },
  {
    "id": "arXiv:2210.12818",
    "title": "Pushing the Efficiency Limit Using Structured Sparse Convolutions",
    "abstract": "Weight pruning is among the most popular approaches for compressing deep\nconvolutional neural networks. Recent work suggests that in a randomly\ninitialized deep neural network, there exist sparse subnetworks that achieve\nperformance comparable to the original network. Unfortunately, finding these\nsubnetworks involves iterative stages of training and pruning, which can be\ncomputationally expensive. We propose Structured Sparse Convolution (SSC),\nwhich leverages the inherent structure in images to reduce the parameters in\nthe convolutional filter. This leads to improved efficiency of convolutional\narchitectures compared to existing methods that perform pruning at\ninitialization. We show that SSC is a generalization of commonly used layers\n(depthwise, groupwise and pointwise convolution) in ``efficient\narchitectures.'' Extensive experiments on well-known CNN models and datasets\nshow the effectiveness of the proposed method. Architectures based on SSC\nachieve state-of-the-art performance compared to baselines on CIFAR-10,\nCIFAR-100, Tiny-ImageNet, and ImageNet classification benchmarks.",
    "descriptor": "\nComments: Accepted at the IEEE Winter Conference on Applications of Computer Vision, WACV 2023\n",
    "authors": [
      "Vinay Kumar Verma",
      "Nikhil Mehta",
      "Shijing Si",
      "Ricardo Henao",
      "Lawrence Carin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12818"
  },
  {
    "id": "arXiv:2210.12820",
    "title": "An Interpretable Deep Semantic Segmentation Method for Earth Observation",
    "abstract": "Earth observation is fundamental for a range of human activities including\nflood response as it offers vital information to decision makers. Semantic\nsegmentation plays a key role in mapping the raw hyper-spectral data coming\nfrom the satellites into a human understandable form assigning class labels to\neach pixel. In this paper, we introduce a prototype-based interpretable deep\nsemantic segmentation (IDSS) method, which is highly accurate as well as\ninterpretable. Its parameters are in orders of magnitude less than the number\nof parameters used by deep networks such as U-Net and are clearly interpretable\nby humans. The proposed here IDSS offers a transparent structure that allows\nusers to inspect and audit the algorithm's decision. Results have demonstrated\nthat IDSS could surpass other algorithms, including U-Net, in terms of IoU\n(Intersection over Union) total water and Recall total water. We used\nWorldFloods data set for our experiments and plan to use the semantic\nsegmentation results combined with masks for permanent water to detect flood\nevents.",
    "descriptor": "",
    "authors": [
      "Ziyang Zhang",
      "Plamen Angelov",
      "Eduardo Soares",
      "Nicolas Longepe",
      "Pierre Philippe Mathieu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12820"
  },
  {
    "id": "arXiv:2210.12826",
    "title": "Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization",
    "abstract": "We introduce an approach to generating videos based on a series of given\nlanguage descriptions. Frames of the video are generated sequentially and\noptimized by guidance from the CLIP image-text encoder; iterating through\nlanguage descriptions, weighting the current description higher than others. As\nopposed to optimizing through an image generator model itself, which tends to\nbe computationally heavy, the proposed approach computes the CLIP loss directly\nat the pixel level, achieving general content at a speed suitable for near\nreal-time systems. The approach can generate videos in up to 720p resolution,\nvariable frame-rates, and arbitrary aspect ratios at a rate of 1-2 frames per\nsecond. Please visit our website to view videos and access our open-source\ncode: https://pschaldenbrand.github.io/text2video/ .",
    "descriptor": "",
    "authors": [
      "Peter Schaldenbrand",
      "Zhixuan Liu",
      "Jean Oh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12826"
  },
  {
    "id": "arXiv:2210.12828",
    "title": "Towards Pragmatic Production Strategies for Natural Language Generation  Tasks",
    "abstract": "This position paper proposes a conceptual framework for the design of Natural\nLanguage Generation (NLG) systems that follow efficient and effective\nproduction strategies in order to achieve complex communicative goals. In this\ngeneral framework, efficiency is characterised as the parsimonious regulation\nof production and comprehension costs while effectiveness is measured with\nrespect to task-oriented and contextually grounded communicative goals. We\nprovide concrete suggestions for the estimation of goals, costs, and utility\nvia modern statistical methods, demonstrating applications of our framework to\nthe classic pragmatic task of visually grounded referential games and to\nabstractive text summarisation, two popular generation tasks with real-world\napplications. In sum, we advocate for the development of NLG systems that learn\nto make pragmatic production decisions from experience, by reasoning about\ngoals, costs, and utility in a human-like way.",
    "descriptor": "\nComments: In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)\n",
    "authors": [
      "Mario Giulianelli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12828"
  },
  {
    "id": "arXiv:2210.12841",
    "title": "A Cooperative Reinforcement Learning Environment for Detecting and  Penalizing Betrayal",
    "abstract": "In this paper we present a Reinforcement Learning environment that leverages\nagent cooperation and communication, aimed at detection, learning and\nultimately penalizing betrayal patterns that emerge in the behavior of\nself-interested agents. We provide a description of game rules, along with\ninteresting cases of betrayal and trade-offs that arise. Preliminary\nexperimental investigations illustrate a) betrayal emergence, b) deceptive\nagents outperforming honest baselines and b) betrayal detection based on\nclassification of behavioral features, which surpasses probabilistic detection\nbaselines. Finally, we propose approaches for penalizing betrayal, list\ndirections for future work and suggest interesting extensions of the\nenvironment towards capturing and exploring increasingly complex patterns of\nsocial interactions.",
    "descriptor": "",
    "authors": [
      "Nikiforos Pittaras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.12841"
  },
  {
    "id": "arXiv:2210.12843",
    "title": "Delving into Masked Autoencoders for Multi-Label Thorax Disease  Classification",
    "abstract": "Vision Transformer (ViT) has become one of the most popular neural\narchitectures due to its great scalability, computational efficiency, and\ncompelling performance in many vision tasks. However, ViT has shown inferior\nperformance to Convolutional Neural Network (CNN) on medical tasks due to its\ndata-hungry nature and the lack of annotated medical data. In this paper, we\npre-train ViTs on 266,340 chest X-rays using Masked Autoencoders (MAE) which\nreconstruct missing pixels from a small part of each image. For comparison,\nCNNs are also pre-trained on the same 266,340 X-rays using advanced\nself-supervised methods (e.g., MoCo v2). The results show that our pre-trained\nViT performs comparably (sometimes better) to the state-of-the-art CNN\n(DenseNet-121) for multi-label thorax disease classification. This performance\nis attributed to the strong recipes extracted from our empirical studies for\npre-training and fine-tuning ViT. The pre-training recipe signifies that\nmedical reconstruction requires a much smaller proportion of an image (10% vs.\n25%) and a more moderate random resized crop range (0.5~1.0 vs. 0.2~1.0)\ncompared with natural imaging. Furthermore, we remark that in-domain transfer\nlearning is preferred whenever possible. The fine-tuning recipe discloses that\nlayer-wise LR decay, RandAug magnitude, and DropPath rate are significant\nfactors to consider. We hope that this study can direct future research on the\napplication of Transformers to a larger variety of medical imaging tasks.",
    "descriptor": "\nComments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2023)\n",
    "authors": [
      "Junfei Xiao",
      "Yutong Bai",
      "Alan Yuille",
      "Zongwei Zhou"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12843"
  },
  {
    "id": "arXiv:2210.12846",
    "title": "EUREKA: EUphemism Recognition Enhanced through Knn-based methods and  Augmentation",
    "abstract": "We introduce EUREKA, an ensemble-based approach for performing automatic\neuphemism detection. We (1) identify and correct potentially mislabelled rows\nin the dataset, (2) curate an expanded corpus called EuphAug, (3) leverage\nmodel representations of Potentially Euphemistic Terms (PETs), and (4) explore\nusing representations of semantically close sentences to aid in classification.\nUsing our augmented dataset and kNN-based methods, EUREKA was able to achieve\nstate-of-the-art results on the public leaderboard of the Euphemism Detection\nShared Task, ranking first with a macro F1 score of 0.881. Our code is\navailable at https://github.com/sedrickkeh/EUREKA.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 Figurative Language Workshop; first place for Euphemism Detection Shared Task. Code at this https URL\n",
    "authors": [
      "Sedrick Scott Keh",
      "Rohit K. Bharadwaj",
      "Emmy Liu",
      "Simone Tedeschi",
      "Varun Gangal",
      "Roberto Navigli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12846"
  },
  {
    "id": "arXiv:2210.12849",
    "title": "Learning to Advise Humans By Leveraging Algorithm Discretion",
    "abstract": "Expert decision-makers (DMs) in high-stakes AI-advised (AIDeT) settings\nreceive and reconcile recommendations from AI systems before making their final\ndecisions. We identify distinct properties of these settings which are key to\ndeveloping AIDeT models that effectively benefit team performance. First, DMs\nin AIDeT settings exhibit algorithm discretion behavior (ADB), i.e., an\nidiosyncratic tendency to imperfectly accept or reject algorithmic\nrecommendations for any given decision task. Second, DMs incur contradiction\ncosts from exerting decision-making resources (e.g., time and effort) when\nreconciling AI recommendations that contradict their own judgment. Third, the\nhuman'simperfect discretion and reconciliation costs introduce the need for the\nAI to offer advice selectively. We refer to the task of developing AI to advise\nhumans in AIDeT settings as learning to advise} and we address this task by\nfirst introducing the AIDeT-Learning Framework. Additionally, we argue that\nleveraging the human partner's ADB is key to maximizing the AIDeT's decision\naccuracy while regularizing for contradiction costs. Finally, we instantiate\nour framework to develop TeamRules (TR): an algorithm that produces rule-based\nmodels and recommendations for AIDeT settings. TR is optimized to selectively\nadvise a human and to trade-off contradiction costs and team accuracy for a\ngiven environment by leveraging the human partner's ADB. Evaluations on\nsynthetic and real-world benchmark datasets with a variety of simulated human\naccuracy and discretion behaviors show that TR robustly improves the team's\nobjective across settings over interpretable, rule-based alternatives.",
    "descriptor": "",
    "authors": [
      "Nicholas Wolczynski",
      "Maytal Saar-Tsechansky",
      "Tong Wang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.12849"
  },
  {
    "id": "arXiv:2210.12851",
    "title": "Lazy Incremental Search for Efficient Replanning with Bounded  Suboptimality Guarantees",
    "abstract": "We present a lazy incremental search algorithm, Lifelong-GLS (L-GLS), along\nwith its bounded suboptimal version, Bounded L-GLS (B-LGLS) that combine the\nsearch efficiency of incremental search algorithms with the evaluation\nefficiency of lazy search algorithms for fast replanning in problem domains\nwhere edge-evaluations are more expensive than vertex-expansions. The proposed\nalgorithms generalize Lifelong Planning A* (LPA*) and its bounded suboptimal\nversion, Truncated LPA* (TLPA*), within the Generalized Lazy Search (GLS)\nframework, so as to restrict expensive edge evaluations only to the current\nshortest subpath when the cost-to-come inconsistencies are propagated during\nrepair. We also present dynamic versions of the L-GLS and B-LGLS algorithms,\ncalled Generalized D* (GD*) and Bounded Generalized D* (B-GD*), respectively,\nfor efficient replanning with non-stationary queries, designed specifically for\nnavigation of mobile robots. We prove that the proposed algorithms are complete\nand correct in finding a solution that is guaranteed not to exceed the optimal\nsolution cost by a user-chosen factor. Our numerical and experimental results\nsupport the claim that the proposed integration of the incremental and lazy\nsearch frameworks can help find solutions faster compared to the regular\nincremental or regular lazy search algorithms when the underlying graph\nrepresentation changes often.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2105.12076\n",
    "authors": [
      "Jaein Lim",
      "Mahdi Ghanei",
      "R. Connor Lawson",
      "Siddhartha Srinivasa",
      "Panagiotis Tsiotras"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12851"
  },
  {
    "id": "arXiv:2210.12852",
    "title": "1st Place Solution of The Robust Vision Challenge (RVC) 2022 Semantic  Segmentation Track",
    "abstract": "This report describes the winner solution to the semantic segmentation task\nof the Robust Vision Challenge on ECCV 2022. Our method adopts the FAN-B-Hybrid\nmodel as the encoder and uses Segformer as the segmentation framework. The\nmodel is trained on a combined dataset containing images from 9 datasets\n(ADE20K, Cityscapes, Mapillary Vistas, ScanNet, VIPER, Wilddash2, IDD, BDD, and\nCOCO) with a simple dataset balancing strategy. All the original labels are\nprojected to a 256-class unified label space, and the model is trained with\nnaive cross-entropy loss. Without significant hyperparameters tuning or any\nspecific loss weighting, our solution ranks 1st on all the required semantic\nsegmentation benchmarks from multiple domains (ADE20K, Cityscapes, Mapillary\nVistas, ScanNet, VIPER, and Wilddash2). Our method could be served as a strong\nbaseline for the multi-domain segmentation task and our codebase could be\nhelpful to future work. Code will be available at\nhttps://github.com/lambert-x/RVC_Segmentation.",
    "descriptor": "\nComments: Winner Solution to The Robust Vision Challenge 2022 Semantic Segmentation Track\n",
    "authors": [
      "Junfei Xiao",
      "Zhichao Xu",
      "Shiyi Lan",
      "Zhiding Yu",
      "Alan Yuille",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12852"
  },
  {
    "id": "arXiv:2210.12854",
    "title": "Artificial life using the book and bookmarker",
    "abstract": "Reproduction, development, and individual interactions are essential topics\nin artificial life. The cellular automata, which can handle these in a\ncomposite way, is highly restricted in its form and behavior because it\nrepresents life as a pattern of cells. In contrast, the virtual creatures\nproposed by Sims have a very high degree of freedom in terms of morphology and\nbehavior. However, they have limited expressive capacity in terms of those\nviewpoints. In this study, we carefully extract the characteristics of these\ntwo models and propose a new artificial life model. The virtual creatures found\nin the proposed model have unique survival strategies and lifestyles. They have\nacquired interesting properties in reproduction, development, and individual\ninteractions while having freedom in morphology and behavior.",
    "descriptor": "",
    "authors": [
      "Keishu Utimula"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12854"
  },
  {
    "id": "arXiv:2210.12856",
    "title": "Symmetry and Variance: Generative Parametric Modelling of Historical  Brick Wall Patterns",
    "abstract": "This study integrates artificial intelligence and computational design tools\nto extract information from architectural heritage. Photogrammetry-based point\ncloud models of brick walls from the Anatolian Seljuk period are analysed in\nterms of the interrelated units of construction, simultaneously considering\nboth the inherent symmetries and irregularities. The real-world data is used as\ninput for acquiring the stochastic parameters of spatial relations and a set of\nparametric shape rules to recreate designs of existing and hypothetical brick\nwalls within the style. The motivation is to be able to generate large data\nsets for machine learning of the style and to devise procedures for robotic\nproduction of such designs with repetitive units.",
    "descriptor": "\nComments: 10 pages, 7 Figures. This paper is published at \"Symmetry: Art and Science | 12th SIS-Symmetry Congress\"\n",
    "authors": [
      "Sevgi Altun",
      "Mustafa Cem Gunes",
      "Yusuf H. Sahin",
      "Alican Mertan",
      "Gozde Unal",
      "Mine Ozkar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12856"
  },
  {
    "id": "arXiv:2210.12857",
    "title": "Bootstrapping meaning through listening: Unsupervised learning of spoken  sentence embeddings",
    "abstract": "Inducing semantic representations directly from speech signals is a highly\nchallenging task but has many useful applications in speech mining and spoken\nlanguage understanding. This study tackles the unsupervised learning of\nsemantic representations for spoken utterances. Through converting speech\nsignals into hidden units generated from acoustic unit discovery, we propose\nWavEmbed, a multimodal sequential autoencoder that predicts hidden units from a\ndense representation of speech. Secondly, we also propose S-HuBERT to induce\nmeaning through knowledge distillation, in which a sentence embedding model is\nfirst trained on hidden units and passes its knowledge to a speech encoder\nthrough contrastive learning. The best performing model achieves a moderate\ncorrelation (0.5~0.6) with human judgments, without relying on any labels or\ntranscriptions. Furthermore, these models can also be easily extended to\nleverage textual transcriptions of speech to learn much better speech\nembeddings that are strongly correlated with human annotations. Our proposed\nmethods are applicable to the development of purely data-driven systems for\nspeech mining, indexing and search.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Jian Zhu",
      "Zuoyu Tian",
      "Yadong Liu",
      "Cong Zhang",
      "Chia-wen Lo"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12857"
  },
  {
    "id": "arXiv:2210.12858",
    "title": "Kadabra: Adapting Kademlia for the Decentralized Web",
    "abstract": "Blockchains have become the catalyst for a growing movement to create a more\ndecentralized Internet. A fundamental operation of applications in a\ndecentralized Internet is data storage and retrieval. As today's blockchains\nare limited in their storage functionalities, in recent years a number of\npeer-to-peer data storage networks have emerged based on the Kademlia\ndistributed hash table protocol. However, existing Kademlia implementations are\nnot efficient enough to support fast data storage and retrieval operations\nnecessary for (decentralized) Web applications. In this paper, we present\nKadabra, a decentralized protocol for computing the routing table entries in\nKademlia to accelerate lookups. Kadabra is motivated by the multi-armed bandit\nproblem, and can automatically adapt to heterogeneity and dynamism in the\nnetwork. Experimental results show Kadabra achieving between 15-50% lower\nlookup latencies compared to state-of-the-art baselines.",
    "descriptor": "\nComments: 26 pages, 19 figures\n",
    "authors": [
      "Yunqi Zhang",
      "Shaileshh Bojja Venkatakrishnan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Data Structures and Algorithms (cs.DS)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.12858"
  },
  {
    "id": "arXiv:2210.12859",
    "title": "A Stack-Free Traversal Algorithm for Left-Balanced k-d Trees",
    "abstract": "We present an algorithm that allows for find-closest-point and kNN-style\ntraversals of left-balanced k-d trees, without the need for either recursion or\nsoftware-managed stacks; instead using only current and last previously\ntraversed node to compute which node to traverse next.",
    "descriptor": "",
    "authors": [
      "Ingo Wald"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.12859"
  },
  {
    "id": "arXiv:2210.12861",
    "title": "Tight relative estimation in the mean of Bernoulli random variables",
    "abstract": "Given a stream of Bernoulli random variables, consider the problem of\nestimating the mean of the random variable within a specified relative error\nwith a specified probability of failure. Until now, the Gamma Bernoulli\nApproximation Scheme (GBAS) was the method that accomplished this goal using\nthe smallest number of average samples. In this work, a new method is\nintroduced that is faster when the mean is bounded away from zero. The process\nuses a two-stage process together with some simple inequalities to get rigorous\nbounds on the error probability.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Mark Huber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.12861"
  },
  {
    "id": "arXiv:2210.12863",
    "title": "Overview of Dialogue Robot Competition 2022",
    "abstract": "Although many competitions have been held on dialogue systems in the past, no\ncompetition has been organized specifically for dialogue with humanoid robots.\nAs the first such attempt in the world, we held a dialogue robot competition in\n2020 to compare the performances of interactive robots using an android that\nclosely resembles a human. Dialogue Robot Competition 2022 (DRC2022) was the\nsecond competition, held in August 2022. The task and regulations followed\nthose of the first competition, while the evaluation method was improved and\nthe event was internationalized. The competition has two rounds, a preliminary\nround and the final round. In the preliminary round, twelve participating teams\ncompeted in performance of a dialogue robot in the manner of a field\nexperiment, and then three of those teams were selected as finalists. The final\nround will be held on October 25, 2022, in the Robot Competition session of\nIROS2022. This paper provides an overview of the task settings and evaluation\nmethod of DRC2022 and the results of the preliminary round.",
    "descriptor": "\nComments: This paper is part of the proceedings of the Dialogue Robot Competition 2022\n",
    "authors": [
      "Takashi Minato",
      "Ryuichiro Higashinaka",
      "Kurima Sakai",
      "Tomo Funayama",
      "Hiromitsu Nishizaki",
      "Takayuki Nagai"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.12863"
  },
  {
    "id": "arXiv:2210.12864",
    "title": "K-SAM: Sharpness-Aware Minimization at the Speed of SGD",
    "abstract": "Sharpness-Aware Minimization (SAM) has recently emerged as a robust technique\nfor improving the accuracy of deep neural networks. However, SAM incurs a high\ncomputational cost in practice, requiring up to twice as much computation as\nvanilla SGD. The computational challenge posed by SAM arises because each\niteration requires both ascent and descent steps and thus double the gradient\ncomputations. To address this challenge, we propose to compute gradients in\nboth stages of SAM on only the top-k samples with highest loss. K-SAM is simple\nand extremely easy-to-implement while providing significant generalization\nboosts over vanilla SGD at little to no additional cost.",
    "descriptor": "\nComments: 13 pages, 2 figures\n",
    "authors": [
      "Renkun Ni",
      "Ping-yeh Chiang",
      "Jonas Geiping",
      "Micah Goldblum",
      "Andrew Gordon Wilson",
      "Tom Goldstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12864"
  },
  {
    "id": "arXiv:2210.12865",
    "title": "Knowledge Transfer from Answer Ranking to Answer Generation",
    "abstract": "Recent studies show that Question Answering (QA) based on Answer Sentence\nSelection (AS2) can be improved by generating an improved answer from the top-k\nranked answer sentences (termed GenQA). This allows for synthesizing the\ninformation from multiple candidates into a concise, natural-sounding answer.\nHowever, creating large-scale supervised training data for GenQA models is very\nchallenging. In this paper, we propose to train a GenQA model by transferring\nknowledge from a trained AS2 model, to overcome the aforementioned issue.\nFirst, we use an AS2 model to produce a ranking over answer candidates for a\nset of questions. Then, we use the top ranked candidate as the generation\ntarget, and the next k top ranked candidates as context for training a GenQA\nmodel. We also propose to use the AS2 model prediction scores for loss\nweighting and score-conditioned input/output shaping, to aid the knowledge\ntransfer. Our evaluation on three public and one large industrial datasets\ndemonstrates the superiority of our approach over the AS2 baseline, and GenQA\ntrained using supervised data.",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Matteo Gabburo",
      "Rik Koncel-Kedziorski",
      "Siddhant Garg",
      "Luca Soldaini",
      "Alessandro Moschitti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12865"
  },
  {
    "id": "arXiv:2210.12867",
    "title": "Deep Equilibrium Approaches to Diffusion Models",
    "abstract": "Diffusion-based generative models are extremely effective in generating\nhigh-quality images, with generated samples often surpassing the quality of\nthose produced by other models under several metrics. One distinguishing\nfeature of these models, however, is that they typically require long sampling\nchains to produce high-fidelity images. This presents a challenge not only from\nthe lenses of sampling time, but also from the inherent difficulty in\nbackpropagating through these chains in order to accomplish tasks such as model\ninversion, i.e. approximately finding latent states that generate known images.\nIn this paper, we look at diffusion models through a different perspective,\nthat of a (deep) equilibrium (DEQ) fixed point model. Specifically, we extend\nthe recent denoising diffusion implicit model (DDIM; Song et al. 2020), and\nmodel the entire sampling chain as a joint, multivariate fixed point system.\nThis setup provides an elegant unification of diffusion and equilibrium models,\nand shows benefits in 1) single image sampling, as it replaces the fully-serial\ntypical sampling process with a parallel one; and 2) model inversion, where we\ncan leverage fast gradients in the DEQ setting to much more quickly find the\nnoise that generates a given image. The approach is also orthogonal and thus\ncomplementary to other methods used to reduce the sampling time, or improve\nmodel inversion. We demonstrate our method's strong performance across several\ndatasets, including CIFAR10, CelebA, and LSUN Bedrooms and Churches.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Ashwini Pokle",
      "Zhengyang Geng",
      "Zico Kolter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12867"
  },
  {
    "id": "arXiv:2210.12870",
    "title": "Imbalanced Class Data Performance Evaluation and Improvement using Novel  Generative Adversarial Network-based Approach: SSG and GBO",
    "abstract": "Class imbalance in a dataset is one of the major challenges that can\nsignificantly impact the performance of machine learning models resulting in\nbiased predictions. Numerous techniques have been proposed to address class\nimbalanced problems, including, but not limited to, Oversampling,\nUndersampling, and cost-sensitive approaches. Due to its ability to generate\nsynthetic data, oversampling techniques such as the Synthetic Minority\nOversampling Technique (SMOTE) is among the most widely used methodology by\nresearchers. However, one of SMOTE's potential disadvantages is that newly\ncreated minor samples may overlap with major samples. As an effect, the\nprobability of ML models' biased performance towards major classes increases.\nRecently, generative adversarial network (GAN) has garnered much attention due\nto its ability to create almost real samples. However, GAN is hard to train\neven though it has much potential. This study proposes two novel techniques:\nGAN-based Oversampling (GBO) and Support Vector Machine-SMOTE-GAN (SSG) to\novercome the limitations of the existing oversampling approaches. The\npreliminary computational result shows that SSG and GBO performed better on the\nexpanded imbalanced eight benchmark datasets than the original SMOTE. The study\nalso revealed that the minor sample generated by SSG demonstrates Gaussian\ndistributions, which is often difficult to achieve using original SMOTE.",
    "descriptor": "",
    "authors": [
      "Md Manjurul Ahsan",
      "Md Shahin Ali",
      "Zahed Siddique"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12870"
  },
  {
    "id": "arXiv:2210.12871",
    "title": "Tighter Abstract Queries in Neural Network Verification",
    "abstract": "Neural networks have become critical components of reactive systems in\nvarious domains within computer science. Despite their excellent performance,\nusing neural networks entails numerous risks that stem from our lack of ability\nto understand and reason about their behavior. Due to these risks, various\nformal methods have been proposed for verifying neural networks; but\nunfortunately, these typically struggle with scalability barriers. Recent\nattempts have demonstrated that abstraction-refinement approaches could play a\nsignificant role in mitigating these limitations; but these approaches can\noften produce networks that are so abstract, that they become unsuitable for\nverification. To deal with this issue, we present CEGARETTE, a novel\nverification mechanism where both the system and the property are abstracted\nand refined simultaneously. We observe that this approach allows us to produce\nabstract networks which are both small and sufficiently accurate, allowing for\nquick verification times while avoiding a large number of refinement steps. For\nevaluation purposes, we implemented CEGARETTE as an extension to the recently\nproposed CEGAR-NN framework. Our results are very promising, and demonstrate a\nsignificant improvement in performance over multiple benchmarks.",
    "descriptor": "",
    "authors": [
      "Elazar Cohen",
      "Yizhak Yisrael Elboher",
      "Clark Barrett",
      "Guy Katz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.12871"
  },
  {
    "id": "arXiv:2210.12872",
    "title": "Socio-cognitive Optimization of Time-delay Control Problems using  Evolutionary Metaheuristics",
    "abstract": "Metaheuristics are universal optimization algorithms which should be used for\nsolving difficult problems, unsolvable by classic approaches. In this paper we\naim at constructing novel socio-cognitive metaheuristic based on castes, and\napply several versions of this algorithm to optimization of time-delay system\nmodel. Besides giving the background and the details of the proposed algorithms\nwe apply them to optimization of selected variants of the problem and discuss\nthe results.",
    "descriptor": "\nComments: In proceedings of the Intelligent Systems IS2022 Conference, October 12-14, 2022, Warsaw, Poland\n",
    "authors": [
      "Piotr Kipinski",
      "Hubert Guzowski",
      "Aleksandra Urbanczyk",
      "Maciej Smolka",
      "Marek Kisiel-Dorohinicki",
      "Aleksander Byrski",
      "Zuzana Kominkova Oplatkova",
      "Roman Senkerik",
      "Libor Pekar",
      "Radek Matusu",
      "Frantisek Gazdos"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12872"
  },
  {
    "id": "arXiv:2210.12873",
    "title": "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated  Learning",
    "abstract": "Federated Learning (FL) is a distributed learning paradigm that enables\ndifferent parties to train a model together for high quality and strong privacy\nprotection. In this scenario, individual participants may get compromised and\nperform backdoor attacks by poisoning the data (or gradients). Existing work on\nrobust aggregation and certified FL robustness does not study how hardening\nbenign clients can affect the global model (and the malicious clients). In this\nwork, we theoretically analyze the connection among cross-entropy loss, attack\nsuccess rate, and clean accuracy in this setting. Moreover, we propose a\ntrigger reverse engineering based defense and show that our method can achieve\nrobustness improvement with guarantee (i.e., reducing the attack success rate)\nwithout affecting benign accuracy. We conduct comprehensive experiments across\ndifferent datasets and attack settings. Our results on eight competing SOTA\ndefense methods show the empirical superiority of our method on both\nsingle-shot and continuous FL backdoor attacks.",
    "descriptor": "\nComments: 24 pages\n",
    "authors": [
      "Kaiyuan Zhang",
      "Guanhong Tao",
      "Qiuling Xu",
      "Siyuan Cheng",
      "Shengwei An",
      "Yingqi Liu",
      "Shiwei Feng",
      "Guangyu Shen",
      "Pin-Yu Chen",
      "Shiqing Ma",
      "Xiangyu Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12873"
  },
  {
    "id": "arXiv:2210.12874",
    "title": "Tail Batch Sampling: Approximating Global Contrastive Losses as  Optimization over Batch Assignments",
    "abstract": "Contrastive Learning has recently achieved state-of-the-art performance in a\nwide range of tasks. Many contrastive learning approaches use mined hard\nnegatives to make batches more informative during training but these approaches\nare inefficient as they increase epoch length proportional to the number of\nmined negatives and require frequent updates of nearest neighbor indices or\nmining from recent batches. In this work, we provide an alternative to hard\nnegative mining in supervised contrastive learning, Tail Batch Sampling (TBS),\nan efficient approximation to the batch assignment problem that upper bounds\nthe gap between the global and training losses, $\\mathcal{L}^{Global} -\n\\mathcal{L}^{Train}$. TBS \\textbf{improves state-of-the-art performance} in\nsentence embedding (+0.37 Spearman) and code-search tasks (+2.2\\% MRR), is easy\nto implement - requiring only a few additional lines of code, does not maintain\nexternal data structures such as nearest neighbor indices, is more\ncomputationally efficient when compared to the most minimal hard negative\nmining approaches, and makes no changes to the model being trained.",
    "descriptor": "\nComments: 18 pages, 5 figures\n",
    "authors": [
      "Vin Sachidananda",
      "Ziyi Yang",
      "Chenguang Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12874"
  },
  {
    "id": "arXiv:2210.12877",
    "title": "A Secure Design Pattern Approach Toward Tackling Lateral-Injection  Attacks",
    "abstract": "Software weaknesses that create attack surfaces for adversarial exploits,\nsuch as lateral SQL injection (LSQLi) attacks, are usually introduced during\nthe design phase of software development. Security design patterns are\nsometimes applied to tackle these weaknesses. However, due to the stealthy\nnature of lateral-based attacks, employing traditional security patterns to\naddress these threats is insufficient. Hence, we present SEAL, a secure design\nthat extrapolates architectural, design, and implementation abstraction levels\nto delegate security strategies toward tackling LSQLi attacks. We evaluated\nSEAL using case study software, where we assumed the role of an adversary and\ninjected several attack vectors tasked with compromising the confidentiality\nand integrity of its database. Our evaluation of SEAL demonstrated its capacity\nto address LSQLi attacks.",
    "descriptor": "\nComments: 4 pages, 3 figures. Accepted to The 15th IEEE International Conference on Security of Information and Networks (SIN)\n",
    "authors": [
      "Chidera Biringa",
      "G\u00f6khan Kul"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12877"
  },
  {
    "id": "arXiv:2210.12878",
    "title": "IDD-3D: Indian Driving Dataset for 3D Unstructured Road Scenes",
    "abstract": "Autonomous driving and assistance systems rely on annotated data from traffic\nand road scenarios to model and learn the various object relations in complex\nreal-world scenarios. Preparation and training of deploy-able deep learning\narchitectures require the models to be suited to different traffic scenarios\nand adapt to different situations. Currently, existing datasets, while\nlarge-scale, lack such diversities and are geographically biased towards mainly\ndeveloped cities. An unstructured and complex driving layout found in several\ndeveloping countries such as India poses a challenge to these models due to the\nsheer degree of variations in the object types, densities, and locations. To\nfacilitate better research toward accommodating such scenarios, we build a new\ndataset, IDD-3D, which consists of multi-modal data from multiple cameras and\nLiDAR sensors with 12k annotated driving LiDAR frames across various traffic\nscenarios. We discuss the need for this dataset through statistical comparisons\nwith existing datasets and highlight benchmarks on standard 3D object detection\nand tracking tasks in complex layouts. Code and data available at\nhttps://github.com/shubham1810/idd3d_kit.git",
    "descriptor": "\nComments: 10 pages, 8 figures, 5 tables, Accepted in Winter Conference on Applications of Computer Vision (WACV 2023)\n",
    "authors": [
      "Shubham Dokania",
      "A.H. Abdul Hafez",
      "Anbumani Subramanian",
      "Manmohan Chandraker",
      "C.V. Jawahar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12878"
  },
  {
    "id": "arXiv:2210.12881",
    "title": "A Control Theoretic Approach to Infrastructure-Centric Blockchain  Tokenomics",
    "abstract": "There are a multitude of Blockchain-based physical infrastructure systems,\noperating on a crypto-currency enabled token economy, where infrastructure\nsuppliers are rewarded with tokens for enabling, validating, managing and/or\nsecuring the system. However, today's token economies are largely designed\nwithout infrastructure systems in mind, and often operate with a fixed token\nsupply (e.g., Bitcoin). This paper argues that token economies for\ninfrastructure networks should be structured differently - they should\ncontinually incentivize new suppliers to join the network to provide services\nand support to the ecosystem. As such, the associated token rewards should\ngracefully scale with the size of the decentralized system, but should be\ncarefully balanced with consumer demand to manage inflation and be designed to\nultimately reach an equilibrium. To achieve such an equilibrium, the\ndecentralized token economy should be adaptable and controllable so that it\nmaximizes the total utility of all users, such as achieving stable (overall\nnon-inflationary) token economies.\nOur main contribution is to model infrastructure token economies as dynamical\nsystems - the circulating token supply, price, and consumer demand change as a\nfunction of the payment to nodes and costs to consumers for infrastructure\nservices. Crucially, this dynamical systems view enables us to leverage tools\nfrom mathematical control theory to optimize the overall decentralized\nnetwork's performance. Moreover, our model extends easily to a Stackelberg game\nbetween the controller and the nodes, which we use for robust, strategic\npricing. In short, we develop predictive, optimization-based controllers that\noutperform traditional algorithmic stablecoin heuristics by up to $2.4 \\times$\nin simulations based on real demand data from existing decentralized wireless\nnetworks.",
    "descriptor": "",
    "authors": [
      "Oguzhan Akcin",
      "Robert P. Streit",
      "Benjamin Oommen",
      "Sriram Vishwanath",
      "Sandeep Chinchali"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2210.12881"
  },
  {
    "id": "arXiv:2210.12883",
    "title": "A Greek Parliament Proceedings Dataset for Computational Linguistics and  Political Analysis",
    "abstract": "Large, diachronic datasets of political discourse are hard to come across,\nespecially for resource-lean languages such as Greek. In this paper, we\nintroduce a curated dataset of the Greek Parliament Proceedings that extends\nchronologically from 1989 up to 2020. It consists of more than 1 million\nspeeches with extensive metadata, extracted from 5,355 parliamentary record\nfiles. We explain how it was constructed and the challenges that we had to\novercome. The dataset can be used for both computational linguistics and\npolitical analysis-ideally, combining the two. We present such an application,\nshowing (i) how the dataset can be used to study the change of word usage\nthrough time, (ii) between significant historical events and political parties,\n(iii) by evaluating and employing algorithms for detecting semantic shifts.",
    "descriptor": "\nComments: Accepted to the 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks\n",
    "authors": [
      "Konstantina Dritsa",
      "Kaiti Thoma",
      "John Pavlopoulos",
      "Panos Louridas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12883"
  },
  {
    "id": "arXiv:2210.12887",
    "title": "Retrieval Augmentation for Commonsense Reasoning: A Unified Approach",
    "abstract": "A common thread of retrieval-augmented methods in the existing literature\nfocuses on retrieving encyclopedic knowledge, such as Wikipedia, which\nfacilitates well-defined entity and relation spaces that can be modeled.\nHowever, applying such methods to commonsense reasoning tasks faces two unique\nchallenges, i.e., the lack of a general large-scale corpus for retrieval and a\ncorresponding effective commonsense retriever. In this paper, we systematically\ninvestigate how to leverage commonsense knowledge retrieval to improve\ncommonsense reasoning tasks. We proposed a unified framework of\nretrieval-augmented commonsense reasoning (called RACo), including a newly\nconstructed commonsense corpus with over 20 million documents and novel\nstrategies for training a commonsense retriever. We conducted experiments on\nfour different commonsense reasoning tasks. Extensive evaluation results showed\nthat our proposed RACo can significantly outperform other knowledge-enhanced\nmethod counterparts, achieving new SoTA performance on the CommonGen and CREAK\nleaderboards.",
    "descriptor": "\nComments: EMNLP 2022 (main)\n",
    "authors": [
      "Wenhao Yu",
      "Chenguang Zhu",
      "Zhihan Zhang",
      "Shuohang Wang",
      "Zhuosheng Zhang",
      "Yuwei Fang",
      "Meng Jiang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12887"
  },
  {
    "id": "arXiv:2210.12889",
    "title": "DALL-E 2 Fails to Reliably Capture Common Syntactic Processes",
    "abstract": "Machine intelligence is increasingly being linked to claims about sentience,\nlanguage processing, and an ability to comprehend and transform natural\nlanguage into a range of stimuli. We systematically analyze the ability of\nDALL-E 2 to capture 8 grammatical phenomena pertaining to compositionality that\nare widely discussed in linguistics and pervasive in human language: binding\nprinciples and coreference, passives, structural ambiguity, negation, word\norder, double object constructions, sentence coordination, ellipsis, and\ncomparatives. Whereas young children routinely master these phenomena, learning\nsystematic mappings between syntax and semantics, DALL-E 2 is unable to\nreliably infer meanings that are consistent with the syntax. These results\nchallenge recent claims concerning the capacity of such systems to understand\nof human language. We make available the full set of test materials as a\nbenchmark for future testing.",
    "descriptor": "",
    "authors": [
      "Evelina Leivada",
      "Elliot Murphy",
      "Gary Marcus"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12889"
  },
  {
    "id": "arXiv:2210.12892",
    "title": "AACHER: Assorted Actor-Critic Deep Reinforcement Learning with Hindsight  Experience Replay",
    "abstract": "Actor learning and critic learning are two components of the outstanding and\nmostly used Deep Deterministic Policy Gradient (DDPG) reinforcement learning\nmethod. Since actor and critic learning plays a significant role in the overall\nrobot's learning, the performance of the DDPG approach is relatively sensitive\nand unstable as a result. We propose a multi-actor-critic DDPG for reliable\nactor-critic learning to further enhance the performance and stability of DDPG.\nThis multi-actor-critic DDPG is then integrated with Hindsight Experience\nReplay (HER) to form our new deep learning framework called AACHER. AACHER uses\nthe average value of multiple actors or critics to substitute the single actor\nor critic in DDPG to increase resistance in the case when one actor or critic\nperforms poorly. Numerous independent actors and critics can also gain\nknowledge from the environment more broadly. We implemented our proposed AACHER\non goal-based environments: AuboReach, FetchReach-v1, FetchPush-v1,\nFetchSlide-v1, and FetchPickAndPlace-v1. For our experiments, we used various\ninstances of actor/critic combinations, among which A10C10 and A20C20 were the\nbest-performing combinations. Overall results show that AACHER outperforms the\ntraditional algorithm (DDPG+HER) in all of the actor/critic number combinations\nthat are used for evaluation. When used on FetchPickAndPlace-v1, the\nperformance boost for A20C20 is as high as roughly 3.8 times the success rate\nin DDPG+HER.",
    "descriptor": "",
    "authors": [
      "Adarsh Sehgal",
      "Muskan Sehgal",
      "Hung Manh La"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12892"
  },
  {
    "id": "arXiv:2210.12893",
    "title": "On Universality of the S Combinator",
    "abstract": "In combinatory logic it is known that the set of two combinators K and S are\nuniversal; in the sense that any other combinator can be expressed in terms of\nthese two. K combinator can not be expressed only in terms of the S combinator.\nThis will answer a question raised by Stephen Wolfram as ``Is the S combinator\non its own computation universal?''",
    "descriptor": "",
    "authors": [
      "Farrokh Vatan"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.12893"
  },
  {
    "id": "arXiv:2210.12896",
    "title": "IDRL: Identifying Identities in Multi-Agent Reinforcement Learning with  Ambiguous Identities",
    "abstract": "Multi-agent reinforcement learning(MARL) is a prevalent learning paradigm for\nsolving stochastic games. In previous studies, agents in a game are defined to\nbe teammates or enemies beforehand, and the relation of the agents is fixed\nthroughout the game. Those works can hardly work in the games where the\ncompetitive and collaborative relationships are not public and dynamically\nchanging, which is decided by the \\textit{identities} of the agents. How to\nlearn a successful policy in such a situation where the identities of agents\nare ambiguous is still a problem. Focusing on this problem, in this work, we\ndevelop a novel MARL framework: IDRL, which identifies the identities of the\nagents dynamically and then chooses the corresponding policy to perform in the\ntask. In the IDRL framework, a relation network is constructed to deduce the\nidentities of the multi-agents through feeling the kindness and hostility\nunleashed by other agents; a dangerous network is built to estimate the risk of\nthe identification. We also propose an intrinsic reward to help train the\nrelation network and the dangerous network to get a trade-off between the need\nto maximize external reward and the accuracy of identification. After\nidentifying the cooperation-competition pattern among the agents, the proposed\nmethod IDRL applies one of the off-the-shelf MARL methods to learn the policy.\nTaking the poker game \\textit{red-10} as the experiment environment,\nexperiments show that the IDRL can achieve superior performance compared to the\nother MARL methods. Significantly, the relation network has the par performance\nto identify the identities of agents with top human players; the dangerous\nnetwork reasonably avoids the risk of imperfect identification.",
    "descriptor": "",
    "authors": [
      "Shijie Han",
      "Peng liu",
      "Siyuan Li"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12896"
  },
  {
    "id": "arXiv:2210.12899",
    "title": "SpikeSim: An end-to-end Compute-in-Memory Hardware Evaluation Tool for  Benchmarking Spiking Neural Networks",
    "abstract": "SNNs are an active research domain towards energy efficient machine\nintelligence. Compared to conventional ANNs, SNNs use temporal spike data and\nbio-plausible neuronal activation functions such as Leaky-Integrate\nFire/Integrate Fire (LIF/IF) for data processing. However, SNNs incur\nsignificant dot-product operations causing high memory and computation overhead\nin standard von-Neumann computing platforms. Today, In-Memory Computing (IMC)\narchitectures have been proposed to alleviate the \"memory-wall bottleneck\"\nprevalent in von-Neumann architectures. Although recent works have proposed\nIMC-based SNN hardware accelerators, the following have been overlooked- 1) the\nadverse effects of crossbar non-ideality on SNN performance due to repeated\nanalog dot-product operations over multiple time-steps, 2) hardware overheads\nof essential SNN-specific components such as the LIF/IF and data communication\nmodules. To this end, we propose SpikeSim, a tool that can perform realistic\nperformance, energy, latency and area evaluation of IMC-mapped SNNs. SpikeSim\nconsists of a practical monolithic IMC architecture called SpikeFlow for\nmapping SNNs. Additionally, the non-ideality computation engine (NICE) and\nenergy-latency-area (ELA) engine performs hardware-realistic evaluation of\nSpikeFlow-mapped SNNs. Based on 65nm CMOS implementation and experiments on\nCIFAR10, CIFAR100 and TinyImagenet datasets, we find that the LIF/IF neuronal\nmodule has significant area contribution (>11% of the total hardware area). We\npropose SNN topological modifications leading to 1.24x and 10x reduction in the\nneuronal module's area and the overall energy-delay-product value,\nrespectively. Furthermore, in this work, we perform a holistic comparison\nbetween IMC implemented ANN and SNNs and conclude that lower number of\ntime-steps are the key to achieve higher throughput and energy-efficiency for\nSNNs compared to 4-bit ANNs.",
    "descriptor": "\nComments: 14 pages, 22 figures\n",
    "authors": [
      "Abhishek Moitra",
      "Abhiroop Bhattacharjee",
      "Runcong Kuang",
      "Gokul Krishnan",
      "Yu Cao",
      "Priyadarshini Panda"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12899"
  },
  {
    "id": "arXiv:2210.12902",
    "title": "Event-Centric Question Answering via Contrastive Learning and Invertible  Event Transformation",
    "abstract": "Human reading comprehension often requires reasoning of event semantic\nrelations in narratives, represented by Event-centric Question-Answering (QA).\nTo address event-centric QA, we propose a novel QA model with contrastive\nlearning and invertible event transformation, call TranCLR. Our proposed model\nutilizes an invertible transformation matrix to project semantic vectors of\nevents into a common event embedding space, trained with contrastive learning,\nand thus naturally inject event semantic knowledge into mainstream QA\npipelines. The transformation matrix is fine-tuned with the annotated event\nrelation types between events that occurred in questions and those in answers,\nusing event-aware question vectors. Experimental results on the Event Semantic\nRelation Reasoning (ESTER) dataset show significant improvements in both\ngenerative and extractive settings compared to the existing strong baselines,\nachieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact\nMatch (EM) score under the multi-answer setting. Qualitative analysis reveals\nthe high quality of the generated answers by TranCLR, demonstrating the\nfeasibility of injecting event knowledge into QA model learning. Our code and\nmodels can be found at https://github.com/LuJunru/TranCLR.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Junru Lu",
      "Xingwei Tan",
      "Gabriele Pergola",
      "Lin Gui",
      "Yulan He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12902"
  },
  {
    "id": "arXiv:2210.12903",
    "title": "Gallery Filter Network for Person Search",
    "abstract": "In person search, we aim to localize a query person from one scene in other\ngallery scenes. The cost of this search operation is dependent on the number of\ngallery scenes, making it beneficial to reduce the pool of likely scenes. We\ndescribe and demonstrate the Gallery Filter Network (GFN), a novel module which\ncan efficiently discard gallery scenes from the search process, and benefit\nscoring for persons detected in remaining scenes. We show that the GFN is\nrobust under a range of different conditions by testing on different retrieval\nsets, including cross-camera, occluded, and low-resolution scenarios. In\naddition, we develop the base SeqNeXt person search model, which improves and\nsimplifies the original SeqNet model. We show that the SeqNeXt+GFN combination\nyields significant performance gains over other state-of-the-art methods on the\nstandard PRW and CUHK-SYSU person search datasets. To aid experimentation for\nthis and other models, we provide standardized tooling for the data processing\nand evaluation pipeline typically used for person search research.",
    "descriptor": "\nComments: WACV 2023; Code: this https URL\n",
    "authors": [
      "Lucas Jaffe",
      "Avideh Zakhor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12903"
  },
  {
    "id": "arXiv:2210.12905",
    "title": "Visualizing the Obvious: A Concreteness-based Ensemble Model for Noun  Property Prediction",
    "abstract": "Neural language models encode rich knowledge about entities and their\nrelationships which can be extracted from their representations using probing.\nCommon properties of nouns (e.g., red strawberries, small ant) are, however,\nmore challenging to extract compared to other types of knowledge because they\nare rarely explicitly stated in texts. We hypothesize this to mainly be the\ncase for perceptual properties which are obvious to the participants in the\ncommunication. We propose to extract these properties from images and use them\nin an ensemble model, in order to complement the information that is extracted\nfrom language models. We consider perceptual properties to be more concrete\nthan abstract properties (e.g., interesting, flawless). We propose to use the\nadjectives' concreteness score as a lever to calibrate the contribution of each\nsource (text vs. images). We evaluate our ensemble model in a ranking task\nwhere the actual properties of a noun need to be ranked higher than other\nnon-relevant properties. Our results show that the proposed combination of text\nand images greatly improves noun property prediction compared to powerful\ntext-based language models.",
    "descriptor": "\nComments: Findings of EMNLP 2022; The first two authors contributed equally\n",
    "authors": [
      "Yue Yang",
      "Artemis Panagopoulou",
      "Marianna Apidianaki",
      "Mark Yatskar",
      "Chris Callison-Burch"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12905"
  },
  {
    "id": "arXiv:2210.12906",
    "title": "Iterative Detection and Decoding for Cell-Free Massive Multiuser MIMO  with LDPC Codes",
    "abstract": "This paper proposes an iterative detection and decoding (IDD) scheme for a\ncell free massive multiple input multiple output (CF-mMIMO) system. Users send\ncoded data to the access points (APs), which is jointly detected at central\nprocessing unit (CPU). The symbols are exchanged iteratively in the form of log\nlikelihood ratios (LLRs) between the detector and the low-density parity check\ncodes (LPDC) decoder, increasing the coded system's performance. We propose a\nlist-based multi-feedback diversity with successive interference cancellation\n(MF-SIC) to improve the performance of the CF-mMIMO. Furthermore, the proposed\ndetector is compared with the parallel interference cancellation (PIC) and\nMF-PIC schemes. Finally, the bit error rate (BER) performance of CF-mMIMO is\ncompared with the co-located mMIMO (Col-mMIMO).",
    "descriptor": "\nComments: 4 figures, 8 pages\n",
    "authors": [
      "T. Ssettumba",
      "R. Di Renna",
      "L. Landau",
      "R. C. de Lamare"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12906"
  },
  {
    "id": "arXiv:2210.12908",
    "title": "Predicting the Citation Count and CiteScore of Journals One Year in  Advance",
    "abstract": "Prediction of the future performance of academic journals is a task that can\nbenefit a variety of stakeholders including editorial staff, publishers,\nindexing services, researchers, university administrators and granting\nagencies. Using historical data on journal performance, this can be framed as a\nmachine learning regression problem. In this work, we study two such regression\ntasks: 1) prediction of the number of citations a journal will receive during\nthe next calendar year, and 2) prediction of the Elsevier CiteScore a journal\nwill be assigned for the next calendar year. To address these tasks, we first\ncreate a dataset of historical bibliometric data for journals indexed in\nScopus. We propose the use of neural network models trained on our dataset to\npredict the future performance of journals. To this end, we perform feature\nselection and model configuration for a Multi-Layer Perceptron and a Long\nShort-Term Memory. Through experimental comparisons to heuristic prediction\nbaselines and classical machine learning models, we demonstrate superior\nperformance in our proposed models for the prediction of future citation and\nCiteScore values.",
    "descriptor": "",
    "authors": [
      "William Croft",
      "J\u00f6rg-R\u00fcdiger Sack"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12908"
  },
  {
    "id": "arXiv:2210.12910",
    "title": "Specializing Multi-domain NMT via Penalizing Low Mutual Information",
    "abstract": "Multi-domain Neural Machine Translation (NMT) trains a single model with\nmultiple domains. It is appealing because of its efficacy in handling multiple\ndomains within one model. An ideal multi-domain NMT should learn distinctive\ndomain characteristics simultaneously, however, grasping the domain peculiarity\nis a non-trivial task. In this paper, we investigate domain-specific\ninformation through the lens of mutual information (MI) and propose a new\nobjective that penalizes low MI to become higher. Our method achieved the\nstate-of-the-art performance among the current competitive multi-domain NMT\nmodels. Also, we empirically show our objective promotes low MI to be higher\nresulting in domain-specialized multi-domain NMT.",
    "descriptor": "\nComments: Accepted in EMNLP 2022\n",
    "authors": [
      "Jiyoung Lee",
      "Hantae Kim",
      "Hyunchang Cho",
      "Edward Choi",
      "Cheonbok Park"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12910"
  },
  {
    "id": "arXiv:2210.12914",
    "title": "A Novel Adaptive Causal Sampling Method for Physics-Informed Neural  Networks",
    "abstract": "Physics-Informed Neural Networks (PINNs) have become a kind of attractive\nmachine learning method for obtaining solutions of partial differential\nequations (PDEs). Training PINNs can be seen as a semi-supervised learning\ntask, in which only exact values of initial and boundary points can be obtained\nin solving forward problems, and in the whole spatio-temporal domain\ncollocation points are sampled without exact labels, which brings training\ndifficulties. Thus the selection of collocation points and sampling methods are\nquite crucial in training PINNs. Existing sampling methods include fixed and\ndynamic types, and in the more popular latter one, sampling is usually\ncontrolled by PDE residual loss. We point out that it is not sufficient to only\nconsider the residual loss in adaptive sampling and sampling should obey\ntemporal causality. We further introduce temporal causality into adaptive\nsampling and propose a novel adaptive causal sampling method to improve the\nperformance and efficiency of PINNs. Numerical experiments of several PDEs with\nhigh-order derivatives and strong nonlinearity, including Cahn Hilliard and KdV\nequations, show that the proposed sampling method can improve the performance\nof PINNs with few collocation points. We demonstrate that by utilizing such a\nrelatively simple sampling method, prediction performance can be improved up to\ntwo orders of magnitude compared with state-of-the-art results with almost no\nextra computation cost, especially when points are limited.",
    "descriptor": "",
    "authors": [
      "Jia Guo",
      "Haifeng Wang",
      "Chenping Hou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12914"
  },
  {
    "id": "arXiv:2210.12915",
    "title": "Robust Ellipse Fitting Based on Maximum Correntropy Criterion With  Variable Center",
    "abstract": "The presence of outliers can significantly degrade the performance of ellipse\nfitting methods. We develop an ellipse fitting method that is robust to\noutliers based on the maximum correntropy criterion with variable center\n(MCC-VC), where a Laplacian kernel is used. For single ellipse fitting, we\nformulate a non-convex optimization problem to estimate the kernel bandwidth\nand center and divide it into two subproblems, each estimating one parameter.\nWe design sufficiently accurate convex approximation to each subproblem such\nthat computationally efficient closed-form solutions are obtained. The two\nsubproblems are solved in an alternate manner until convergence is reached. We\nalso investigate coupled ellipses fitting. While there exist multiple ellipses\nfitting methods that can be used for coupled ellipses fitting, we develop a\ncouple ellipses fitting method by exploiting the special structure. Having\nunknown association between data points and ellipses, we introduce an\nassociation vector for each data point and formulate a non-convex mixed-integer\noptimization problem to estimate the data associations, which is approximately\nsolved by relaxing it into a second-order cone program. Using the estimated\ndata associations, we extend the proposed method to achieve the final coupled\nellipses fitting. The proposed method is shown to have significantly better\nperformance over the existing methods in both simulated data and real images.",
    "descriptor": "\nComments: 13 pages, 9 figures. Submitted to IEEE Transactions on Image Processing\n",
    "authors": [
      "Wei Wang",
      "Gang Wang",
      "Chenlong Hu",
      "K. C. Ho"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12915"
  },
  {
    "id": "arXiv:2210.12916",
    "title": "Explaining epsilon in differential privacy through the lens of  information theory",
    "abstract": "The study of leakage measures for privacy has been a subject of intensive\nresearch and is an important aspect of understanding how privacy leaks occur in\ncomputer programs. Differential privacy has been a focal point in the privacy\ncommunity for some years and yet its leakage characteristics are not completely\nunderstood. In this paper we bring together two areas of research --\ninformation theory and the g-leakage framework of quantitative information flow\n(QIF) -- to give an operational interpretation for the epsilon parameter of\ndifferential privacy. We find that epsilon emerges as a capacity measure in\nboth frameworks; via (log)-lift, a popular measure in information theory; and\nvia max-case g-leakage, which describes the leakage of any system to Bayesian\nadversaries modelled using ``worst-case'' assumptions under the QIF framework.\nOur characterisation resolves an important question of interpretability of\nepsilon and consolidates a number of disparate results covering the literature\nof both information theory and quantitative information flow.",
    "descriptor": "",
    "authors": [
      "Natasha Fernandes",
      "Annabelle McIver",
      "Parastoo Sadeghi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12916"
  },
  {
    "id": "arXiv:2210.12917",
    "title": "A Comparative Qualitative and Quantitative Analysis of the Performance  of Security Options for Message Protocols: Fog Computing Scenario",
    "abstract": "We analyze the utilization of publish-subscribe protocols in IoT and Fog\nComputing and challenges around security configuration, performance, and\nqualitative characteristics. Such problems with security configuration lead to\nsignificant disruptions and high operation costs. Yet, These issues can be\nprevented by selecting the appropriate transmission technology for each\nconfiguration, considering the variations in sizing, installation, sensor\nprofile, distribution, security, networking, and locality. This work aims to\npresent a comparative qualitative and quantitative analysis around diverse\nconfigurations, focusing on Smart Agriculture's scenario and specifically the\ncase of fish-farming. As result, we applied a data generation workbench to\ncreate datasets of relevant research data and compared the results in terms of\nperformance, resource utilization, security, and resilience. Also, we provide a\nqualitative analysis of use case scenarios for the quantitative data produced.\nAs a contribution, this robust analysis provides a blueprint to decision\nsupport for Fog Computing engineers analyzing the best protocol to apply in\nvarious configurations.",
    "descriptor": "",
    "authors": [
      "Wesley dos Reis Bezerra",
      "Fernando Koch",
      "Carlos Becker Westphall"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12917"
  },
  {
    "id": "arXiv:2210.12918",
    "title": "Unsupervised Object Representation Learning using Translation and  Rotation Group Equivariant VAE",
    "abstract": "In many imaging modalities, objects of interest can occur in a variety of\nlocations and poses (i.e. are subject to translations and rotations in 2d or\n3d), but the location and pose of an object does not change its semantics (i.e.\nthe object's essence). That is, the specific location and rotation of an\nairplane in satellite imagery, or the 3d rotation of a chair in a natural\nimage, or the rotation of a particle in a cryo-electron micrograph, do not\nchange the intrinsic nature of those objects. Here, we consider the problem of\nlearning semantic representations of objects that are invariant to pose and\nlocation in a fully unsupervised manner. We address shortcomings in previous\napproaches to this problem by introducing TARGET-VAE, a translation and\nrotation group-equivariant variational autoencoder framework. TARGET-VAE\ncombines three core innovations: 1) a rotation and translation\ngroup-equivariant encoder architecture, 2) a structurally disentangled\ndistribution over latent rotation, translation, and a\nrotation-translation-invariant semantic object representation, which are\njointly inferred by the approximate inference network, and 3) a spatially\nequivariant generator network. In comprehensive experiments, we show that\nTARGET-VAE learns disentangled representations without supervision that\nsignificantly improve upon, and avoid the pathologies of, previous methods.\nWhen trained on images highly corrupted by rotation and translation, the\nsemantic representations learned by TARGET-VAE are similar to those learned on\nconsistently posed objects, dramatically improving clustering in the semantic\nlatent space. Furthermore, TARGET-VAE is able to perform remarkably accurate\nunsupervised pose and location inference. We expect methods like TARGET-VAE\nwill underpin future approaches for unsupervised object generation, pose\nprediction, and object detection.",
    "descriptor": "",
    "authors": [
      "Alireza Nasiri",
      "Tristan Bepler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12918"
  },
  {
    "id": "arXiv:2210.12920",
    "title": "Efficient User Scheduling for Uplink Hybrid Satellite-Terrestrial  Communication",
    "abstract": "Due to increasing demands of seamless connection and massive information\nexchange across the world, the integrated satellite-terrestrial communication\nsystems develop rapidly. To shed lights on the design of this system, we\nconsider an uplink communication model consisting of a single satellite, a\nsingle terrestrial station and multiple ground users. The terrestrial station\nuses decode-and-forward (DF) to facilitate the communication between ground\nusers and the satellite. The channel between the satellite and the terrestrial\nstation is assumed to be a quasi-static shadowed Rician fading channel, while\nthe channels between the terrestrial station and ground users are assumed to\nexperience independent quasi-static Rayleigh fading. We consider two cases of\nchannel state information (CSI) availability. When instantaneous CSI is\navailable, we derive the instantaneous achievable sum rate of all ground users\nand formulate an optimization problem to maximize the sum rate. When only\nchannel distribution information (CDI) is available, we derive a closed-form\nexpression for the outage probability and formulate another optimization\nproblem to minimize the outage probability. Both optimization problems\ncorrespond to scheduling algorithms for ground users. For both cases, we\npropose low-complexity user scheduling algorithms and demonstrate the\nefficiency of our scheduling algorithms via numerical simulations.",
    "descriptor": "\nComments: To appear in IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Lina Zhu",
      "Lin Bai",
      "Lin Zhou",
      "Jinho Choi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12920"
  },
  {
    "id": "arXiv:2210.12921",
    "title": "Investigating the effect of domain selection on automatic speech  recognition performance: a case study on Bangladeshi Bangla",
    "abstract": "The performance of data-driven natural language processing systems is\ncontingent upon the quality of corpora. However, principal corpus design\ncriteria are often not identified and examined adequately, particularly in the\nspeech processing discipline. Speech corpora development requires additional\nattention with regard to clean/noisy, read/spontaneous, multi-talker speech,\naccents/dialects, etc. Domain selection is also a crucial decision point in\nspeech corpus development. In this study, we demonstrate the significance of\ndomain selection by assessing a state-of-the-art Bangla automatic speech\nrecognition (ASR) model on a novel multi-domain Bangladeshi Bangla ASR\nevaluation benchmark - BanSpeech, which contains 7.2 hours of speech and 9802\nutterances from 19 distinct domains. The ASR model has been trained with deep\nconvolutional neural network (CNN), layer normalization technique, and\nConnectionist Temporal Classification (CTC) loss criterion on SUBAK.KO, a\nmostly read speech corpus for the low-resource and morphologically rich\nlanguage Bangla. Experimental evaluation reveals the ASR model on SUBAK.KO\nfaces difficulty recognizing speech from domains with mostly spontaneous speech\nand has a high number of out-of-vocabulary (OOV) words. The same ASR model, on\nthe other hand, performs better in read speech domains and contains fewer OOV\nwords. In addition, we report the outcomes of our experiments with layer\nnormalization, input feature extraction, number of convolutional layers, etc.,\nand set a baseline on SUBAK.KO. The BanSpeech will be publicly available to\nmeet the need for a challenging evaluation benchmark for Bangla ASR.",
    "descriptor": "\nComments: To be submitted\n",
    "authors": [
      "Ahnaf Mozib Samin",
      "M. Humayan Kobir",
      "Md. Mushtaq Shahriyar Rafee",
      "M. Firoz Ahmed",
      "Shafkat Kibria",
      "M. Shahidur Rahman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.12921"
  },
  {
    "id": "arXiv:2210.12922",
    "title": "BARS: A Benchmark for Airport Runway Segmentation",
    "abstract": "Airport runway segmentation can effectively reduce the accident rate during\nthe landing phase, which has the largest risk of flight accidents. With the\nrapid development of deep learning, related methods have good performance on\nsegmentation tasks and can be well adapted to complex scenes. However, the lack\nof large-scale, publicly available datasets in this field makes the development\nof methods based on deep learning difficult. Therefore, we propose a Benchmark\nfor Airport Runway Segmentation, named BARS. Meanwhile, a semi-automatic\nannotation pipeline is designed to reduce the workload of annotation. BARS has\nthe largest dataset with the richest categories and the only instance\nannotation in the field. The dataset, which is collected using the X-Plane\nsimulation platform, contains 10,002 images and 29,347 instances with three\ncategories. We evaluate eight representative instance segmentation methods on\nBARS and analyze their performance. Based on the characteristic of the airport\nrunway with a regular shape, we propose a plug-and-play smoothing\npost-processing module (SPPM) and a contour point constraint loss (CPCL)\nfunction to smooth segmentation results for mask-based and contour-based\nmethods, respectively. Furthermore, a novel evaluation metric named average\nsmoothness (AS) is developed to measure smoothness. The experiments show that\nexisting instance segmentation methods can achieve prediction results with good\nperformance on BARS. SPPM and CPCL can improve the average accuracy by 0.9% and\n1.13%, respectively. And the average smoothness enhancements for SPPM and CPCL\nare more than 50% and 28%, respectively. Our work will be released at\nhttps://github.com/c-wenhui/BARS.",
    "descriptor": "\nComments: 14pages,8 figures, 4 tables\n",
    "authors": [
      "Wenhui Chen",
      "Zhijiang Zhang",
      "Liang Yu",
      "Yichun Tai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12922"
  },
  {
    "id": "arXiv:2210.12924",
    "title": "OLLA: Decreasing the Memory Usage of Neural Networks by Optimizing the  Lifetime and Location of Arrays",
    "abstract": "The size of deep neural networks has grown exponentially in recent years.\nUnfortunately, hardware devices have not kept pace with the rapidly increasing\nmemory requirements. To cope with this, researchers have turned to techniques\nsuch as spilling and recomputation, which increase training time, or reduced\nprecision and model pruning, which can affect model accuracy. We present OLLA,\nan algorithm that optimizes the lifetime and memory location of the tensors\nused to train neural networks. Our method reduces the memory usage of existing\nneural networks, without needing any modification to the models or their\ntraining procedures. We formulate the problem as a joint integer linear program\n(ILP). We present several techniques to simplify the encoding of the problem,\nand enable our approach to scale to the size of state-of-the-art neural\nnetworks using an off-the-shelf ILP solver. We experimentally demonstrate that\nOLLA only takes minutes if not seconds to allow the training of neural networks\nusing one-third less memory on average.",
    "descriptor": "",
    "authors": [
      "Benoit Steiner",
      "Mostafa Elhoushi",
      "Jacob Kahn",
      "James Hegarty"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12924"
  },
  {
    "id": "arXiv:2210.12925",
    "title": "TIARA: Multi-grained Retrieval for Robust Question Answering over Large  Knowledge Bases",
    "abstract": "Pre-trained language models (PLMs) have shown their effectiveness in multiple\nscenarios. However, KBQA remains challenging, especially regarding coverage and\ngeneralization settings. This is due to two main factors: i) understanding the\nsemantics of both questions and relevant knowledge from the KB; ii) generating\nexecutable logical forms with both semantic and syntactic correctness. In this\npaper, we present a new KBQA model, TIARA, which addresses those issues by\napplying multi-grained retrieval to help the PLM focus on the most relevant KB\ncontexts, viz., entities, exemplary logical forms, and schema items. Moreover,\nconstrained decoding is used to control the output space and reduce generation\nerrors. Experiments over important benchmarks demonstrate the effectiveness of\nour approach. TIARA outperforms previous SOTA, including those using PLMs or\noracle entity annotations, by at least 4.1 and 1.1 F1 points on GrailQA and\nWebQuestionsSP, respectively.",
    "descriptor": "",
    "authors": [
      "Yiheng Shu",
      "Zhiwei Yu",
      "Yuhan Li",
      "B\u00f6rje F. Karlsson",
      "Tingting Ma",
      "Yuzhong Qu",
      "Chin-Yew Lin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12925"
  },
  {
    "id": "arXiv:2210.12926",
    "title": "Exploring Euphemism Detection in Few-Shot and Zero-Shot Settings",
    "abstract": "This work builds upon the Euphemism Detection Shared Task proposed in the\nEMNLP 2022 FigLang Workshop, and extends it to few-shot and zero-shot settings.\nWe demonstrate a few-shot and zero-shot formulation using the dataset from the\nshared task, and we conduct experiments in these settings using RoBERTa and\nGPT-3. Our results show that language models are able to classify euphemistic\nterms relatively well even on new terms unseen during training, indicating that\nit is able to capture higher-level concepts related to euphemisms.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 Figurative Language Workshop (Euphemism Detection Shared Task). Official code at this https URL\n",
    "authors": [
      "Sedrick Scott Keh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12926"
  },
  {
    "id": "arXiv:2210.12927",
    "title": "The Design and Realization of Multi-agent Obstacle Avoidance based on  Reinforcement Learning",
    "abstract": "Intelligence agents and multi-agent systems play important roles in scenes\nlike the control system of grouped drones, and multi-agent navigation and\nobstacle avoidance which is the foundational function of advanced application\nhas great importance. In multi-agent navigation and obstacle avoidance tasks,\nthe decision-making interactions and dynamic changes of agents are difficult\nfor traditional route planning algorithms or reinforcement learning algorithms\nwith the increased complexity of the environment. The classical multi-agent\nreinforcement learning algorithm, Multi-agent deep deterministic policy\ngradient(MADDPG), solved precedent algorithms' problems of having unstationary\ntraining process and unable to deal with environment randomness. However,\nMADDPG ignored the temporal message hidden beneath agents' interaction with the\nenvironment. Besides, due to its CTDE technique which let each agent's critic\nnetwork to calculate over all agents' action and the whole environment\ninformation, it lacks ability to scale to larger amount of agents. To deal with\nMADDPG's ignorance of the temporal information of the data, this article\nproposes a new algorithm called MADDPG-LSTMactor, which combines MADDPG with\nLong short term memory (LSTM). By using agent's observations of continuous\ntimesteps as the input of its policy network, it allows the LSTM layer to\nprocess the hidden temporal message. Experimental result demonstrated that this\nalgorithm had better performance in scenarios where the amount of agents is\nsmall. Besides, to solve MADDPG's drawback of not being efficient in scenarios\nwhere agents are too many, this article puts forward a light-weight MADDPG\n(MADDPG-L) algorithm, which simplifies the input of critic network. The result\nof experiments showed that this algorithm had better performance than MADDPG\nwhen the amount of agents was large.",
    "descriptor": "",
    "authors": [
      "Enyu Zhao",
      "Chanjuan Liu",
      "Houfu Su",
      "Yang Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12927"
  },
  {
    "id": "arXiv:2210.12928",
    "title": "GFlowOut: Dropout with Generative Flow Networks",
    "abstract": "Bayesian Inference offers principled tools to tackle many critical problems\nwith modern neural networks such as poor calibration and generalization, and\ndata inefficiency. However, scaling Bayesian inference to large architectures\nis challenging and requires restrictive approximations. Monte Carlo Dropout has\nbeen widely used as a relatively cheap way for approximate Inference and to\nestimate uncertainty with deep neural networks. Traditionally, the dropout mask\nis sampled independently from a fixed distribution. Recent works show that the\ndropout mask can be viewed as a latent variable, which can be inferred with\nvariational inference. These methods face two important challenges: (a) the\nposterior distribution over masks can be highly multi-modal which can be\ndifficult to approximate with standard variational inference and (b) it is not\ntrivial to fully utilize sample-dependent information and correlation among\ndropout masks to improve posterior estimation. In this work, we propose\nGFlowOut to address these issues. GFlowOut leverages the recently proposed\nprobabilistic framework of Generative Flow Networks (GFlowNets) to learn the\nposterior distribution over dropout masks. We empirically demonstrate that\nGFlowOut results in predictive distributions that generalize better to\nout-of-distribution data, and provide uncertainty estimates which lead to\nbetter performance in downstream tasks.",
    "descriptor": "",
    "authors": [
      "Dianbo Liu",
      "Moksh Jain",
      "Bonaventure Dossou",
      "Qianli Shen",
      "Salem Lahlou",
      "Anirudh Goyal",
      "Nikolay Malkin",
      "Chris Emezue",
      "Dinghuai Zhang",
      "Nadhir Hassen",
      "Xu Ji",
      "Kenji Kawaguchi",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12928"
  },
  {
    "id": "arXiv:2210.12929",
    "title": "Finding Memo: Extractive Memorization in Constrained Sequence Generation  Tasks",
    "abstract": "Memorization presents a challenge for several constrained Natural Language\nGeneration (NLG) tasks such as Neural Machine Translation (NMT), wherein the\nproclivity of neural models to memorize noisy and atypical samples reacts\nadversely with the noisy (web crawled) datasets. However, previous studies of\nmemorization in constrained NLG tasks have only focused on counterfactual\nmemorization, linking it to the problem of hallucinations. In this work, we\npropose a new, inexpensive algorithm for extractive memorization (exact\ntraining data generation under insufficient context) in constrained sequence\ngeneration tasks and use it to study extractive memorization and its effects in\nNMT. We demonstrate that extractive memorization poses a serious threat to NMT\nreliability by qualitatively and quantitatively characterizing the memorized\nsamples as well as the model behavior in their vicinity. Based on empirical\nobservations, we develop a simple algorithm which elicits non-memorized\ntranslations of memorized samples from the same model, for a large fraction of\nsuch samples. Finally, we show that the proposed algorithm could also be\nleveraged to mitigate memorization in the model through finetuning. We have\nreleased the code to reproduce our results at\nhttps://github.com/vyraun/Finding-Memo.",
    "descriptor": "\nComments: EMNLP Findings 2022\n",
    "authors": [
      "Vikas Raunak",
      "Arul Menezes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12929"
  },
  {
    "id": "arXiv:2210.12930",
    "title": "Social norms of fairness with reputation-based role assignment in the  dictator game",
    "abstract": "A vast body of experiments share the view that social norms are major factors\nfor the emergence of fairness in a population of individuals playing the\ndictator game (DG). Recently, to explore which social norms are conducive to\nsustaining cooperation has obtained considerable concern. However, thus far few\nstudies have investigated how social norms influence the evolution of fairness\nby means of indirect reciprocity. In this study, we propose an indirect\nreciprocal model of the DG and consider that an individual can be assigned as\nthe dictator due to its good reputation. We investigate the `leading eight'\nnorms and all second-order social norms by a two-timescale theoretical\nanalysis. We show that when role assignment is based on reputation, four of the\n`leading eight' norms, including stern judging and simple standing, lead to a\nhigh level of fairness, which increases with the selection intensity. Our work\nalso reveals that not only the correct treatment of making a fair split with\ngood recipients but also distinguishing unjustified unfair split from justified\nunfair split matters in elevating the level of fairness.",
    "descriptor": "\nComments: Chaos, Accepted: 3 October 2022\n",
    "authors": [
      "Qing Li",
      "Songtao Li",
      "Yanling Zhang",
      "Xiaojie Chen",
      "Shuo Yang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.12930"
  },
  {
    "id": "arXiv:2210.12933",
    "title": "Multi-Agent Path Finding via Tree LSTM",
    "abstract": "In recent years, Multi-Agent Path Finding (MAPF) has attracted attention from\nthe fields of both Operations Research (OR) and Reinforcement Learning (RL).\nHowever, in the 2021 Flatland3 Challenge, a competition on MAPF, the best RL\nmethod scored only 27.9, far less than the best OR method. This paper proposes\na new RL solution to Flatland3 Challenge, which scores 125.3, several times\nhigher than the best RL solution before. We creatively apply a novel network\narchitecture, TreeLSTM, to MAPF in our solution. Together with several other RL\ntechniques, including reward shaping, multiple-phase training, and centralized\ncontrol, our solution is comparable to the top 2-3 OR methods.",
    "descriptor": "\nComments: In submission to AAAI23-MAPF\n",
    "authors": [
      "Yuhao Jiang",
      "Kunjie Zhang",
      "Qimai Li",
      "Jiaxin Chen",
      "Xiaolong Zhu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.12933"
  },
  {
    "id": "arXiv:2210.12935",
    "title": "360-MLC: Multi-view Layout Consistency for Self-training and  Hyper-parameter Tuning",
    "abstract": "We present 360-MLC, a self-training method based on multi-view layout\nconsistency for finetuning monocular room-layout models using unlabeled\n360-images only. This can be valuable in practical scenarios where a\npre-trained model needs to be adapted to a new data domain without using any\nground truth annotations. Our simple yet effective assumption is that multiple\nlayout estimations in the same scene must define a consistent geometry\nregardless of their camera positions. Based on this idea, we leverage a\npre-trained model to project estimated layout boundaries from several camera\nviews into the 3D world coordinate. Then, we re-project them back to the\nspherical coordinate and build a probability function, from which we sample the\npseudo-labels for self-training. To handle unconfident pseudo-labels, we\nevaluate the variance in the re-projected boundaries as an uncertainty value to\nweight each pseudo-label in our loss function during training. In addition,\nsince ground truth annotations are not available during training nor in\ntesting, we leverage the entropy information in multiple layout estimations as\na quantitative metric to measure the geometry consistency of the scene,\nallowing us to evaluate any layout estimator for hyper-parameter tuning,\nincluding model selection without ground truth annotations. Experimental\nresults show that our solution achieves favorable performance against\nstate-of-the-art methods when self-training from three publicly available\nsource datasets to a unique, newly labeled dataset consisting of multi-view of\nthe same scenes.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Bolivar Solarte",
      "Chin-Hsuan Wu",
      "Yueh-Cheng Liu",
      "Yi-Hsuan Tsai",
      "Min Sun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12935"
  },
  {
    "id": "arXiv:2210.12936",
    "title": "Selecting and Composing Learning Rate Policies for Deep Neural Networks",
    "abstract": "The choice of learning rate (LR) functions and policies has evolved from a\nsimple fixed LR to the decaying LR and the cyclic LR, aiming to improve the\naccuracy and reduce the training time of Deep Neural Networks (DNNs). This\npaper presents a systematic approach to selecting and composing an LR policy\nfor effective DNN training to meet desired target accuracy and reduce training\ntime within the pre-defined training iterations. It makes three original\ncontributions. First, we develop an LR tuning mechanism for auto-verification\nof a given LR policy with respect to the desired accuracy goal under the\npre-defined training time constraint. Second, we develop an LR policy\nrecommendation system (LRBench) to select and compose good LR policies from the\nsame and/or different LR functions through dynamic tuning, and avoid bad\nchoices, for a given learning task, DNN model and dataset. Third, we extend\nLRBench by supporting different DNN optimizers and show the significant mutual\nimpact of different LR policies and different optimizers. Evaluated using\npopular benchmark datasets and different DNN models (LeNet, CNN3, ResNet), we\nshow that our approach can effectively deliver high DNN test accuracy,\noutperform the existing recommended default LR policies, and reduce the DNN\ntraining time by 1.6$\\sim$6.7$\\times$ to meet a targeted model accuracy.",
    "descriptor": "\nComments: To appear on ACM Transactions on Intelligent Systems and Technology. The source codes: LRBench (this https URL)\n",
    "authors": [
      "Yanzhao Wu",
      "Ling Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12936"
  },
  {
    "id": "arXiv:2210.12940",
    "title": "Heterogeneous Information Crossing on Graphs for Session-based  Recommender Systems",
    "abstract": "Recommender systems are fundamental information filtering techniques to\nrecommend content or items that meet users' personalities and potential needs.\nAs a crucial solution to address the difficulty of user identification and\nunavailability of historical information, session-based recommender systems\nprovide recommendation services that only rely on users' behaviors in the\ncurrent session. However, most existing studies are not well-designed for\nmodeling heterogeneous user behaviors and capturing the relationships between\nthem in practical scenarios. To fill this gap, in this paper, we propose a\nnovel graph-based method, namely Heterogeneous Information Crossing on Graphs\n(HICG). HICG utilizes multiple types of user behaviors in the sessions to\nconstruct heterogeneous graphs, and captures users' current interests with\ntheir long-term preferences by effectively crossing the heterogeneous\ninformation on the graphs. In addition, we also propose an enhanced version,\nnamed HICG-CL, which incorporates contrastive learning (CL) technique to\nenhance item representation ability. By utilizing the item co-occurrence\nrelationships across different sessions, HICG-CL improves the recommendation\nperformance of HICG. We conduct extensive experiments on three real-world\nrecommendation datasets, and the results verify that (i) HICG achieves the\nstate-of-the-art performance by utilizing multiple types of behaviors on the\nheterogeneous graph. (ii) HICG-CL further significantly improves the\nrecommendation performance of HICG by the proposed contrastive learning module.",
    "descriptor": "\nComments: Accepted by ACM Transactions on the Web (TWEB)\n",
    "authors": [
      "Xiaolin Zheng",
      "Rui Wu",
      "Zhongxuan Han",
      "Chaochao Chen",
      "Linxun Chen",
      "Bing Han"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12940"
  },
  {
    "id": "arXiv:2210.12941",
    "title": "Are we really making much progress in unsupervised graph outlier  detection? Revisiting the problem with new insight and superior method",
    "abstract": "A large number of studies on Graph Outlier Detection (GOD) have emerged in\nrecent years due to its wide applications, in which Unsupervised Node Outlier\nDetection (UNOD) on attributed networks is an important area. UNOD focuses on\ndetecting two kinds of typical outliers in graphs: the structural outlier and\nthe contextual outlier. Most existing works conduct the experiments based on\nthe datasets with injected outliers. However, we find that the most widely-used\noutlier injection approach has a serious data leakage issue. By only utilizing\nsuch data leakage, a simple approach can achieve the state-of-the-art\nperformance in detecting outliers. In addition, we observe that most existing\nalgorithms have performance drops with varied injection settings. The other\nmajor issue is on balanced detection performance between the two types of\noutliers, which has not been considered by existing studies. In this paper, we\nanalyze the cause of the data leakage issue in depth since the injection\napproach is a building block to advance UNOD. Moreover, we devise a novel\nvariance-based model to detect structural outliers, which is more robust to\ndifferent injection settings. On top of this, we propose a new framework,\nVariance-based Graph Outlier Detection (VGOD), which combines our\nvariance-based model and attribute reconstruction model to detect outliers in a\nbalanced way. Finally, we conduct extensive experiments to demonstrate the\neffectiveness and the efficiency of VGOD. The results on 5 real-world datasets\nvalidate that VGOD achieves not only the best performance in detecting outliers\nbut also a balanced detection performance between structural and contextual\noutliers.",
    "descriptor": "",
    "authors": [
      "Yihong Huang",
      "Liping Wang",
      "Fan Zhang",
      "Xuemin Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12941"
  },
  {
    "id": "arXiv:2210.12942",
    "title": "Are Current Task-oriented Dialogue Systems Able to Satisfy Impolite  Users?",
    "abstract": "Task-oriented dialogue (TOD) systems have assisted users on many tasks,\nincluding ticket booking and service inquiries. While existing TOD systems have\nshown promising performance in serving customer needs, these systems mostly\nassume that users would interact with the dialogue agent politely. This\nassumption is unrealistic as impatient or frustrated customers may also\ninteract with TOD systems impolitely. This paper aims to address this research\ngap by investigating impolite users' effects on TOD systems. Specifically, we\nconstructed an impolite dialogue corpus and conducted extensive experiments to\nevaluate the state-of-the-art TOD systems on our impolite dialogue corpus. Our\nexperimental results show that existing TOD systems are unable to handle\nimpolite user utterances. We also present a data augmentation method to improve\nTOD performance in impolite dialogues. Nevertheless, handling impolite\ndialogues remains a very challenging research task. We hope by releasing the\nimpolite dialogue corpus and establishing the benchmark evaluations, more\nresearchers are encouraged to investigate this new challenging research task.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Zhiqiang Hu",
      "Roy Kaa-Wei Lee",
      "Nancy F. Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12942"
  },
  {
    "id": "arXiv:2210.12944",
    "title": "Deep Edge Intelligence: Architecture, Key Features, Enabling  Technologies and Challenges",
    "abstract": "With the breakthroughs in Deep Learning, recent years have witnessed a\nmassive surge in Artificial Intelligence applications and services. Meanwhile,\nthe rapid advances in Mobile Computing and Internet of Things has also given\nrise to billions of mobile and smart sensing devices connected to the Internet,\ngenerating zettabytes of data at the network edge. The opportunity to combine\nthese two domains of technologies to power interconnected devices with\nintelligence is likely to pave the way for a new wave of technology\nrevolutions. Embracing this technology revolution, in this article, we present\na novel computing vision named Deep Edge Intelligence (DEI). DEI employs Deep\nLearning, Artificial Intelligence, Cloud and Edge Computing, 5G/6G networks,\nInternet of Things, Microservices, etc. aiming to provision reliable and secure\nintelligence services to every person and organisation at any place with better\nuser experience. The vision, system architecture, key layers and features of\nDEI are also detailed. Finally, we reveal the key enabling technologies and\nresearch challenges associated with it.",
    "descriptor": "",
    "authors": [
      "Prabath Abeysekara",
      "Hai Dong",
      "A.K. Qin"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12944"
  },
  {
    "id": "arXiv:2210.12945",
    "title": "Revisiting Sparse Convolutional Model for Visual Recognition",
    "abstract": "Despite strong empirical performance for image classification, deep neural\nnetworks are often regarded as ``black boxes'' and they are difficult to\ninterpret. On the other hand, sparse convolutional models, which assume that a\nsignal can be expressed by a linear combination of a few elements from a\nconvolutional dictionary, are powerful tools for analyzing natural images with\ngood theoretical interpretability and biological plausibility. However, such\nprincipled models have not demonstrated competitive performance when compared\nwith empirically designed deep networks. This paper revisits the sparse\nconvolutional modeling for image classification and bridges the gap between\ngood empirical performance (of deep learning) and good interpretability (of\nsparse convolutional models). Our method uses differentiable optimization\nlayers that are defined from convolutional sparse coding as drop-in\nreplacements of standard convolutional layers in conventional deep neural\nnetworks. We show that such models have equally strong empirical performance on\nCIFAR-10, CIFAR-100, and ImageNet datasets when compared to conventional neural\nnetworks. By leveraging stable recovery property of sparse modeling, we further\nshow that such models can be much more robust to input corruptions as well as\nadversarial perturbations in testing through a simple proper trade-off between\nsparse regularization and data reconstruction terms. Source code can be found\nat https://github.com/Delay-Xili/SDNet.",
    "descriptor": "\nComments: 17 pages. Accepted by NeurIPS2022\n",
    "authors": [
      "Xili Dai",
      "Mingyang Li",
      "Pengyuan Zhai",
      "Shengbang Tong",
      "Xingjian Gao",
      "Shao-Lun Huang",
      "Zhihui Zhu",
      "Chong You",
      "Yi Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12945"
  },
  {
    "id": "arXiv:2210.12947",
    "title": "IT-RUDA: Information Theory Assisted Robust Unsupervised Domain  Adaptation",
    "abstract": "Distribution shift between train (source) and test (target) datasets is a\ncommon problem encountered in machine learning applications. One approach to\nresolve this issue is to use the Unsupervised Domain Adaptation (UDA) technique\nthat carries out knowledge transfer from a label-rich source domain to an\nunlabeled target domain. Outliers that exist in either source or target\ndatasets can introduce additional challenges when using UDA in practice. In\nthis paper, $\\alpha$-divergence is used as a measure to minimize the\ndiscrepancy between the source and target distributions while inheriting\nrobustness, adjustable with a single parameter $\\alpha$, as the prominent\nfeature of this measure. Here, it is shown that the other well-known\ndivergence-based UDA techniques can be derived as special cases of the proposed\nmethod. Furthermore, a theoretical upper bound is derived for the loss in the\ntarget domain in terms of the source loss and the initial $\\alpha$-divergence\nbetween the two domains. The robustness of the proposed method is validated\nthrough testing on several benchmarked datasets in open-set and partial UDA\nsetups where extra classes existing in target and source datasets are\nconsidered as outliers.",
    "descriptor": "",
    "authors": [
      "Shima Rashidi",
      "Ruwan Tennakoon",
      "Aref Miri Rekavandi",
      "Papangkorn Jessadatavornwong",
      "Amanda Freis",
      "Garret Huff",
      "Mark Easton",
      "Adrian Mouritz",
      "Reza Hoseinnezhad",
      "Alireza Bab-Hadiashar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12947"
  },
  {
    "id": "arXiv:2210.12949",
    "title": "Enhancing Label Consistency on Document-level Named Entity Recognition",
    "abstract": "Named entity recognition (NER) is a fundamental part of extracting\ninformation from documents in biomedical applications. A notable advantage of\nNER is its consistency in extracting biomedical entities in a document context.\nAlthough existing document NER models show consistent predictions, they still\ndo not meet our expectations. We investigated whether the adjectives and\nprepositions within an entity cause a low label consistency, which results in\ninconsistent predictions. In this paper, we present our method, ConNER, which\nenhances the label dependency of modifiers (e.g., adjectives and prepositions)\nto achieve higher label agreement. ConNER refines the draft labels of the\nmodifiers to improve the output representations of biomedical entities. The\neffectiveness of our method is demonstrated on four popular biomedical NER\ndatasets; in particular, its efficacy is proved on two datasets with 7.5-8.6%\nabsolute improvements in the F1 score. We interpret that our ConNER method is\neffective on datasets that have intrinsically low label consistency. In the\nqualitative analysis, we demonstrate how our approach makes the NER model\ngenerate consistent predictions. Our code and resources are available at\nhttps://github.com/dmis-lab/ConNER/.",
    "descriptor": "",
    "authors": [
      "Minbyul Jeong",
      "Jaewoo Kang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12949"
  },
  {
    "id": "arXiv:2210.12952",
    "title": "Ares: A System-Oriented Wargame Framework for Adversarial ML",
    "abstract": "Since the discovery of adversarial attacks against machine learning models\nnearly a decade ago, research on adversarial machine learning has rapidly\nevolved into an eternal war between defenders, who seek to increase the\nrobustness of ML models against adversarial attacks, and adversaries, who seek\nto develop better attacks capable of weakening or defeating these defenses.\nThis domain, however, has found little buy-in from ML practitioners, who are\nneither overtly concerned about these attacks affecting their systems in the\nreal world nor are willing to trade off the accuracy of their models in pursuit\nof robustness against these attacks.\nIn this paper, we motivate the design and implementation of Ares, an\nevaluation framework for adversarial ML that allows researchers to explore\nattacks and defenses in a realistic wargame-like environment. Ares frames the\nconflict between the attacker and defender as two agents in a reinforcement\nlearning environment with opposing objectives. This allows the introduction of\nsystem-level evaluation metrics such as time to failure and evaluation of\ncomplex strategies such as moving target defenses. We provide the results of\nour initial exploration involving a white-box attacker against an adversarially\ntrained defender.",
    "descriptor": "\nComments: Presented at the DLS Workshop at S&P 2022\n",
    "authors": [
      "Farhan Ahmed",
      "Pratik Vaishnavi",
      "Kevin Eykholt",
      "Amir Rahmati"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12952"
  },
  {
    "id": "arXiv:2210.12954",
    "title": "Message Passing-Based Joint User Activity Detection and Channel  Estimation for Temporally-Correlated Massive Access",
    "abstract": "This paper studies the temporally-correlated massive access system where a\nlarge number of users communicate with the base station sporadically and\ncontinue transmitting data in the following frames in high probability when\nbeing active. To exploit both the sparsity and the temporal correlations in the\nuser activities, we formulate the joint user activity detection and channel\nestimation problem in multiple consecutive frames as a dynamic compressed\nsensing (DCS) problem. Particularly, the problem is proposed to be solved under\nBayesian inference to fully utilize the channel statistics and the activity\nevolution process. The hybrid generalized approximate message passing (HyGAMP)\nframework is leveraged to design a HyGAMP-DCS algorithm, which can nearly\nachieve the Bayesian optimality with efficient computations. Specifically, a\nGAMP part for channel estimation and an MP part for activity likelihood update\nare included in the proposed algorithm, then the extrinsic information is\nexchanged between them for performance enhancement. Moveover, we develop the\nexpectation maximization HyGAMP-DCS (EM-HyGAMP-DCS) algorithm to adaptively\nlearn the hyperparameters during the estimation procedure when the system\nstatistics are unavailable. Particularly, the analytical tool of state\nevolution is provided to find the appropriate hyperparameter initialization\nthat ensures EM-HyGAMP-DCS to achieve satisfied performance and fast\nconvergence. From the simulation results, it is validated that our proposed\nalgorithm can significantly outperform the existing methods.",
    "descriptor": "\nComments: 32 pages, 14 figures, major revision\n",
    "authors": [
      "Weifeng Zhu",
      "Meixia Tao",
      "Xiaojun Yuan",
      "Yunfeng Guan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.12954"
  },
  {
    "id": "arXiv:2210.12955",
    "title": "The Case for Accelerating BFT Protocols Using In-Network Ordering",
    "abstract": "Mission critical systems deployed in data centers today are facing more\nsophisticated failures. Byzantine fault tolerant (BFT) protocols are capable of\nmasking these types of failures, but are rarely deployed due to their\nperformance cost and complexity. In this work, we propose a new approach to\ndesigning high performance BFT protocols in data centers. By re-examining the\nordering responsibility between the network and the BFT protocol, we advocate a\nnew abstraction offered by the data center network infrastructure. Concretely,\nwe design a new authenticated ordered multicast primitive (AOM) that provides\ntransferable authentication and non-equivocation guarantees. Feasibility of the\ndesign is demonstrated by two hardware implementations of AOM -- one using HMAC\nand the other using public key cryptography for authentication -- on\nnew-generation programmable switches. We then co-design a new BFT protocol,\nMatrix, that leverages the guarantees of AOM to eliminate cross-replica\ncoordination and authentication in the common case. Evaluation results show\nthat Matrix outperforms state-of-the-art protocols on both latency and\nthroughput metrics by a wide margin, demonstrating the benefit of our new\nnetwork ordering abstraction for BFT systems.",
    "descriptor": "\nComments: 19 pages, 5 figures, under submission\n",
    "authors": [
      "Guangda Sun",
      "Xin Zhe Khooi",
      "Yunfan Li",
      "Mingliang Jiang",
      "Jialin Li"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.12955"
  },
  {
    "id": "arXiv:2210.12957",
    "title": "On the optimization and pruning for Bayesian deep learning",
    "abstract": "The goal of Bayesian deep learning is to provide uncertainty quantification\nvia the posterior distribution. However, exact inference over the weight space\nis computationally intractable due to the ultra-high dimensions of the neural\nnetwork. Variational inference (VI) is a promising approach, but naive\napplication on weight space does not scale well and often underperform on\npredictive accuracy. In this paper, we propose a new adaptive variational\nBayesian algorithm to train neural networks on weight space that achieves high\npredictive accuracy. By showing that there is an equivalence to Stochastic\nGradient Hamiltonian Monte Carlo(SGHMC) with preconditioning matrix, we then\npropose an MCMC within EM algorithm, which incorporates the spike-and-slab\nprior to capture the sparsity of the neural network. The EM-MCMC algorithm\nallows us to perform optimization and model pruning within one-shot. We\nevaluate our methods on CIFAR-10, CIFAR-100 and ImageNet datasets, and\ndemonstrate that our dense model can reach the state-of-the-art performance and\nour sparse model perform very well compared to previously proposed pruning\nschemes.",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Xiongwen Ke",
      "Yanan Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.12957"
  },
  {
    "id": "arXiv:2210.12958",
    "title": "Composition, Attention, or Both?",
    "abstract": "In this paper, we propose a novel architecture called Composition Attention\nGrammars (CAGs) that recursively compose subtrees into a single vector\nrepresentation with a composition function, and selectively attend to previous\nstructural information with a self-attention mechanism. We investigate whether\nthese components -- the composition function and the self-attention mechanism\n-- can both induce human-like syntactic generalization. Specifically, we train\nlanguage models (LMs) with and without these two components with the model\nsizes carefully controlled, and evaluate their syntactic generalization\nperformance against six test circuits on the SyntaxGym benchmark. The results\ndemonstrated that the composition function and the self-attention mechanism\nboth play an important role to make LMs more human-like, and closer inspection\nof linguistic phenomenon implied that the composition function allowed\nsyntactic features, but not semantic features, to percolate into subtree\nrepresentations.",
    "descriptor": "\nComments: Accepted by Findings of EMNLP 2022\n",
    "authors": [
      "Ryo Yoshida",
      "Yohei Oseki"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12958"
  },
  {
    "id": "arXiv:2210.12964",
    "title": "Non-Contrastive Learning-based Behavioural Biometrics for Smart IoT  Devices",
    "abstract": "Behaviour biometrics are being explored as a viable alternative to overcome\nthe limitations of traditional authentication methods such as passwords and\nstatic biometrics. Also, they are being considered as a viable authentication\nmethod for IoT devices such as smart headsets with AR/VR capabilities,\nwearables, and erables, that do not have a large form factor or the ability to\nseamlessly interact with the user. Recent behavioural biometric solutions use\ndeep learning models that require large amounts of annotated training data.\nCollecting such volumes of behaviour biometrics data raises privacy and\nusability concerns. To this end, we propose using SimSiam-based non-contrastive\nself-supervised learning to improve the label efficiency of behavioural\nbiometric systems. The key idea is to use large volumes of unlabelled (and\nanonymised) data to build good feature extractors that can be subsequently used\nin supervised settings. Using two EEG datasets, we show that at lower amounts\nof labelled data, non-contrastive learning performs 4%-11% more than\nconventional methods such as supervised learning and data augmentation. We also\nshow that, in general, self-supervised learning methods perform better than\nother baselines. Finally, through careful experimentation, we show various\nmodifications that can be incorporated into the non-contrastive learning\nprocess to archive high performance.",
    "descriptor": "\nComments: NA\n",
    "authors": [
      "Oshan Jayawardana",
      "Fariza Rashid",
      "Suranga Seneviratne"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12964"
  },
  {
    "id": "arXiv:2210.12965",
    "title": "High-Resolution Image Editing via Multi-Stage Blended Diffusion",
    "abstract": "Diffusion models have shown great results in image generation and in image\nediting. However, current approaches are limited to low resolutions due to the\ncomputational cost of training diffusion models for high-resolution generation.\nWe propose an approach that uses a pre-trained low-resolution diffusion model\nto edit images in the megapixel range. We first use Blended Diffusion to edit\nthe image at a low resolution, and then upscale it in multiple stages, using a\nsuper-resolution model and Blended Diffusion. Using our approach, we achieve\nhigher visual fidelity than by only applying off the shelf super-resolution\nmethods to the output of the diffusion model. We also obtain better global\nconsistency than directly using the diffusion model at a higher resolution.",
    "descriptor": "\nComments: Machine Learning for Creativity and Design Workshop at NeurIPS 2022\n",
    "authors": [
      "Johannes Ackermann",
      "Minjun Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12965"
  },
  {
    "id": "arXiv:2210.12971",
    "title": "Holistically-Attracted Wireframe Parsing: From Supervised to  Self-Supervised Learning",
    "abstract": "This paper presents Holistically-Attracted Wireframe Parsing (HAWP) for 2D\nimages using both fully supervised and self-supervised learning paradigms. At\nthe core is a parsimonious representation that encodes a line segment using a\nclosed-form 4D geometric vector, which enables lifting line segments in\nwireframe to an end-to-end trainable holistic attraction field that has\nbuilt-in geometry-awareness, context-awareness and robustness. The proposed\nHAWP consists of three components: generating line segment and end-point\nproposal, binding line segment and end-point, and end-point-decoupled\nlines-of-interest verification. For self-supervised learning, a\nsimulation-to-reality pipeline is exploited in which a HAWP is first trained\nusing synthetic data and then used to ``annotate\" wireframes in real images\nwith Homographic Adaptation. With the self-supervised annotations, a HAWP model\nfor real images is trained from scratch. In experiments, the proposed HAWP\nachieves state-of-the-art performance in both the Wireframe dataset and the\nYorkUrban dataset in fully-supervised learning. It also demonstrates a\nsignificantly better repeatability score than prior arts with much more\nefficient training in self-supervised learning. Furthermore, the\nself-supervised HAWP shows great potential for general wireframe parsing\nwithout onerous wireframe labels.",
    "descriptor": "\nComments: Journal extension of arXiv:2003.01663\n",
    "authors": [
      "Nan Xue",
      "Tianfu Wu",
      "Song Bai",
      "Fu-Dong Wang",
      "Gui-Song Xia",
      "Liangpei Zhang",
      "Philip H.S. Torr"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12971"
  },
  {
    "id": "arXiv:2210.12974",
    "title": "Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks",
    "abstract": "Fusing deep learning models trained on separately located clients into a\nglobal model in a one-shot communication round is a straightforward\nimplementation of Federated Learning. Although current model fusion methods are\nshown experimentally valid in fusing neural networks with almost identical\narchitectures, they are rarely theoretically analyzed. In this paper, we reveal\nthe phenomenon of neuron disturbing, where neurons from heterogeneous local\nmodels interfere with each other mutually. We give detailed explanations from a\nBayesian viewpoint combining the data heterogeneity among clients and\nproperties of neural networks. Furthermore, to validate our findings, we\npropose an experimental method that excludes neuron disturbing and fuses neural\nnetworks via adaptively selecting a local model, called AMS, to execute the\nprediction according to the input. The experiments demonstrate that AMS is more\nrobust in data heterogeneity than general model fusion and ensemble methods.\nThis implies the necessity of considering neural disturbing in model fusion.\nBesides, AMS is available for fusing models with varying architectures as an\nexperimental algorithm, and we also list several possible extensions of AMS for\nfuture work.",
    "descriptor": "\nComments: 16 pages, 3 figures\n",
    "authors": [
      "Biao Zhang",
      "Peng Xiao",
      "Shuqin Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12974"
  },
  {
    "id": "arXiv:2210.12977",
    "title": "Language-free Training for Zero-shot Video Grounding",
    "abstract": "Given an untrimmed video and a language query depicting a specific temporal\nmoment in the video, video grounding aims to localize the time interval by\nunderstanding the text and video simultaneously. One of the most challenging\nissues is an extremely time- and cost-consuming annotation collection,\nincluding video captions in a natural language form and their corresponding\ntemporal regions. In this paper, we present a simple yet novel training\nframework for video grounding in the zero-shot setting, which learns a network\nwith only video data without any annotation. Inspired by the recent\nlanguage-free paradigm, i.e. training without language data, we train the\nnetwork without compelling the generation of fake (pseudo) text queries into a\nnatural language form. Specifically, we propose a method for learning a video\ngrounding model by selecting a temporal interval as a hypothetical correct\nanswer and considering the visual feature selected by our method in the\ninterval as a language feature, with the help of the well-aligned\nvisual-language space of CLIP. Extensive experiments demonstrate the prominence\nof our language-free training framework, outperforming the existing zero-shot\nvideo grounding method and even several weakly-supervised approaches with large\nmargins on two standard datasets.",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Dahye Kim",
      "Jungin Park",
      "Jiyoung Lee",
      "Seongheon Park",
      "Kwanghoon Sohn"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12977"
  },
  {
    "id": "arXiv:2210.12979",
    "title": "Multi-Type Conversational Question-Answer Generation with Closed-ended  and Unanswerable Questions",
    "abstract": "Conversational question answering (CQA) facilitates an incremental and\ninteractive understanding of a given context, but building a CQA system is\ndifficult for many domains due to the problem of data scarcity. In this paper,\nwe introduce a novel method to synthesize data for CQA with various question\ntypes, including open-ended, closed-ended, and unanswerable questions. We\ndesign a different generation flow for each question type and effectively\ncombine them in a single, shared framework. Moreover, we devise a hierarchical\nanswerability classification (hierarchical AC) module that improves quality of\nthe synthetic data while acquiring unanswerable questions. Manual inspections\nshow that synthetic data generated with our framework have characteristics very\nsimilar to those of human-generated conversations. Across four domains, CQA\nsystems trained on our synthetic data indeed show good performance close to the\nsystems trained on human-annotated data.",
    "descriptor": "\nComments: AACL-IJCNLP 2022\n",
    "authors": [
      "Seonjeong Hwang",
      "Yunsu Kim",
      "Gary Geunbae Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12979"
  },
  {
    "id": "arXiv:2210.12985",
    "title": "Maknuune: A Large Open Palestinian Arabic Lexicon",
    "abstract": "We present Maknuune, a large open lexicon for the Palestinian Arabic dialect.\nMaknuune has over 36K entries from 17K lemmas, and 3.7K roots. All entries\ninclude diacritized Arabic orthography, phonological transcription and English\nglosses. Some entries are enriched with additional information such as broken\nplurals and templatic feminine forms, associated phrases and collocations,\nStandard Arabic glosses, and examples or notes on grammar, usage, or location\nof collected entry.",
    "descriptor": "",
    "authors": [
      "Shahd Dibas",
      "Christian Khairallah",
      "Nizar Habash",
      "Omar Fayez Sadi",
      "Tariq Sairafy",
      "Karmel Sarabta",
      "Abrar Ardah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.12985"
  },
  {
    "id": "arXiv:2210.12989",
    "title": "Robust Object Detection in Remote Sensing Imagery with Noisy and Sparse  Geo-Annotations (Full Version)",
    "abstract": "Recently, the availability of remote sensing imagery from aerial vehicles and\nsatellites constantly improved. For an automated interpretation of such data,\ndeep-learning-based object detectors achieve state-of-the-art performance.\nHowever, established object detectors require complete, precise, and correct\nbounding box annotations for training. In order to create the necessary\ntraining annotations for object detectors, imagery can be georeferenced and\ncombined with data from other sources, such as points of interest localized by\nGPS sensors. Unfortunately, this combination often leads to poor object\nlocalization and missing annotations. Therefore, training object detectors with\nsuch data often results in insufficient detection performance. In this paper,\nwe present a novel approach for training object detectors with extremely noisy\nand incomplete annotations. Our method is based on a teacher-student learning\nframework and a correction module accounting for imprecise and missing\nannotations. Thus, our method is easy to use and can be combined with arbitrary\nobject detectors. We demonstrate that our approach improves standard detectors\nby 37.1\\% $AP_{50}$ on a noisy real-world remote-sensing dataset. Furthermore,\nour method achieves great performance gains on two datasets with synthetic\nnoise. Code is available at\n\\url{https://github.com/mxbh/robust_object_detection}.",
    "descriptor": "",
    "authors": [
      "Maximilian Bernhard",
      "Matthias Schubert"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12989"
  },
  {
    "id": "arXiv:2210.12990",
    "title": "Optimal activity and battery scheduling algorithm using load and solar  generation forecasts",
    "abstract": "Energy usage optimal scheduling has attracted great attention in the power\nsystem community, where various methodologies have been proposed. However, in\nreal-world applications, the optimal scheduling problems require reliable\nenergy forecasting, which is scarcely discussed as a joint solution to the\nscheduling problem. The 5\\textsuperscript{th} IEEE Computational Intelligence\nSociety (IEEE-CIS) competition raised a practical problem of decreasing the\nelectricity bill by scheduling building activities, where forecasting the solar\nenergy generation and building consumption is a necessity. To solve this\nproblem, we propose a technical sequence for tackling the solar PV and demand\nforecast and optimal scheduling problems, where solar generation prediction\nmethods and an optimal university lectures scheduling algorithm are proposed.",
    "descriptor": "\nComments: 6 pages, 4 figures, 3 tables. Accepted for IEEE proceedings as a conference paper for AUPEC 2022\n",
    "authors": [
      "Yogesh Pipada Sunil Kumar",
      "Rui Yuan",
      "Nam Trong Dinh",
      "S. Ali Pourmousavi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12990"
  },
  {
    "id": "arXiv:2210.12996",
    "title": "Static Information Flow Control Made Simpler",
    "abstract": "Static information flow control (IFC) systems provide the ability to restrict\ndata flows within a program, enabling vulnerable functionality or confidential\ndata to be statically isolated from unsecured data or program logic. Despite\nthe wide applicability of IFC as a mechanism for guaranteeing confidentiality\nand integrity -- the fundamental properties on which computer security relies\n-- existing IFC systems have seen little use, requiring users to reason about\ncomplicated mechanisms such as lattices of security labels and dual notions of\nconfidentiality and integrity within these lattices. We propose a system that\ndiverges significantly from previous work on information flow control, opting\nto reason directly about the data that programmers already work with. In doing\nso, we naturally and seamlessly combine the clasically separate notions of\nconfidentiality and integrity into one unified framework, further simplifying\nreasoning. We motivate and showcase our work through two case studies on TLS\nprivate key management: one for Rocket, a popular Rust web framework, and\nanother for Conduit, a server implementation for the Matrix messaging service\nwritten in Rust.",
    "descriptor": "\nComments: 12 pages, 10 figures\n",
    "authors": [
      "Hemant Gouni",
      "Jonathan Aldrich"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.12996"
  },
  {
    "id": "arXiv:2210.12997",
    "title": "Are Current Decoding Strategies Capable of Facing the Challenges of  Visual Dialogue?",
    "abstract": "Decoding strategies play a crucial role in natural language generation\nsystems. They are usually designed and evaluated in open-ended text-only tasks,\nand it is not clear how different strategies handle the numerous challenges\nthat goal-oriented multimodal systems face (such as grounding and\ninformativeness). To answer this question, we compare a wide variety of\ndifferent decoding strategies and hyper-parameter configurations in a Visual\nDialogue referential game. Although none of them successfully balance lexical\nrichness, accuracy in the task, and visual grounding, our in-depth analysis\nallows us to highlight the strengths and weaknesses of each decoding strategy.\nWe believe our findings and suggestions may serve as a starting point for\ndesigning more effective decoding algorithms that handle the challenges of\nVisual Dialogue tasks.",
    "descriptor": "\nComments: Accepted at INLG 2022\n",
    "authors": [
      "Amit Kumar Chaudhary",
      "Alex J. Lucassen",
      "Ioanna Tsani",
      "Alberto Testoni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12997"
  },
  {
    "id": "arXiv:2210.13001",
    "title": "Modeling Information Change in Science Communication with Semantically  Matched Paraphrases",
    "abstract": "Whether the media faithfully communicate scientific information has long been\na core issue to the science community. Automatically identifying paraphrased\nscientific findings could enable large-scale tracking and analysis of\ninformation changes in the science communication process, but this requires\nsystems to understand the similarity between scientific information across\nmultiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND\nINFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific\nfindings annotated for degree of information change. SPICED contains 6,000\nscientific finding pairs extracted from news stories, social media discussions,\nand full texts of original papers. We demonstrate that SPICED poses a\nchallenging task and that models trained on SPICED improve downstream\nperformance on evidence retrieval for fact checking of real-world scientific\nclaims. Finally, we show that models trained on SPICED can reveal large-scale\ntrends in the degrees to which people and organizations faithfully communicate\nnew scientific findings. Data, code, and pre-trained models are available at\nthis http URL",
    "descriptor": "\nComments: In EMNLP 2022; 25 pages; 11 figures; 6 tables\n",
    "authors": [
      "Dustin Wright",
      "Jiaxin Pei",
      "David Jurgens",
      "Isabelle Augenstein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13001"
  },
  {
    "id": "arXiv:2210.13002",
    "title": "An Empirical Revisiting of Linguistic Knowledge Fusion in Language  Understanding Tasks",
    "abstract": "Though linguistic knowledge emerges during large-scale language model\npretraining, recent work attempt to explicitly incorporate human-defined\nlinguistic priors into task-specific fine-tuning. Infusing language models with\nsyntactic or semantic knowledge from parsers has shown improvements on many\nlanguage understanding tasks. To further investigate the effectiveness of\nstructural linguistic priors, we conduct empirical study of replacing parsed\ngraphs or trees with trivial ones (rarely carrying linguistic knowledge e.g.,\nbalanced tree) for tasks in the GLUE benchmark. Encoding with trivial graphs\nachieves competitive or even better performance in fully-supervised and\nfew-shot settings. It reveals that the gains might not be significantly\nattributed to explicit linguistic priors but rather to more feature\ninteractions brought by fusion layers. Hence we call for attention to using\ntrivial graphs as necessary baselines to design advanced knowledge fusion\nmethods in the future.",
    "descriptor": "\nComments: EMNLP 2022 Main Conference\n",
    "authors": [
      "Changlong Yu",
      "Tianyi Xiao",
      "Lingpeng Kong",
      "Yangqiu Song",
      "Wilfred Ng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13002"
  },
  {
    "id": "arXiv:2210.13004",
    "title": "On representation of natural image patches",
    "abstract": "Starting from the first principle I derive an unsupervised learning method\nnamed even code to model local statistics of natural images. The first version\nuses orthogonal bases with independent states to model simple probability\ndistribution of a few pixels. The second version uses a microscopic loss\nfunction to learn a nonlinear sparse binary representation of image patches.\nThe distance in the binary representation space reflects image patch\nsimilarity. The learned model also has local edge detecting and orientation\nselective units like early visual systems.",
    "descriptor": "",
    "authors": [
      "Cheng Guo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.13004"
  },
  {
    "id": "arXiv:2210.13005",
    "title": "Towards Out-of-Distribution Sequential Event Prediction: A Causal  Treatment",
    "abstract": "The goal of sequential event prediction is to estimate the next event based\non a sequence of historical events, with applications to sequential\nrecommendation, user behavior analysis and clinical treatment. In practice, the\nnext-event prediction models are trained with sequential data collected at one\ntime and need to generalize to newly arrived sequences in remote future, which\nrequires models to handle temporal distribution shift from training to testing.\nIn this paper, we first take a data-generating perspective to reveal a negative\nresult that existing approaches with maximum likelihood estimation would fail\nfor distribution shift due to the latent context confounder, i.e., the common\ncause for the historical events and the next event. Then we devise a new\nlearning objective based on backdoor adjustment and further harness variational\ninference to make it tractable for sequence learning problems. On top of that,\nwe propose a framework with hierarchical branching structures for learning\ncontext-specific representations. Comprehensive experiments on diverse tasks\n(e.g., sequential recommendation) demonstrate the effectiveness, applicability\nand scalability of our method with various off-the-shelf models as backbones.",
    "descriptor": "\nComments: in NeurIPS 2022\n",
    "authors": [
      "Chenxiao Yang",
      "Qitian Wu",
      "Qingsong Wen",
      "Zhiqiang Zhou",
      "Liang Sun",
      "Junchi Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.13005"
  },
  {
    "id": "arXiv:2210.13007",
    "title": "Iterative Patch Selection for High-Resolution Image Recognition",
    "abstract": "High-resolution images are prevalent in various applications, such as\nautonomous driving and computer-aided diagnosis. However, training neural\nnetworks on such images is computationally challenging and easily leads to\nout-of-memory errors even on modern GPUs. We propose a simple method, Iterative\nPatch Selection (IPS), which decouples the memory usage from the input size and\nthus enables the processing of arbitrarily large images under tight hardware\nconstraints. IPS achieves this by selecting only the most salient patches,\nwhich are then aggregated into a global representation for image recognition.\nFor both patch selection and aggregation, a cross-attention based transformer\nis introduced, which exhibits a close connection to Multiple Instance Learning.\nOur method demonstrates strong performance and has wide applicability across\ndifferent domains, training regimes and image sizes while using minimal\naccelerator memory. For example, we are able to finetune our model on\nwhole-slide images consisting of up to 250k patches (>16 gigapixels) with only\n5 GB of GPU VRAM at a batch size of 16.",
    "descriptor": "\nComments: 20 pages, 10 figures\n",
    "authors": [
      "Benjamin Bergner",
      "Christoph Lippert",
      "Aravindh Mahendran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.13007"
  },
  {
    "id": "arXiv:2210.13010",
    "title": "Active Reconfigurable Intelligent Surface Aided Surveillance Scheme",
    "abstract": "This letter attempts to design a surveillance scheme by adopting an active\nreconfigurable intelligent surface (RIS). Different from the conventional\npassive RIS, the active RIS could not only adjust the phase shift but also\namplify the amplitude of the reflected signal. With such reflecting, the\nreflected signal of active RIS could jointly adjust the signal to interference\nplus noise ratio (SINR) of the suspicious receiver and the legitimate monitor,\nhence the proactive eavesdropping at the physical layer could be effectively\nrealized. We formulate the optimization problem with the target of maximizing\nthe eavesdropping rate to obtain the optimal reflecting coefficient matrix of\nthe active RIS. The formulated optimization problem is nonconvex fractional\nprogramming and challenging to deal with. We then solve the problem by\napproximating it as a series of convex constraints. Simulation results validate\nthe effectiveness of our designed surveillance scheme and show that the\nproposed active RIS aided surveillance scheme has good performance in terms of\neavesdropping rate compared with the scheme with passive RIS.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Xinyue Hu",
      "Yibo Yi",
      "Kun Li",
      "Hongwei Zhang",
      "Caihong Kai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.13010"
  },
  {
    "id": "arXiv:2210.13011",
    "title": "On All-Action Policy Gradients",
    "abstract": "In this paper, we analyze the variance of stochastic policy gradient with\nmany action samples per state (all-action SPG). We decompose the variance of\nSPG and derive an optimality condition for all-action SPG. The optimality\ncondition shows when all-action SPG should be preferred over single-action\ncounterpart and allows to determine a variance-minimizing sampling scheme in\nSPG estimation. Furthermore, we propose dynamics-all-action (DAA) module, an\naugmentation that allows for all-action sampling without manipulation of the\nenvironment. DAA addresses the problems associated with using a Q-network for\nall-action sampling and can be readily applied to any on-policy SPG algorithm.\nWe find that using DAA with a canonical on-policy algorithm (PPO) yields better\nsample efficiency and higher policy returns on a variety of challenging\ncontinuous action environments.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Michal Nauman",
      "Marek Cygan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13011"
  },
  {
    "id": "arXiv:2210.13013",
    "title": "Towards Understanding Player Behavior in Blockchain Games: A Case Study  of Aavegotchi",
    "abstract": "Blockchain games introduce unique gameplay and incentive mechanisms by\nallowing players to be rewarded with in-game assets or tokens through financial\nactivities. However, most blockchain games are not comparable to traditional\ngames in terms of lifespan and player engagement. In this paper, we try to see\nthe big picture in a small way to explore and determine the impact of gameplay\nand financial factors on player behavior in blockchain games. Taking Aavegotchi\nas an example, we collect one year of operation data to build player profiles.\nWe perform an in-depth analysis of player behavior from the macroscopic data\nand apply an unsupervised clustering method to distinguish the attraction of\nthe gameplay and incentives. Our results reveal that the whole game is held up\nby a small number of players with high-frequent interaction or vast amounts of\nfunds invested. Financial incentives are indispensable for blockchain games for\nthey provide attraction and optional ways for players to engage with the game.\nHowever, financial services are tightly linked to the free market. The game\nwill face an irreversible loss of players when the market experiences\ndepression. For blockchain games, well-designed gameplay should be the\nfundamental basis for the long-lasting retention of players.",
    "descriptor": "",
    "authors": [
      "Yu Jiang",
      "Tian Min",
      "Sizheng Fan",
      "Rongqi Tao",
      "Wei Cai"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13013"
  },
  {
    "id": "arXiv:2210.13014",
    "title": "Geometric Knowledge Distillation: Topology Compression for Graph Neural  Networks",
    "abstract": "We study a new paradigm of knowledge transfer that aims at encoding graph\ntopological information into graph neural networks (GNNs) by distilling\nknowledge from a teacher GNN model trained on a complete graph to a student GNN\nmodel operating on a smaller or sparser graph. To this end, we revisit the\nconnection between thermodynamics and the behavior of GNN, based on which we\npropose Neural Heat Kernel (NHK) to encapsulate the geometric property of the\nunderlying manifold concerning the architecture of GNNs. A fundamental and\nprincipled solution is derived by aligning NHKs on teacher and student models,\ndubbed as Geometric Knowledge Distillation. We develop non- and parametric\ninstantiations and demonstrate their efficacy in various experimental settings\nfor knowledge distillation regarding different types of privileged topological\ninformation and teacher-student schemes.",
    "descriptor": "\nComments: in NeurIPS 2022\n",
    "authors": [
      "Chenxiao Yang",
      "Qitian Wu",
      "Junchi Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13014"
  },
  {
    "id": "arXiv:2210.13015",
    "title": "An Opponent-Aware Reinforcement Learning Method for Team-to-Team  Multi-Vehicle Pursuit via Maximizing Mutual Information Indicator",
    "abstract": "The pursuit-evasion game in Smart City brings a profound impact on the\nMulti-vehicle Pursuit (MVP) problem, when police cars cooperatively pursue\nsuspected vehicles. Existing studies on the MVP problems tend to set evading\nvehicles to move randomly or in a fixed prescribed route. The opponent modeling\nmethod has proven considerable promise in tackling the non-stationary caused by\nthe adversary agent. However, most of them focus on two-player competitive\ngames and easy scenarios without the interference of environments. This paper\nconsiders a Team-to-Team Multi-vehicle Pursuit (T2TMVP) problem in the\ncomplicated urban traffic scene where the evading vehicles adopt the\npre-trained dynamic strategies to execute decisions intelligently. To solve\nthis problem, we propose an opponent-aware reinforcement learning via\nmaximizing mutual information indicator (OARLM2I2) method to improve pursuit\nefficiency in the complicated environment. First, a sequential encoding-based\nopponents joint strategy modeling (SEOJSM) mechanism is proposed to generate\nevading vehicles' joint strategy model, which assists the multi-agent\ndecision-making process based on deep Q-network (DQN). Then, we design a mutual\ninformation-united loss, simultaneously considering the reward fed back from\nthe environment and the effectiveness of opponents' joint strategy model, to\nupdate pursuing vehicles' decision-making process. Extensive experiments based\non SUMO demonstrate our method outperforms other baselines by 21.48% on average\nin reducing pursuit time. The code is available at\n\\url{https://github.com/ANT-ITS/OARLM2I2}.",
    "descriptor": "\nComments: 8 pages, 6 figures, accepted by MSN2022\n",
    "authors": [
      "Qinwen Wang",
      "Xinhang Li",
      "Zheng Yuan",
      "Yiying Yang",
      "Chen Xu",
      "Lin Zhang"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.13015"
  },
  {
    "id": "arXiv:2210.13016",
    "title": "Cards Against AI: Predicting Humor in a Fill-in-the-blank Party Game",
    "abstract": "Humor is an inherently social phenomenon, with humorous utterances shaped by\nwhat is socially and culturally accepted. Understanding humor is an important\nNLP challenge, with many applications to human-computer interactions. In this\nwork we explore humor in the context of Cards Against Humanity -- a party game\nwhere players complete fill-in-the-blank statements using cards that can be\noffensive or politically incorrect. We introduce a novel dataset of 300,000\nonline games of Cards Against Humanity, including 785K unique jokes, analyze it\nand provide insights. We trained machine learning models to predict the winning\njoke per game, achieving performance twice as good (20\\%) as random, even\nwithout any user information. On the more difficult task of judging novel\ncards, we see the models' ability to generalize is moderate. Interestingly, we\nfind that our models are primarily focused on punchline card, with the context\nhaving little impact. Analyzing feature importance, we observe that short,\ncrude, juvenile punchlines tend to win.",
    "descriptor": "\nComments: Conditionally accepted in EMNLP 2022 short findings. 5 pages\n",
    "authors": [
      "Dan Ofer",
      "Dafna Shahaf"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "General Literature (cs.GL)"
    ],
    "url": "https://arxiv.org/abs/2210.13016"
  },
  {
    "id": "arXiv:2210.13020",
    "title": "On Tools for Completeness of Kleene Algebra with Hypotheses",
    "abstract": "In the literature on Kleene algebra, a number of variants have been proposed\nwhich impose additional structure specified by a theory, such as Kleene algebra\nwith tests (KAT) and the recent Kleene algebra with observations (KAO), or make\nspecific assumptions about certain constants, as for instance in NetKAT. Many\nof these variants fit within the unifying perspective offered by Kleene algebra\nwith hypotheses, which comes with a canonical language model constructed from a\ngiven set of hypotheses. For the case of KAT, this model corresponds to the\nfamiliar interpretation of expressions as languages of guarded strings. A\nrelevant question therefore is whether Kleene algebra together with a given set\nof hypotheses is complete with respect to its canonical language model. In this\npaper, we revisit, combine and extend existing results on this question to\nobtain tools for proving completeness in a modular way. We showcase these tools\nby giving new and modular proofs of completeness for KAT, KAO and NetKAT, and\nwe prove completeness for new variants of KAT: KAT extended with a constant for\nthe full relation, KAT extended with a converse operation, and a version of KAT\nwhere the collection of tests only forms a distributive lattice.",
    "descriptor": "",
    "authors": [
      "Damien Pous",
      "Jurriaan Rot",
      "Jana Wagemaker"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.13020"
  },
  {
    "id": "arXiv:2210.13023",
    "title": "FairGen: Fair Synthetic Data Generation",
    "abstract": "With the rising adoption of Machine Learning across the domains like banking,\npharmaceutical, ed-tech, etc, it has become utmost important to adopt\nresponsible AI methods to ensure models are not unfairly discriminating against\nany group. Given the lack of clean training data, generative adversarial\ntechniques are preferred to generate synthetic data with several\nstate-of-the-art architectures readily available across various domains from\nunstructured data such as text, images to structured datasets modelling fraud\ndetection and many more. These techniques overcome several challenges such as\nclass imbalance, limited training data, restricted access to data due to\nprivacy issues. Existing work focusing on generating fair data either works for\na certain GAN architecture or is very difficult to tune across the GANs. In\nthis paper, we propose a pipeline to generate fairer synthetic data independent\nof the GAN architecture. The proposed paper utilizes a pre-processing algorithm\nto identify and remove bias inducing samples. In particular, we claim that\nwhile generating synthetic data most GANs amplify bias present in the training\ndata but by removing these bias inducing samples, GANs essentially focuses more\non real informative samples. Our experimental evaluation on two open-source\ndatasets demonstrates how the proposed pipeline is generating fair data along\nwith improved performance in some cases.",
    "descriptor": "",
    "authors": [
      "Bhushan Chaudhari",
      "Himanshu Choudhary",
      "Aakash Agarwal",
      "Kamna Meena",
      "Tanmoy Bhowmik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13023"
  },
  {
    "id": "arXiv:2210.13024",
    "title": "Investigating the detection of Tortured Phrases in Scientific Literature",
    "abstract": "With the help of online tools, unscrupulous authors can today generate a\npseudo-scientific article and attempt to publish it. Some of these tools work\nby replacing or paraphrasing existing texts to produce new content, but they\nhave a tendency to generate nonsensical expressions. A recent study introduced\nthe concept of 'tortured phrase', an unexpected odd phrase that appears instead\nof the fixed expression. E.g. counterfeit consciousness instead of artificial\nintelligence. The present study aims at investigating how tortured phrases,\nthat are not yet listed, can be detected automatically. We conducted several\nexperiments, including non-neural binary classification, neural binary\nclassification and cosine similarity comparison of the phrase tokens, yielding\nnoticeable results.",
    "descriptor": "",
    "authors": [
      "Puthineath Lay",
      "Martin Lentschat",
      "Cyril Labb\u00e9"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.13024"
  },
  {
    "id": "arXiv:2210.13025",
    "title": "On the Effectiveness of Automated Metrics for Text Generation Systems",
    "abstract": "A major challenge in the field of Text Generation is evaluation because we\nlack a sound theory that can be leveraged to extract guidelines for evaluation\ncampaigns. In this work, we propose a first step towards such a theory that\nincorporates different sources of uncertainty, such as imperfect automated\nmetrics and insufficiently sized test sets. The theory has practical\napplications, such as determining the number of samples needed to reliably\ndistinguish the performance of a set of Text Generation systems in a given\nsetting. We showcase the application of the theory on the WMT 21 and\nSpot-The-Bot evaluation data and outline how it can be leveraged to improve the\nevaluation protocol regarding the reliability, robustness, and significance of\nthe evaluation outcome.",
    "descriptor": "",
    "authors": [
      "Pius von D\u00e4niken",
      "Jan Deriu",
      "Don Tuggener",
      "Mark Cieliebak"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13025"
  },
  {
    "id": "arXiv:2210.13028",
    "title": "Generalised Likelihood Ratio Testing Adversaries through the  Differential Privacy Lens",
    "abstract": "Differential Privacy (DP) provides tight upper bounds on the capabilities of\noptimal adversaries, but such adversaries are rarely encountered in practice.\nUnder the hypothesis testing/membership inference interpretation of DP, we\nexamine the Gaussian mechanism and relax the usual assumption of a\nNeyman-Pearson-Optimal (NPO) adversary to a Generalized Likelihood Test (GLRT)\nadversary. This mild relaxation leads to improved privacy guarantees, which we\nexpress in the spirit of Gaussian DP and $(\\varepsilon, \\delta)$-DP, including\ncomposition and sub-sampling results. We evaluate our results numerically and\nfind them to match the theoretical upper bounds.",
    "descriptor": "",
    "authors": [
      "Georgios Kaissis",
      "Alexander Ziller",
      "Stefan Kolek Martinez de Azagra",
      "Daniel Rueckert"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.13028"
  },
  {
    "id": "arXiv:2210.13029",
    "title": "Multilingual Auxiliary Tasks Training: Bridging the Gap between  Languages for Zero-Shot Transfer of Hate Speech Detection Models",
    "abstract": "Zero-shot cross-lingual transfer learning has been shown to be highly\nchallenging for tasks involving a lot of linguistic specificities or when a\ncultural gap is present between languages, such as in hate speech detection. In\nthis paper, we highlight this limitation for hate speech detection in several\ndomains and languages using strict experimental settings. Then, we propose to\ntrain on multilingual auxiliary tasks -- sentiment analysis, named entity\nrecognition, and tasks relying on syntactic information -- to improve zero-shot\ntransfer of hate speech detection models across languages. We show how hate\nspeech detection models benefit from a cross-lingual {\\em knowledge proxy}\nbrought by auxiliary tasks fine-tuning and highlight these tasks' positive\nimpact on bridging the hate speech linguistic and cultural gap between\nlanguages.",
    "descriptor": "\nComments: Accepted to Findings of AACL-IJCNLP 2022\n",
    "authors": [
      "Syrielle Montariol",
      "Arij Riabi",
      "Djam\u00e9 Seddah"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13029"
  },
  {
    "id": "arXiv:2210.13030",
    "title": "Self-supervised Rewiring of Pre-trained Speech Encoders: Towards Faster  Fine-tuning with Less Labels in Speech Processing",
    "abstract": "Pre-trained speech Transformers have facilitated great success across various\nspeech processing tasks. However, fine-tuning these encoders for downstream\ntasks require sufficiently large training data to converge or to achieve\nstate-of-the-art. In text domain this has been partly attributed to\nsub-optimality of the representation space in pre-trained Transformers. In this\nwork, we take a sober look into pre-trained speech encoders and rewire their\nrepresentation space without requiring any task-specific labels. Our method\nutilises neutrally synthesised version of audio inputs along with frame masking\nto construct positive pairs for contrastive self-supervised learning. When used\nfor augmenting the wav2vec 2 encoder, we observe consistent improvement of\nisotropy in the representation space. Our experiments on 6 speech processing\ntasks, exhibit a significant convergence speedup during task fine-tuning as\nwell as consistent task improvement, specially in low-resource settings.",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "Hao Yang",
      "Jinming Zhao",
      "Gholamreza Haffari",
      "Ehsan Shareghi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13030"
  },
  {
    "id": "arXiv:2210.13034",
    "title": "Subspace-based Set Operations on a Pre-trained Word Embedding Space",
    "abstract": "Word embedding is a fundamental technology in natural language processing. It\nis often exploited for tasks using sets of words, although standard methods for\nrepresenting word sets and set operations remain limited. If we can leverage\nthe advantage of word embedding for such set operations, we can calculate\nsentence similarity and find words that effectively share a concept with a\ngiven word set in a straightforward way. In this study, we formulate\nrepresentations of sets and set operations in a pre-trained word embedding\nspace. Inspired by \\textit{quantum logic}, we propose a novel formulation of\nset operations using subspaces in a pre-trained word embedding space. Based on\nour definitions, we propose two metrics based on the degree to which a word\nbelongs to a set and the similarity between embedding two sets. Our experiments\nwith Text Concept Set Retrieval and Semantic Textual Similarity tasks\ndemonstrated the effectiveness of our proposed method.",
    "descriptor": "",
    "authors": [
      "Yoichi Ishibashi",
      "Sho Yokoi",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13034"
  },
  {
    "id": "arXiv:2210.13039",
    "title": "\"Covid vaccine is against Covid but Oxford vaccine is made at Oxford!\"  Semantic Interpretation of Proper Noun Compounds",
    "abstract": "Proper noun compounds, e.g., \"Covid vaccine\", convey information in a\nsuccinct manner (a \"Covid vaccine\" is a \"vaccine that immunizes against the\nCovid disease\"). These are commonly used in short-form domains, such as news\nheadlines, but are largely ignored in information-seeking applications. To\naddress this limitation, we release a new manually annotated dataset, ProNCI,\nconsisting of 22.5K proper noun compounds along with their free-form semantic\ninterpretations. ProNCI is 60 times larger than prior noun compound datasets\nand also includes non-compositional examples, which have not been previously\nexplored. We experiment with various neural models for automatically generating\nthe semantic interpretations from proper noun compounds, ranging from few-shot\nprompting to supervised learning, with varying degrees of knowledge about the\nconstituent nouns. We find that adding targeted knowledge, particularly about\nthe common noun, results in performance gains of upto 2.8%. Finally, we\nintegrate our model generated interpretations with an existing Open IE system\nand observe an 7.5% increase in yield at a precision of 85%. The dataset and\ncode are available at https://github.com/dair-iitd/pronci.",
    "descriptor": "\nComments: Accepted at EMNLP'22\n",
    "authors": [
      "Keshav Kolluru",
      "Gabriel Stanovsky",
      "Mausam"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13039"
  },
  {
    "id": "arXiv:2210.13041",
    "title": "Learning Neural Radiance Fields from Multi-View Geometry",
    "abstract": "We present a framework, called MVG-NeRF, that combines classical Multi-View\nGeometry algorithms and Neural Radiance Fields (NeRF) for image-based 3D\nreconstruction. NeRF has revolutionized the field of implicit 3D\nrepresentations, mainly due to a differentiable volumetric rendering\nformulation that enables high-quality and geometry-aware novel view synthesis.\nHowever, the underlying geometry of the scene is not explicitly constrained\nduring training, thus leading to noisy and incorrect results when extracting a\nmesh with marching cubes. To this end, we propose to leverage pixelwise depths\nand normals from a classical 3D reconstruction pipeline as geometric priors to\nguide NeRF optimization. Such priors are used as pseudo-ground truth during\ntraining in order to improve the quality of the estimated underlying surface.\nMoreover, each pixel is weighted by a confidence value based on the\nforward-backward reprojection error for additional robustness. Experimental\nresults on real-world data demonstrate the effectiveness of this approach in\nobtaining clean 3D meshes from images, while maintaining competitive\nperformances in novel view synthesis.",
    "descriptor": "\nComments: ECCV 2022 Workshop on \"Learning to Generate 3D Shapes and Scenes\"\n",
    "authors": [
      "Marco Orsingher",
      "Paolo Zani",
      "Paolo Medici",
      "Massimo Bertozzi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13041"
  },
  {
    "id": "arXiv:2210.13043",
    "title": "Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular  data",
    "abstract": "High model performance, on average, can hide that models may systematically\nunderperform on subgroups of the data. We consider the tabular setting, which\nsurfaces the unique issue of outcome heterogeneity - this is prevalent in areas\nsuch as healthcare, where patients with similar features can have different\noutcomes, thus making reliable predictions challenging. To tackle this, we\npropose Data-IQ, a framework to systematically stratify examples into subgroups\nwith respect to their outcomes. We do this by analyzing the behavior of\nindividual examples during training, based on their predictive confidence and,\nimportantly, the aleatoric (data) uncertainty. Capturing the aleatoric\nuncertainty permits a principled characterization and then subsequent\nstratification of data examples into three distinct subgroups (Easy, Ambiguous,\nHard). We experimentally demonstrate the benefits of Data-IQ on four real-world\nmedical datasets. We show that Data-IQ's characterization of examples is most\nrobust to variation across similarly performant (yet different) models,\ncompared to baselines. Since Data-IQ can be used with any ML model (including\nneural networks, gradient boosting etc.), this property ensures consistency of\ndata characterization, while allowing flexible model selection. Taking this a\nstep further, we demonstrate that the subgroups enable us to construct new\napproaches to both feature acquisition and dataset selection. Furthermore, we\nhighlight how the subgroups can inform reliable model usage, noting the\nsignificant impact of the Ambiguous subgroup on model generalization.",
    "descriptor": "\nComments: Presented at NeurIPS 2022\n",
    "authors": [
      "Nabeel Seedat",
      "Jonathan Crabb\u00e9",
      "Ioana Bica",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13043"
  },
  {
    "id": "arXiv:2210.13047",
    "title": "EXP-SC: A Semantic Communication Model Based on Information Framework  Expansion and Knowledge Collision",
    "abstract": "Semantic communication is not obsessed with improving the accuracy of\ntransmitted symbols, but is concerned with expressing the desired meaning that\nthe symbol sequence exactly carried. However, the generation and measurement of\nsemantic messages are still an open problem. Expansions combine simple things\ninto complex systems and even generate intelligence, which is consistent with\nthe evolution of the human language system. We apply this idea to semantic\ncommunication system, quantifying and transmitting semantics by symbol\nsequences, and investigate the semantic information system in a similar way as\nShannon did for digital communication systems. This work was the first to\npropose the concept of semantic expansion and knowledge collision, which may\nprovide a new paradigm for semantic communications. We believe that expansions\nand collisions will be the cornerstone of semantic information theory.",
    "descriptor": "",
    "authors": [
      "Gangtao Xin",
      "Pingyi Fan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Computer Science and Game Theory (cs.GT)",
      "Signal Processing (eess.SP)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.13047"
  },
  {
    "id": "arXiv:2210.13050",
    "title": "Structural generalization is hard for sequence-to-sequence models",
    "abstract": "Sequence-to-sequence (seq2seq) models have been successful across many NLP\ntasks, including ones that require predicting linguistic structure. However,\nrecent work on compositional generalization has shown that seq2seq models\nachieve very low accuracy in generalizing to linguistic structures that were\nnot seen in training. We present new evidence that this is a general limitation\nof seq2seq models that is present not just in semantic parsing, but also in\nsyntactic parsing and in text-to-text tasks, and that this limitation can often\nbe overcome by neurosymbolic models that have linguistic knowledge built in. We\nfurther report on some experiments that give initial answers on the reasons for\nthese limitations.",
    "descriptor": "\nComments: Accepted in EMNLP 2022\n",
    "authors": [
      "Yuekun Yao",
      "Alexander Koller"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13050"
  },
  {
    "id": "arXiv:2210.13053",
    "title": "Foreground Guidance and Multi-Layer Feature Fusion for Unsupervised  Object Discovery with Transformers",
    "abstract": "Unsupervised object discovery (UOD) has recently shown encouraging progress\nwith the adoption of pre-trained Transformer features. However, current methods\nbased on Transformers mainly focus on designing the localization head (e.g.,\nseed selection-expansion and normalized cut) and overlook the importance of\nimproving Transformer features. In this work, we handle UOD task from the\nperspective of feature enhancement and propose FOReground guidance and\nMUlti-LAyer feature fusion for unsupervised object discovery, dubbed FORMULA.\nFirstly, we present a foreground guidance strategy with an off-the-shelf UOD\ndetector to highlight the foreground regions on the feature maps and then\nrefine object locations in an iterative fashion. Moreover, to solve the scale\nvariation issues in object detection, we design a multi-layer feature fusion\nmodule that aggregates features responding to objects at different scales. The\nexperiments on VOC07, VOC12, and COCO 20k show that the proposed FORMULA\nachieves new state-of-the-art results on unsupervised object discovery. The\ncode will be released at https://github.com/VDIGPKU/FORMULA.",
    "descriptor": "\nComments: Accepted in WACV 2023\n",
    "authors": [
      "Zhiwei Lin",
      "Zengyu Yang",
      "Yongtao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13053"
  },
  {
    "id": "arXiv:2210.13054",
    "title": "PARAFAC2-based Coupled Matrix and Tensor Factorizations",
    "abstract": "Coupled matrix and tensor factorizations (CMTF) have emerged as an effective\ndata fusion tool to jointly analyze data sets in the form of matrices and\nhigher-order tensors. The PARAFAC2 model has shown to be a promising\nalternative to the CANDECOMP/PARAFAC (CP) tensor model due to its flexibility\nand capability to handle irregular/ragged tensors. While fusion models based on\na PARAFAC2 model coupled with matrix/tensor decompositions have been recently\nstudied, they are limited in terms of possible regularizations and/or types of\ncoupling between data sets. In this paper, we propose an algorithmic framework\nfor fitting PARAFAC2-based CMTF models with the possibility of imposing various\nconstraints on all modes and linear couplings, using Alternating Optimization\n(AO) and the Alternating Direction Method of Multipliers (ADMM). Through\nnumerical experiments, we demonstrate that the proposed algorithmic approach\naccurately recovers the underlying patterns using various constraints and\nlinear couplings.",
    "descriptor": "",
    "authors": [
      "Carla Schenker",
      "Xiulin Wang",
      "Evrim Acar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13054"
  },
  {
    "id": "arXiv:2210.13055",
    "title": "A Unified Framework for Pun Generation with Humor Principles",
    "abstract": "We propose a unified framework to generate both homophonic and homographic\npuns to resolve the split-up in existing works. Specifically, we incorporate\nthree linguistic attributes of puns to the language models: ambiguity,\ndistinctiveness, and surprise. Our framework consists of three parts: 1) a\ncontext words/phrases selector to promote the aforementioned attributes, 2) a\ngeneration model trained on non-pun sentences to incorporate the context\nwords/phrases into the generation output, and 3) a label predictor that learns\nthe structure of puns which is used to steer the generation model at inference\ntime. Evaluation results on both pun types demonstrate the efficacy of our\nmodel over strong baselines.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Yufei Tian",
      "Divyanshu Sheth",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13055"
  },
  {
    "id": "arXiv:2210.13057",
    "title": "Certifying Induced Subgraphs in Large Graphs",
    "abstract": "We introduce I/O-optimal certifying algorithms for bipartite graphs, as well\nas for the classes of split, threshold, bipartite chain, and trivially perfect\ngraphs. When the input graph is a class member, the certifying algorithm\nreturns a certificate that characterizes this class. Otherwise, it returns a\nforbidden induced subgraph as a certificate for non-membership. On a graph with\n$n$ vertices and $m$ edges, our algorithms take optimal $O(\\text{sort}(n + m))$\nI/Os in the worst case or with high probability for bipartite chain graphs, and\nthe certificates are returned in optimal I/Os. We give implementations for\nsplit and threshold graphs and provide an experimental evaluation.",
    "descriptor": "",
    "authors": [
      "Ulrich Meyer",
      "Hung Tran",
      "Konstantinos Tsakalidis"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13057"
  },
  {
    "id": "arXiv:2210.13060",
    "title": "Is the Envelope Beneficial to Non-Orthogonal Multiple Access?",
    "abstract": "Non-orthogonal multiple access (NOMA) is capable of serving different numbers\nof users in the same time-frequency resource element, and this feature can be\nleveraged to carry additional information. In the orthogonal frequency division\nmultiplexing (OFDM) system, we propose a novel enhanced NOMA scheme, called\nNOMA with informative envelope (NOMA-IE), to explore the flexibility of the\nenvelope of NOMA signals. In this scheme, data bits are conveyed by the\nquantified signal envelope in addition to classic signal constellations. The\nsubcarrier activation patterns of different users are jointly decided by the\nenvelope former. At the receiver, successive interference cancellation (SIC) is\nemployed, and we also introduce the envelope detection coefficient to eliminate\nthe error floor. Theoretical expressions of spectral efficiency and energy\nefficiency are provided for the NOMA-IE. Then, considering the binary phase\nshift keying modulation, we derive the asymptotic bit error rate for the\ntwo-subcarrier OFDM subblock. Afterwards, the expressions are extended to the\nfour-subcarrier case. The analytical results reveal that the imperfect SIC and\nthe index error are the main factors degrading the error performance. The\nnumerical results demonstrate the superiority of the NOMA-IE over the OFDM and\nOFDM-NOMA, especially in the high signal-to-noise ratio (SNR) regime.",
    "descriptor": "\nComments: 30 pages, 9 figures\n",
    "authors": [
      "Ziyi Xie",
      "Wenqiang Yi",
      "Xuanli Wu",
      "Yuanwei Liu",
      "Arumugam Nallanathan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13060"
  },
  {
    "id": "arXiv:2210.13062",
    "title": "A non-sequential hierarchy of message-passing models",
    "abstract": "There is a wide variety of message-passing communication models, ranging from\nsynchronous ''rendez-vous'' communications to fully asynchronous/out-of-order\ncommunications. For large-scale distributed systems, the communication model is\ndetermined by the transport layer of the network, and a few classes of orders\nof message delivery (FIFO, causally ordered) have been identified in the early\ndays of distributed computing. For local-scale message-passing applications,\ne.g., running on a single machine, the communication model may be determined by\nthe actual implementation of message buffers and by how FIFO queues are used.\nWhile large-scale communication models, such as causal ordering, are defined by\nlogical axioms, local-scale models are often defined by an operational\nsemantics. In this work, we connect these two approaches, and we present a\nunified hierarchy of communication models encompassing both large-scale and\nlocal-scale models, based on their non-sequential behaviors. We also show that\nall the communication models we consider can be axiomatised in the monadic\nsecond order logic, and may therefore benefit from several bounded verification\ntechniques based on bounded special treewidth. CCS Concepts: $\\bullet$ Theory\nof computation $\\rightarrow$ Verification by model checking; Modal and temporal\nlogics; Distributed computing models.",
    "descriptor": "",
    "authors": [
      "Cinzia Di Giusto",
      "Davide Ferr\u00e9",
      "Laetitia Laversa",
      "Etienne Lozes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Formal Languages and Automata Theory (cs.FL)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2210.13062"
  },
  {
    "id": "arXiv:2210.13063",
    "title": "Scalable Program Clone Search Through Spectral Analysis",
    "abstract": "We consider the problem of program clone search, i.e. given a target program\nand a repository of known programs (all in executable format), the goal is to\nfind the program in the repository most similar to our target program - with\npotential applications in terms of reverse engineering, program clustering,\nmalware lineage and software theft detection. Recent years have witnessed a\nblooming in code similarity techniques, yet most of them focus on\nfunction-level similarity while we are interested in program-level similarity.\nConsequently, these recent approaches are not directly suited to program clone\nsearch, being either too slow to handle large code bases, not precise enough,\nor not robust against slight variations introduced by compilation or source\ncode versions. We introduce Programs Spectral Similarity (PSS), the first\nspectral analysis dedicated to program-level similarity. PSS reaches a sweet\nspot in terms of precision, speed and robustness. Especially, its one-time\nspectral feature extraction is tailored for large repositories of programs,\nmaking it a perfect fit for program clone search.",
    "descriptor": "",
    "authors": [
      "Tristan Benoit",
      "Jean-Yves Marion",
      "S\u00e9bastien Bardin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13063"
  },
  {
    "id": "arXiv:2210.13064",
    "title": "How Bad is Selfish Driving? Bounding the Inefficiency of Equilibria in  Urban Driving Games",
    "abstract": "We consider the interaction among agents engaging in a driving task and we\nmodel it as general-sum game. This class of games exhibits a plurality of\ndifferent equilibria posing the issue of equilibrium selection. While selecting\nthe most efficient equilibrium (in term of social cost) is often impractical\nfrom a computational standpoint, in this work we study the (in)efficiency of\nany equilibrium players might agree to play. More specifically, we bound the\nequilibrium inefficiency by modeling driving games as particular type of\ncongestion games over spatio-temporal resources. We obtain novel guarantees\nthat refine existing bounds on the Price of Anarchy (PoA) as a function of\nproblem-dependent game parameters. For instance, the relative trade-off between\nproximity costs and personal objectives such as comfort and progress. Although\nthe obtained guarantees concern open-loop trajectories, we observe efficient\nequilibria even when agents employ closed-loop policies trained via\ndecentralized multi-agent reinforcement learning.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Alessandro Zanardi",
      "Pier Giuseppe Sessa",
      "Nando K\u00e4slin",
      "Saverio Bolognani",
      "Andrea Censi",
      "Emilio Frazzoli"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.13064"
  },
  {
    "id": "arXiv:2210.13066",
    "title": "Benchmarking Deformable Object Manipulation with Differentiable Physics",
    "abstract": "Deformable Object Manipulation (DOM) is of significant importance to both\ndaily and industrial applications. Recent successes in differentiable physics\nsimulators allow learning algorithms to train a policy with analytic gradients\nthrough environment dynamics, which significantly facilitates the development\nof DOM algorithms. However, existing DOM benchmarks are either\nsingle-object-based or non-differentiable. This leaves the questions of 1) how\na task-specific algorithm performs on other tasks and 2) how a\ndifferentiable-physics-based algorithm compares with the non-differentiable\nones in general. In this work, we present DaXBench, a differentiable DOM\nbenchmark with a wide object and task coverage. DaXBench includes 9 challenging\nhigh-fidelity simulated tasks, covering rope, cloth, and liquid manipulation\nwith various difficulty levels. To better understand the performance of general\nalgorithms on different DOM tasks, we conduct comprehensive experiments over\nrepresentative DOM methods, ranging from planning to imitation learning and\nreinforcement learning. In addition, we provide careful empirical studies of\nexisting decision-making algorithms based on differentiable physics, and\ndiscuss their limitations, as well as potential future directions.",
    "descriptor": "",
    "authors": [
      "Siwei Chen",
      "Cunjun Yu",
      "Yiqing Xu",
      "Linfeng Li",
      "Xiao Ma",
      "Zhongwen Xu",
      "David Hsu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13066"
  },
  {
    "id": "arXiv:2210.13067",
    "title": "10 hours data is all you need",
    "abstract": "We propose a novel procedure to generate pseudo mandarin speech data named as\nCAMP (character audio mix up), which aims at generating audio from a character\nscale. We also raise a method for building a mandarin character scale audio\ndatabase adaptive to CAMP named as META-AUDIO, which makes full use of audio\ndata and can greatly increase the data diversity of the database. Experiments\nshow that our CAMP method is simple and quite effective. For example, we train\nmodels with 10 hours of audio data in AISHELL-1 and pseudo audio data generated\nby CAMP, and achieve a competitive 11.07 character error rate (CER). Besides,\nwe also perform training with only 10 hours of audio data in AIDATATANG dataset\nand pseudo audio data generated by CAMP, which again achieves a competitive\n8.26 CER.",
    "descriptor": "",
    "authors": [
      "Zeping Min",
      "Qian Ge",
      "Zhong Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13067"
  },
  {
    "id": "arXiv:2210.13069",
    "title": "Factorized structure of the long-range two-electron integrals tensor and  its application in quantum chemistry",
    "abstract": "We introduce two new approximation methods for the numerical evaluation of\nthe long-range Coulomb potential and the approximation of the resulting high\ndimensional Two-Electron Integrals tensor (TEI) with long-range interactions\narising in molecular simulations. The first method exploits the tensorized\nstructure of the compressed two-electron integrals obtained through\ntwo-dimensional Chebyshev interpolation combined with Gaussian quadrature. The\nsecond method is based on the Fast Multipole Method (FMM). Numerical\nexperiments for different medium size molecules on high quality basis sets\noutline the efficiency of the two methods. Detailed algorithmic is provided in\nthis paper as well as numerical comparison of the introduced approaches.",
    "descriptor": "",
    "authors": [
      "Siwar Badreddine",
      "Igor Chollet",
      "Laura Grigori"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13069"
  },
  {
    "id": "arXiv:2210.13070",
    "title": "Perception of the Environment",
    "abstract": "This chapter discusses the intricacies of cybersecurity agents' perception.\nIt addresses the complexity of perception and illuminates how perception shapes\nand influences the decision-making process. It then explores the necessary\nconsiderations when crafting the world representation and discusses the power\nand bandwidth constraints of perception and the underlying issues of AICA's\ntrust in perception. On these foundations, it provides the reader with a guide\nto developing perception models for AICA, discussing the trade-offs of each\nobjective state approximation. The guide is written in the context of the CYST\ncybersecurity simulation engine, which aims to closely model cybersecurity\ninteractions and can be used as a basis for developing AICA. Because CYST is\nfreely available, the reader is welcome to try implementing and evaluating the\nproposed methods for themselves.",
    "descriptor": "\nComments: 3. chapter of the book \"Autonomous Intelligent Agents for Cyber Defense\" by Alexander Kott, to be published as a part of the book series \"Advances in Information Security\" by Springer\n",
    "authors": [
      "Martin Drasar"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13070"
  },
  {
    "id": "arXiv:2210.13075",
    "title": "Hardness in Markov Decision Processes: Theory and Practice",
    "abstract": "Meticulously analysing the empirical strengths and weaknesses of\nreinforcement learning methods in hard (challenging) environments is essential\nto inspire innovations and assess progress in the field. In tabular\nreinforcement learning, there is no well-established standard selection of\nenvironments to conduct such analysis, which is partially due to the lack of a\nwidespread understanding of the rich theory of hardness of environments. The\ngoal of this paper is to unlock the practical usefulness of this theory through\nfour main contributions. First, we present a systematic survey of the theory of\nhardness, which also identifies promising research directions. Second, we\nintroduce Colosseum, a pioneering package that enables empirical hardness\nanalysis and implements a principled benchmark composed of environments that\nare diverse with respect to different measures of hardness. Third, we present\nan empirical analysis that provides new insights into computable measures.\nFinally, we benchmark five tabular agents in our newly proposed benchmark.\nWhile advancing the theoretical understanding of hardness in non-tabular\nreinforcement learning remains essential, our contributions in the tabular\nsetting are intended as solid steps towards a principled non-tabular benchmark.\nAccordingly, we benchmark four agents in non-tabular versions of Colosseum\nenvironments, obtaining results that demonstrate the generality of tabular\nhardness measures.",
    "descriptor": "",
    "authors": [
      "Michelangelo Conserva",
      "Paulo Rauber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13075"
  },
  {
    "id": "arXiv:2210.13076",
    "title": "Towards Unifying Reference Expression Generation and Comprehension",
    "abstract": "Reference Expression Generation (REG) and Comprehension (REC) are two highly\ncorrelated tasks. Modeling REG and REC simultaneously for utilizing the\nrelation between them is a promising way to improve both. However, the problem\nof distinct inputs, as well as building connections between them in a single\nmodel, brings challenges to the design and training of the joint model. To\naddress the problems, we propose a unified model for REG and REC, named UniRef.\nIt unifies these two tasks with the carefully-designed Image-Region-Text Fusion\nlayer (IRTF), which fuses the image, region and text via the image\ncross-attention and region cross-attention. Additionally, IRTF could generate\npseudo input regions for the REC task to enable a uniform way for sharing the\nidentical representation space across the REC and REG. We further propose\nVision-conditioned Masked Language Modeling (VMLM) and Text-Conditioned Region\nPrediction (TRP) to pre-train UniRef model on multi-granular corpora. The VMLM\nand TRP are directly related to REG and REC, respectively, but could help each\nother. We conduct extensive experiments on three benchmark datasets, RefCOCO,\nRefCOCO+ and RefCOCOg. Experimental results show that our model outperforms\nprevious state-of-the-art methods on both REG and REC.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 (main conference)\n",
    "authors": [
      "Duo Zheng",
      "Tao Kong",
      "Ya Jing",
      "Jiaan Wang",
      "Xiaojie Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13076"
  },
  {
    "id": "arXiv:2210.13077",
    "title": "EpipolarNVS: leveraging on Epipolar geometry for single-image Novel View  Synthesis",
    "abstract": "Novel-view synthesis (NVS) can be tackled through different approaches,\ndepending on the general setting: a single source image to a short video\nsequence, exact or noisy camera pose information, 3D-based information such as\npoint clouds etc. The most challenging scenario, the one where we stand in this\nwork, only considers a unique source image to generate a novel one from another\nviewpoint. However, in such a tricky situation, the latest learning-based\nsolutions often struggle to integrate the camera viewpoint transformation.\nIndeed, the extrinsic information is often passed as-is, through a\nlow-dimensional vector. It might even occur that such a camera pose, when\nparametrized as Euler angles, is quantized through a one-hot representation.\nThis vanilla encoding choice prevents the learnt architecture from inferring\nnovel views on a continuous basis (from a camera pose perspective). We claim it\nexists an elegant way to better encode relative camera pose, by leveraging\n3D-related concepts such as the epipolar constraint. We, therefore, introduce\nan innovative method that encodes the viewpoint transformation as a 2D feature\nimage. Such a camera encoding strategy gives meaningful insights to the network\nregarding how the camera has moved in space between the two views. By encoding\nthe camera pose information as a finite number of coloured epipolar lines, we\ndemonstrate through our experiments that our strategy outperforms vanilla\nencoding.",
    "descriptor": "\nComments: Accepted at BMVC 2022 - 21 pages\n",
    "authors": [
      "Ga\u00e9tan Landreau",
      "Mohamed Tamaazousti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13077"
  },
  {
    "id": "arXiv:2210.13079",
    "title": "mm-Wave Radar Hand Shape Classification Using Deformable Transformers",
    "abstract": "A novel, real-time, mm-Wave radar-based static hand shape classification\nalgorithm and implementation are proposed. The method finds several\napplications in low cost and privacy sensitive touchless control technology\nusing 60 Ghz radar as the sensor input. As opposed to prior Range-Doppler image\nbased 2D classification solutions, our method converts raw radar data to 3D\nsparse cartesian point clouds.The demonstrated 3D radar neural network model\nusing deformable transformers significantly surpasses the performance results\nset by prior methods which either utilize custom signal processing or apply\ngeneric convolutional techniques on Range-Doppler FFT images. Experiments are\nperformed on an internally collected dataset using an off-the-shelf radar\nsensor.",
    "descriptor": "",
    "authors": [
      "Athmanarayanan Lakshmi Narayanan",
      "Asma Beevi K. T",
      "Haoyang Wu",
      "Jingyi Ma",
      "W. Margaret Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13079"
  },
  {
    "id": "arXiv:2210.13082",
    "title": "Are Deep Sequence Classifiers Good at Non-Trivial Generalization?",
    "abstract": "Recent advances in deep learning models for sequence classification have\ngreatly improved their classification accuracy, specially when large training\nsets are available. However, several works have suggested that under some\nsettings the predictions made by these models are poorly calibrated. In this\nwork we study binary sequence classification problems and we look at model\ncalibration from a different perspective by asking the question: Are deep\nlearning models capable of learning the underlying target class distribution?\nWe focus on sparse sequence classification, that is problems in which the\ntarget class is rare and compare three deep learning sequence classification\nmodels. We develop an evaluation that measures how well a classifier is\nlearning the target class distribution. In addition, our evaluation\ndisentangles good performance achieved by mere compression of the training\nsequences versus performance achieved by proper model generalization. Our\nresults suggest that in this binary setting the deep-learning models are indeed\nable to learn the underlying class distribution in a non-trivial manner, i.e.\nby proper generalization beyond data compression.",
    "descriptor": "",
    "authors": [
      "Francesco Cazzaro",
      "Ariadna Quattoni",
      "Xavier Carreras"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13082"
  },
  {
    "id": "arXiv:2210.13083",
    "title": "Scalable Representation Learning in Linear Contextual Bandits with  Constant Regret Guarantees",
    "abstract": "We study the problem of representation learning in stochastic contextual\nlinear bandits. While the primary concern in this domain is usually to find\nrealizable representations (i.e., those that allow predicting the reward\nfunction at any context-action pair exactly), it has been recently shown that\nrepresentations with certain spectral properties (called HLS) may be more\neffective for the exploration-exploitation task, enabling LinUCB to achieve\nconstant (i.e., horizon-independent) regret. In this paper, we propose\nBanditSRL, a representation learning algorithm that combines a novel\nconstrained optimization problem to learn a realizable representation with good\nspectral properties with a generalized likelihood ratio test to exploit the\nrecovered representation and avoid excessive exploration. We prove that\nBanditSRL can be paired with any no-regret algorithm and achieve constant\nregret whenever an HLS representation is available. Furthermore, BanditSRL can\nbe easily combined with deep neural networks and we show how regularizing\ntowards HLS representations is beneficial in standard benchmarks.",
    "descriptor": "\nComments: Accepted at Neurips 2022\n",
    "authors": [
      "Andrea Tirinzoni",
      "Matteo Papini",
      "Ahmed Touati",
      "Alessandro Lazaric",
      "Matteo Pirotta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13083"
  },
  {
    "id": "arXiv:2210.13084",
    "title": "Full-Text Argumentation Mining on Scientific Publications",
    "abstract": "Scholarly Argumentation Mining (SAM) has recently gained attention due to its\npotential to help scholars with the rapid growth of published scientific\nliterature. It comprises two subtasks: argumentative discourse unit recognition\n(ADUR) and argumentative relation extraction (ARE), both of which are\nchallenging since they require e.g. the integration of domain knowledge, the\ndetection of implicit statements, and the disambiguation of argument structure.\nWhile previous work focused on dataset construction and baseline methods for\nspecific document sections, such as abstract or results, full-text scholarly\nargumentation mining has seen little progress. In this work, we introduce a\nsequential pipeline model combining ADUR and ARE for full-text SAM, and provide\na first analysis of the performance of pretrained language models (PLMs) on\nboth subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus,\noutperforming the previous best reported result by a large margin (+7% F1). We\nalso present the first results for ARE, and thus for the full AM pipeline, on\nthis benchmark dataset. Our detailed error analysis reveals that non-contiguous\nADUs as well as the interpretation of discourse connectors pose major\nchallenges and that data annotation needs to be more consistent.",
    "descriptor": "",
    "authors": [
      "Arne Binder",
      "Bhuvanesh Verma",
      "Leonhard Hennig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13084"
  },
  {
    "id": "arXiv:2210.13086",
    "title": "Legal-Tech Open Diaries: Lesson learned on how to develop and deploy  light-weight models in the era of humongous Language Models",
    "abstract": "In the era of billion-parameter-sized Language Models (LMs), start-ups have\nto follow trends and adapt their technology accordingly. Nonetheless, there are\nopen challenges since the development and deployment of large models comes with\na need for high computational resources and has economical consequences. In\nthis work, we follow the steps of the R&D group of a modern legal-tech start-up\nand present important insights on model development and deployment. We start\nfrom ground zero by pre-training multiple domain-specific multi-lingual LMs\nwhich are a better fit to contractual and regulatory text compared to the\navailable alternatives (XLM-R). We present benchmark results of such models in\na half-public half-private legal benchmark comprising 5 downstream tasks\nshowing the impact of larger model size. Lastly, we examine the impact of a\nfull-scale pipeline for model compression which includes: a) Parameter Pruning,\nb) Knowledge Distillation, and c) Quantization: The resulting models are much\nmore efficient without sacrificing performance at large.",
    "descriptor": "\nComments: 10 pages, long paper at NLLP Workshop 2022 proceedings\n",
    "authors": [
      "Stelios Maroudas",
      "Sotiris Legkas",
      "Prodromos Malakasiotis",
      "Ilias Chalkidis"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13086"
  },
  {
    "id": "arXiv:2210.13088",
    "title": "Your Router is My Prober: Measuring IPv6 Networks via ICMP Rate Limiting  Side Channels",
    "abstract": "Active Internet measurements face challenges when some measurements require\nmany remote vantage points. In this paper, we propose a novel technique for\nmeasuring remote IPv6 networks via side channels in ICMP rate limiting, a\nrequired function for IPv6 nodes to limit the rate at which ICMP error messages\nare generated. This technique, iVantage, can to some extent use 1.1M remote\nrouters distributed in 9.5k autonomous systems and 182 countries as our\n\"vantage points\". We apply iVantage to two different, but both challenging\nmeasurement tasks: 1) measuring the deployment of inbound source address\nvalidation (ISAV) and 2) measuring reachability between arbitrary Internet\nnodes. We accomplish these two tasks from only one local vantage point without\ncontrolling the targets or relying on other services within the target\nnetworks. Our large-scale ISAV measurements cover ~50% of all IPv6 autonomous\nsystems and find ~79% of them are vulnerable to spoofing, which is the most\nlarge-scale measurement study of IPv6 ISAV to date. Our method for reachability\nmeasurements achieves over 80% precision and recall in our evaluation. Finally,\nwe perform an Internet-wide measurement of the ICMP rate limiting\nimplementations, present a detailed discussion on ICMP rate limiting,\nparticularly the potential security and privacy risks in the mechanism of ICMP\nrate limiting, and provide possible mitigation measures. We make our code\navailable to the community.",
    "descriptor": "",
    "authors": [
      "Long Pan",
      "Jiahai Yang",
      "Lin He",
      "Zhiliang Wang",
      "Leyao Nie",
      "Guanglei Song",
      "Yaozhong Liu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13088"
  },
  {
    "id": "arXiv:2210.13089",
    "title": "An agent-based epidemics simulation to compare and explain screening and  vaccination prioritisation strategies",
    "abstract": "This paper describes an agent-based model of epidemics dynamics. This model\nis willingly simplified, as its goal is not to predict the evolution of the\nepidemics, but to explain the underlying mechanisms in an interactive way. This\nmodel allows to compare screening prioritisation strategies, as well as\nvaccination priority strategies, on a virtual population. The model is\nimplemented in Netlogo in different simulators, published online to let people\nexperiment with them. This paper reports on the model design, implementation,\nand experimentations. In particular we have compared screening strategies to\nevaluate the epidemics vs control it by quarantining infectious people; and we\nhave compared vaccinating older people with more risk factors, vs younger\npeople with more social contacts.",
    "descriptor": "",
    "authors": [
      "Carole Adam",
      "Helene Arduin"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13089"
  },
  {
    "id": "arXiv:2210.13094",
    "title": "DNA tile self-assembly for 3D-surfaces and genus identification",
    "abstract": "We introduce a new DNA tile self-assembly model: the Surface Flexible Tile\nAssembly Model (SFTAM), where 2D tiles are placed on host 3D surfaces made of\naxis-parallel unit cubes glued together by their faces, called polycubes. The\nbonds are flexible, so that the assembly can bind on the edges of the polycube.\nWe are interested in the study of SFTAM self-assemblies on 3D surfaces which\nare not always embeddable in the Euclidean plane, in order to compare their\ndifferent behaviors.\nWe focus on a family of polycubes called cuboids. Order-0 cuboids are\npolycubes that have six rectangular faces, and order-1 cuboids are made from\ntwo order-0 cuboids by substracting one from the other. Thus, order-1 cuboids\ncan be of genus~0 or of genus~1 (then they contain a tunnel). We are interested\nin the genus of these structures, and we present a SFTAM tile assembly system\nthat determines the genus of a given order-1 cuboid. The SFTAM tile assembly\nsystem which we design, contains a specific set $Y$ of tile types with the\nfollowing properties. If the assembly is made on a host order-1 cuboid $C$ of\ngenus~0, no tile of $Y$ appears in any producible assembly, but if $C$ has\ngenus~1, every terminal assembly contains at least one tile of $Y$.\nThus, we are able to distinguish the host surfaces according to their genus,\nby the tiles used in the assembly.",
    "descriptor": "",
    "authors": [
      "Florent Becker",
      "Shahrzad Heydarshahi"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.13094"
  },
  {
    "id": "arXiv:2210.13097",
    "title": "Locality-Preserving Minimal Perfect Hashing of k-mers",
    "abstract": "Minimal perfect hashing is the problem of mapping a static set of $n$\ndistinct keys into the address space $\\{1,\\ldots,n\\}$ bijectively. It is\nwell-known that $n\\log_2 e$ bits are necessary to specify a minimal perfect\nhash function $f$, when no additional knowledge of the input keys is to be\nused. However, it is often the case in practice that the input keys have\nintrinsic relationships that we can exploit to lower the bit complexity of $f$.\nFor example, consider a string and the set of all its distinct sub-strings of\nlength $k$ - the so-called $k$-mers of the string. Two consecutive $k$-mers in\nthe string have a strong intrinsic relationship in that they share an overlap\nof $k-1$ symbols. Hence, it seems intuitively possible to beat the classic\n$\\log_2 e$ bits/key barrier in this case. Moreover, we would like $f$ to map\nconsecutive $k$-mers to consecutive addresses, as to preserve as much as\npossible the relationships between the keys also in the co-domain\n$\\{1,\\ldots,n\\}$. This is a useful feature in practice as it guarantees a\ncertain degree of locality of reference for $f$, resulting in a better\nevaluation time when querying consecutive $k$-mers from a string. Motivated by\nthese premises, we initiate the study of a new type of locality-preserving\nminimal perfect hash functions designed for $k$-mers extracted consecutively\nfrom a string (or collections of strings). We show a theoretic lower bound on\nthe bit complexity of any $(1-\\varepsilon)$-locality-preserving MPHF, for a\nparameter $0 < \\varepsilon < 1$. The complexity is lower than $n\\log_2 e$ bits\nfor sufficiently small $\\varepsilon$. We propose a construction that approaches\nthe theoretic minimum space for growing $k$ and present a practical\nimplementation of the method.",
    "descriptor": "",
    "authors": [
      "Giulio Ermanno Pibiri",
      "Yoshihiro Shibuya",
      "Antoine Limasset"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13097"
  },
  {
    "id": "arXiv:2210.13101",
    "title": "Towards an efficient Iris Recognition System on Embedded Devices",
    "abstract": "Iris Recognition (IR) is one of the market's most reliable and accurate\nbiometric systems. Today, it is challenging to build NIR-capturing devices\nunder the premise of hardware price reduction. Commercial NIR sensors are\nprotected from modification. The process of building a new device is not\ntrivial because it is required to start from scratch with the process of\ncapturing images with quality, calibrating operational distances, and building\nlightweight software such as eyes/iris detectors and segmentation sub-systems.\nIn light of such challenges, this work aims to develop and implement iris\nrecognition software in an embedding system and calibrate NIR in a contactless\nbinocular setup. We evaluate and contrast speed versus performance obtained\nwith two embedded computers and infrared cameras. Further, a lightweight\nsegmenter sub-system called \"Unet_xxs\" is proposed, which can be used for iris\nsemantic segmentation under restricted memory resources.",
    "descriptor": "",
    "authors": [
      "Daniel P. Benalcazar",
      "Juan E. Tapia",
      "Mauricio Vasquez",
      "Leonardo Causa",
      "Enrique Lopez Droguett",
      "Christoph Busch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13101"
  },
  {
    "id": "arXiv:2210.13103",
    "title": "Deep Grey-Box Modeling With Adaptive Data-Driven Models Toward  Trustworthy Estimation of Theory-Driven Models",
    "abstract": "The combination of deep neural nets and theory-driven models, which we call\ndeep grey-box modeling, can be inherently interpretable to some extent thanks\nto the theory backbone. Deep grey-box models are usually learned with a\nregularized risk minimization to prevent a theory-driven part from being\noverwritten and ignored by a deep neural net. However, an estimation of the\ntheory-driven part obtained by uncritically optimizing a regularizer can hardly\nbe trustworthy when we are not sure what regularizer is suitable for the given\ndata, which may harm the interpretability. Toward a trustworthy estimation of\nthe theory-driven part, we should analyze regularizers' behavior to compare\ndifferent candidates and to justify a specific choice. In this paper, we\npresent a framework that enables us to analyze a regularizer's behavior\nempirically with a slight change in the neural net's architecture and the\ntraining objective.",
    "descriptor": "\nComments: 16 pages, 8 figures\n",
    "authors": [
      "Naoya Takeishi",
      "Alexandros Kalousis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13103"
  },
  {
    "id": "arXiv:2210.13107",
    "title": "An Analytical Estimation of Spiking Neural Networks Energy Efficiency",
    "abstract": "Spiking Neural Networks are a type of neural networks where neurons\ncommunicate using only spikes. They are often presented as a low-power\nalternative to classical neural networks, but few works have proven these\nclaims to be true. In this work, we present a metric to estimate the energy\nconsumption of SNNs independently of a specific hardware. We then apply this\nmetric on SNNs processing three different data types (static, dynamic and\nevent-based) representative of real-world applications. As a result, all of our\nSNNs are 6 to 8 times more efficient than their FNN counterparts.",
    "descriptor": "\nComments: Accepted for ICONIP 2022 Conference\n",
    "authors": [
      "Edgar Lemaire",
      "Loic Cordone",
      "Andrea Castagnetti",
      "Pierre-Emmanuel Novac",
      "Jonathan Courtois",
      "Benoit Miramond"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.13107"
  },
  {
    "id": "arXiv:2210.13108",
    "title": "Heat Demand Forecasting with Multi-Resolutional Representation of  Heterogeneous Temporal Ensemble",
    "abstract": "One of the primal challenges faced by utility companies is ensuring efficient\nsupply with minimal greenhouse gas emissions. The advent of smart meters and\nsmart grids provide an unprecedented advantage in realizing an optimised supply\nof thermal energies through proactive techniques such as load forecasting. In\nthis paper, we propose a forecasting framework for heat demand based on neural\nnetworks where the time series are encoded as scalograms equipped with the\ncapacity of embedding exogenous variables such as weather, and\nholiday/non-holiday. Subsequently, CNNs are utilized to predict the heat load\nmulti-step ahead. Finally, the proposed framework is compared with other\nstate-of-the-art methods, such as SARIMAX and LSTM. The quantitative results\nfrom retrospective experiments show that the proposed framework consistently\noutperforms the state-of-the-art baseline method with real-world data acquired\nfrom Denmark. A minimal mean error of 7.54% for MAPE and 417kW for RMSE is\nachieved with the proposed framework in comparison to all other methods.",
    "descriptor": "",
    "authors": [
      "Adithya Ramachandran",
      "Satyaki Chatterjee",
      "Siming Bayer",
      "Andreas Maier",
      "Thorkil Flensmark"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13108"
  },
  {
    "id": "arXiv:2210.13109",
    "title": "Domain Adaptive Segmentation of Electron Microscopy with Sparse Point  Annotations",
    "abstract": "Accurate segmentation of organelle instances, e.g., mitochondria, is\nessential for electron microscopy analysis. Despite the outstanding performance\nof fully supervised methods, they highly rely on sufficient per-pixel annotated\ndata and are sensitive to domain shift. Aiming to develop a highly\nannotation-efficient approach with competitive performance, we focus on\nweakly-supervised domain adaptation (WDA) with a type of extremely sparse and\nweak annotation demanding minimal annotation efforts, i.e., sparse point\nannotations on only a small subset of object instances. To reduce performance\ndegradation arising from domain shift, we explore multi-level transferable\nknowledge through conducting three complementary tasks, i.e., counting,\ndetection, and segmentation, constituting a task pyramid with different levels\nof domain invariance. The intuition behind this is that after investigating a\nrelated source domain, it is much easier to spot similar objects in the target\ndomain than to delineate their fine boundaries. Specifically, we enforce\ncounting estimation as a global constraint to the detection with sparse\nsupervision, which further guides the segmentation. A cross-position\ncut-and-paste augmentation is introduced to further compensate for the\nannotation sparsity. Extensive validations show that our model with only 15\\%\npoint annotations can achieve comparable performance as supervised models and\nshows robustness to annotation selection.",
    "descriptor": "\nComments: 7pages, 4 figures\n",
    "authors": [
      "Dafei Qiu",
      "Jiajin Yi",
      "Jialin Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13109"
  },
  {
    "id": "arXiv:2210.13110",
    "title": "On DoS Resiliency Analysis of Networked Control Systems: Trade-Off  Between Jamming Actions and Network Delays",
    "abstract": "This letter deals with the problem of quantifying resiliency of Networked\nControl Systems (NCSs) to Denial-of-Service (DoS) attacks and variable network\ndelays. Internal exponential stability and $\\mathcal{L}_2$ external stability\nare studied. The closed-loop system is augmented with an auxiliary timer\nvariable and analyzed in a hybrid system framework. Lyapunov-like conditions\nare given to ensure $0$-input global exponential stability and $\\mathcal{L}_2$\nexternal stability. A computationally affordable algorithm based on linear\nmatrix inequalities is devised to provide trade-off curves between maximum\nlength of DoS attacks and largest network delays. Finally, the effectiveness of\nthe proposed approach is shown in a numerical example.",
    "descriptor": "\nComments: Matches the published version\n",
    "authors": [
      "Roberto Merco",
      "Francesco Ferrante",
      "Pierluigi Pisu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13110"
  },
  {
    "id": "arXiv:2210.13111",
    "title": "Federated and Meta learning over Non-Wireless and Wireless Networks: A  Tutorial",
    "abstract": "In recent years, various machine learning (ML) solutions have been developed\nto solve resource management, interference management, autonomy, and\ndecision-making problems in non-wireless and wireless networks. Standard ML\napproaches require collecting data at a central server for training, which\ncannot preserve the data privacy of devices. To address this issue, federated\nlearning (FL) is an effective method to allow edge devices to collaboratively\ntrain ML models without sharing local datasets for data privacy. Typically, FL\nfocuses on learning a global model for a given task and all devices and hence\ncannot adapt the model to devices with different data distributions. In such\ncases, meta learning can be employed to adapt learning models to different data\ndistributions using a few data samples. In this tutorial, we conduct a\ncomprehensive review on FL, meta learning, and federated meta learning\n(FedMeta). Compared to other tutorial papers, our objective is to leverage how\nFL/meta-learning/FedMeta can be designed, optimized, and evolved over\nnon-wireless and wireless networks. Furthermore, we analyze not only the\nrelationship among these learning algorithms but also their advantages and\ndisadvantages in real-world applications.",
    "descriptor": "",
    "authors": [
      "Xiaonan Liu",
      "Yansha Deng",
      "Arumugam Nallanathan",
      "Mehdi Bennis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13111"
  },
  {
    "id": "arXiv:2210.13112",
    "title": "Optimization-Based Motion Planning for Autonomous Parking Considering  Dynamic Obstacle: A Hierarchical Framework",
    "abstract": "We present a hierarchical framework based on graph search and model\npredictive control (MPC) for electric autonomous vehicle (EAV) parking\nmaneuvers in a tight environment. At high-level, only static obstacles are\nconsidered, and the scenario-based hybrid A* (SHA*), which is faster than the\ntraditional hybrid A*, is designed to provide an initial guess (also known as a\nglobal path) for the parking task. To extract the velocity and acceleration\nprofile from an initial guess, an optimal control problem (OCP) is built. At\nthe low level, an NMPC-based strategy is used to avoid dynamic obstacles (also\nknown as local planning). The efficacy of SHA* is evaluated through 148\ndifferent simulation schemes and the proposed hierarchical parking framework is\ndemonstrated through a real-time parallel parking simulation.",
    "descriptor": "\nComments: conference\n",
    "authors": [
      "Xuemin Chi",
      "Zhitao Liu",
      "Jihao Huang",
      "Feng Hong",
      "Hongye Su"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13112"
  },
  {
    "id": "arXiv:2210.13113",
    "title": "Interactive inference: a multi-agent model of cooperative joint actions",
    "abstract": "We advance a novel computational model of multi-agent, cooperative joint\nactions that is grounded in the cognitive framework of active inference. The\nmodel assumes that to solve a joint task, such as pressing together a red or\nblue button, two (or more) agents engage in a process of interactive inference.\nEach agent maintains probabilistic beliefs about the goal of the joint task\n(e.g., should we press the red or blue button?) and updates them by observing\nthe other agent's movements, while in turn selecting movements that make his\nown intentions legible and easy to infer by the other agent (i.e., sensorimotor\ncommunication). Over time, the interactive inference aligns both the beliefs\nand the behavioral strategies of the agents, hence ensuring the success of the\njoint action. We exemplify the functioning of the model in two simulations. The\nfirst simulation illustrates a ''leaderless'' joint action. It shows that when\ntwo agents lack a strong preference about their joint task goal, they jointly\ninfer it by observing each other's movements. In turn, this helps the\ninteractive alignment of their beliefs and behavioral strategies. The second\nsimulation illustrates a \"leader-follower\" joint action. It shows that when one\nagent (\"leader\") knows the true joint goal, it uses sensorimotor communication\nto help the other agent (\"follower\") infer it, even if doing this requires\nselecting a more costly individual plan. These simulations illustrate that\ninteractive inference supports successful multi-agent joint actions and\nreproduces key cognitive and behavioral dynamics of \"leaderless\" and\n\"leader-follower\" joint actions observed in human-human experiments. In sum,\ninteractive inference provides a cognitively inspired, formal framework to\nrealize cooperative joint actions and consensus in multi-agent systems.",
    "descriptor": "\nComments: 35 pages, 16 figures\n",
    "authors": [
      "Domenico Maisto",
      "Francesco Donnarumma",
      "Giovanni Pezzulo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Multiagent Systems (cs.MA)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.13113"
  },
  {
    "id": "arXiv:2210.13115",
    "title": "Non-conforming interface conditions for the second-order wave equation",
    "abstract": "Imposition methods of interface conditions for the second-order wave equation\nwith non-conforming grids is considered. The spatial discretization is based on\nhigh order finite differences with summation-by-parts properties. Previously\npresented solution methods for this problem, based on the simultaneous\napproximation term (SAT) method, have shown to introduce significant stiffness.\nThis can lead to highly inefficient schemes. Here, two new methods of imposing\nthe interface conditions to avoid the stiffness problems are presented: 1) a\nprojection method and 2) a hybrid between the projection method and the SAT\nmethod. Numerical experiments are performed using traditional and\norder-preserving interpolation operators. Both of the novel methods retain the\naccuracy and convergence behavior of the previously developed SAT method but\nare significantly less stiff.",
    "descriptor": "\nComments: 17 pages, 1 figure. Submitted to Journal of Scientific Computing 29 May 2022\n",
    "authors": [
      "Gustav Eriksson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13115"
  },
  {
    "id": "arXiv:2210.13118",
    "title": "Unsupervised Term Extraction for Highly Technical Domains",
    "abstract": "Term extraction is an information extraction task at the root of knowledge\ndiscovery platforms. Developing term extractors that are able to generalize\nacross very diverse and potentially highly technical domains is challenging, as\nannotations for domains requiring in-depth expertise are scarce and expensive\nto obtain. In this paper, we describe the term extraction subsystem of a\ncommercial knowledge discovery platform that targets highly technical fields\nsuch as pharma, medical, and material science. To be able to generalize across\ndomains, we introduce a fully unsupervised annotator (UA). It extracts terms by\ncombining novel morphological signals from sub-word tokenization with\nterm-to-topic and intra-term similarity metrics, computed using general-domain\npre-trained sentence-encoders. The annotator is used to implement a\nweakly-supervised setup, where transformer-models are fine-tuned (or\npre-trained) over the training data generated by running the UA over large\nunlabeled corpora. Our experiments demonstrate that our setup can improve the\npredictive performance while decreasing the inference latency on both CPUs and\nGPUs. Our annotators provide a very competitive baseline for all the cases\nwhere annotations are not available.",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (industry). 8 pages, 3 figures, 3 tables\n",
    "authors": [
      "Francesco Fusco",
      "Peter Staar",
      "Diego Antognini"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13118"
  },
  {
    "id": "arXiv:2210.13119",
    "title": "Cybersecurity in the Smart Grid: Practitioners' Perspective",
    "abstract": "The Smart Grid (SG) is a cornerstone of modern society, providing the energy\nrequired to sustain billions of lives and thousands of industries.\nUnfortunately, as one of the most critical infrastructures of our World, the SG\nis an attractive target for attackers. The problem is aggravated by the\nincreasing adoption of digitalisation, which further increases the SG's\nexposure to cyberthreats. Successful exploitation of such exposure leads to\nentire countries being paralysed, which is an unacceptable -- but ultimately\ninescapable -- risk.\nThis paper aims to mitigate this risk by elucidating the perspective of real\npractitioners on the cybersecurity of the SG. We interviewed 18 entities,\noperating in diverse countries in Europe and covering all domains of the SG --\nfrom energy generation, to its delivery. Our analysis highlights a stark\ncontrast between (a)research and practice, but also between (b) public and\nprivate entities. For instance: some threats appear to be much less dangerous\nthan what is claimed in related papers; some technological paradigms have\ndubious utility for practitioners, but are actively promoted by literature;\nfinally, practitioners may either under- or over-estimate their own\ncybersecurity capabilities. We derive four takeaways that enable future\nendeavours to improve the overall cybersecurity in the SG. We conjecture that\nmost of the problems are due to an improper communication between researchers,\npractitioners and regulatory bodies -- which, despite sharing a common goal,\ntend to neglect the viewpoint of the other `spheres'.",
    "descriptor": "",
    "authors": [
      "Jacqueline Meyer",
      "Giovanni Apruzzese"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13119"
  },
  {
    "id": "arXiv:2210.13124",
    "title": "Cipherfix: Mitigating Ciphertext Side-Channel Attacks in Software",
    "abstract": "Trusted execution environments are quickly rising in popularity as they\nenable to run workloads in the cloud without having to trust cloud service\nproviders, by offering additional hardware-assisted security guarantees. One\nkey mechanism for server-grade TEEs is main memory encryption, as it not only\nprevents system-level attackers from reading the TEE's content, but also\nprovides protection against physical, off-chip attackers. The recent\nCipherleaks attacks show that the memory encryption system of AMD SEV-SNP and\npotentially other TEEs are vulnerable to a new kind of attack, dubbed the\nciphertext side-channel. The ciphertext side-channel allows to leak secret data\nfrom TEE-protected implementations by analyzing ciphertext patterns exhibited\ndue to deterministic memory encryption. It cannot be mitigated by current best\npractices like data-oblivious constant-time code. As these ciphertext leakages\nare inherent to deterministic memory encryption, a hardware fix on existing\nsystems is unlikely. Thus, in this paper, we present a software-based, drop-in\nsolution that can harden existing binaries such that they can be safely\nexecuted under TEEs vulnerable to ciphertext side-channels. We combine taint\ntracking with both static and dynamic binary instrumentation to find sensitive\nmemory locations and prevent the leakage by masking secret data before it gets\nwritten to memory. This way, although the memory encryption remains\ndeterministic, we destroy any secret-dependent patterns in encrypted memory. We\nshow that our proof-of-concept implementation can protect constant-time EdDSA\nand ECDSA implementations against ciphertext side-channels.",
    "descriptor": "",
    "authors": [
      "Jan Wichelmann",
      "Anna P\u00e4tschke",
      "Luca Wilke",
      "Thomas Eisenbarth"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13124"
  },
  {
    "id": "arXiv:2210.13125",
    "title": "Iris super-resolution using CNNs: is photo-realism important to iris  recognition?",
    "abstract": "The use of low-resolution images adopting more relaxed acquisition conditions\nsuch as mobile phones and surveillance videos is becoming increasingly common\nin iris recognition nowadays. Concurrently, a great variety of single image\nsuper-resolution techniques are emerging, especially with the use of\nconvolutional neural networks (CNNs). The main objective of these methods is to\ntry to recover finer texture details generating more photo-realistic images\nbased on the optimisation of an objective function depending basically on the\nCNN architecture and training approach. In this work, the authors explore\nsingle image super-resolution using CNNs for iris recognition. For this, they\ntest different CNN architectures and use different training databases,\nvalidating their approach on a database of 1.872 near infrared iris images and\non a mobile phone image database. They also use quality assessment, visual\nresults and recognition experiments to verify if the photo-realism provided by\nthe CNNs which have already proven to be effective for natural images can\nreflect in a better recognition rate for iris recognition. The results show\nthat using deeper architectures trained with texture databases that provide a\nbalance between edge preservation and the smoothness of the method can lead to\ngood results in the iris recognition process.",
    "descriptor": "\nComments: Published at IET Biometrics\n",
    "authors": [
      "Eduardo Ribeiro",
      "Andreas Uhl",
      "Fernando Alonso-Fernandez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13125"
  },
  {
    "id": "arXiv:2210.13129",
    "title": "Facial Soft Biometrics for Recognition in the Wild: Recent Works,  Annotation, and COTS Evaluation",
    "abstract": "The role of soft biometrics to enhance person recognition systems in\nunconstrained scenarios has not been extensively studied. Here, we explore the\nutility of the following modalities: gender, ethnicity, age, glasses, beard,\nand moustache. We consider two assumptions: 1) manual estimation of soft\nbiometrics and 2) automatic estimation from two commercial off-the-shelf\nsystems (COTS). All experiments are reported using the labeled faces in the\nwild (LFW) database. First, we study the discrimination capabilities of soft\nbiometrics standalone. Then, experiments are carried out fusing soft biometrics\nwith two state-of-the-art face recognition systems based on deep learning. We\nobserve that soft biometrics is a valuable complement to the face modality in\nunconstrained scenarios, with relative improvements up to 40%/15% in the\nverification performance when using manual/automatic soft biometrics\nestimation. Results are reproducible as we make public our manual annotations\nand COTS outputs of soft biometrics over LFW, as well as the face recognition\nscores.",
    "descriptor": "\nComments: Published at IEEE Transactions on Information Forensics and Security\n",
    "authors": [
      "Ester Gonzalez-Sosa",
      "Julian Fierrez",
      "Ruben Vera-Rodriguez",
      "Fernando Alonso-Fernandez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13129"
  },
  {
    "id": "arXiv:2210.13131",
    "title": "Boundary and interface methods for energy stable finite difference  discretizations of the dynamic beam equation",
    "abstract": "We consider energy stable summation by parts finite difference methods\n(SBP-FD) for the homogeneous and piecewise homogeneous dynamic beam equation\n(DBE). Previously the constant coefficient problem has been solved with SBP-FD\ntogether with penalty terms (SBP-SAT) to impose boundary conditions. In this\nwork we revisit this problem and compare SBP-SAT to the projection method\n(SBP-P). We also consider the DBE with discontinuous coefficients and present\nnovel SBP-SAT, SBP-P and hybrid SBP-SAT-P discretizations for imposing\ninterface conditions. Numerical experiments show that all methods considered\nare similar in terms of accuracy, but that SBP-P can be more computationally\nefficient (less restrictive time step requirement for explicit time integration\nmethods) for both the constant and piecewise constant coefficient problems.",
    "descriptor": "\nComments: 28 pages, 5 figures. Submitted to Journal of Computational Physics 26 July 2021\n",
    "authors": [
      "Gustav Eriksson",
      "Jonatan Werpers",
      "David Niemel\u00e4",
      "Niklas Wik",
      "Valter Zethrin",
      "Ken Mattsson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13131"
  },
  {
    "id": "arXiv:2210.13134",
    "title": "Multilingual Multimodal Learning with Machine Translated Text",
    "abstract": "Most vision-and-language pretraining research focuses on English tasks.\nHowever, the creation of multilingual multimodal evaluation datasets (e.g.\nMulti30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality\ntraining data that is both multilingual and multimodal. In this paper, we\ninvestigate whether machine translating English multimodal data can be an\neffective proxy for the lack of readily available multilingual data. We call\nthis framework TD-MML: Translated Data for Multilingual Multimodal Learning,\nand it can be applied to any multimodal dataset and model. We apply it to both\npretraining and fine-tuning data with a state-of-the-art model. In order to\nprevent models from learning from low-quality translated text, we propose two\nmetrics for automatically removing such translations from the resulting\ndatasets. In experiments on five tasks across 20 languages in the IGLUE\nbenchmark, we show that translated data can provide a useful signal for\nmultilingual multimodal learning, both at pretraining and fine-tuning.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Chen Qiu",
      "Dan Oneata",
      "Emanuele Bugliarello",
      "Stella Frank",
      "Desmond Elliott"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13134"
  },
  {
    "id": "arXiv:2210.13136",
    "title": "Path association rule mining",
    "abstract": "Graph association rule mining is a data mining technique used for discovering\nregularities in graph data. In this study, we propose a novel concept, {\\it\npath association rule mining}, to discover the correlations of path patterns\nthat frequently appear in a given graph. Reachability path patterns (i.e.,\nexistence of paths from a vertex to another vertex) are applied in our concept\nto discover diverse regularities. We show that the problem is NP-hard, and we\ndevelop an efficient algorithm in which the anti-monotonic property is used on\npath patterns. Subsequently, we develop approximation and parallelization\ntechniques to efficiently and scalably discover rules. We use real-life graphs\nto experimentally verify the effective",
    "descriptor": "",
    "authors": [
      "Yuya Sasaki"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.13136"
  },
  {
    "id": "arXiv:2210.13138",
    "title": "Fractional Order Runge-Kutta Methods",
    "abstract": "This paper investigates, a new class of fractional order Runge-Kutta (FORK)\nmethods for numerical\napproximation to the solution of fractional differential equations (FDEs). By\nusing the Caputo generalized\nTaylor formula and the total differential for Caputo fractional derivative,\nwe construct explicit and implicit FORK methods, as the well-known\nRunge-Kutta schemes for ordinary differential equations. In the proposed\nmethod, due to the dependence of fractional derivatives to a fixed base point\n$t_0,$ we had to modify the right-hand side of the given equation in all\nsteps of\nthe FORK methods. Some coefficients for explicit and implicit FORK schemes\nare presented. The convergence analysis of the proposed method is also\ndiscussed. Numerical experiments clarify the effectiveness and robustness of\nthe method.",
    "descriptor": "",
    "authors": [
      "F. Ghoreishi",
      "R. Ghaffari"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13138"
  },
  {
    "id": "arXiv:2210.13148",
    "title": "DAGformer: Directed Acyclic Graph Transformer",
    "abstract": "In many fields, such as natural language processing and computer vision, the\nTransformer architecture has become the standard. Recently, the Transformer\narchitecture has also attracted a growing amount of interest in graph\nrepresentation learning since it naturally overcomes some graph neural network\n(GNNs) restrictions. In this work, we focus on a special yet widely used class\nof graphs-DAGs. We propose the directed acyclic graph Transformer, DAGformer, a\nTransformer architecture that processes information according to the\nreachability relation defined by the partial order. DAGformer is simple and\nflexible, allowing it to be used with various transformer-based models. We show\nthat our architecture achieves state-of-the-art performance on representative\nDAG datasets, outperforming all previous approaches.",
    "descriptor": "",
    "authors": [
      "Yuankai Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13148"
  },
  {
    "id": "arXiv:2210.13149",
    "title": "Binary Graph Convolutional Network with Capacity Exploration",
    "abstract": "The current success of Graph Neural Networks (GNNs) usually relies on loading\nthe entire attributed graph for processing, which may not be satisfied with\nlimited memory resources, especially when the attributed graph is large. This\npaper pioneers to propose a Binary Graph Convolutional Network (Bi-GCN), which\nbinarizes both the network parameters and input node attributes and exploits\nbinary operations instead of floating-point matrix multiplications for network\ncompression and acceleration. Meanwhile, we also propose a new gradient\napproximation based back-propagation method to properly train our Bi-GCN.\nAccording to the theoretical analysis, our Bi-GCN can reduce the memory\nconsumption by an average of ~31x for both the network parameters and input\ndata, and accelerate the inference speed by an average of ~51x, on three\ncitation networks, i.e., Cora, PubMed, and CiteSeer. Besides, we introduce a\ngeneral approach to generalize our binarization method to other variants of\nGNNs, and achieve similar efficiencies. Although the proposed Bi-GCN and\nBi-GNNs are simple yet efficient, these compressed networks may also possess a\npotential capacity problem, i.e., they may not have enough storage capacity to\nlearn adequate representations for specific tasks. To tackle this capacity\nproblem, an Entropy Cover Hypothesis is proposed to predict the lower bound of\nthe width of Bi-GNN hidden layers. Extensive experiments have demonstrated that\nour Bi-GCN and Bi-GNNs can give comparable performances to the corresponding\nfull-precision baselines on seven node classification datasets and verified the\neffectiveness of our Entropy Cover Hypothesis for solving the capacity problem.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2010.07565\n",
    "authors": [
      "Junfu Wang",
      "Yuanfang Guo",
      "Liang Yang",
      "Yunhong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13149"
  },
  {
    "id": "arXiv:2210.13150",
    "title": "A PAC-Bayesian Generalization Bound for Equivariant Networks",
    "abstract": "Equivariant networks capture the inductive bias about the symmetry of the\nlearning task by building those symmetries into the model. In this paper, we\nstudy how equivariance relates to generalization error utilizing PAC Bayesian\nanalysis for equivariant networks, where the transformation laws of feature\nspaces are determined by group representations. By using perturbation analysis\nof equivariant networks in Fourier domain for each layer, we derive norm-based\nPAC-Bayesian generalization bounds. The bound characterizes the impact of group\nsize, and multiplicity and degree of irreducible representations on the\ngeneralization error and thereby provide a guideline for selecting them. In\ngeneral, the bound indicates that using larger group size in the model improves\nthe generalization error substantiated by extensive numerical experiments.",
    "descriptor": "\nComments: 41 pages, 15 figures, accepted at NeurIPS 2022\n",
    "authors": [
      "Arash Behboodi",
      "Gabriele Cesa",
      "Taco Cohen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13150"
  },
  {
    "id": "arXiv:2210.13151",
    "title": "Good governance and national information transparency: A comparative  study of 117 countries",
    "abstract": "Information transparency is a major building block of responsible\ngovernments. We explored factors influencing the information transparency of\n117 world nations. After controlling for the effects of confounding variables\nof wealth (GDP per capita), corruption rate, population density, human capital,\nand telecommunication infrastructure, we found that the good governance indices\n(democracy, economy, and management) were strong and stable predictors of\ninformation transparency of world nations.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Mahmood Khosrowjerdi"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13151"
  },
  {
    "id": "arXiv:2210.13153",
    "title": "Reachability-Aware Laplacian Representation in Reinforcement Learning",
    "abstract": "In Reinforcement Learning (RL), Laplacian Representation (LapRep) is a\ntask-agnostic state representation that encodes the geometry of the\nenvironment. A desirable property of LapRep stated in prior works is that the\nEuclidean distance in the LapRep space roughly reflects the reachability\nbetween states, which motivates the usage of this distance for reward shaping.\nHowever, we find that LapRep does not necessarily have this property in\ngeneral: two states having small distance under LapRep can actually be far away\nin the environment. Such mismatch would impede the learning process in reward\nshaping. To fix this issue, we introduce a Reachability-Aware Laplacian\nRepresentation (RA-LapRep), by properly scaling each dimension of LapRep.\nDespite the simplicity, we demonstrate that RA-LapRep can better capture the\ninter-state reachability as compared to LapRep, through both theoretical\nexplanations and experimental results. Additionally, we show that this\nimprovement yields a significant boost in reward shaping performance and also\nbenefits bottleneck state discovery.",
    "descriptor": "",
    "authors": [
      "Kaixin Wang",
      "Kuangqi Zhou",
      "Jiashi Feng",
      "Bryan Hooi",
      "Xinchao Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13153"
  },
  {
    "id": "arXiv:2210.13156",
    "title": "Empirical analysis of PGA-MAP-Elites for Neuroevolution in Uncertain  Domains",
    "abstract": "Quality-Diversity algorithms, among which MAP-Elites, have emerged as\npowerful alternatives to performance-only optimisation approaches as they\nenable generating collections of diverse and high-performing solutions to an\noptimisation problem. However, they are often limited to low-dimensional search\nspaces and deterministic environments. The recently introduced Policy Gradient\nAssisted MAP-Elites (PGA-MAP-Elites) algorithm overcomes this limitation by\npairing the traditional Genetic operator of MAP-Elites with a gradient-based\noperator inspired by Deep Reinforcement Learning. This new operator guides\nmutations toward high-performing solutions using policy-gradients. In this\nwork, we propose an in-depth study of PGA-MAP-Elites. We demonstrate the\nbenefits of policy-gradients on the performance of the algorithm and the\nreproducibility of the generated solutions when considering uncertain domains.\nWe first prove that PGA-MAP-Elites is highly performant in both deterministic\nand uncertain high-dimensional environments, decorrelating the two challenges\nit tackles. Secondly, we show that in addition to outperforming all the\nconsidered baselines, the collections of solutions generated by PGA-MAP-Elites\nare highly reproducible in uncertain environments, approaching the\nreproducibility of solutions found by Quality-Diversity approaches built\nspecifically for uncertain applications. Finally, we propose an ablation and\nin-depth analysis of the dynamic of the policy-gradients-based variation. We\ndemonstrate that the policy-gradient variation operator is determinant to\nguarantee the performance of PGA-MAP-Elites but is only essential during the\nearly stage of the process, where it finds high-performing regions of the\nsearch space.",
    "descriptor": "\nComments: submitted to Transactions on Evolutionary Learning and Optimization\n",
    "authors": [
      "Manon Flageat",
      "Felix Chalumeau",
      "Antoine Cully"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.13156"
  },
  {
    "id": "arXiv:2210.13159",
    "title": "Towards an Understanding of Long-Tailed Runtimes of SLS Algorithms",
    "abstract": "The satisfiability problem is one of the most famous problems in computer\nscience. Its NP-completeness has been used to argue that SAT is intractable.\nHowever, there have been tremendous advances that allow SAT solvers to solve\ninstances with millions of variables. A particularly successful paradigm is\nstochastic local search.\nIn most cases, there are different ways of formulating the underlying\nproblem. While it is known that this has an impact on the runtime of solvers,\nfinding a helpful formulation is generally non-trivial. The recently introduced\nGapSAT solver [Lorenz and W\\\"orz 2020] demonstrated a successful way to improve\nthe performance of an SLS solver on average by learning additional information\nwhich logically entails from the original problem. Still, there were cases in\nwhich the performance slightly deteriorated. This justifies in-depth\ninvestigations into how learning logical implications affects runtimes for SLS.\nIn this work, we propose a method for generating logically equivalent problem\nformulations, generalizing the ideas of GapSAT. This allows a rigorous\nmathematical study of the effect on the runtime of SLS solvers. If the\nmodification process is treated as random, Johnson SB distributions provide a\nperfect characterization of the hardness. Since the observed Johnson SB\ndistributions approach lognormal distributions, our analysis also suggests that\nthe hardness is long-tailed. As a second contribution, we theoretically prove\nthat restarts are useful for long-tailed distributions. This implies that\nadditional restarts can further refine all algorithms employing above mentioned\nmodification technique. Since the empirical studies compellingly suggest that\nthe runtime distributions follow Johnson SB distributions, we investigate this\nproperty theoretically. We succeed in proving that the runtimes for\nSch\\\"oning's random walk algorithm are approximately Johnson SB.",
    "descriptor": "\nComments: Full-length version of the article in ACM Journal of Experimental Algorithmics (JEA). arXiv admin note: text overlap with arXiv:2107.00378\n",
    "authors": [
      "Jan-Hendrik Lorenz",
      "Florian W\u00f6rz"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Artificial Intelligence (cs.AI)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.13159"
  },
  {
    "id": "arXiv:2210.13160",
    "title": "Algorithms for geometrical operations with NURBS surfaces",
    "abstract": "The aim of the paper is to show algorithms for geometrical manipulations on\nNURBS surfaces. These include generating NURBS surfaces that pass through given\npoints, calculating the minimum distance to a point and include line to surface\nand surface to surface intersections.",
    "descriptor": "\nComments: A technical note explaining algorithms that are useful for software developers\n",
    "authors": [
      "Gernot Beer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13160"
  },
  {
    "id": "arXiv:2210.13163",
    "title": "Bilingual Synchronization: Restoring Translational Relationships with  Editing Operations",
    "abstract": "Machine Translation (MT) is usually viewed as a one-shot process that\ngenerates the target language equivalent of some source text from scratch. We\nconsider here a more general setting which assumes an initial target sequence,\nthat must be transformed into a valid translation of the source, thereby\nrestoring parallelism between source and target. For this bilingual\nsynchronization task, we consider several architectures (both autoregressive\nand non-autoregressive) and training regimes, and experiment with multiple\npractical settings such as simulated interactive MT, translating with\nTranslation Memory (TM) and TM cleaning. Our results suggest that one single\ngeneric edit-based system, once fine-tuned, can compare with, or even\noutperform, dedicated systems specifically trained for these tasks.",
    "descriptor": "\nComments: EMNLP 2022 main conference\n",
    "authors": [
      "Jitao Xu",
      "Josep Crego",
      "Fran\u00e7ois Yvon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13163"
  },
  {
    "id": "arXiv:2210.13167",
    "title": "Exploring Self-Attention for Crop-type Classification Explainability",
    "abstract": "Automated crop-type classification using Sentinel-2 satellite time series is\nessential to support agriculture monitoring. Recently, deep learning models\nbased on transformer encoders became a promising approach for crop-type\nclassification. Using explainable machine learning to reveal the inner workings\nof these models is an important step towards improving stakeholders' trust and\nefficient agriculture monitoring.\nIn this paper, we introduce a novel explainability framework that aims to\nshed a light on the essential crop disambiguation patterns learned by a\nstate-of-the-art transformer encoder model. More specifically, we process the\nattention weights of a trained transformer encoder to reveal the critical dates\nfor crop disambiguation and use domain knowledge to uncover the phenological\nevents that support the model performance. We also present a sensitivity\nanalysis approach to understand better the attention capability for revealing\ncrop-specific phenological events.\nWe report compelling results showing that attention patterns strongly relate\nto key dates, and consequently, to the critical phenological events for\ncrop-type classification. These findings might be relevant for improving\nstakeholder trust and optimizing agriculture monitoring processes.\nAdditionally, our sensitivity analysis demonstrates the limitation of attention\nweights for identifying the important events in the crop phenology as we\nempirically show that the unveiled phenological events depend on the other\ncrops in the data considered during training.",
    "descriptor": "",
    "authors": [
      "Ivica Obadic",
      "Ribana Roscher",
      "Dario Augusto Borges Oliveira",
      "Xiao Xiang Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13167"
  },
  {
    "id": "arXiv:2210.13168",
    "title": "Proficiency assessment of L2 spoken English using wav2vec 2.0",
    "abstract": "The increasing demand for learning English as a second language has led to a\ngrowing interest in methods for automatically assessing spoken language\nproficiency. Most approaches use hand-crafted features, but their efficacy\nrelies on their particular underlying assumptions and they risk discarding\npotentially salient information about proficiency. Other approaches rely on\ntranscriptions produced by ASR systems which may not provide a faithful\nrendition of a learner's utterance in specific scenarios (e.g., non-native\nchildren's spontaneous speech). Furthermore, transcriptions do not yield any\ninformation about relevant aspects such as intonation, rhythm or prosody. In\nthis paper, we investigate the use of wav2vec 2.0 for assessing overall and\nindividual aspects of proficiency on two small datasets, one of which is\npublicly available. We find that this approach significantly outperforms the\nBERT-based baseline system trained on ASR and manual transcriptions used for\ncomparison.",
    "descriptor": "\nComments: Accepted at SLT 2022\n",
    "authors": [
      "Stefano Bann\u00f2",
      "Marco Matassoni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13168"
  },
  {
    "id": "arXiv:2210.13171",
    "title": "Distributed DeeP-LCC for cooperatively smoothing large-scale mixed  traffic flow via connected and automated vehicles",
    "abstract": "Cooperative control of connected and automated vehicles (CAVs) promises\nsmoother traffic flow. In mixed traffic, where human-driven vehicles with\nunknown dynamics coexist, data-driven control techniques, particularly the\nrecently proposed DeeP-LCC (Data-EnablEd Predictive Leading Cruise Control),\ndirectly utilizes measurable traffic data to achieve CAV safe and optimal\ncontrol. However, the centralized control setting in most data-driven\nstrategies prohibits their large-scale application. To improve the scalability\nof DeeP-LCC, this paper proposes a cooperative DeeP-LCC formulation and its\ndistributed implementation algorithm. In cooperative DeeP-LCC, the traffic\nsystem is naturally partitioned into multiple subsystems, where each CAV\ncollects local trajectory data for predictions, and the cross-subsystem\ninteraction is formulated as a coupling constraint. Then, we employ the\nAlternating Direction Method of Multipliers (ADMM) to design the distributed\nDeeP-LCC algorithm, which achieves computation and communication efficiency and\ntrajectory data privacy. Large-scale experiments on 100 vehicles with 20 CAVs\nreveal the real-time wave-dampening potential of distributed DeeP-LCC, saving\nup to 32.53% fuel consumption.",
    "descriptor": "\nComments: 24 pages, 8 figures\n",
    "authors": [
      "Jiawei Wang",
      "Yingzhao Lian",
      "Yuning Jiang",
      "Qing Xu",
      "Keqiang Li",
      "Colin N. Jones"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13171"
  },
  {
    "id": "arXiv:2210.13181",
    "title": "The Better Your Syntax, the Better Your Semantics? Probing Pretrained  Language Models for the English Comparative Correlative",
    "abstract": "Construction Grammar (CxG) is a paradigm from cognitive linguistics\nemphasising the connection between syntax and semantics. Rather than rules that\noperate on lexical items, it posits constructions as the central building\nblocks of language, i.e., linguistic units of different granularity that\ncombine syntax and semantics. As a first step towards assessing the\ncompatibility of CxG with the syntactic and semantic knowledge demonstrated by\nstate-of-the-art pretrained language models (PLMs), we present an investigation\nof their capability to classify and understand one of the most commonly studied\nconstructions, the English comparative correlative (CC). We conduct experiments\nexamining the classification accuracy of a syntactic probe on the one hand and\nthe models' behaviour in a semantic application task on the other, with BERT,\nRoBERTa, and DeBERTa as the example PLMs. Our results show that all three\ninvestigated PLMs are able to recognise the structure of the CC but fail to use\nits meaning. While human-like performance of PLMs on many NLP tasks has been\nalleged, this indicates that PLMs still suffer from substantial shortcomings in\ncentral domains of linguistic knowledge.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Leonie Weissweiler",
      "Valentin Hofmann",
      "Abdullatif K\u00f6ksal",
      "Hinrich Sch\u00fctze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13181"
  },
  {
    "id": "arXiv:2210.13182",
    "title": "Simultaneous Improvement of ML Model Fairness and Performance by  Identifying Bias in Data",
    "abstract": "Machine learning models built on datasets containing discriminative instances\nattributed to various underlying factors result in biased and unfair outcomes.\nIt's a well founded and intuitive fact that existing bias mitigation strategies\noften sacrifice accuracy in order to ensure fairness. But when AI engine's\nprediction is used for decision making which reflects on revenue or operational\nefficiency such as credit risk modelling, it would be desirable by the business\nif accuracy can be somehow reasonably preserved. This conflicting requirement\nof maintaining accuracy and fairness in AI motivates our research. In this\npaper, we propose a fresh approach for simultaneous improvement of fairness and\naccuracy of ML models within a realistic paradigm. The essence of our work is a\ndata preprocessing technique that can detect instances ascribing a specific\nkind of bias that should be removed from the dataset before training and we\nfurther show that such instance removal will have no adverse impact on model\naccuracy. In particular, we claim that in the problem settings where instances\nexist with similar feature but different labels caused by variation in\nprotected attributes , an inherent bias gets induced in the dataset, which can\nbe identified and mitigated through our novel scheme. Our experimental\nevaluation on two open-source datasets demonstrates how the proposed method can\nmitigate bias along with improving rather than degrading accuracy, while\noffering certain set of control for end user.",
    "descriptor": "",
    "authors": [
      "Bhushan Chaudhari",
      "Akash Agarwal",
      "Tanmoy Bhowmik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13182"
  },
  {
    "id": "arXiv:2210.13184",
    "title": "DPU-v2: Energy-efficient execution of irregular directed acyclic graphs",
    "abstract": "A growing number of applications like probabilistic machine learning, sparse\nlinear algebra, robotic navigation, etc., exhibit irregular data flow\ncomputation that can be modeled with directed acyclic graphs (DAGs). The\nirregularity arises from the seemingly random connections of nodes, which makes\nthe DAG structure unsuitable for vectorization on CPU or GPU. Moreover, the\nnodes usually represent a small number of arithmetic operations that cannot\namortize the overhead of launching tasks/kernels for each node, further posing\nchallenges for parallel execution.\nTo enable energy-efficient execution, this work proposes DAG processing unit\n(DPU) version 2, a specialized processor architecture optimized for irregular\nDAGs with static connectivity. It consists of a tree-structured datapath for\nefficient data reuse, a customized banked register file, and interconnects\ntuned to support irregular register accesses. DPU-v2 is utilized effectively\nthrough a targeted compiler that systematically maps operations to the\ndatapath, minimizes register bank conflicts, and avoids pipeline hazards.\nFinally, a design space exploration identifies the optimal architecture\nconfiguration that minimizes the energy-delay product. This hardware-software\nco-optimization approach results in a speedup of 1.4$\\times$, 3.5$\\times$, and\n14$\\times$ over a state-of-the-art DAG processor ASIP, a CPU, and a GPU,\nrespectively, while also achieving a lower energy-delay product. In this way,\nthis work takes an important step toward enabling an embedded execution of\nemerging DAG workloads.",
    "descriptor": "",
    "authors": [
      "Nimish Shah",
      "Wannes Meert",
      "Marian Verhelst"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.13184"
  },
  {
    "id": "arXiv:2210.13186",
    "title": "Meta Input: How to Leverage Off-the-Shelf Deep Neural Networks",
    "abstract": "These days, although deep neural networks (DNNs) have achieved a noticeable\nprogress in a wide range of research area, it lacks the adaptability to be\nemployed in the real-world applications because of the environment discrepancy\nproblem. Such a problem originates from the difference between training and\ntesting environments, and it is widely known that it causes serious performance\ndegradation, when a pretrained DNN model is applied to a new testing\nenvironment. Therefore, in this paper, we introduce a novel approach that\nallows end-users to exploit pretrained DNN models in their own testing\nenvironment without modifying the models. To this end, we present a\n\\textit{meta input} which is an additional input transforming the distribution\nof testing data to be aligned with that of training data. The proposed meta\ninput can be optimized with a small number of testing data only by considering\nthe relation between testing input data and its output prediction. Also, it\ndoes not require any knowledge of the network's internal architecture and\nmodification of its weight parameters. Then, the obtained meta input is added\nto testing data in order to shift the distribution of testing data to that of\noriginally used training data. As a result, end-users can exploit well-trained\nmodels in their own testing environment which can differ from the training\nenvironment. We validate the effectiveness and versatility of the proposed meta\ninput by showing the robustness against the environment discrepancy through the\ncomprehensive experiments with various tasks.",
    "descriptor": "",
    "authors": [
      "Minsu Kim",
      "Youngjoon Yu",
      "Sungjune Park",
      "Yong Man Ro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13186"
  },
  {
    "id": "arXiv:2210.13188",
    "title": "Dissecting Deep Metric Learning Losses for Image-Text Retrieval",
    "abstract": "Visual-Semantic Embedding (VSE) is a prevalent approach in image-text\nretrieval by learning a joint embedding space between the image and language\nmodalities where semantic similarities would be preserved. The triplet loss\nwith hard-negative mining has become the de-facto objective for most VSE\nmethods. Inspired by recent progress in deep metric learning (DML) in the image\ndomain which gives rise to new loss functions that outperform triplet loss, in\nthis paper, we revisit the problem of finding better objectives for VSE in\nimage-text matching. Despite some attempts in designing losses based on\ngradient movement, most DML losses are defined empirically in the embedding\nspace. Instead of directly applying these loss functions which may lead to\nsub-optimal gradient updates in model parameters, in this paper we present a\nnovel Gradient-based Objective AnaLysis framework, or \\textit{GOAL}, to\nsystematically analyze the combinations and reweighting of the gradients in\nexisting DML functions. With the help of this analysis framework, we further\npropose a new family of objectives in the gradient space exploring different\ngradient combinations. In the event that the gradients are not integrable to a\nvalid loss function, we implement our proposed objectives such that they would\ndirectly operate in the gradient space instead of on the losses in the\nembedding space. Comprehensive experiments have demonstrated that our novel\nobjectives have consistently improved performance over baselines across\ndifferent visual/text features and model frameworks. We also showed the\ngeneralizability of the GOAL framework by extending it to other models using\ntriplet family losses including vision-language model with heavy cross-modal\ninteractions and have achieved state-of-the-art results on the image-text\nretrieval tasks on COCO and Flick30K.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2201.11307\n",
    "authors": [
      "Hong Xuan",
      "Xi Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13188"
  },
  {
    "id": "arXiv:2210.13191",
    "title": "Navigating the challenges in creating complex data systems: a  development philosophy",
    "abstract": "In this perspective, we argue that despite the democratization of powerful\ntools for data science and machine learning over the last decade, developing\nthe code for a trustworthy and effective data science system (DSS) is getting\nharder. Perverse incentives and a lack of widespread software engineering (SE)\nskills are among many root causes we identify that naturally give rise to the\ncurrent systemic crisis in reproducibility of DSSs. We analyze why SE and\nbuilding large complex systems is, in general, hard. Based on these insights,\nwe identify how SE addresses those difficulties and how we can apply and\ngeneralize SE methods to construct DSSs that are fit for purpose. We advocate\ntwo key development philosophies, namely that one should incrementally grow --\nnot biphasically plan and build -- DSSs, and one should always employ two types\nof feedback loops during development: one which tests the code's correctness\nand another that evaluates the code's efficacy.",
    "descriptor": "",
    "authors": [
      "S\u00f6ren Dittmer",
      "Michael Roberts",
      "Julian Gilbey",
      "Ander Biguri",
      "AIX-COVNET Collaboration",
      "Jacobus Preller",
      "James H.F. Rudd",
      "John A.D. Aston",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13191"
  },
  {
    "id": "arXiv:2210.13199",
    "title": "Informed Sampling-based Collision Avoidance with Least Deviation from  the Nominal Path",
    "abstract": "This paper addresses local path re-planning for $n$-dimensional systems by\nintroducing an informed sampling scheme and cost function to achieve collision\navoidance with minimum deviation from an (optimal) nominal path. The proposed\ninformed subset consists of the union of ellipsoids along the specified nominal\npath, such that the subset efficiently encapsulates all points along the\nnominal path. The cost function penalizes large deviations from the nominal\npath, thereby ensuring current safety in the face of potential collisions while\nretaining most of the overall efficiency of the nominal path. The proposed\nmethod is demonstrated on scenarios related to the navigation of autonomous\nmarine crafts.",
    "descriptor": "\nComments: Accepted for publication at IROS'2022\n",
    "authors": [
      "Thomas T. Enevoldsen",
      "Roberto Galeazzi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13199"
  },
  {
    "id": "arXiv:2210.13202",
    "title": "Online Information Retrieval Evaluation using the STELLA Framework",
    "abstract": "Involving users in early phases of software development has become a common\nstrategy as it enables developers to consider user needs from the beginning.\nOnce a system is in production, new opportunities to observe, evaluate and\nlearn from users emerge as more information becomes available. Gathering\ninformation from users to continuously evaluate their behavior is a common\npractice for commercial software, while the Cranfield paradigm remains the\npreferred option for Information Retrieval (IR) and recommendation systems in\nthe academic world. Here we introduce the Infrastructures for Living Labs\nSTELLA project which aims to create an evaluation infrastructure allowing\nexperimental systems to run along production web-based academic search systems\nwith real users. STELLA combines user interactions and log files analyses to\nenable large-scale A/B experiments for academic search.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2203.05430\n",
    "authors": [
      "Timo Breuer",
      "Narges Tavakolpoursaleh",
      "Johann Schaible",
      "Daniel Hienert",
      "Philipp Schaer",
      "Leyla Jael Castro"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.13202"
  },
  {
    "id": "arXiv:2210.13205",
    "title": "Applying Autonomous Hybrid Agent-based Computing to Difficult  Optimization Problems",
    "abstract": "Evolutionary multi-agent systems (EMASs) are very good at dealing with\ndifficult, multi-dimensional problems, their efficacy was proven theoretically\nbased on analysis of the relevant Markov-Chain based model. Now the research\ncontinues on introducing autonomous hybridization into EMAS. This paper focuses\non a proposed hybrid version of the EMAS, and covers selection and introduction\nof a number of hybrid operators and defining rules for starting the hybrid\nsteps of the main algorithm. Those hybrid steps leverage existing, well-known\nand proven to be efficient metaheuristics, and integrate their results into the\nmain algorithm. The discussed modifications are evaluated based on a number of\ndifficult continuous-optimization benchmarks.",
    "descriptor": "",
    "authors": [
      "Mateusz Godzik",
      "Jacek Dajda",
      "Marek Kisiel-Dorohinicki",
      "Aleksander Byrski",
      "Leszek Rutkowski",
      "Patryk Orzechowski",
      "Joost Wagenaar",
      "Jason H. Moore"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.13205"
  },
  {
    "id": "arXiv:2210.13207",
    "title": "GeoAI at ACM SIGSPATIAL: The New Frontier of Geospatial Artificial  Intelligence Research",
    "abstract": "Geospatial Artificial Intelligence (GeoAI) is an interdisciplinary field\nenjoying tremendous adoption. However, the efficient design and implementation\nof GeoAI systems face many open challenges. This is mainly due to the lack of\nnon-standardized approaches to artificial intelligence tool development,\ninadequate platforms, and a lack of multidisciplinary engagements, which all\nmotivate domain experts to seek a shared stage with scientists and engineers to\nsolve problems of significant impact on society. Since its inception in 2017,\nthe GeoAI series of workshops has been co-located with the Association for\nComputing Machinery International Conference on Advances in Geographic\nInformation Systems. The workshop series has fostered a nexus for\ngeoscientists, computer scientists, engineers, entrepreneurs, and\ndecision-makers, from academia, industry, and government to engage in\nartificial intelligence, spatiotemporal data computing, and geospatial data\nscience research, motivated by various challenges. In this article, we revisit\nand discuss the state of GeoAI open research directions, the recent\ndevelopments, and an emerging agenda calling for a continued cross-disciplinary\ncommunity engagement.",
    "descriptor": "\nComments: 12 pages, 1 figure, 1 table\n",
    "authors": [
      "Dalton Lunga",
      "Yingjie Hu",
      "Shawn Newsam",
      "Song Gao",
      "Bruno Martins",
      "Lexie Yang",
      "Xueqing Deng"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13207"
  },
  {
    "id": "arXiv:2210.13210",
    "title": "Mutual Information Alleviates Hallucinations in Abstractive  Summarization",
    "abstract": "Despite significant progress in the quality of language generated from\nabstractive summarization models, these models still exhibit the tendency to\nhallucinate, i.e., output content not supported by the source document. A\nnumber of works have tried to fix--or at least uncover the source of--the\nproblem with limited success. In this paper, we identify a simple criterion\nunder which models are significantly more likely to assign more probability to\nhallucinated content during generation: high model uncertainty. This finding\noffers a potential explanation for hallucinations: models default to favoring\ntext with high marginal probability, i.e., high-frequency occurrences in the\ntraining set, when uncertain about a continuation. It also motivates possible\nroutes for real-time intervention during decoding to prevent such\nhallucinations. We propose a decoding strategy that switches to optimizing for\npointwise mutual information of the source and target token--rather than purely\nthe probability of the target token--when the model exhibits uncertainty.\nExperiments on the XSum dataset show that our method decreases the probability\nof hallucinated tokens while maintaining the Rouge and BertS scores of\ntop-performing decoding strategies.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Liam van der Poel",
      "Ryan Cotterell",
      "Clara Meister"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13210"
  },
  {
    "id": "arXiv:2210.13212",
    "title": "A Dimension-Augmented Physics-Informed Neural Network (DaPINN) with High  Level Accuracy and Efficiency",
    "abstract": "Physics-informed neural networks (PINNs) have been widely applied in\ndifferent fields due to their effectiveness in solving partial differential\nequations (PDEs). However, the accuracy and efficiency of PINNs need to be\nconsiderably improved for scientific and commercial use. To address this issue,\nwe systematically propose a novel dimension-augmented physics-informed neural\nnetwork (DaPINN), which simultaneously and significantly improves the accuracy\nand efficiency of the PINN. In the DaPINN model, we introduce inductive bias in\nthe neural network to enhance network generalizability by adding a special\nregularization term to the loss function. Furthermore, we manipulate the\nnetwork input dimension by inserting additional sample features and\nincorporating the expanded dimensionality in the loss function. Moreover, we\nverify the effectiveness of power series augmentation, Fourier series\naugmentation and replica augmentation, in both forward and backward problems.\nIn most experiments, the error of DaPINN is 1$\\sim$2 orders of magnitude lower\nthan that of PINN. The results show that the DaPINN outperforms the original\nPINN in terms of both accuracy and efficiency with a reduced dependence on the\nnumber of sample points. We also discuss the complexity of the DaPINN and its\ncompatibility with other methods.",
    "descriptor": "\nComments: 33 pages, 12 figures\n",
    "authors": [
      "Weilong Guan",
      "Kaihan Yang",
      "Yinsheng Chen",
      "Zhong Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.13212"
  },
  {
    "id": "arXiv:2210.13225",
    "title": "Biologically Plausible Variational Policy Gradient with Spiking  Recurrent Winner-Take-All Networks",
    "abstract": "One stream of reinforcement learning research is exploring biologically\nplausible models and algorithms to simulate biological intelligence and fit\nneuromorphic hardware. Among them, reward-modulated spike-timing-dependent\nplasticity (R-STDP) is a recent branch with good potential in energy\nefficiency. However, current R-STDP methods rely on heuristic designs of local\nlearning rules, thus requiring task-specific expert knowledge. In this paper,\nwe consider a spiking recurrent winner-take-all network, and propose a new\nR-STDP method, spiking variational policy gradient (SVPG), whose local learning\nrules are derived from the global policy gradient and thus eliminate the need\nfor heuristic designs. In experiments of MNIST classification and Gym\nInvertedPendulum, our SVPG achieves good training performance, and also\npresents better robustness to various kinds of noises than conventional\nmethods.",
    "descriptor": "\nComments: Accepted to BMVC 2022\n",
    "authors": [
      "Zhile Yang",
      "Shangqi Guo",
      "Ying Fang",
      "Jian K. Liu"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.13225"
  },
  {
    "id": "arXiv:2210.13229",
    "title": "Computation of Laplacian eigenvalues of two-dimensional shapes with  dihedral symmetry",
    "abstract": "We numerically compute the lowest Laplacian eigenvalues of several\ntwo-dimensional shapes with dihedral symmetry at arbitrary precision\narithmetic. Our approach is based on the method of particular solutions with\ndomain decomposition. We are particularly interested in asymptotic expansions\nof the eigenvalues $\\lambda(n)$ of shapes with $n$ edges that are of the form\n$\\lambda(n) \\sim x\\sum_{k=0}^{\\infty} \\frac{C_k(x)}{n^k}$ where $x$ is the\nlimiting eigenvalue for $n\\rightarrow \\infty$. Expansions of this form have\npreviously only been known for regular polygons with Dirichlet boundary\ncondition and (quite surprisingly) involve Riemann zeta values and\nsingle-valued multiple zeta values, which makes them interesting to study. We\nprovide numerical evidence for closed-form expressions of higher order $C_k(x)$\nand give more examples of shapes for which such expansions are possible\n(including regular polygons with Neumann boundary condition, regular star\npolygons and star shapes with sinusoidal boundary).",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "David Berghaus",
      "Robert Stephen Jones",
      "Hartmut Monien",
      "Danylo Radchenko"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)",
      "Number Theory (math.NT)",
      "Spectral Theory (math.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13229"
  },
  {
    "id": "arXiv:2210.13230",
    "title": "Comparing Exploratory Graphical Analyses and Unique Variable Analysis to  Other Dimension Reduction Methods On Machine Learning Algorithms",
    "abstract": "Developing interpretable machine learning models has become an increasingly\nimportant issue. One way in which data scientists have been able to develop\ninterpretable models has been to use dimension reduction techniques. In this\npaper, we examine several dimension reduction techniques including two recent\napproaches developed in the network psychometrics literature called exploratory\ngraph analysis (EGA) and unique variable analysis (UVA). We compared EGA and\nUVA with two other dimension reduction techniques common in the machine\nlearning literature (principal component analysis and independent component\nanalysis) as well as no reduction to the variables real data. We show that EGA\nand UVA perform as well as the other reduction techniques or no reduction.\nConsistent with previous literature, we show that dimension reduction can\ndecrease, increase, or provide the same accuracy as no reduction of variables.\nOur tentative results find that dimension reduction tends to lead to better\nperformance when used for classification tasks.",
    "descriptor": "",
    "authors": [
      "Sean H. Merritt",
      "Alexander P. Christensen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.13230"
  },
  {
    "id": "arXiv:2210.13231",
    "title": "Analysing Training-Data Leakage from Gradients through Linear Systems  and Gradient Matching",
    "abstract": "Recent works have demonstrated that it is possible to reconstruct training\nimages and their labels from gradients of an image-classification model when\nits architecture is known. Unfortunately, there is still an incomplete\ntheoretical understanding of the efficacy and failure of these gradient-leakage\nattacks. In this paper, we propose a novel framework to analyse training-data\nleakage from gradients that draws insights from both analytic and\noptimisation-based gradient-leakage attacks. We formulate the reconstruction\nproblem as solving a linear system from each layer iteratively, accompanied by\ncorrections using gradient matching. Under this framework, we claim that the\nsolubility of the reconstruction problem is primarily determined by that of the\nlinear system at each layer. As a result, we are able to partially attribute\nthe leakage of the training data in a deep network to its architecture. We also\npropose a metric to measure the level of security of a deep learning model\nagainst gradient-based attacks on the training data.",
    "descriptor": "\nComments: To appear at the 33rd British Machine Vision Conference 2022\n",
    "authors": [
      "Cangxiong Chen",
      "Neill D. F. Campbell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13231"
  },
  {
    "id": "arXiv:2210.13232",
    "title": "A Complete Recipe for Bayesian Knowledge Transfer: Object Tracking",
    "abstract": "The problem of sequentially transferring from a source object track and a\nmodel to another Bayesian filter has become ubiquitous. Due to the lack of a\nstructural model that can capture the dependence among different models, the\ntransfer may not be fully specified. In this paper, we introduce a novel\nBayesian model that accounts for the model-jump from which the object can\nchoose a model and follow. We aim to track the trajectory of the object while\nsequentially transferring from the source object to the target object. The main\nidea is to impute the dynamical model while tracking the object and estimating\nthe state parameters of the moving object according to discretized dynamic\nsystems. We demonstrate this procedure can handle the model mismatch as it\nsequentially corrects the predictive model. Particularly, for a fixed number of\nmotion models, the object can learn what motion to follow at each time step. We\nemploy a prior model for each model and then adaptively correct for changing\none model to another to robustly estimate object trajectory under various\nmotions. More concretely, we propose a robust Bayesian recipe to handle the\nmodel-jump and then integrate it with a Markov chain Monte Carlo (MCMC)\napproach to sample from the posterior distribution. We demonstrate through\nexperiments the advantage of accounting for model-jump in our proposed method\nfor knowledge transfer between learning tasks in Bayesian transfer learning.",
    "descriptor": "",
    "authors": [
      "Bahman Moraffah",
      "Antonia Papandreou-Suppappola"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13232"
  },
  {
    "id": "arXiv:2210.13235",
    "title": "Chaos Theory and Adversarial Robustness",
    "abstract": "Neural Networks, being susceptible to adversarial attacks, should face a\nstrict level of scrutiny before being deployed in critical or adversarial\napplications. This paper uses ideas from Chaos Theory to explain, analyze, and\nquantify the degree to which Neural Networks are susceptible to or robust\nagainst adversarial attacks. Our results show that susceptibility to attack\ngrows significantly with the depth of the model, which has significant safety\nimplications for the design of Neural Networks for production environments. We\nalso demonstrate how to quickly and easily approximate the certified robustness\nradii for extremely large models, which until now has been computationally\ninfeasible to calculate directly, as well as show a clear relationship between\nour new susceptibility metric and post-attack accuracy.",
    "descriptor": "\nComments: 13 pages, 6 figures\n",
    "authors": [
      "Jonathan S. Kent"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13235"
  },
  {
    "id": "arXiv:2210.13236",
    "title": "Universal and Independent: Multilingual Probing Framework for Exhaustive  Model Interpretation and Evaluation",
    "abstract": "Linguistic analysis of language models is one of the ways to explain and\ndescribe their reasoning, weaknesses, and limitations. In the probing part of\nthe model interpretability research, studies concern individual languages as\nwell as individual linguistic structures. The question arises: are the detected\nregularities linguistically coherent, or on the contrary, do they dissonate at\nthe typological scale? Moreover, the majority of studies address the inherent\nset of languages and linguistic structures, leaving the actual typological\ndiversity knowledge out of scope. In this paper, we present and apply the\nGUI-assisted framework allowing us to easily probe a massive number of\nlanguages for all the morphosyntactic features present in the Universal\nDependencies data. We show that reflecting the anglo-centric trend in NLP over\nthe past years, most of the regularities revealed in the mBERT model are\ntypical for the western-European languages. Our framework can be integrated\nwith the existing probing toolboxes, model cards, and leaderboards, allowing\npractitioners to use and share their standard probing methods to interpret\nmultilingual models. Thus we propose a toolkit to systematize the multilingual\nflaws in multilingual models, providing a reproducible experimental setup for\n104 languages and 80 morphosyntactic features.\nhttps://github.com/AIRI-Institute/Probing_framework",
    "descriptor": "\nComments: Accepted to BlackBoxNLP, EMNLP 2022\n",
    "authors": [
      "Oleg Serikov",
      "Vitaly Protasov",
      "Ekaterina Voloshina",
      "Viktoria Knyazkova",
      "Tatiana Shavrina"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13236"
  },
  {
    "id": "arXiv:2210.13242",
    "title": "SurferMonkey: A Decentralized Anonymous Blockchain Intercommunication  System via Zero Knowledge Proofs",
    "abstract": "Blockchain intercommunication systems enable the exchanges of messages\nbetween blockchains. This interoperability promotes innovation, unlocks\nliquidity and access to assets. However, blockchains are isolated systems that\noriginally were not designed for interoperability. This makes cross-chain\ncommunication, or bridges for short, insecure by nature. More precisely,\ncross-chain systems face security challenges in terms of selfish rational\nplayers such as maximal extractable value (MEV) and censorship.\nWe propose to solve these challenges using zero knowledge proofs (ZKPs) for\ncross-chain communication. Securing cross-chain communication is remarkably\nmore complex than securing single-chain events as such a system must preserve\nuser security against both on- and off-chain analysis.\nTo achieve this goal, we propose the following pair of contributions: the\nDACT protocol and the SurferMonkey infrastructure that supports the DACT\nprotocol. The DACT protocol is a global solution for the anonymity and security\nchallenges of agnostic blockchain intercommunication. DACT breaks on- and\noff-chain analysis thanks to the use of ZKPs. SurferMonkey is a decentralized\ninfrastructure that implements DACT in practice. Since SurferMonkey works at\nthe blockchain application layer, any decentralized application (dApp) can use\nSurferMonkey to send any type of message to a dApp on another blockchain. With\nSurferMonkey, users can neither be censored nor be exposed to MEV. By applying\ndecentralized proactive security, we obtain resilience against selfish rational\nplayers, and raise the security bar against cyberattacks. We have implemented a\nproof of concept (PoC) of SurferMonkey by reverse engineering Tornado Cash and\nby applying IDEN3 ZKP circuits. SurferMonkey enables new usecases, ranging from\nanonymous voting and gaming, to a new phase of anonymous decentralized finance\n(aDeFi).",
    "descriptor": "",
    "authors": [
      "Miguel D\u00edaz Montiel",
      "Rachid Guerraoui",
      "Pierre-Louis Roman"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.13242"
  },
  {
    "id": "arXiv:2210.13252",
    "title": "Library and Information Science Scholarly Journals Publishing  Simulation: A Study",
    "abstract": "The author's productivity is assessed based on publications, which requires a\nlot of motivation and time. Manuscripts get through several steps before being\naccepted and published. The purpose of this paper is to understand the time gap\nbetween acceptance to the publication of manuscripts in reputed journals of\nLibrary and Information Science.\nThis paper is useful to contemporary researchers for knowing the journal\npublication duration. In this paper, we discussed the refereed and index\njournals in the field of library and information science. For this study, we\ncollected the data from six LIS journals which were published from the 2020\nJanuary to December Asian region. The study focuses on detailed analyses of\njournal processing and publishing duration. The major contribution of this\nstudy gives the six LIS journal processing time they are: author manuscript\nsubmitted to accepted, accepted to published, and submitted to published\nperiod.",
    "descriptor": "",
    "authors": [
      "Priyanka Sinha",
      "Subaveerapandiyan A"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2210.13252"
  },
  {
    "id": "arXiv:2210.13255",
    "title": "Local Connection Reinforcement Learning Method for Efficient Control of  Robotic Peg-in-Hole Assembly",
    "abstract": "Traditional control methods of robotic peg-in-hole assembly rely on complex\ncontact state analysis. Reinforcement learning (RL) is gradually becoming a\npreferred method of controlling robotic peg-in-hole assembly tasks. However,\nthe training process of RL is quite time-consuming because RL methods are\nalways globally connected, which means all state components are assumed to be\nthe input of policies for all action components, thus increasing action space\nand state space to be explored. In this paper, we first define continuous space\nserialized Shapley value (CS3) and construct a connection graph to clarify the\ncorrelativity of action components on state components. Then we propose a local\nconnection reinforcement learning (LCRL) method based on the connection graph,\nwhich eliminates the influence of irrelevant state components on the selection\nof action components. The simulation and experiment results demonstrate that\nthe control strategy obtained through LCRL method improves the stability and\nrapidity of the control process. LCRL method will enhance the data-efficiency\nand increase the final reward of the training process.",
    "descriptor": "",
    "authors": [
      "Yuhang Gai",
      "Jiwen Zhang",
      "Dan Wu",
      "Ken Chen"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13255"
  },
  {
    "id": "arXiv:2210.13261",
    "title": "Physical Layer Security -- from Theory to Practice",
    "abstract": "A large spectrum of technologies are collectively dubbed as physical layer\nsecurity (PLS), ranging from wiretap coding, secret key generation (SKG),\nauthentication using physical unclonable functions (PUFs), localization / RF\nfingerprinting, anomaly detection monitoring the physical layer (PHY) and\nhardware. Despite the fact that the fundamental limits of PLS have long been\ncharacterized, incorporating PLS in future wireless security standards requires\nfurther steps in terms of channel engineering and pre-processing. Reflecting\nupon the growing discussion in our community, in this critical review paper, we\nask some important questions with respect to the key hurdles in the practical\ndeployment of PLS in 6G, but also present some research directions and possible\nsolutions, in particular our vision for context-aware 6G security that\nincorporates PLS.",
    "descriptor": "",
    "authors": [
      "Miroslav Mitev",
      "Thuy M. Pham",
      "Arsenia Chorti",
      "Andre Noll Barreto",
      "Gerhard Fettweis"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13261"
  },
  {
    "id": "arXiv:2210.13263",
    "title": "Driver Locations Harvesting Attack on pRide",
    "abstract": "Privacy preservation in Ride-Hailing Services (RHS) is intended to protect\nprivacy of drivers and riders. pRide, published in IEEE Trans. Vehicular\nTechnology 2021, is a prediction based privacy-preserving RHS protocol to match\nriders with an optimum driver. In the protocol, the Service Provider (SP)\nhomomorphically computes Euclidean distances between encrypted locations of\ndrivers and rider. Rider selects an optimum driver using decrypted distances\naugmented by a new-ride-emergence prediction. To improve the effectiveness of\ndriver selection, the paper proposes an enhanced version where each driver\ngives encrypted distances to each corner of her grid. To thwart a rider from\nusing these distances to launch an inference attack, the SP blinds these\ndistances before sharing them with the rider. In this work, we propose a\npassive attack where an honest-but-curious adversary rider who makes a single\nride request and receives the blinded distances from SP can recover the\nconstants used to blind the distances. Using the unblinded distances, rider to\ndriver distance and Google Nearest Road API, the adversary can obtain the\nprecise locations of responding drivers. We conduct experiments with random\non-road driver locations for four different cities. Our experiments show that\nwe can determine the precise locations of at least 80% of the drivers\nparticipating in the enhanced pRide protocol.",
    "descriptor": "",
    "authors": [
      "Shyam Murthy",
      "Srinivas Vivek"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13263"
  },
  {
    "id": "arXiv:2210.13269",
    "title": "IQUAFLOW: A new framework to measure image quality",
    "abstract": "IQUAFLOW is a new image quality framework that provides a set of tools to\nassess image quality. The user can add custom metrics that can be easily\nintegrated. Furthermore, iquaflow allows to measure quality by using the\nperformance of AI models trained on the images as a proxy. This also helps to\neasily make studies of performance degradation of several modifications of the\noriginal dataset, for instance, with images reconstructed after different\nlevels of lossy compression; satellite images would be a use case example,\nsince they are commonly compressed before downloading to the ground. In this\nsituation, the optimization problem consists in finding the smallest images\nthat provide yet sufficient quality to meet the required performance of the\ndeep learning algorithms. Thus, a study with iquaflow is suitable for such\ncase. All this development is wrapped in Mlflow: an interactive tool used to\nvisualize and summarize the results. This document describes different use\ncases and provides links to their respective repositories. To ease the creation\nof new studies, we include a cookie-cutter repository. The source code, issue\ntracker and aforementioned repositories are all hosted on GitHub\nhttps://github.com/satellogic/iquaflow.",
    "descriptor": "",
    "authors": [
      "P. Gall\u00e9s",
      "K. Takats",
      "M. Hern\u00e1ndez-Cabronero",
      "D. Berga",
      "L. Pega",
      "L. Riordan-Chen",
      "C. Garcia",
      "G. Becker",
      "A. Garriga",
      "A. Bukva",
      "J. Serra-Sagrist\u00e0",
      "D. Vilaseca",
      "J. Mar\u00edn"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13269"
  },
  {
    "id": "arXiv:2210.13270",
    "title": "Generating Hierarchical Explanations on Text Classification Without  Connecting Rules",
    "abstract": "The opaqueness of deep NLP models has motivated the development of methods\nfor interpreting how deep models predict. Recently, work has introduced\nhierarchical attribution, which produces a hierarchical clustering of words,\nalong with an attribution score for each cluster. However, existing work on\nhierarchical attribution all follows the connecting rule, limiting the cluster\nto a continuous span in the input text. We argue that the connecting rule as an\nadditional prior may undermine the ability to reflect the model decision\nprocess faithfully. To this end, we propose to generate hierarchical\nexplanations without the connecting rule and introduce a framework for\ngenerating hierarchical clusters. Experimental results and further analysis\nshow the effectiveness of the proposed method in providing high-quality\nexplanations for reflecting model predicting process.",
    "descriptor": "",
    "authors": [
      "Yiming Ju",
      "Yuanzhe Zhang",
      "Kang Liu",
      "Jun Zhao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13270"
  },
  {
    "id": "arXiv:2210.13274",
    "title": "HAZniCS -- Software Components for Multiphysics Problems",
    "abstract": "We introduce the software toolbox HAZniCS for solving interface-coupled\nmultiphysics problems. HAZniCS is a suite of modules that combines the\nwell-known FEniCS framework for finite element discretization with solver and\ngraph library HAZmath. The focus of the paper is on the design and\nimplementation of a pool of robust and efficient solver algorithms which tackle\nissues related to the complex interfacial coupling of the physical problems\noften encountered in applications in brain biomechanics. The robustness and\nefficiency of the numerical algorithms and methods is shown in several\nnumerical examples, namely the Darcy-Stokes equations that model flow of\ncerebrospinal fluid in the human brain and the mixed-dimensional model of\nelectrodiffusion in the brain tissue.",
    "descriptor": "",
    "authors": [
      "Ana Budisa",
      "Xiaozhe Hu",
      "Miroslav Kuchta",
      "Kent-Andre Mardal",
      "Ludmil Zikatanov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13274"
  },
  {
    "id": "arXiv:2210.13277",
    "title": "Provably Doubly Accelerated Federated Learning: The First Theoretically  Successful Combination of Local Training and Compressed Communication",
    "abstract": "In the modern paradigm of federated learning, a large number of users are\ninvolved in a global learning task, in a collaborative way. They alternate\nlocal computations and two-way communication with a distant orchestrating\nserver. Communication, which can be slow and costly, is the main bottleneck in\nthis setting. To reduce the communication load and therefore accelerate\ndistributed gradient descent, two strategies are popular: 1) communicate less\nfrequently; that is, perform several iterations of local computations between\nthe communication rounds; and 2) communicate compressed information instead of\nfull-dimensional vectors. In this paper, we propose the first algorithm for\ndistributed optimization and federated learning, which harnesses these two\nstrategies jointly and converges linearly to an exact solution, with a doubly\naccelerated rate: our algorithm benefits from the two acceleration mechanisms\nprovided by local training and compression, namely a better dependency on the\ncondition number of the functions and on the dimension of the model,\nrespectively.",
    "descriptor": "",
    "authors": [
      "Laurent Condat",
      "Ivan Agarsky",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.13277"
  },
  {
    "id": "arXiv:2210.13281",
    "title": "Analyzing the Use of Influence Functions for Instance-Specific Data  Filtering in Neural Machine Translation",
    "abstract": "Customer feedback can be an important signal for improving commercial machine\ntranslation systems. One solution for fixing specific translation errors is to\nremove the related erroneous training instances followed by re-training of the\nmachine translation system, which we refer to as instance-specific data\nfiltering. Influence functions (IF) have been shown to be effective in finding\nsuch relevant training examples for classification tasks such as image\nclassification, toxic speech detection and entailment task. Given a probing\ninstance, IF find influential training examples by measuring the similarity of\nthe probing instance with a set of training examples in gradient space. In this\nwork, we examine the use of influence functions for Neural Machine Translation\n(NMT). We propose two effective extensions to a state of the art influence\nfunction and demonstrate on the sub-problem of copied training examples that IF\ncan be applied more generally than handcrafted regular expressions.",
    "descriptor": "\nComments: Accepted at WMT 2022\n",
    "authors": [
      "Tsz Kin Lam",
      "Eva Hasler",
      "Felix Hieber"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13281"
  },
  {
    "id": "arXiv:2210.13283",
    "title": "Content Transfer Across Multiple Screens with Combined Eye-Gaze and  Touch Interaction -- A Replication Study",
    "abstract": "In this paper, we describe the results of replicating one of our studies from\ntwo years ago which compares two techniques for transferring content across\nmultiple screens in VR. Results from the previous study have shown that a\ncombined gaze and touch input can outperform a bimanual touch-only input in\nterms of task completion time, simulator sickness, task load and usability.\nExcept for the simulator sickness, these findings could be validated by the\nreplication. The difference with regards to simulator sickness and variations\nin absolute scores of the other measures could be explained by a different set\nof user with less VR experience.",
    "descriptor": "",
    "authors": [
      "Verena Biener",
      "Jens Grubert"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13283"
  },
  {
    "id": "arXiv:2210.13287",
    "title": "Control and Design Optimization of an Electric Vehicle Transmission  Using Analytical Modeling Methods",
    "abstract": "This paper introduces a framework to systematically optimize the control and\ndesign of an electric vehicle transmission, connecting powertrain sizing\nstudies to detailed gearbox design methods. To this end, we first create\nanalytical models of individual components: gears, shafts, bearings, clutches,\nand synchronizers. Second, we construct a transmission by systematically\nconfiguring a topology with these components. Third, we place the composed\ntransmission within a powertrain and vehicle model, and compute the\nminimum-energy control and design, employing solving algorithms that provide\nglobal optimality guarantees. Finally, we carry out the control and design\noptimization of a fixed- and two-gear transmission for a compact family\nelectric vehicle, whereby we observe that a two-gear transmission can improve\nthe energy consumption by 0.8 %, while also achieving requirements on\ngradeability and performance. We validate our framework by recreating the\ntransmission that is mounted in the benchmark test case vehicle and recomputing\nthe energy consumption over the New European Driving Cycle, where we notice an\nerror in energy consumption of 0.2 %, affirming that our methods are suitable\nfor gear ratio selection in powertrain design optimization.",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Olaf Borsboom",
      "Thijs de Mooy",
      "Mauro Salazar",
      "Theo Hofman"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13287"
  },
  {
    "id": "arXiv:2210.13289",
    "title": "Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR)  for Metaverses",
    "abstract": "Metaverse is expected to emerge as a new paradigm for the next-generation\nInternet, providing fully immersive and personalised experiences to socialize,\nwork, and play in self-sustaining and hyper-spatio-temporal virtual world(s).\nThe advancements in different technologies like augmented reality, virtual\nreality, extended reality (XR), artificial intelligence (AI), and 5G/6G\ncommunication will be the key enablers behind the realization of AI-XR\nmetaverse applications. While AI itself has many potential applications in the\naforementioned technologies (e.g., avatar generation, network optimization,\netc.), ensuring the security of AI in critical applications like AI-XR\nmetaverse applications is profoundly crucial to avoid undesirable actions that\ncould undermine users' privacy and safety, consequently putting their lives in\ndanger. To this end, we attempt to analyze the security, privacy, and\ntrustworthiness aspects associated with the use of various AI techniques in\nAI-XR metaverse applications. Specifically, we discuss numerous such challenges\nand present a taxonomy of potential solutions that could be leveraged to\ndevelop secure, private, robust, and trustworthy AI-XR applications. To\nhighlight the real implications of AI-associated adversarial threats, we\ndesigned a metaverse-specific case study and analyzed it through the\nadversarial lens. Finally, we elaborate upon various open issues that require\nfurther research interest from the community.",
    "descriptor": "\nComments: 24 pages, 11 figures\n",
    "authors": [
      "Adnan Qayyum",
      "Muhammad Atif Butt",
      "Hassan Ali",
      "Muhammad Usman",
      "Osama Halabi",
      "Ala Al-Fuqaha",
      "Qammer H. Abbasi",
      "Muhammad Ali Imran",
      "Junaid Qadir"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13289"
  },
  {
    "id": "arXiv:2210.13290",
    "title": "If You Are Careful, So Am I! How Robot Communicative Motions Can  Influence Human Approach in a Joint Task",
    "abstract": "As humans, we have a remarkable capacity for reading the characteristics of\nobjects only by observing how another person carries them. Indeed, how we\nperform our actions naturally embeds information on the item features.\nCollaborative robots can achieve the same ability by modulating the strategy\nused to transport objects with their end-effector. A contribution in this sense\nwould promote spontaneous interactions by making an implicit yet effective\ncommunication channel available. This work investigates if humans correctly\nperceive the implicit information shared by a robotic manipulator through its\nmovements during a dyadic collaboration task. Exploiting a generative approach,\nwe designed robot actions to convey virtual properties of the transported\nobjects, particularly to inform the partner if any caution is required to\nhandle the carried item. We found that carefulness is correctly interpreted\nwhen observed through the robot movements. In the experiment, we used identical\nempty plastic cups; nevertheless, participants approached them differently\ndepending on the attitude shown by the robot: humans change how they reach for\nthe object, being more careful whenever the robot does the same. This emerging\nform of motor contagion is entirely spontaneous and happens even if the task\ndoes not require it.",
    "descriptor": "\nComments: Accepted to ICSR 2022, 14th International Conference on Social Robotics, preprint version\n",
    "authors": [
      "Linda Lastrico",
      "Nuno Ferreira Duarte",
      "Alessandro Carf\u00ec",
      "Francesco Rea",
      "Fulvio Mastrogiovanni",
      "Alessandra Sciutti",
      "Jos\u00e9 Santos-Victor"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13290"
  },
  {
    "id": "arXiv:2210.13291",
    "title": "NVIDIA FLARE: Federated Learning from Simulation to Real-World",
    "abstract": "Federated learning (FL) enables building robust and generalizable AI models\nby leveraging diverse datasets from multiple collaborators without centralizing\nthe data. We created NVIDIA FLARE as an open-source software development kit\n(SDK) to make it easier for data scientists to use FL in their research and\nreal-world applications. The SDK includes solutions for state-of-the-art FL\nalgorithms and federated machine learning approaches, which facilitate building\nworkflows for distributed learning across enterprises and enable platform\ndevelopers to create a secure, privacy-preserving offering for multiparty\ncollaboration utilizing homomorphic encryption or differential privacy. The SDK\nis a lightweight, flexible, and scalable Python package, and allows researchers\nto bring their data science workflows implemented in any training libraries\n(PyTorch, TensorFlow, XGBoost, or even NumPy) and apply them in real-world FL\nsettings. This paper introduces the key design principles of FLARE and\nillustrates some use cases (e.g., COVID analysis) with customizable FL\nworkflows that implement different privacy-preserving algorithms.\nCode is available at https://github.com/NVIDIA/NVFlare.",
    "descriptor": "\nComments: Accepted at the International Workshop on Federated Learning, NeurIPS 2022, New Orleans, USA (this https URL)\n",
    "authors": [
      "Holger R. Roth",
      "Yan Cheng",
      "Yuhong Wen",
      "Isaac Yang",
      "Ziyue Xu",
      "Yuan-Ting Hsieh",
      "Kristopher Kersten",
      "Ahmed Harouni",
      "Can Zhao",
      "Kevin Lu",
      "Zhihong Zhang",
      "Wenqi Li",
      "Andriy Myronenko",
      "Dong Yang",
      "Sean Yang",
      "Nicola Rieke",
      "Abood Quraini",
      "Chester Chen",
      "Daguang Xu",
      "Nic Ma",
      "Prerna Dogra",
      "Mona Flores",
      "Andrew Feng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Networking and Internet Architecture (cs.NI)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.13291"
  },
  {
    "id": "arXiv:2210.13296",
    "title": "Semantic Image Segmentation with Deep Learning for Vine Leaf Phenotyping",
    "abstract": "Plant phenotyping refers to a quantitative description of the plants\nproperties, however in image-based phenotyping analysis, our focus is primarily\non the plants anatomical, ontogenetical and physiological properties.This\ntechnique reinforced by the success of Deep Learning in the field of image\nbased analysis is applicable to a wide range of research areas making\nhigh-throughput screens of plants possible, reducing the time and effort needed\nfor phenotypic characterization.In this study, we use Deep Learning methods\n(supervised and unsupervised learning based approaches) to semantically segment\ngrapevine leaves images in order to develop an automated object detection\n(through segmentation) system for leaf phenotyping which will yield information\nregarding their structure and function.In these directions we studied several\ndeep learning approaches with promising results as well as we reported some\nfuture challenging tasks in the area of precision agriculture.Our work\ncontributes to plant lifecycle monitoring through which dynamic traits such as\ngrowth and development can be captured and quantified, targeted intervention\nand selective application of agrochemicals and grapevine variety identification\nwhich are key prerequisites in sustainable agriculture.",
    "descriptor": "\nComments: 7th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture (AGRICONTROL 2022)\n",
    "authors": [
      "Petros N. Tamvakis",
      "Chairi Kiourt",
      "Alexandra D. Solomou",
      "George Ioannakis",
      "Nestoras C. Tsirliganis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13296"
  },
  {
    "id": "arXiv:2210.13304",
    "title": "ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and  Effective Text Generation",
    "abstract": "We study the text generation task under the approach of pre-trained language\nmodels (PLMs). Typically, an auto-regressive (AR) method is adopted for\ngenerating texts in a token-by-token manner. Despite many advantages of AR\ngeneration, it usually suffers from inefficient inference. Therefore,\nnon-autoregressive (NAR) models are proposed to generate all target tokens\nsimultaneously. However, NAR models usually generate texts of lower quality due\nto the absence of token dependency in the output text. In this paper, we\npropose ELMER: an efficient and effective PLM for NAR text generation to\nexplicitly model the token dependency during NAR generation. By leveraging the\nearly exit technique, ELMER enables the token generations at different layers,\naccording to their prediction confidence (a more confident token will exit at a\nlower layer). Besides, we propose a novel pre-training objective, Layer\nPermutation Language Modeling, to pre-train ELMER by permuting the exit layer\nfor each token in sequences. Experiments on three text generation tasks show\nthat ELMER significantly outperforms NAR models and further narrows the\nperformance gap with AR PLMs (\\eg ELMER (29.92) vs BART (30.61) ROUGE-L in\nXSUM) while achieving over 10 times inference speedup.",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference (long paper)\n",
    "authors": [
      "Junyi Li",
      "Tianyi Tang",
      "Wayne Xin Zhao",
      "Jian-Yun Nie",
      "Ji-Rong Wen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13304"
  },
  {
    "id": "arXiv:2210.13305",
    "title": "BoundED: Neural Boundary and Edge Detection in 3D Point Clouds via Local  Neighborhood Statistics",
    "abstract": "Extracting high-level structural information from 3D point clouds is\nchallenging but essential for tasks like urban planning or autonomous driving\nrequiring an advanced understanding of the scene at hand. Existing approaches\nare still not able to produce high-quality results consistently while being\nfast enough to be deployed in scenarios requiring interactivity. We propose to\nutilize a novel set of features describing the local neighborhood on a\nper-point basis via first and second order statistics as input for a simple and\ncompact classification network to distinguish between non-edge, sharp-edge, and\nboundary points in the given data. Leveraging this feature embedding enables\nour algorithm to outperform the state-of-the-art techniques in terms of quality\nand processing time.",
    "descriptor": "",
    "authors": [
      "Lukas Bode",
      "Michael Weinmann",
      "Reinhard Klein"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.13305"
  },
  {
    "id": "arXiv:2210.13311",
    "title": "Different Tunes Played with Equal Skill: Exploring a Unified  Optimization Subspace for Delta Tuning",
    "abstract": "Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the\nnew paradigm for using pre-trained language models (PLMs). Up to now, various\nDETs with distinct design elements have been proposed, achieving performance on\npar with fine-tuning. However, the mechanisms behind the above success are\nstill under-explored, especially the connections among various DETs. To fathom\nthe mystery, we hypothesize that the adaptations of different DETs could all be\nreparameterized as low-dimensional optimizations in a unified optimization\nsubspace, which could be found by jointly decomposing independent solutions of\ndifferent DETs. Then we explore the connections among different DETs by\nconducting optimization within the subspace. In experiments, we find that, for\na certain DET, conducting optimization simply in the subspace could achieve\ncomparable performance to its original space, and the found solution in the\nsubspace could be transferred to another DET and achieve non-trivial\nperformance. We also visualize the performance landscape of the subspace and\nfind that there exists a substantial region where different DETs all perform\nwell. Finally, we extend our analysis and show the strong connections between\nfine-tuning and DETs.",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Jing Yi",
      "Weize Chen",
      "Yujia Qin",
      "Yankai Lin",
      "Ning Ding",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13311"
  },
  {
    "id": "arXiv:2210.13312",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "abstract": "Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial.\nIn this work, we examine the open question of social intelligence and Theory\nof Mind in modern NLP systems from an empirical and theory-based perspective.\nWe show that one of today's largest language models (GPT-3; Brown et al., 2020)\nlacks this kind of social intelligence out-of-the box, using two tasks:\nSocialIQa (Sap et al., 2019), which measures models' ability to understand\nintents and reactions of participants of social interactions, and ToMi (Le et\nal., 2019), which measures whether models can infer mental states and realities\nof participants of situations.\nOur results show that models struggle substantially at these Theory of Mind\ntasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi,\nrespectively. To conclude, we draw on theories from pragmatics to contextualize\nthis shortcoming of large language models, by examining the limitations\nstemming from their data, neural architecture, and training paradigms.\nChallenging the prevalent narrative that only scale is needed, we posit that\nperson-centric NLP approaches might be more effective towards neural Theory of\nMind.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Maarten Sap",
      "Ronan LeBras",
      "Daniel Fried",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13312"
  },
  {
    "id": "arXiv:2210.13313",
    "title": "Learning and Covering Sums of Independent Random Variables with  Unbounded Support",
    "abstract": "We study the problem of covering and learning sums $X = X_1 + \\cdots + X_n$\nof independent integer-valued random variables $X_i$ (SIIRVs) with unbounded,\nor even infinite, support. De et al. at FOCS 2018, showed that the maximum\nvalue of the collective support of $X_i$'s necessarily appears in the sample\ncomplexity of learning $X$. In this work, we address two questions: (i) Are\nthere general families of SIIRVs with unbounded support that can be learned\nwith sample complexity independent of both $n$ and the maximal element of the\nsupport? (ii) Are there general families of SIIRVs with unbounded support that\nadmit proper sparse covers in total variation distance? As for question (i), we\nprovide a set of simple conditions that allow the unbounded SIIRV to be learned\nwith complexity $\\text{poly}(1/\\epsilon)$ bypassing the aforementioned lower\nbound. We further address question (ii) in the general setting where each\nvariable $X_i$ has unimodal probability mass function and is a different member\nof some, possibly multi-parameter, exponential family $\\mathcal{E}$ that\nsatisfies some structural properties. These properties allow $\\mathcal{E}$ to\ncontain heavy tailed and non log-concave distributions. Moreover, we show that\nfor every $\\epsilon > 0$, and every $k$-parameter family $\\mathcal{E}$ that\nsatisfies some structural assumptions, there exists an algorithm with\n$\\tilde{O}(k) \\cdot \\text{poly}(1/\\epsilon)$ samples that learns a sum of $n$\narbitrary members of $\\mathcal{E}$ within $\\epsilon$ in TV distance. The output\nof the learning algorithm is also a sum of random variables whose distribution\nlies in the family $\\mathcal{E}$. En route, we prove that any discrete unimodal\nexponential family with bounded constant-degree central moments can be\napproximated by the family corresponding to a bounded subset of the initial\n(unbounded) parameter space.",
    "descriptor": "\nComments: 60 pages, 0 figures. Accepted to the Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Alkis Kalavasis",
      "Konstantinos Stavropoulos",
      "Manolis Zampetakis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.13313"
  },
  {
    "id": "arXiv:2210.13315",
    "title": "Local discontinuous Galerkin method for a third order singularly  perturbed problem of convection-diffusion type",
    "abstract": "The local discontinuous Galerkin (LDG) method is studied for a third-order\nsingularly perturbed problem of the convection-diffusion type. Based on a\nregularity assumption for the exact solution, we prove almost $O(N^{-(k+1/2)})$\n(up to a logarithmic factor) energy-norm convergence uniformly in the\nperturbation parameter. Here, $k\\geq 0$ is the maximum degree of piecewise\npolynomials used in discrete space, and $N$ is the number of mesh elements. The\nresults are valid for the three types of layer-adapted meshes: Shishkin-type,\nBakhvalov-Shishkin type, and Bakhvalov-type. Numerical experiments are\nconducted to test the theoretical results.",
    "descriptor": "\nComments: 21 pages, 22 figures\n",
    "authors": [
      "Li Yan",
      "Zhoufeng Wang",
      "Yao Cheng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13315"
  },
  {
    "id": "arXiv:2210.13317",
    "title": "Planning Coordinated Human-Robot Motions with Neural Network Full-Body  Prediction Models",
    "abstract": "Numerical optimization has become a popular approach to plan smooth motion\ntrajectories for robots. However, when sharing space with humans, balancing\nproperly safety, comfort and efficiency still remains challenging. This is\nnotably the case because humans adapt their behavior to that of the robot,\nraising the need for intricate planning and prediction. In this paper, we\npropose a novel optimization-based motion planning algorithm, which generates\nrobot motions, while simultaneously maximizing the human trajectory likelihood\nunder a data-driven predictive model. Considering planning and prediction\ntogether allows us to formulate objective and constraint functions in the joint\nhuman-robot state space. Key to the approach are added latent space modifiers\nto a differentiable human predictive model based on a dedicated recurrent\nneural network. These modifiers allow to change the human prediction within\nmotion optimization. We empirically evaluate our method using the publicly\navailable MoGaze dataset. Our results indicate that the proposed framework\noutperforms current baselines for planning handover trajectories and avoiding\ncollisions between a robot and a human. Our experiments demonstrate\ncollaborative motion trajectories, where both, the human prediction and the\nrobot plan, adapt to each other.",
    "descriptor": "",
    "authors": [
      "Philipp Kratzer",
      "Marc Toussaint",
      "Jim Mainprice"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13317"
  },
  {
    "id": "arXiv:2210.13319",
    "title": "MARS: Meta-Learning as Score Matching in the Function Space",
    "abstract": "Meta-learning aims to extract useful inductive biases from a set of related\ndatasets. In Bayesian meta-learning, this is typically achieved by constructing\na prior distribution over neural network parameters. However, specifying\nfamilies of computationally viable prior distributions over the\nhigh-dimensional neural network parameters is difficult. As a result, existing\napproaches resort to meta-learning restrictive diagonal Gaussian priors,\nseverely limiting their expressiveness and performance. To circumvent these\nissues, we approach meta-learning through the lens of functional Bayesian\nneural network inference, which views the prior as a stochastic process and\nperforms inference in the function space. Specifically, we view the\nmeta-training tasks as samples from the data-generating process and formalize\nmeta-learning as empirically estimating the law of this stochastic process. Our\napproach can seamlessly acquire and represent complex prior knowledge by\nmeta-learning the score function of the data-generating process marginals\ninstead of parameter space priors. In a comprehensive benchmark, we demonstrate\nthat our method achieves state-of-the-art performance in terms of predictive\naccuracy and substantial improvements in the quality of uncertainty estimates.",
    "descriptor": "",
    "authors": [
      "Krunoslav Lehman Pavasovic",
      "Jonas Rothfuss",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13319"
  },
  {
    "id": "arXiv:2210.13321",
    "title": "Dual-Pixel Raindrop Removal",
    "abstract": "Removing raindrops in images has been addressed as a significant task for\nvarious computer vision applications. In this paper, we propose the first\nmethod using a Dual-Pixel (DP) sensor to better address the raindrop removal.\nOur key observation is that raindrops attached to a glass window yield\nnoticeable disparities in DP's left-half and right-half images, while almost no\ndisparity exists for in-focus backgrounds. Therefore, DP disparities can be\nutilized for robust raindrop detection. The DP disparities also brings the\nadvantage that the occluded background regions by raindrops are shifted between\nthe left-half and the right-half images. Therefore, fusing the information from\nthe left-half and the right-half images can lead to more accurate background\ntexture recovery. Based on the above motivation, we propose a DP Raindrop\nRemoval Network (DPRRN) consisting of DP raindrop detection and DP fused\nraindrop removal. To efficiently generate a large amount of training data, we\nalso propose a novel pipeline to add synthetic raindrops to real-world\nbackground DP images. Experimental results on synthetic and real-world datasets\ndemonstrate that our DPRRN outperforms existing state-of-the-art methods,\nespecially showing better robustness to real-world situations. Our source code\nand datasets are available at this http URL",
    "descriptor": "\nComments: Accepted by BMVC2022 (Oral)\n",
    "authors": [
      "Yizhou Li",
      "Yusuke Monno",
      "Masatoshi Okutomi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13321"
  },
  {
    "id": "arXiv:2210.13325",
    "title": "ICSSIM-A Framework for Building Industrial Control Systems Security  Simulation Testbeds",
    "abstract": "With the advent of smart industry, Industrial Control Systems (ICS) are\nincreasingly using Cloud, IoT, and other services to meet Industry 4.0 targets.\nThe connectivity inherent in these services exposes such systems to increased\ncybersecurity risks. To protect ICSs against cyberattacks, intrusion detection\nsystems and intrusion prevention systems empowered by machine learning are used\nto detect abnormal behavior of the systems. Operational ICSs are not safe\nenvironments to research intrusion detection systems due to the possibility of\ncatastrophic risks. Therefore, realistic ICS testbeds enable researchers to\nanalyze and validate their intrusion detection algorithms in a controlled\nenvironment. Although various ICS testbeds have been developed, researchers'\naccess to a low-cost, adaptable, and customizable testbed that can accurately\nsimulate industrial control systems and suits security research is still an\nimportant issue.\nIn this paper, we present ICSSIM, a framework for simulating customized\nvirtual ICS security testbeds cyber threats and attacks can be investigated,\nand mitigations evaluated. ICSSIM aims to produce extendable, versatile,\nreproducible, low-cost, and comprehensive ICS testbeds with realistic details\nand high fidelity. ICSSIM is built on top of the Docker container technology,\nwhich provides realistic network emulation and runs ICS components on isolated\nprivate operating system kernels. ICSSIM reduces the time for developing ICS\ncomponents and offers physical process modelling using software and hardware in\nthe loop simulation. We demonstrated ICSSIM by creating a testbed and\nvalidating its functionality by showing how different cyberattacks can be\napplied.",
    "descriptor": "\nComments: 36 pages, 10 figures\n",
    "authors": [
      "Alireza Dehlaghi Ghadima",
      "Ali Balador",
      "Hans Hanssona",
      "Mauro Contic"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13325"
  },
  {
    "id": "arXiv:2210.13326",
    "title": "Clean Text and Full-Body Transformer: Microsoft's Submission to the  WMT22 Shared Task on Sign Language Translation",
    "abstract": "This paper describes Microsoft's submission to the first shared task on sign\nlanguage translation at WMT 2022, a public competition tackling sign language\nto spoken language translation for Swiss German sign language. The task is very\nchallenging due to data scarcity and an unprecedented vocabulary size of more\nthan 20k words on the target side. Moreover, the data is taken from real\nbroadcast news, includes native signing and covers scenarios of long videos.\nMotivated by recent advances in action recognition, we incorporate full body\ninformation by extracting features from a pre-trained I3D model and applying a\nstandard transformer network. The accuracy of the system is further improved by\napplying careful data cleaning on the target text. We obtain BLEU scores of 0.6\nand 0.78 on the test and dev set respectively, which is the best score among\nthe participants of the shared task. Also in the human evaluation the\nsubmission reaches the first place. The BLEU score is further improved to 1.08\non the dev set by applying features extracted from a lip reading model.",
    "descriptor": "\nComments: accepted for publication at WMT2022\n",
    "authors": [
      "Subhadeep Dey",
      "Abhilash Pal",
      "Cyrine Chaabani",
      "Oscar Koller"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13326"
  },
  {
    "id": "arXiv:2210.13329",
    "title": "Decimated Prony's Method for Stable Super-resolution",
    "abstract": "We study recovery of amplitudes and nodes of a finite impulse train from a\nlimited number of equispaced noisy frequency samples. This problem is known as\nsuper-resolution (SR) under sparsity constraints and has numerous applications,\nincluding direction of arrival and finite rate of innovation sampling. Prony's\nmethod is an algebraic technique which fully recovers the signal parameters in\nthe absence of measurement noise. In the presence of noise, Prony's method may\nexperience significant loss of accuracy, especially when the separation between\nDirac pulses is smaller than the Nyquist-Shannon-Rayleigh (NSR) limit. In this\nwork we combine Prony's method with a recently established decimation technique\nfor analyzing the SR problem in the regime where the distance between two or\nmore pulses is much smaller than the NSR limit. We show that our approach\nattains optimal asymptotic stability in the presence of noise. Our result\nchallenges the conventional belief that Prony-type methods tend to be highly\nnumerically unstable.",
    "descriptor": "\nComments: 5 pages, 10 figures\n",
    "authors": [
      "Rami Katz",
      "Nuha Diab",
      "Dmitry Batenkov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13329"
  },
  {
    "id": "arXiv:2210.13334",
    "title": "Real-time Speech Interruption Analysis: From Cloud to Client Deployment",
    "abstract": "Meetings are an essential form of communication for all types of\norganizations, and remote collaboration systems have been much more widely used\nsince the COVID-19 pandemic. One major issue with remote meetings is that it is\nchallenging for remote participants to interrupt and speak. We have recently\ndeveloped the first speech interruption analysis model, which detects failed\nspeech interruptions, shows very promising performance, and is being deployed\nin the cloud. To deliver this feature in a more cost-efficient and\nenvironment-friendly way, we reduced the model complexity and size to ship the\nWavLM_SI model in client devices. In this paper, we first describe how we\nsuccessfully improved the True Positive Rate (TPR) at a 1% False Positive Rate\n(FPR) from 50.9% to 68.3% for the failed speech interruption detection model by\ntraining on a larger dataset and fine-tuning. We then shrank the model size\nfrom 222.7 MB to 9.3 MB with an acceptable loss in accuracy and reduced the\ncomplexity from 31.2 GMACS (Giga Multiply-Accumulate Operations per Second) to\n4.3 GMACS. We also estimated the environmental impact of the complexity\nreduction, which can be used as a general guideline for large Transformer-based\nmodels, and thus make those models more accessible with less computation\noverhead.",
    "descriptor": "",
    "authors": [
      "Quchen Fu",
      "Szu-Wei Fu",
      "Yaran Fan",
      "Yu Wu",
      "Zhuo Chen",
      "Jayant Gupchup",
      "Ross Cutler"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13334"
  },
  {
    "id": "arXiv:2210.13339",
    "title": "(LA)yer-neigh(BOR) Sampling: Defusing Neighborhood Explosion in GNNs",
    "abstract": "Graph Neural Networks have recently received a significant attention,\nhowever, training them at a large scale still remains a challenge. Minibatch\ntraining coupled with sampling is used to alleviate this challenge. Even so\nexisting approaches either suffer from the neighborhood explosion phenomenon or\ndo not have good performance. To deal with these issues, we propose a new\nsampling algorithm called LAyer-neighBOR sampling (LABOR). It is designed to be\na direct replacement for Neighborhood Sampling with the same fanout\nhyperparameter while sampling much fewer vertices, without sacrificing quality.\nBy design, the variance of the estimator of each vertex matches Neighbor\nSampling from the point of view of a single vertex. In our experiments, we\ndemonstrate the superiority of our approach when it comes to model convergence\nbehaviour against Neighbor Sampling and also the other Layer Sampling\napproaches under the same limited vertex sampling budget constraints.",
    "descriptor": "",
    "authors": [
      "Muhammed Fatih Bal\u0131n",
      "\u00dcmit V. \u00c7ataly\u00fcrek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13339"
  },
  {
    "id": "arXiv:2210.13344",
    "title": "Augmenting Task-Oriented Dialogue Systems with Relation Extraction",
    "abstract": "The standard task-oriented dialogue pipeline uses intent classification and\nslot-filling to interpret user utterances. While this approach can handle a\nwide range of queries, it does not extract the information needed to handle\nmore complex queries that contain relationships between slots. We propose\nintegration of relation extraction into this pipeline as an effective way to\nexpand the capabilities of dialogue systems. We evaluate our approach by using\nan internal dataset with slot and relation annotations spanning three domains.\nFinally, we show how slot-filling annotation schemes can be simplified once the\nexpressive power of relation annotations is available, reducing the number of\nslots while still capturing the user's intended meaning.",
    "descriptor": "\nComments: DSTC 10 AAAI22 Workshop Paper\n",
    "authors": [
      "Andrew Lee",
      "Zhenguo Chen",
      "Kevin Leach",
      "Jonathan K. Kummerfeld"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13344"
  },
  {
    "id": "arXiv:2210.13345",
    "title": "A Novel Antenna Placement Algorithm for Compressive Sensing MIMO Radar",
    "abstract": "In colocated compressive sensing MIMO radar, the measurement matrix is\nspecified by antenna placement. To guarantee an acceptable recovery\nperformance, this measurement matrix should satisfy certain properties, e.g., a\nsmall coherence. Prior work in the literature often employs randomized\nplacement algorithms which optimize the prior distribution of antenna\nlocations. The performance of these algorithms is suboptimal, as they can be\neasily enhanced via expurgation. In this paper, we suggest an iterative antenna\nplacement algorithm which determines the antenna locations deterministically.\nThe proposed algorithm locates jointly the antenna elements on the transmit and\nreceive arrays, such that the coherence of the resulting measurement matrix is\nminimized. Numerical simulations demonstrate that the proposed algorithm\noutperforms significantly the benchmark, even after expurgation.",
    "descriptor": "\nComments: 6 pages, 3 figures\n",
    "authors": [
      "Bastian Eisele",
      "Ali Bereyhi",
      "Ingrid Ullmann",
      "Ralf M\u00fcller"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13345"
  },
  {
    "id": "arXiv:2210.13352",
    "title": "ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition",
    "abstract": "Speech recognition applications cover a range of different audio and text\ndistributions, with different speaking styles, background noise, transcription\npunctuation and character casing. However, many speech recognition systems\nrequire dataset-specific tuning (audio filtering, punctuation removal and\nnormalisation of casing), therefore assuming a-priori knowledge of both the\naudio and text distributions. This tuning requirement can lead to systems\nfailing to generalise to other datasets and domains. To promote the development\nof multi-domain speech systems, we introduce the End-to-end Speech Benchmark\n(ESB) for evaluating the performance of a single automatic speech recognition\n(ASR) system across a broad set of speech datasets. Benchmarked systems must\nuse the same data pre- and post-processing algorithm across datasets - assuming\nthe audio and text data distributions are a-priori unknown. We compare a series\nof state-of-the-art (SoTA) end-to-end (E2E) systems on this benchmark,\ndemonstrating how a single speech system can be applied and evaluated on a wide\nrange of data distributions. We find E2E systems to be effective across\ndatasets: in a fair comparison, E2E systems achieve within 2.6% of SoTA systems\ntuned to a specific dataset. Our analysis reveals that transcription artefacts,\nsuch as punctuation and casing, pose difficulties for ASR systems and should be\nincluded in evaluation. We believe E2E benchmarking over a range of datasets\npromotes the research of multi-domain speech recognition systems. ESB is\navailable at https://huggingface.co/esb.",
    "descriptor": "\nComments: 25 pages, 1 figure, submitted to ICLR 2023\n",
    "authors": [
      "Sanchit Gandhi",
      "Patrick von Platen",
      "Alexander M. Rush"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13352"
  },
  {
    "id": "arXiv:2210.13356",
    "title": "Robust Self-Supervised Learning with Lie Groups",
    "abstract": "Deep learning has led to remarkable advances in computer vision. Even so,\ntoday's best models are brittle when presented with variations that differ even\nslightly from those seen during training. Minor shifts in the pose, color, or\nillumination of an object can lead to catastrophic misclassifications.\nState-of-the art models struggle to understand how a set of variations can\naffect different objects. We propose a framework for instilling a notion of how\nobjects vary in more realistic settings. Our approach applies the formalism of\nLie groups to capture continuous transformations to improve models' robustness\nto distributional shifts. We apply our framework on top of state-of-the-art\nself-supervised learning (SSL) models, finding that explicitly modeling\ntransformations with Lie groups leads to substantial performance gains of\ngreater than 10% for MAE on both known instances seen in typical poses now\npresented in new poses, and on unknown instances in any pose. We also apply our\napproach to ImageNet, finding that the Lie operator improves performance by\nalmost 4%. These results demonstrate the promise of learning transformations to\nimprove model robustness.",
    "descriptor": "",
    "authors": [
      "Mark Ibrahim",
      "Diane Bouchacourt",
      "Ari Morcos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13356"
  },
  {
    "id": "arXiv:2210.13358",
    "title": "Novelty Detection in Time Series via Weak Innovations Representation: A  Deep Learning Approach",
    "abstract": "We consider novelty detection in time series with unknown and nonparametric\nprobability structures. A deep learning approach is proposed to causally\nextract an innovations sequence consisting of novelty samples statistically\nindependent of all past samples of the time series. A novelty detection\nalgorithm is developed for the online detection of novel changes in the\nprobability structure in the innovations sequence. A minimax optimality under a\nBayes risk measure is established for the proposed novelty detection method,\nand its robustness and efficacy are demonstrated in experiments using real and\nsynthetic datasets.",
    "descriptor": "",
    "authors": [
      "Xinyi Wang",
      "Mei-jen Lee",
      "Qing Zhao",
      "Lang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13358"
  },
  {
    "id": "arXiv:2210.13361",
    "title": "NASA: Neural Architecture Search and Acceleration for Hardware Inspired  Hybrid Networks",
    "abstract": "Multiplication is arguably the most cost-dominant operation in modern deep\nneural networks (DNNs), limiting their achievable efficiency and thus more\nextensive deployment in resource-constrained applications. To tackle this\nlimitation, pioneering works have developed handcrafted multiplication-free\nDNNs, which require expert knowledge and time-consuming manual iteration,\ncalling for fast development tools. To this end, we propose a Neural\nArchitecture Search and Acceleration framework dubbed NASA, which enables\nautomated multiplication-reduced DNN development and integrates a dedicated\nmultiplication-reduced accelerator for boosting DNNs' achievable efficiency.\nSpecifically, NASA adopts neural architecture search (NAS) spaces that augment\nthe state-of-the-art one with hardware-inspired multiplication-free operators,\nsuch as shift and adder, armed with a novel progressive pretrain strategy (PGP)\ntogether with customized training recipes to automatically search for optimal\nmultiplication-reduced DNNs; On top of that, NASA further develops a dedicated\naccelerator, which advocates a chunk-based template and auto-mapper dedicated\nfor NASA-NAS resulting DNNs to better leverage their algorithmic properties for\nboosting hardware efficiency. Experimental results and ablation studies\nconsistently validate the advantages of NASA's algorithm-hardware co-design\nframework in terms of achievable accuracy and efficiency tradeoffs. Codes are\navailable at https://github.com/RICE-EIC/NASA.",
    "descriptor": "\nComments: Accepted to ICCAD2022\n",
    "authors": [
      "Huihong Shi",
      "Haoran You",
      "Yang Zhao",
      "Zhongfeng Wang",
      "Yingyan Lin"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13361"
  },
  {
    "id": "arXiv:2210.13363",
    "title": "Don't Discard Fixed-Window Audio Segmentation in Speech-to-Text  Translation",
    "abstract": "For real-life applications, it is crucial that end-to-end spoken language\ntranslation models perform well on continuous audio, without relying on\nhuman-supplied segmentation. For online spoken language translation, where\nmodels need to start translating before the full utterance is spoken, most\nprevious work has ignored the segmentation problem. In this paper, we compare\nvarious methods for improving models' robustness towards segmentation errors\nand different segmentation strategies in both offline and online settings and\nreport results on translation quality, flicker and delay. Our findings on five\ndifferent language pairs show that a simple fixed-window audio segmentation can\nperform surprisingly well given the right conditions.",
    "descriptor": "\nComments: accepted to WMT22\n",
    "authors": [
      "Chantal Amrhein",
      "Barry Haddow"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13363"
  },
  {
    "id": "arXiv:2210.13368",
    "title": "System Configuration and Navigation of a Guide Dog Robot: Toward Animal  Guide Dog-Level Guiding Work",
    "abstract": "A robot guide dog has compelling advantages over animal guide dogs for its\ncost-effectiveness, potential for mass production, and low maintenance burden.\nHowever, despite the long history of guide dog robot research, previous studies\nwere conducted with little or no consideration of how the guide dog handler and\nthe guide dog work as a team for navigation. To develop a robotic guiding\nsystem that is genuinely beneficial to blind or visually impaired individuals,\nwe performed qualitative research, including interviews with guide dog handlers\nand trainers and first-hand blindfold walking experiences with various guide\ndogs. Grounded on the facts learned from vivid experience and interviews, we\nbuild a collaborative indoor navigation scheme for a guide dog robot that\nincludes preferred features such as speed and directional control. For\ncollaborative navigation, we propose a semantic-aware local path planner that\nenables safe and efficient guiding work by utilizing semantic information about\nthe environment and considering the handler's position and directional cues to\ndetermine the collision-free path. We evaluate our integrated robotic system by\ntesting guide blindfold walking in indoor settings and demonstrate guide\ndog-like navigation behavior by avoiding obstacles at typical gait speed ($0.7\n\\mathrm{m/s}$).",
    "descriptor": "\nComments: First two authors contributed equally\n",
    "authors": [
      "Hochul Hwang",
      "Tim Xia",
      "Ibrahima Keita",
      "Ken Suzuki",
      "Joydeep Biswas",
      "Sunghoon I. Lee",
      "Donghyun Kim"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13368"
  },
  {
    "id": "arXiv:2210.13371",
    "title": "Time-Varying ALIP Model and Robust Foot-Placement Control for  Underactuated Bipedal Robot Walking on a Swaying Rigid Surface",
    "abstract": "Controller design for bipedal walking on dynamic rigid surfaces (DRSes),\nwhich are rigid surfaces moving in the inertial frame (e.g., ships and\nairplanes), remains largely uninvestigated. This paper introduces a\nhierarchical control approach that achieves stable underactuated bipedal robot\nwalking on a horizontally oscillating DRS. The highest layer of our approach is\na real-time motion planner that generates desired global behaviors (i.e., the\ncenter of mass trajectories and footstep locations) by stabilizing a\nreduced-order robot model. One key novelty of this layer is the derivation of\nthe reduced-order model by analytically extending the angular momentum based\nlinear inverted pendulum (ALIP) model from stationary to horizontally moving\nsurfaces. The other novelty is the development of a discrete-time\nfoot-placement controller that exponentially stabilizes the hybrid, linear,\ntime-varying ALIP model. The middle layer of the proposed approach is a walking\npattern generator that translates the desired global behaviors into the robot's\nfull-body reference trajectories for all directly actuated degrees of freedom.\nThe lowest layer is an input-output linearizing controller that exponentially\ntracks those full-body reference trajectories based on the full-order, hybrid,\nnonlinear robot dynamics. Simulations of planar underactuated bipedal walking\non a swaying DRS confirm that the proposed framework ensures the walking\nstability under different DRS motions and gait types.",
    "descriptor": "",
    "authors": [
      "Yuan Gao",
      "Yukai Gong",
      "Victor Paredes",
      "Ayonga Hereid",
      "Yan Gu"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13371"
  },
  {
    "id": "arXiv:2210.13373",
    "title": "Local Metric Learning for Off-Policy Evaluation in Contextual Bandits  with Continuous Actions",
    "abstract": "We consider local kernel metric learning for off-policy evaluation (OPE) of\ndeterministic policies in contextual bandits with continuous action spaces. Our\nwork is motivated by practical scenarios where the target policy needs to be\ndeterministic due to domain requirements, such as prescription of treatment\ndosage and duration in medicine. Although importance sampling (IS) provides a\nbasic principle for OPE, it is ill-posed for the deterministic target policy\nwith continuous actions. Our main idea is to relax the target policy and pose\nthe problem as kernel-based estimation, where we learn the kernel metric in\norder to minimize the overall mean squared error (MSE). We present an analytic\nsolution for the optimal metric, based on the analysis of bias and variance.\nWhereas prior work has been limited to scalar action spaces or kernel bandwidth\nselection, our work takes a step further being capable of vector action spaces\nand metric optimization. We show that our estimator is consistent, and\nsignificantly reduces the MSE compared to baseline OPE methods through\nexperiments on various domains.",
    "descriptor": "",
    "authors": [
      "Haanvid Lee",
      "Jongmin Lee",
      "Yunseon Choi",
      "Wonseok Jeon",
      "Byung-Jun Lee",
      "Yung-Kyun Noh",
      "Kee-Eung Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13373"
  },
  {
    "id": "arXiv:2210.13376",
    "title": "Comparison of Entropy Calculation Methods for Ransomware Encrypted File  Identification",
    "abstract": "Ransomware is a malicious class of software that utilises encryption to\nimplement an attack on system availability. The target's data remains encrypted\nand is held captive by the attacker until a ransom demand is met. A common\napproach used by many crypto-ransomware detection techniques is to monitor file\nsystem activity and attempt to identify encrypted files being written to disk,\noften using a file's entropy as an indicator of encryption. However, often in\nthe description of these techniques, little or no discussion is made as to why\na particular entropy calculation technique is selected or any justification\ngiven as to why one technique is selected over the alternatives. The Shannon\nmethod of entropy calculation is the most commonly-used technique when it comes\nto file encryption identification in crypto-ransomware detection techniques.\nOverall, correctly encrypted data should be indistinguishable from random data,\nso apart from the standard mathematical entropy calculations such as\nChi-Square, Shannon Entropy and Serial Correlation, the test suites used to\nvalidate the output from pseudo-random number generators would also be suited\nto perform this analysis. he hypothesis being that there is a fundamental\ndifference between different entropy methods and that the best methods may be\nused to better detect ransomware encrypted files. The paper compares the\naccuracy of 53 distinct tests in being able to differentiate between encrypted\ndata and other file types. The testing is broken down into two phases, the\nfirst phase is used to identify potential candidate tests, and a second phase\nwhere these candidates are thoroughly evaluated. To ensure that the tests were\nsufficiently robust, the NapierOne dataset is used. This dataset contains\nthousands of examples of the most commonly used file types, as well as examples\nof files that have been encrypted by crypto-ransomware.",
    "descriptor": "",
    "authors": [
      "Simon R Davies",
      "Richard Macfarlane",
      "William J. Buchanan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13376"
  },
  {
    "id": "arXiv:2210.13378",
    "title": "ADLight: A Universal Approach of Traffic Signal Control with Augmented  Data Using Reinforcement Learning",
    "abstract": "Traffic signal control has the potential to reduce congestion in dynamic\nnetworks. Recent studies show that traffic signal control with reinforcement\nlearning (RL) methods can significantly reduce the average waiting time.\nHowever, a shortcoming of existing methods is that they require model\nretraining for new intersections with different structures. In this paper, we\npropose a novel reinforcement learning approach with augmented data (ADLight)\nto train a universal model for intersections with different structures. We\npropose a new agent design incorporating features on movements and actions with\nset current phase duration to allow the generalized model to have the same\nstructure for different intersections. A new data augmentation method named\n\\textit{movement shuffle} is developed to improve the generalization\nperformance. We also test the universal model with new intersections in\nSimulation of Urban MObility (SUMO). The results show that the performance of\nour approach is close to the models trained in a single environment directly\n(only a 5% loss of average waiting time), and we can reduce more than 80% of\ntraining time, which saves a lot of computational resources in scalable\noperations of traffic lights.",
    "descriptor": "",
    "authors": [
      "Maonan Wang",
      "Yutong Xu",
      "Xi Xiong",
      "Yuheng Kan",
      "Chengcheng Xu",
      "Man-On Pun"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13378"
  },
  {
    "id": "arXiv:2210.13382",
    "title": "Emergent world representations: Exploring a sequence model trained on a  synthetic task",
    "abstract": "Language models show a surprising range of capabilities, but the source of\ntheir apparent competence is unclear. Do these networks just memorize a\ncollection of surface statistics, or do they rely on internal representations\nof the process that generates the sequences they see? We investigate this\nquestion by applying a variant of the GPT model to the task of predicting legal\nmoves in a simple board game, Othello. Although the network has no a priori\nknowledge of the game or its rules, we uncover evidence of an emergent\nnonlinear internal representation of the board state. Interventional\nexperiments indicate this representation can be used to control the output of\nthe network and create \"latent saliency maps\" that can help explain predictions\nin human terms.",
    "descriptor": "\nComments: code: this https URL\n",
    "authors": [
      "Kenneth Li",
      "Aspen K. Hopkins",
      "David Bau",
      "Fernanda Vi\u00e9gas",
      "Hanspeter Pfister",
      "Martin Wattenberg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13382"
  },
  {
    "id": "arXiv:2210.13383",
    "title": "Evaluating Long-Term Memory in 3D Mazes",
    "abstract": "Intelligent agents need to remember salient information to reason in\npartially-observed environments. For example, agents with a first-person view\nshould remember the positions of relevant objects even if they go out of view.\nSimilarly, to effectively navigate through rooms agents need to remember the\nfloor plan of how rooms are connected. However, most benchmark tasks in\nreinforcement learning do not test long-term memory in agents, slowing down\nprogress in this important research direction. In this paper, we introduce the\nMemory Maze, a 3D domain of randomized mazes specifically designed for\nevaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze\nmeasures long-term memory separate from confounding agent abilities and\nrequires the agent to localize itself by integrating information over time.\nWith Memory Maze, we propose an online reinforcement learning benchmark, a\ndiverse offline dataset, and an offline probing evaluation. Recording a human\nplayer establishes a strong baseline and verifies the need to build up and\nretain memories, which is reflected in their gradually increasing rewards\nwithin each episode. We find that current algorithms benefit from training with\ntruncated backpropagation through time and succeed on small mazes, but fall\nshort of human performance on the large mazes, leaving room for future\nalgorithmic designs to be evaluated on the Memory Maze.",
    "descriptor": "\nComments: Project website: this https URL\n",
    "authors": [
      "Jurgis Pasukonis",
      "Timothy Lillicrap",
      "Danijar Hafner"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13383"
  },
  {
    "id": "arXiv:2210.13385",
    "title": "Resilience and Load Balancing in Fog Networks: A Multi-Criteria Decision  Analysis Approach",
    "abstract": "The advent of Cloud Computing enabled the proliferation of IoT applications\nfor smart environments. However, the distance of these resources makes them\nunsuitable for delay-sensitive applications. Hence, Fog Computing has emerged\nto provide such capabilities in proximity to end devices through distributed\nresources. These limited resources can collaborate to serve distributed IoT\napplication workflows using the concept of stateless micro Fog service\nreplicas, which provides resiliency and maintains service availability in the\nface of failures. Load balancing supports this collaboration by optimally\nassigning workloads to appropriate services, i.e., distributing the load among\nFog nodes to fairly utilize compute and network resources and minimize\nexecution delays. In this paper, we propose using ELECTRE, a Multi-Criteria\nDecision Analysis (MCDA) approach, to efficiently balance the load in Fog\nenvironments. We considered multiple objectives to make service selection\ndecisions, including compute and network load information. We evaluate our\napproach in a realistic unbalanced topological setup with heterogeneous\nworkload requirements. To the best of our knowledge, this is the first time\nELECTRE-based methods are used to balance the load in Fog environments. Through\nsimulations, we compared the performance of our proposed approach with\ntraditional baseline methods that are commonly used in practice, namely random,\nRound-Robin, nearest node, and fastest service selection algorithms. In terms\nof the overall system performance, our approach outperforms these methods with\nup to 67% improvement.",
    "descriptor": "\nComments: 20 pages, 22 figures, 4 tables\n",
    "authors": [
      "Maad Ebrahim",
      "Abdelhakim Hafid"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.13385"
  },
  {
    "id": "arXiv:2210.13386",
    "title": "Contraction of Locally Differentially Private Mechanisms",
    "abstract": "We investigate the contraction properties of locally differentially private\nmechanisms. More specifically, we derive tight upper bounds on the divergence\nbetween $PK$ and $QK$ output distributions of an $\\epsilon$-LDP mechanism $K$\nin terms of a divergence between the corresponding input distributions $P$ and\n$Q$, respectively. Our first main technical result presents a sharp upper bound\non the $\\chi^2$-divergence $\\chi^2(PK\\|QK)$ in terms of $\\chi^2(P\\|Q)$ and\n$\\epsilon$. We also show that the same result holds for a large family of\ndivergences, including KL-divergence and squared Hellinger distance. The second\nmain technical result gives an upper bound on $\\chi^2(PK\\|QK)$ in terms of\ntotal variation distance $TV(P, Q)$ and $\\epsilon$. We then utilize these\nbounds to establish locally private versions of the Cram\\'er-Rao bound, Le\nCam's, Assouad's, and the mutual information methods, which are powerful tools\nfor bounding minimax estimation risks. These results are shown to lead to\nbetter privacy analyses than the state-of-the-arts in several statistical\nproblems such as entropy and discrete distribution estimation, non-parametric\ndensity estimation, and hypothesis testing.",
    "descriptor": "",
    "authors": [
      "Shahab Asoodeh",
      "Huanyu Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13386"
  },
  {
    "id": "arXiv:2210.13387",
    "title": "Towards a Higher-Order Mathematical Operational Semantics",
    "abstract": "Compositionality proofs in higher-order languages are notoriously involved,\nand general semantic frameworks guaranteeing compositionality are hard to come\nby. In particular, Turi and Plotkin's bialgebraic abstract GSOS framework,\nwhich has been successfully applied to obtain off-the-shelf compositionality\nresults for first-order languages, so far does not apply to higher-order\nlanguages. In the present work, we develop a theory of abstract GSOS\nspecifications for higher-order languages, in effect transferring the core\nprinciples of Turi and Plotkin's framework to a higher-order setting. In our\ntheory, the operational semantics of higher-order languages is represented by\ncertain dinatural transformations that we term pointed higher-order GSOS laws.\nWe give a general compositionality result that applies to all systems specified\nin this way, and we discuss how compositionality of the SKI calculus and the\n$\\lambda$-calculus w.r.t. a strong variant of Abramsky's applicative\nbisimilarity are obtained as instances.",
    "descriptor": "",
    "authors": [
      "Sergey Goncharov",
      "Stefan Milius",
      "Lutz Schr\u00f6der",
      "Stelios Tsampas",
      "Henning Urbat"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Programming Languages (cs.PL)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2210.13387"
  },
  {
    "id": "arXiv:2210.13388",
    "title": "Focused Concatenation for Context-Aware Neural Machine Translation",
    "abstract": "A straightforward approach to context-aware neural machine translation\nconsists in feeding the standard encoder-decoder architecture with a window of\nconsecutive sentences, formed by the current sentence and a number of sentences\nfrom its context concatenated to it. In this work, we propose an improved\nconcatenation approach that encourages the model to focus on the translation of\nthe current sentence, discounting the loss generated by target context. We also\npropose an additional improvement that strengthen the notion of sentence\nboundaries and of relative sentence distance, facilitating model compliance to\nthe context-discounted objective. We evaluate our approach with both\naverage-translation quality metrics and contrastive test sets for the\ntranslation of inter-sentential discourse phenomena, proving its superiority to\nthe vanilla concatenation approach and other sophisticated context-aware\nsystems.",
    "descriptor": "\nComments: WMT 2022 (camera ready)\n",
    "authors": [
      "Lorenzo Lupo",
      "Marco Dinarelli",
      "Laurent Besacier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13388"
  },
  {
    "id": "arXiv:2210.13389",
    "title": "A Regularized Conditional GAN for Posterior Sampling in Inverse Problems",
    "abstract": "In inverse problems, one seeks to reconstruct an image from incomplete and/or\ndegraded measurements. Such problems arise in magnetic resonance imaging (MRI),\ncomputed tomography, deblurring, superresolution, inpainting, and other\napplications. It is often the case that many image hypotheses are consistent\nwith both the measurements and prior information, and so the goal is not to\nrecover a single ``best'' hypothesis but rather to explore the space of\nprobable hypotheses, i.e., to sample from the posterior distribution. In this\nwork, we propose a regularized conditional Wasserstein GAN that can generate\ndozens of high-quality posterior samples per second. Using quantitative\nevaluation metrics like conditional Fr\\'{e}chet inception distance, we\ndemonstrate that our method produces state-of-the-art posterior samples in both\nmulticoil MRI and inpainting applications.",
    "descriptor": "",
    "authors": [
      "Matthew Bendel",
      "Rizwan Ahmad",
      "Philip Schniter"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13389"
  },
  {
    "id": "arXiv:2210.13391",
    "title": "Explaining Translationese: why are Neural Classifiers Better and what do  they Learn?",
    "abstract": "Recent work has shown that neural feature- and representation-learning, e.g.\nBERT, achieves superior performance over traditional manual feature engineering\nbased approaches, with e.g. SVMs, in translationese classification tasks.\nPrevious research did not show $(i)$ whether the difference is because of the\nfeatures, the classifiers or both, and $(ii)$ what the neural classifiers\nactually learn. To address $(i)$, we carefully design experiments that swap\nfeatures between BERT- and SVM-based classifiers. We show that an SVM fed with\nBERT representations performs at the level of the best BERT classifiers, while\nBERT learning and using handcrafted features performs at the level of an SVM\nusing handcrafted features. This shows that the performance differences are due\nto the features. To address $(ii)$ we use integrated gradients and find that\n$(a)$ there is indication that information captured by hand-crafted features is\nonly a subset of what BERT learns, and $(b)$ part of BERT's top performance\nresults are due to BERT learning topic differences and spurious correlations\nwith translationese.",
    "descriptor": "\nComments: 16 pages, 7 figures, 4 tables. The first 2 authors contributed equally. Accepted to BlackboxNLP 2022 (at EMNLP 2022)\n",
    "authors": [
      "Kwabena Amponsah-Kaakyire",
      "Daria Pylypenko",
      "Josef van Genabith",
      "Cristina Espa\u00f1a-Bonet"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13391"
  },
  {
    "id": "arXiv:2210.13393",
    "title": "We need to talk about random seeds",
    "abstract": "Modern neural network libraries all take as a hyperparameter a random seed,\ntypically used to determine the initial state of the model parameters. This\nopinion piece argues that there are some safe uses for random seeds: as part of\nthe hyperparameter search to select a good model, creating an ensemble of\nseveral models, or measuring the sensitivity of the training algorithm to the\nrandom seed hyperparameter. It argues that some uses for random seeds are\nrisky: using a fixed random seed for \"replicability\" and varying only the\nrandom seed to create score distributions for performance comparison. An\nanalysis of 85 recent publications from the ACL Anthology finds that more than\n50% contain risky uses of random seeds.",
    "descriptor": "",
    "authors": [
      "Steven Bethard"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13393"
  },
  {
    "id": "arXiv:2210.13395",
    "title": "Improved Bi-point Rounding Algorithms and a Golden Barrier for  $k$-Median",
    "abstract": "The current best approximation algorithms for $k$-median rely on first\nobtaining a structured fractional solution known as a bi-point solution, and\nthen rounding it to an integer solution. We improve this second step by\nunifying and refining previous approaches. We describe a hierarchy of\nincreasingly-complex partitioning schemes for the facilities, along with\ncorresponding sets of algorithms and factor-revealing non-linear programs. We\nprove that the third layer of this hierarchy is a $2.613$-approximation,\nimproving upon the current best ratio of $2.675$, while no layer can be proved\nbetter than $2.588$ under the proposed analysis.\nOn the negative side, we give a family of bi-point solutions which cannot be\napproximated better than the square root of the golden ratio, even if allowed\nto open $k+o(k)$ facilities. This gives a barrier to current approaches for\nobtaining an approximation better than $2 \\sqrt{\\phi} \\approx 2.544$.\nAltogether we reduce the approximation gap of bi-point solutions by two thirds.",
    "descriptor": "",
    "authors": [
      "Kishen N. Gowda",
      "Thomas Pensyl",
      "Aravind Srinivasan",
      "Khoa Trinh"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13395"
  },
  {
    "id": "arXiv:2210.13396",
    "title": "Offline congestion games: How feedback type affects data coverage  requirement",
    "abstract": "This paper investigates when one can efficiently recover an approximate Nash\nEquilibrium (NE) in offline congestion games.The existing dataset coverage\nassumption in offline general-sum games inevitably incurs a dependency on the\nnumber of actions, which can be exponentially large in congestion games. We\nconsider three different types of feedback with decreasing revealed\ninformation. Starting from the facility-level (a.k.a., semi-bandit) feedback,\nwe propose a novel one-unit deviation coverage condition and give a\npessimism-type algorithm that can recover an approximate NE. For the\nagent-level (a.k.a., bandit) feedback setting, interestingly, we show the\none-unit deviation coverage condition is not sufficient. On the other hand, we\nconvert the game to multi-agent linear bandits and show that with a generalized\ndata coverage assumption in offline linear bandits, we can efficiently recover\nthe approximate NE. Lastly, we consider a novel type of feedback, the\ngame-level feedback where only the total reward from all agents is revealed.\nAgain, we show the coverage assumption for the agent-level feedback setting is\ninsufficient in the game-level feedback setting, and with a stronger version of\nthe data coverage assumption for linear bandits, we can recover an approximate\nNE. Together, our results constitute the first study of offline congestion\ngames and imply formal separations between different types of feedback.",
    "descriptor": "\nComments: 20 pages, 3 figures\n",
    "authors": [
      "Haozhe Jiang",
      "Qiwen Cui",
      "Zhihan Xiong",
      "Maryam Fazel",
      "Simon S. Du"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.13396"
  },
  {
    "id": "arXiv:2210.13397",
    "title": "Development of Hybrid ASR Systems for Low Resource Medical Domain  Conversational Telephone Speech",
    "abstract": "In recent years, ASR systems have reached remarkable performance on specific\ntasks for which sufficient amounts of training data are available, like e.g.\nLibriSpeech. However, varying acoustic and recording conditions and speaking\nstyles and a lack of sufficient in-domain training data still pose challenges\nto the development of accurate models. In this work, we present our efforts for\nthe development of ASR systems for a conversational telephone speech\ntranslation task in the medical domain for three languages (Arabic, German,\nVietnamese) to support emergency room interaction between physician and patient\nacross language barriers. We study different training schedules and data\ncombination approaches in order to improve the system's performance, as well as\nanalyze where limited available data is used most efficiently.",
    "descriptor": "\nComments: ASR System Paper for HYKIST project\n",
    "authors": [
      "Christoph L\u00fcscher",
      "Mohammad Zeineldeen",
      "Zijian Yang",
      "Peter Vieting",
      "Khai Le-Duc",
      "Weiyue Wang",
      "Ralf Schl\u00fcter",
      "Hermann Ney"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.13397"
  },
  {
    "id": "arXiv:2210.13399",
    "title": "Does Mode of Digital Contact Tracing Affect User Willingness to Share  Information? A Quantitative Study",
    "abstract": "Digital contact tracing can limit the spread of infectious diseases.\nNevertheless, there remain barriers to attaining sufficient adoption. In this\nstudy, we investigate how willingness to participate in contact tracing is\naffected by two critical factors: the modes of data collection and the type of\ndata collected. We conducted a scenario-based survey study among 220\nrespondents in the United States (U.S.) to understand their perceptions about\ncontact tracing associated with automated and manual contact tracing methods.\nThe findings indicate a promising use of smartphones and a combination of\npublic health officials and medical health records as information sources.\nThrough a quantitative analysis, we describe how different modalities and\nindividual demographic factors may affect user compliance in providing four key\npieces of information to contact tracing.",
    "descriptor": "\nComments: 18 pages, 11 figures, 13 tables\n",
    "authors": [
      "Camellia Zakaria",
      "Pin Sym Foong",
      "Chang Siang Lim",
      "Pavithren V. S. Pakianathan",
      "Gerald Huat Choon Koh",
      "Simon Tangi Perrault"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.13399"
  },
  {
    "id": "arXiv:2210.13401",
    "title": "Entity-level Sentiment Analysis in Contact Center Telephone  Conversations",
    "abstract": "Entity-level sentiment analysis predicts the sentiment about entities\nmentioned in a given text. It is very useful in a business context to\nunderstand user emotions towards certain entities, such as products or\ncompanies. In this paper, we demonstrate how we developed an entity-level\nsentiment analysis system that analyzes English telephone conversation\ntranscripts in contact centers to provide business insight. We present two\napproaches, one entirely based on the transformer-based DistilBERT model, and\nanother that uses a convolutional neural network supplemented with some\nheuristic rules.",
    "descriptor": "",
    "authors": [
      "Xue-Yong Fu",
      "Cheng Chen",
      "Md Tahmid Rahman Laskar",
      "Shayna Gardiner",
      "Pooja Hiranandani",
      "Shashi Bhushan TN"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13401"
  },
  {
    "id": "arXiv:2210.13403",
    "title": "Task-Driven In-Hand Manipulation of Unknown Objects with Tactile Sensing",
    "abstract": "Manipulation of objects in-hand without an object model is a foundational\nskill for many tasks in unstructured environments. In many cases, vision-only\napproaches may not be feasible; for example, due to occlusion in cluttered\nspaces. In this paper, we introduce a method to reorient unknown objects by\nincrementally building a probabilistic estimate of the object shape and pose\nduring task-driven manipulation. Our method leverages Bayesian optimization to\nstrategically trade-off exploration of the global object shape with efficient\ntask completion. We demonstrate our approach on a Tactile-Enabled Roller\nGrasper, a gripper that rolls objects in hand while continuously collecting\ntactile data. We evaluate our method in simulation on a set of randomly\ngenerated objects and find that our method reliably reorients objects while\nsignificantly reducing the exploration time needed to do so. On the Roller\nGrasper hardware, we show successful qualitative reconstruction of the object\nmodel. In summary, this work (1) presents a system capable of simultaneously\nlearning unknown 3D object shape and pose using tactile sensing; and (2)\ndemonstrates that task-driven exploration results in more efficient object\nmanipulation than the common paradigm of complete object exploration before\ntask-completion.",
    "descriptor": "",
    "authors": [
      "Chaoyi Pan",
      "Marion Lepert",
      "Shenli Yuan",
      "Rika Antonova",
      "Jeannette Bohg"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13403"
  },
  {
    "id": "arXiv:2210.13404",
    "title": "Contrastive Representation Learning for Gaze Estimation",
    "abstract": "Self-supervised learning (SSL) has become prevalent for learning\nrepresentations in computer vision. Notably, SSL exploits contrastive learning\nto encourage visual representations to be invariant under various image\ntransformations. The task of gaze estimation, on the other hand, demands not\njust invariance to various appearances but also equivariance to the geometric\ntransformations. In this work, we propose a simple contrastive representation\nlearning framework for gaze estimation, named Gaze Contrastive Learning\n(GazeCLR). GazeCLR exploits multi-view data to promote equivariance and relies\non selected data augmentation techniques that do not alter gaze directions for\ninvariance learning. Our experiments demonstrate the effectiveness of GazeCLR\nfor several settings of the gaze estimation task. Particularly, our results\nshow that GazeCLR improves the performance of cross-domain gaze estimation and\nyields as high as 17.2% relative improvement. Moreover, the GazeCLR framework\nis competitive with state-of-the-art representation learning methods for\nfew-shot evaluation. The code and pre-trained models are available at\nhttps://github.com/jswati31/gazeclr.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022 Gaze Meets ML Workshop (Spotlight)\n",
    "authors": [
      "Swati Jindal",
      "Roberto Manduchi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.13404"
  },
  {
    "id": "arXiv:2210.13414",
    "title": "Thermodynamics-informed neural networks for physically realistic mixed  reality",
    "abstract": "The imminent impact of immersive technologies in society urges for active\nresearch in real-time and interactive physics simulation for virtual worlds to\nbe realistic. In this context, realistic means to be compliant to the laws of\nphysics. In this paper we present a method for computing the dynamic response\nof (possibly non-linear and dissipative) deformable objects induced by\nreal-time user interactions in mixed reality using deep learning. The\ngraph-based architecture of the method ensures the thermodynamic consistency of\nthe predictions, whereas the visualization pipeline allows a natural and\nrealistic user experience. Two examples of virtual solids interacting with\nvirtual or physical solids in mixed reality scenarios are provided to prove the\nperformance of the method.",
    "descriptor": "\nComments: 11 pages, 7 figures\n",
    "authors": [
      "Quercus Hern\u00e1ndez",
      "Alberto Bad\u00edas",
      "Francisco Chinesta",
      "El\u00edas Cueto"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Artificial Intelligence (cs.AI)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13414"
  },
  {
    "id": "arXiv:2210.13416",
    "title": "A continuous trainable filter for convolution with unstructured data",
    "abstract": "Convolutional Neural Network (CNN) is one of the most important architectures\nin deep learning. The fundamental building block of a CNN is a trainable\nfilter, represented as a discrete grid, used to perform convolution on discrete\ninput data. In this work, we propose a continuous version of a trainable\nconvolutional filter able to work also with unstructured data. This new\nframework allows exploring CNNs beyond discrete domains, enlarging the usage of\nthis important learning technique for many more complex problems. Our\nexperiments show that the continuous filter can achieve a level of accuracy\ncomparable to the state-of-the-art discrete filter, and that it can be used in\ncurrent deep learning architectures as a building block to solve problems with\nunstructured domains as well.",
    "descriptor": "",
    "authors": [
      "Dario Coscia",
      "Laura Meneghetti",
      "Nicola Demo",
      "Giovanni Stabile",
      "Gianluigi Rozza"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13416"
  },
  {
    "id": "arXiv:2210.13417",
    "title": "Avalon: A Benchmark for RL Generalization Using Procedurally Generated  Worlds",
    "abstract": "Despite impressive successes, deep reinforcement learning (RL) systems still\nfall short of human performance on generalization to new tasks and environments\nthat differ from their training. As a benchmark tailored for studying RL\ngeneralization, we introduce Avalon, a set of tasks in which embodied agents in\nhighly diverse procedural 3D worlds must survive by navigating terrain, hunting\nor gathering food, and avoiding hazards. Avalon is unique among existing RL\nbenchmarks in that the reward function, world dynamics, and action space are\nthe same for every task, with tasks differentiated solely by altering the\nenvironment; its 20 tasks, ranging in complexity from eat and throw to hunt and\nnavigate, each create worlds in which the agent must perform specific skills in\norder to survive. This setup enables investigations of generalization within\ntasks, between tasks, and to compositional tasks that require combining skills\nlearned from previous tasks. Avalon includes a highly efficient simulator, a\nlibrary of baselines, and a benchmark with scoring metrics evaluated against\nhundreds of hours of human performance, all of which are open-source and\npublicly available. We find that standard RL baselines make progress on most\ntasks but are still far from human performance, suggesting Avalon is\nchallenging enough to advance the quest for generalizable RL.",
    "descriptor": "\nComments: Accepted to NeurIPS Datasets and Benchmarks 2022. Video and links to all code, data, etc can be found at this https URL\n",
    "authors": [
      "Joshua Albrecht",
      "Abraham J. Fetterman",
      "Bryden Fogelman",
      "Ellie Kitanidis",
      "Bartosz Wr\u00f3blewski",
      "Nicole Seo",
      "Michael Rosenthal",
      "Maksis Knutins",
      "Zachary Polizzi",
      "James B. Simon",
      "Kanjun Qiu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13417"
  },
  {
    "id": "arXiv:2210.13420",
    "title": "Twitter Users' Behavioral Response to Toxic Replies",
    "abstract": "Online toxic attacks, such as harassment, trolling, and hate speech have been\nlinked to an increase in offline violence and negative psychological effects on\nvictims. In this paper, we studied the impact of toxicity on users' online\nbehavior. We collected a sample of 79.8k Twitter conversations. Then, through a\nlongitudinal study, for nine weeks, we tracked and compared the behavioral\nreactions of authors, who were toxicity victims, with those who were not. We\nfound that toxicity victims show a combination of the following behavioral\nreactions: avoidance, revenge, countermeasures, and negotiation. We performed\nstatistical tests to understand the significance of the contribution of toxic\nreplies toward user behaviors while considering confounding factors, such as\nthe structure of conversations and the user accounts' visibility,\nidentifiability, and activity level. Interestingly, we found that compared to\nother random authors, victims are more likely to engage in conversations, reply\nin a toxic way, and unfollow toxicity instigators. Even if the toxicity is\ndirected at other participants, the root authors are more likely to engage in\nthe conversations and reply in a toxic way. However, victims who have verified\naccounts are less likely to participate in conversations or respond by posting\ntoxic comments. In addition, replies are more likely to be removed in\nconversations with a larger percentage of toxic nested replies and toxic\nreplies directed at other users. Our results can assist further studies in\ndeveloping more effective detection and intervention methods for reducing the\nnegative consequences of toxicity on social media.",
    "descriptor": "",
    "authors": [
      "Ana Aleksandric",
      "Sayak Saha Roy",
      "Shirin Nilizadeh"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.13420"
  },
  {
    "id": "arXiv:2210.13421",
    "title": "Evaluation of Position and Velocity Based Forward Dynamics Compliance  Control (FDCC) for Robotic Interactions in Position Controlled Robots",
    "abstract": "In robotic manipulation, end-effector compliance is an essential precondition\nfor performing contact-rich tasks, such as machining, assembly, and human-robot\ninteraction. Most robotic arms are position-controlled stiff systems at a\nhardware level. Thus, adding compliance becomes essential. Compliance in those\nsystems has been recently achieved using Forward dynamics compliance control\n(FDCC), which, owing to its virtual forward dynamics model, can be implemented\non both position and velocity-controlled robots. This paper evaluates the\nchoice of control interface (and hence the control domain), which, although\nconsidered trivial, is essential due to differences in their characteristics.\nIn some cases, the choice is restricted to the available hardware interface.\nHowever, given the option to choose, the velocity-based control interface makes\na better candidate for compliance control because of smoother compliant\nbehaviour, reduced interaction forces, and work done. To prove these points, in\nthis paper FDCC is evaluated on the UR10e six-DOF manipulator with velocity and\nposition control modes. The evaluation is based on force-control benchmarking\nmetrics using 3D-printed artefacts. Real experiments favour the choice of\nvelocity control over position control.",
    "descriptor": "\nComments: Submitted to RA-L on 15th Sept 2022, for associated video see: this https URL\n",
    "authors": [
      "Mohatashem Reyaz Makhdoomi",
      "Vivek Muralidharan",
      "Juan Sandoval",
      "Miguel Olivares-Mendez",
      "Carol Martinez"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13421"
  },
  {
    "id": "arXiv:2210.13428",
    "title": "PseudoAugment: Learning to Use Unlabeled Data for Data Augmentation in  Point Clouds",
    "abstract": "Data augmentation is an important technique to improve data efficiency and\nsave labeling cost for 3D detection in point clouds. Yet, existing augmentation\npolicies have so far been designed to only utilize labeled data, which limits\nthe data diversity. In this paper, we recognize that pseudo labeling and data\naugmentation are complementary, thus propose to leverage unlabeled data for\ndata augmentation to enrich the training data. In particular, we design three\nnovel pseudo-label based data augmentation policies (PseudoAugments) to fuse\nboth labeled and pseudo-labeled scenes, including frames (PseudoFrame), objecta\n(PseudoBBox), and background (PseudoBackground). PseudoAugments outperforms\npseudo labeling by mitigating pseudo labeling errors and generating diverse\nfused training scenes. We demonstrate PseudoAugments generalize across\npoint-based and voxel-based architectures, different model capacity and both\nKITTI and Waymo Open Dataset. To alleviate the cost of hyperparameter tuning\nand iterative pseudo labeling, we develop a population-based data augmentation\nframework for 3D detection, named AutoPseudoAugment. Unlike previous works that\nperform pseudo-labeling offline, our framework performs PseudoAugments and\nhyperparameter tuning in one shot to reduce computational cost. Experimental\nresults on the large-scale Waymo Open Dataset show our method outperforms\nstate-of-the-art auto data augmentation method (PPBA) and self-training method\n(pseudo labeling). In particular, AutoPseudoAugment is about 3X and 2X data\nefficient on vehicle and pedestrian tasks compared to prior arts. Notably,\nAutoPseudoAugment nearly matches the full dataset training results, with just\n10% of the labeled run segments on the vehicle detection task.",
    "descriptor": "",
    "authors": [
      "Zhaoqi Leng",
      "Shuyang Cheng",
      "Benjamin Caine",
      "Weiyue Wang",
      "Xiao Zhang",
      "Jonathon Shlens",
      "Mingxing Tan",
      "Dragomir Anguelov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13428"
  },
  {
    "id": "arXiv:2210.13431",
    "title": "Instruction-Following Agents with Jointly Pre-Trained Vision-Language  Models",
    "abstract": "Humans are excellent at understanding language and vision to accomplish a\nwide range of tasks. In contrast, creating general instruction-following\nembodied agents remains a difficult challenge. Prior work that uses pure\nlanguage-only models lack visual grounding, making it difficult to connect\nlanguage instructions with visual observations. On the other hand, methods that\nuse pre-trained vision-language models typically come with divided language and\nvisual representations, requiring designing specialized network architecture to\nfuse them together. We propose a simple yet effective model for robots to solve\ninstruction-following tasks in vision-based environments. Our \\ours method\nconsists of a multimodal transformer that encodes visual observations and\nlanguage instructions, and a policy transformer that predicts actions based on\nencoded representations. The multimodal transformer is pre-trained on millions\nof image-text pairs and natural language text, thereby producing generic\ncross-modal representations of observations and instructions. The policy\ntransformer keeps track of the full history of observations and actions, and\npredicts actions autoregressively. We show that this unified transformer model\noutperforms all state-of-the-art pre-trained or trained-from-scratch methods in\nboth single-task and multi-task settings. Our model also shows better model\nscalability and generalization ability than prior work.",
    "descriptor": "",
    "authors": [
      "Hao Liu",
      "Lisa Lee",
      "Kimin Lee",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13431"
  },
  {
    "id": "arXiv:2210.13432",
    "title": "FCM: Forgetful Causal Masking Makes Causal Language Models Better  Zero-Shot Learners",
    "abstract": "Large language models (LLM) trained using the next-token-prediction\nobjective, such as GPT3 and PaLM, have revolutionized natural language\nprocessing in recent years by showing impressive zero-shot and few-shot\ncapabilities across a wide range of tasks. In this work, we propose a simple\ntechnique that significantly boosts the performance of LLMs without adding\ncomputational cost. Our key observation is that, by performing the next token\nprediction task with randomly selected past tokens masked out, we can improve\nthe quality of the learned representations for downstream language\nunderstanding tasks. We hypothesize that randomly masking past tokens prevents\nover-attending to recent tokens and encourages attention to tokens in the\ndistant past. By randomly masking input tokens in the PaLM model, we show that\nwe can significantly improve 1B and 8B PaLM's zero-shot performance on the\nSuperGLUE benchmark from 55.7 to 59.2 and from 61.6 to 64.0, respectively. Our\nlargest 8B model matches the score of PaLM with an average score of 64, despite\nthe fact that PaLM is trained on a much larger dataset (780B tokens) of\nhigh-quality conversation and webpage data, while ours is trained on the\nsmaller C4 dataset (180B tokens). Experimental results show that our method\nalso improves PaLM's zero and few-shot performance on a diverse suite of tasks,\nincluding commonsense reasoning, natural language inference and cloze\ncompletion. Moreover, we show that our technique also helps representation\nlearning, significantly improving PaLM's finetuning results.",
    "descriptor": "",
    "authors": [
      "Hao Liu",
      "Xinyang Geng",
      "Lisa Lee",
      "Igor Mordatch",
      "Sergey Levine",
      "Sharan Narang",
      "Pieter Abbeel"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13432"
  },
  {
    "id": "arXiv:2210.13435",
    "title": "Dichotomy of Control: Separating What You Can Control from What You  Cannot",
    "abstract": "Future- or return-conditioned supervised learning is an emerging paradigm for\noffline reinforcement learning (RL), where the future outcome (i.e., return)\nassociated with an observed action sequence is used as input to a policy\ntrained to imitate those same actions. While return-conditioning is at the\nheart of popular algorithms such as decision transformer (DT), these methods\ntend to perform poorly in highly stochastic environments, where an occasional\nhigh return can arise from randomness in the environment rather than the\nactions themselves. Such situations can lead to a learned policy that is\ninconsistent with its conditioning inputs; i.e., using the policy to act in the\nenvironment, when conditioning on a specific desired return, leads to a\ndistribution of real returns that is wildly different than desired. In this\nwork, we propose the dichotomy of control (DoC), a future-conditioned\nsupervised learning framework that separates mechanisms within a policy's\ncontrol (actions) from those beyond a policy's control (environment\nstochasticity). We achieve this separation by conditioning the policy on a\nlatent variable representation of the future, and designing a mutual\ninformation constraint that removes any information from the latent variable\nassociated with randomness in the environment. Theoretically, we show that DoC\nyields policies that are consistent with their conditioning inputs, ensuring\nthat conditioning a learned policy on a desired high-return future outcome will\ncorrectly induce high-return behavior. Empirically, we show that DoC is able to\nachieve significantly better performance than DT on environments that have\nhighly stochastic rewards and transition",
    "descriptor": "",
    "authors": [
      "Mengjiao Yang",
      "Dale Schuurmans",
      "Pieter Abbeel",
      "Ofir Nachum"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13435"
  },
  {
    "id": "arXiv:2210.13439",
    "title": "Cascading Biases: Investigating the Effect of Heuristic Annotation  Strategies on Data and Models",
    "abstract": "Cognitive psychologists have documented that humans use cognitive heuristics,\nor mental shortcuts, to make quick decisions while expending less effort. While\nperforming annotation work on crowdsourcing platforms, we hypothesize that such\nheuristic use among annotators cascades on to data quality and model\nrobustness. In this work, we study cognitive heuristic use in the context of\nannotating multiple-choice reading comprehension datasets. We propose tracking\nannotator heuristic traces, where we tangibly measure low-effort annotation\nstrategies that could indicate usage of various cognitive heuristics. We find\nevidence that annotators might be using multiple such heuristics, based on\ncorrelations with a battery of psychological tests. Importantly, heuristic use\namong annotators determines data quality along several dimensions: (1) known\nbiased models, such as partial input models, more easily solve examples\nauthored by annotators that rate highly on heuristic use, (2) models trained on\nannotators scoring highly on heuristic use don't generalize as well, and (3)\nheuristic-seeking annotators tend to create qualitatively less challenging\nexamples. Our findings suggest that tracking heuristic usage among annotators\ncan potentially help with collecting challenging datasets and diagnosing model\nbiases.",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Chaitanya Malaviya",
      "Sudeep Bhatia",
      "Mark Yatskar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13439"
  },
  {
    "id": "arXiv:2210.13440",
    "title": "Reliability-Aware Prediction via Uncertainty Learning for Person Image  Retrieval",
    "abstract": "Current person image retrieval methods have achieved great improvements in\naccuracy metrics. However, they rarely describe the reliability of the\nprediction. In this paper, we propose an Uncertainty-Aware Learning (UAL)\nmethod to remedy this issue. UAL aims at providing reliability-aware\npredictions by considering data uncertainty and model uncertainty\nsimultaneously. Data uncertainty captures the ``noise\" inherent in the sample,\nwhile model uncertainty depicts the model's confidence in the sample's\nprediction. Specifically, in UAL, (1) we propose a sampling-free data\nuncertainty learning method to adaptively assign weights to different samples\nduring training, down-weighting the low-quality ambiguous samples. (2) we\nleverage the Bayesian framework to model the model uncertainty by assuming the\nparameters of the network follow a Bernoulli distribution. (3) the data\nuncertainty and the model uncertainty are jointly learned in a unified network,\nand they serve as two fundamental criteria for the reliability assessment: if a\nprobe is high-quality (low data uncertainty) and the model is confident in the\nprediction of the probe (low model uncertainty), the final ranking will be\nassessed as reliable. Experiments under the risk-controlled settings and the\nmulti-query settings show the proposed reliability assessment is effective. Our\nmethod also shows superior performance on three challenging benchmarks under\nthe vanilla single query settings.",
    "descriptor": "",
    "authors": [
      "Zhaopeng Dou",
      "Zhongdao Wang",
      "Weihua Chen",
      "Yali Li",
      "Shengjin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13440"
  },
  {
    "id": "arXiv:2210.13445",
    "title": "Monocular Dynamic View Synthesis: A Reality Check",
    "abstract": "We study the recent progress on dynamic view synthesis (DVS) from monocular\nvideo. Though existing approaches have demonstrated impressive results, we show\na discrepancy between the practical capture process and the existing\nexperimental protocols, which effectively leaks in multi-view signals during\ntraining. We define effective multi-view factors (EMFs) to quantify the amount\nof multi-view signal present in the input capture sequence based on the\nrelative camera-scene motion. We introduce two new metrics: co-visibility\nmasked image metrics and correspondence accuracy, which overcome the issue in\nexisting protocols. We also propose a new iPhone dataset that includes more\ndiverse real-life deformation sequences. Using our proposed experimental\nprotocol, we show that the state-of-the-art approaches observe a 1-2 dB drop in\nmasked PSNR in the absence of multi-view cues and 4-5 dB drop when modeling\ncomplex motion. Code and data can be found at https://hangg7.com/dycheck.",
    "descriptor": "\nComments: NeurIPS 2022. Project page: this https URL Code: this https URL\n",
    "authors": [
      "Hang Gao",
      "Ruilong Li",
      "Shubham Tulsiani",
      "Bryan Russell",
      "Angjoo Kanazawa"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13445"
  },
  {
    "id": "arXiv:2210.13446",
    "title": "Flying Trot Control Method for Quadruped Robot Based on Trajectory  Planning",
    "abstract": "An intuitive control method for the flying trot, which combines offline\ntrajectory planning with real-time balance control, is presented. The motion\nfeatures of running animals in the vertical direction were analysed using the\nspring-load-inverted-pendulum (SLIP) model, and the foot trajectory of the\nrobot was planned, so the robot could run similar to an animal capable of\nvertical flight, according to the given height and speed of the trunk. To\nimprove the robustness of running, a posture control method based on a foot\nacceleration adjustment is proposed. A novel kinematic based CoM observation\nmethod and CoM regulation method is present to enhance the stability of\nlocomotion. To reduce the impact force when the robot interacts with the\nenvironment, the virtual model control method is used in the control of the\nfoot trajectory to achieve active compliance. By selecting the proper\nparameters for the virtual model, the oscillation motion of the virtual model\nand the planning motion of the support foot are synchronized to avoid the large\ndisturbance caused by the oscillation motion of the virtual model in relation\nto the robot motion. The simulation and experiment using the quadruped robot\nBilly are reported. In the experiment, the maximum speed of the robot could\nreach 4.73 times the body length per second, which verified the feasibility of\nthe control method.",
    "descriptor": "\nComments: 30 pages, 20 figures, journal\n",
    "authors": [
      "Hongge Wang",
      "Hui Chai",
      "Bin Chen",
      "Aizhen Xie",
      "Rui Song",
      "Bo Su"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13446"
  },
  {
    "id": "arXiv:2210.13447",
    "title": "Precision Machine Learning",
    "abstract": "We explore unique considerations involved in fitting ML models to data with\nvery high precision, as is often required for science applications. We\nempirically compare various function approximation methods and study how they\nscale with increasing parameters and data. We find that neural networks can\noften outperform classical approximation methods on high-dimensional examples,\nby auto-discovering and exploiting modular structures therein. However, neural\nnetworks trained with common optimizers are less powerful for low-dimensional\ncases, which motivates us to study the unique properties of neural network loss\nlandscapes and the corresponding optimization challenges that arise in the high\nprecision regime. To address the optimization issue in low dimensions, we\ndevelop training tricks which enable us to train neural networks to extremely\nlow loss, close to the limits allowed by numerical precision.",
    "descriptor": "",
    "authors": [
      "Eric J. Michaud",
      "Ziming Liu",
      "Max Tegmark"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.13447"
  },
  {
    "id": "arXiv:2210.13448",
    "title": "EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form  Summarization in the Legal Domain",
    "abstract": "Existing summarization datasets come with two main drawbacks: (1) They tend\nto focus on overly exposed domains, such as news articles or wiki-like texts,\nand (2) are primarily monolingual, with few multilingual datasets. In this\nwork, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated\ndocument summaries of legal acts from the European Union law platform\n(EUR-Lex). Documents and their respective summaries exist as cross-lingual\nparagraph-aligned data in several of the 24 official European languages,\nenabling access to various cross-lingual and lower-resourced summarization\nsetups. We obtain up to 1,500 document/summary pairs per language, including a\nsubset of 375 cross-lingually aligned legal acts with texts available in all 24\nlanguages. In this work, the data acquisition process is detailed and key\ncharacteristics of the resource are compared to existing summarization\nresources. In particular, we illustrate challenging sub-problems and open\nquestions on the dataset that could help the facilitation of future research in\nthe direction of domain-specific cross-lingual summarization. Limited by the\nextreme length and language diversity of samples, we further conduct\nexperiments with suitable extractive monolingual and cross-lingual baselines\nfor future work. Code for the extraction as well as access to our data and\nbaselines is available online at: https://github.com/achouhan93/eur-lex-sum.",
    "descriptor": "",
    "authors": [
      "Dennis Aumiller",
      "Ashish Chouhan",
      "Michael Gertz"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13448"
  },
  {
    "id": "arXiv:2210.13449",
    "title": "Controlled Text Reduction",
    "abstract": "Producing a reduced version of a source text, as in generic or focused\nsummarization, inherently involves two distinct subtasks: deciding on targeted\ncontent and generating a coherent text conveying it. While some popular\napproaches address summarization as a single end-to-end task, prominent works\nsupport decomposed modeling for individual subtasks. Further, semi-automated\ntext reduction is also very appealing, where users may identify targeted\ncontent while models would generate a corresponding coherent summary.\nIn this paper, we focus on the second subtask, of generating coherent text\ngiven pre-selected content. Concretely, we formalize \\textit{Controlled Text\nReduction} as a standalone task, whose input is a source text with marked spans\nof targeted content (\"highlighting\"). A model then needs to generate a coherent\ntext that includes all and only the target information. We advocate the\npotential of such models, both for modular fully-automatic summarization, as\nwell as for semi-automated human-in-the-loop use cases. Facilitating proper\nresearch, we crowdsource high-quality dev and test datasets for the task.\nFurther, we automatically generate a larger \"silver\" training dataset from\navailable summarization benchmarks, leveraging a pretrained summary-source\nalignment model. Finally, employing these datasets, we present a supervised\nbaseline model, showing promising results and insightful analyses.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Aviv Slobodkin",
      "Paul Roit",
      "Eran Hirsch",
      "Ori Ernst",
      "Ido Dagan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.13449"
  },
  {
    "id": "arXiv:2210.13452",
    "title": "MetaFormer Baselines for Vision",
    "abstract": "MetaFormer, the abstracted architecture of Transformer, has been found to\nplay a significant role in achieving competitive performance. In this paper, we\nfurther explore the capacity of MetaFormer, again, without focusing on token\nmixer design: we introduce several baseline models under MetaFormer using the\nmost basic or common mixers, and summarize our observations as follows: (1)\nMetaFormer ensures solid lower bound of performance. By merely adopting\nidentity mapping as the token mixer, the MetaFormer model, termed\nIdentityFormer, achieves >80% accuracy on ImageNet-1K. (2) MetaFormer works\nwell with arbitrary token mixers. When specifying the token mixer as even a\nrandom matrix to mix tokens, the resulting model RandFormer yields an accuracy\nof >81%, outperforming IdentityFormer. Rest assured of MetaFormer's results\nwhen new token mixers are adopted. (3) MetaFormer effortlessly offers\nstate-of-the-art results. With just conventional token mixers dated back five\nyears ago, the models instantiated from MetaFormer already beat state of the\nart. (a) ConvFormer outperforms ConvNeXt. Taking the common depthwise separable\nconvolutions as the token mixer, the model termed ConvFormer, which can be\nregarded as pure CNNs, outperforms the strong CNN model ConvNeXt. (b) CAFormer\nsets new record on ImageNet-1K. By simply applying depthwise separable\nconvolutions as token mixer in the bottom stages and vanilla self-attention in\nthe top stages, the resulting model CAFormer sets a new record on ImageNet-1K:\nit achieves an accuracy of 85.5% at 224x224 resolution, under normal supervised\ntraining without external data or distillation. In our expedition to probe\nMetaFormer, we also find that a new activation, StarReLU, reduces 71% FLOPs of\nactivation compared with GELU yet achieves better performance. We expect\nStarReLU to find great potential in MetaFormer-like models alongside other\nneural networks.",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Weihao Yu",
      "Chenyang Si",
      "Pan Zhou",
      "Mi Luo",
      "Yichen Zhou",
      "Jiashi Feng",
      "Shuicheng Yan",
      "Xinchao Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13452"
  },
  {
    "id": "arXiv:2210.12158",
    "title": "Graph Coloring via Neural Networks for Haplotype Assembly and Viral  Quasispecies Reconstruction",
    "abstract": "Understanding genetic variation, e.g., through mutations, in organisms is\ncrucial to unravel their effects on the environment and human health. A\nfundamental characterization can be obtained by solving the haplotype assembly\nproblem, which yields the variation across multiple copies of chromosomes.\nVariations among fast evolving viruses that lead to different strains (called\nquasispecies) are also deciphered with similar approaches. In both these cases,\nhigh-throughput sequencing technologies that provide oversampled mixtures of\nlarge noisy fragments (reads) of genomes, are used to infer constituent\ncomponents (haplotypes or quasispecies). The problem is harder for polyploid\nspecies where there are more than two copies of chromosomes. State-of-the-art\nneural approaches to solve this NP-hard problem do not adequately model\nrelations among the reads that are important for deconvolving the input signal.\nWe address this problem by developing a new method, called NeurHap, that\ncombines graph representation learning with combinatorial optimization. Our\nexperiments demonstrate substantially better performance of NeurHap in real and\nsynthetic datasets compared to competing approaches.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Hansheng Xue",
      "Vaibhav Rajan",
      "Yu Lin"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12158"
  },
  {
    "id": "arXiv:2210.12161",
    "title": "Task-Based Assessment for Neural Networks: Evaluating Undersampled MRI  Reconstructions based on Human Observer Signal Detection",
    "abstract": "Recent research has explored using neural networks to reconstruct\nundersampled magnetic resonance imaging (MRI) data. Because of the complexity\nof the artifacts in the reconstructed images, there is a need to develop\ntask-based approaches of image quality. Common metrics for evaluating image\nquality like the normalized root mean squared error (NRMSE) and structural\nsimilarity (SSIM) are global metrics which average out impact of subtle\nfeatures in the images. Using measures of image quality which incorporate a\nsubtle signal for a specific task allow for image quality assessment which\nlocally evaluates the effect of undersampling on a signal. We used a U-Net to\nreconstruct under-sampled images with 2x, 3x, 4x and 5x fold 1-D undersampling\nrates. Cross validation was performed for a 500 and a 4000 image training set\nwith both structural similarity (SSIM) and mean squared error (MSE) losses. A\ntwo alternative forced choice (2-AFC) observer study was carried out for\ndetecting a subtle signal (small blurred disk) from images with the 4000 image\ntraining set. We found that for both loss functions and training set sizes, the\nhuman observer performance on the 2-AFC studies led to a choice of a 2x\nundersampling but the SSIM and NRMSE led to a choice of a 3x undersampling. For\nthis task, SSIM and NRMSE led to an overestimate of the achievable\nundersampling using a U-Net before a steep loss of image quality when compared\nto the performance of human observers in the detection of a subtle lesion.",
    "descriptor": "",
    "authors": [
      "Joshua D. Herman",
      "Rachel E. Roca",
      "Alexandra G. O'Neill",
      "Marcus L. Wong",
      "Sajan G. Lingala",
      "Angel R. Pineda"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12161"
  },
  {
    "id": "arXiv:2210.12166",
    "title": "Equivalence Checking of Parameterized Quantum Circuits: Verifying the  Compilation of Variational Quantum Algorithms",
    "abstract": "Variational quantum algorithms have been introduced as a promising class of\nquantum-classical hybrid algorithms that can already be used with the noisy\nquantum computing hardware available today by employing parameterized quantum\ncircuits. Considering the non-trivial nature of quantum circuit compilation and\nthe subtleties of quantum computing, it is essential to verify that these\nparameterized circuits have been compiled correctly. Established equivalence\nchecking procedures that handle parameter-free circuits already exist. However,\nno methodology capable of handling circuits with parameters has been proposed\nyet. This work fills this gap by showing that verifying the equivalence of\nparameterized circuits can be achieved in a purely symbolic fashion using an\nequivalence checking approach based on the ZX-calculus. At the same time,\nproofs of inequality can be efficiently obtained with conventional methods by\ntaking advantage of the degrees of freedom inherent to parameterized circuits.\nWe implemented the corresponding methods and proved that the resulting\nmethodology is complete. Experimental evaluations (using the entire parametric\nansatz circuit library provided by Qiskit as benchmarks) demonstrate the\nefficacy of the proposed approach. The implementation is open source and\npublicly available as part of the equivalence checking tool QCEC\n(https://github.com/cda-tum/qcec) which is part of the Munich Quantum Toolkit\n(MQT).",
    "descriptor": "\nComments: 7 pages, 3 figures, 2 tables, 28th Asia and South Pacific Design Automation Conference (ASPDAC '23)\n",
    "authors": [
      "Tom Peham",
      "Lukas Burgholzer",
      "Robert Wille"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2210.12166"
  },
  {
    "id": "arXiv:2210.12175",
    "title": "High-Fidelity Visual Structural Inspections through Transformers and  Learnable Resizers",
    "abstract": "Visual inspection is the predominant technique for evaluating the condition\nof civil infrastructure. The recent advances in unmanned aerial vehicles (UAVs)\nand artificial intelligence have made the visual inspections faster, safer, and\nmore reliable. Camera-equipped UAVs are becoming the new standard in the\nindustry by collecting massive amounts of visual data for human inspectors.\nMeanwhile, there has been significant research on autonomous visual inspections\nusing deep learning algorithms, including semantic segmentation. While UAVs can\ncapture high-resolution images of buildings' fa\\c{c}ades, high-resolution\nsegmentation is extremely challenging due to the high computational memory\ndemands. Typically, images are uniformly downsized at the price of losing fine\nlocal details. Contrarily, breaking the images into multiple smaller patches\ncan cause a loss of global contextual in-formation. We propose a hybrid\nstrategy that can adapt to different inspections tasks by managing the global\nand local semantics trade-off. The framework comprises a compound,\nhigh-resolution deep learning architecture equipped with an attention-based\nsegmentation model and learnable downsampler-upsampler modules designed for\noptimal efficiency and in-formation retention. The framework also utilizes\nvision transformers on a grid of image crops aiming for high precision learning\nwithout downsizing. An augmented inference technique is used to boost the\nperformance and re-duce the possible loss of context due to grid cropping.\nComprehensive experiments have been performed on 3D physics-based graphics\nmodels synthetic environments in the Quake City dataset. The proposed framework\nis evaluated using several metrics on three segmentation tasks: component type,\ncomponent damage state, and global damage (crack, rebar, spalling).",
    "descriptor": "",
    "authors": [
      "Kareem Eltouny",
      "Seyedomid Sajedi",
      "Xiao Liang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12175"
  },
  {
    "id": "arXiv:2210.12183",
    "title": "Weighted Coordinates Poset Block Codes",
    "abstract": "Given $[n]=\\{1,2,\\ldots,n\\}$, a partial order $\\preceq$ on $[n]$, a label map\n$\\pi : [n] \\rightarrow \\mathbb{N}$ defined by $\\pi(i) = k_i$ with\n$\\sum_{i=1}^{n}\\pi (i) = N$, the direct sum $ \\mathbb{F}_{q}^{k_1} \\oplus\n\\mathbb{F}_{q}^{k_2}\\oplus \\ldots \\oplus \\mathbb{F}_{q}^{k_n} $ of $\n\\mathbb{F}_q^N $, and a weight function $w$ on $ \\mathbb{F}_q $, we define a\nposet block metric $d_{(P,w,\\pi)}$ on $\\mathbb{F}_{q}^{N}$ based on the poset\n$P=([n],\\preceq)$. The metric $d_{(P,w,\\pi)}$ is said to be weighted\ncoordinates poset block metric ($(P,w,\\pi)$-metric). It extends the weighted\ncoordinates poset metric ($(P,w)$-metric) introduced by L. Panek and J. A.\nPinheiro and generalizes the poset block metric ($(P,\\pi)$-metric) introduced\nby M. M. S. Alves et al. We determine the complete weight distribution of a\n$(P,w,\\pi)$-space, thereby obtaining it for $(P,w)$-space, $(P,\\pi)$-space,\n$\\pi$-space, and $P$-space as special cases. We obtain the Singleton bound for\n$(P,w,\\pi)$-codes and for $(P,w)$-codes as well. In particular, we re-obtain\nthe Singleton bound for any code with respect to $(P,\\pi)$-metric and\n$P$-metric. Moreover, packing radius and Singleton bound for NRT block codes\nare found.",
    "descriptor": "\nComments: 18 PAGES\n",
    "authors": [
      "Atul Kumar Shriwastva",
      "R. S. Selvaraj"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12183"
  },
  {
    "id": "arXiv:2210.12194",
    "title": "GraphNeT: Graph neural networks for neutrino telescope event  reconstruction",
    "abstract": "GraphNeT is an open-source python framework aimed at providing high quality,\nuser friendly, end-to-end functionality to perform reconstruction tasks at\nneutrino telescopes using graph neural networks (GNNs). GraphNeT makes it fast\nand easy to train complex models that can provide event reconstruction with\nstate-of-the-art performance, for arbitrary detector configurations, with\ninference times that are orders of magnitude faster than traditional\nreconstruction techniques. GNNs from GraphNeT are flexible enough to be applied\nto data from all neutrino telescopes, including future projects such as IceCube\nextensions or P-ONE. This means that GNN-based reconstruction can be used to\nprovide state-of-the-art performance on most reconstruction tasks in neutrino\ntelescopes, at real-time event rates, across experiments and physics analyses,\nwith vast potential impact for neutrino and astro-particle physics.",
    "descriptor": "\nComments: 6 pages, 1 figure. Code can be found at this https URL . Submitted to the Journal of Open Source Software (JOSS)\n",
    "authors": [
      "Andreas S\u00f8gaard",
      "Rasmus F. \u00d8rs\u00f8e",
      "Leon Bozianu",
      "Morten Holm",
      "Kaare Endrup Iversen",
      "Tim Guggenmos",
      "Martin Ha Minh",
      "Philipp Eller",
      "Troels C. Petersen"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2210.12194"
  },
  {
    "id": "arXiv:2210.12235",
    "title": "Sequential Gradient Descent and Quasi-Newton's Method for Change-Point  Analysis",
    "abstract": "One common approach to detecting change-points is minimizing a cost function\nover possible numbers and locations of change-points. The framework includes\nseveral well-established procedures, such as the penalized likelihood and\nminimum description length. Such an approach requires finding the cost value\nrepeatedly over different segments of the data set, which can be time-consuming\nwhen (i) the data sequence is long and (ii) obtaining the cost value involves\nsolving a non-trivial optimization problem. This paper introduces a new\nsequential method (SE) that can be coupled with gradient descent (SeGD) and\nquasi-Newton's method (SeN) to find the cost value effectively. The core idea\nis to update the cost value using the information from previous steps without\nre-optimizing the objective function. The new method is applied to change-point\ndetection in generalized linear models and penalized regression. Numerical\nstudies show that the new approach can be orders of magnitude faster than the\nPruned Exact Linear Time (PELT) method without sacrificing estimation accuracy.",
    "descriptor": "",
    "authors": [
      "Xianyang Zhang",
      "Trisha Dawn"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12235"
  },
  {
    "id": "arXiv:2210.12236",
    "title": "Uncertain Evidence in Probabilistic Models and Stochastic Simulators",
    "abstract": "We consider the problem of performing Bayesian inference in probabilistic\nmodels where observations are accompanied by uncertainty, referred to as\n`uncertain evidence'. In many real-world scenarios, such uncertainty stems from\nmeasurement errors associated with observable quantities in probabilistic\nmodels. We explore how to interpret uncertain evidence, and by extension the\nimportance of proper interpretation as it pertains to inference about latent\nvariables. We consider a recently-proposed method `stochastic evidence' as well\nas revisit two older methods: Jeffrey's rule and virtual evidence. We devise\nconcrete guidelines on how to account for uncertain evidence and we provide new\ninsights, particularly regarding consistency. To showcase the impact of\ndifferent interpretations of the same uncertain evidence, we carry out\nexperiments in which we compare inference results associated with each\ninterpretation.",
    "descriptor": "",
    "authors": [
      "Andreas Munk",
      "Alexander Mead",
      "Frank Wood"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12236"
  },
  {
    "id": "arXiv:2210.12243",
    "title": "Rainbow cycles for matchings, single edges, and triangles",
    "abstract": "Given a family $\\mathcal{F}=(F_1, \\ldots,F_m)$ of sets of edges, a set $F$ of\nedges is said to be rainbow for $\\mathcal{F}$ if each of the edges in $F$ is\ntaken from a distinct $F_i$. The rainbow girth $rgirth(\\mathcal{F})$ of\n$\\mathcal{F}$ is the minimal length of a cycle that is rainbow with respect to\n$\\mathcal{F}$.\nAs a generalization of the famous Caccetta-H\\\"aggkvist conjecture, Aharoni\nconjectured that if in the above $m=n$, $F_i \\subseteq E(K_n)$ and $|F_i|=k$\nfor all $1\\le i \\le n$, then $rgirth (\\mathcal{F}) \\le \\lceil \\frac{n}{k}\n\\rceil$. In \\cite{AharoniGuo} it was shown that if all sets $F_i$ are matchings\nof size $2$ then $rgirth(\\mathcal{F})=O(\\log n)$, and in \\cite{ABCGZ2022} the\nsame was shown when all sets $F_i$ are triangles. In this note we study the\nmixed case: assuming that $\\alpha n$ sets $F_i$ are matchings of size 2, $\\beta\nn$ are triangles, and $\\gamma n$ are single edges -- in what range of values of\n$(\\alpha, \\beta, \\gamma)$, $rgirth(\\mathcal{F})$ is logarithmic?",
    "descriptor": "\nComments: 7 pages\n",
    "authors": [
      "He Guo"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.12243"
  },
  {
    "id": "arXiv:2210.12272",
    "title": "Implicit Offline Reinforcement Learning via Supervised Learning",
    "abstract": "Offline Reinforcement Learning (RL) via Supervised Learning is a simple and\neffective way to learn robotic skills from a dataset collected by policies of\ndifferent expertise levels. It is as simple as supervised learning and Behavior\nCloning (BC), but takes advantage of return information. On datasets collected\nby policies of similar expertise, implicit BC has been shown to match or\noutperform explicit BC. Despite the benefits of using implicit models to learn\nrobotic skills via BC, offline RL via Supervised Learning algorithms have been\nlimited to explicit models. We show how implicit models can leverage return\ninformation and match or outperform explicit algorithms to acquire robotic\nskills from fixed datasets. Furthermore, we show the close relationship between\nour implicit methods and other popular RL via Supervised Learning algorithms to\nprovide a unified framework. Finally, we demonstrate the effectiveness of our\nmethod on high-dimension manipulation and locomotion tasks.",
    "descriptor": "",
    "authors": [
      "Alexandre Piche",
      "Rafael Pardinas",
      "David Vazquez",
      "Igor Mordatch",
      "Chris Pal"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12272"
  },
  {
    "id": "arXiv:2210.12289",
    "title": "Complexity and Ramsey Largeness of Sets of Oracles Separating Complexity  Classes",
    "abstract": "We prove two sets of results concerning computational complexity classes. The\nfirst concerns a variation of the random oracle hypothesis posed by Bennett and\nGill after they showed that relative to a randomly chosen oracle, P not equal\nNP with probability 1. This hypothesis was quickly disproven in several ways,\nmost famously in 1992 with the result that IP equals PSPACE, in spite of the\nclasses being shown unequal with probability 1. Here we propose a variation of\nwhat it means to be ``large'' using the Ellentuck topology. In this new\ncontext, we demonstrate that the set of oracles separating NP and co-NP is not\nsmall, and obtain similar results for the separation of PSPACE from PH along\nwith the separation of NP from BQP. We demonstrate that this version of the\nhypothesis turns it into a sufficient condition for unrelativized\nrelationships, at least in the three cases considered here. Second, we example\nthe descriptive complexity of the classes of oracles providing the separations\nfor these various classes, and determine their exact placement in the Borel\nhierarchy.",
    "descriptor": "",
    "authors": [
      "Alex Creiner",
      "Stephen Jackson"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.12289"
  },
  {
    "id": "arXiv:2210.12293",
    "title": "DL-Corrector-Remapper: A grid-free bias-correction deep learning  methodology for data-driven high-resolution global weather forecasting",
    "abstract": "Data-driven models, such as FourCastNet (FCN), have shown exemplary\nperformance in high-resolution global weather forecasting. This performance,\nhowever, is based on supervision on mesh-gridded weather data without the\nutilization of raw climate observational data, the gold standard ground truth.\nIn this work we develop a methodology to correct, remap, and fine-tune gridded\nuniform forecasts of FCN so it can be directly compared against observational\nground truth, which is sparse and non-uniform in space and time. This is akin\nto bias correction and post-processing of numerical weather prediction (NWP), a\nroutine operation at meteorological and weather forecasting centers across the\nglobe. The Adaptive Fourier Neural Operator (AFNO) architecture is used as the\nbackbone to learn continuous representations of the atmosphere. The spatially\nand temporally non-uniform output is evaluated by the non-uniform discrete\ninverse Fourier transform (NUIDFT) given the output query locations. We call\nthis network the Deep-Learning-Corrector-Remapper (DLCR). The improvement in\nDLCR's performance against the gold standard ground truth over the baseline's\nperformance shows its potential to correct, remap, and fine-tune the\nmesh-gridded forecasts under the supervision of observations.",
    "descriptor": "",
    "authors": [
      "Tao Ge",
      "Jaideep Pathak",
      "Akshay Subramaniam",
      "Karthik Kashinath"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12293"
  },
  {
    "id": "arXiv:2210.12331",
    "title": "Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from  Brain MRIs",
    "abstract": "Alzheimer's disease (AD) is a neuro-degenerative disease that can cause\ndementia and result severe reduction in brain function inhibiting simple tasks\nespecially if no preventative care is taken. Over 1 in 9 Americans suffer from\nAD induced dementia and unpaid care for people with AD related dementia is\nvalued at $271.6 billion. In this paper, we first review other approaches that\ncould be used for early detection of AD. We then give an overview of our\ndataset that was from the Alzheimer's Disease Neuroimaging Initiative (ADNI)\nand propose a deep Convolutional Neural Network (CNN) architecture consisting\nof 7,866,819 parameters. This model has three different length convolutional\nbranches each comprised of different kernel sizes that can predict whether a\npatient is non-demented, mild-demented, or moderately-demented with a 99.05%\nthree class accuracy.",
    "descriptor": "\nComments: 9 pages, 5 figures\n",
    "authors": [
      "Paul K. Mandal",
      "Rakesh Mahto"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12331"
  },
  {
    "id": "arXiv:2210.12334",
    "title": "Adaptive Data Fusion for Multi-task Non-smooth Optimization",
    "abstract": "We study the problem of multi-task non-smooth optimization that arises\nubiquitously in statistical learning, decision-making and risk management. We\ndevelop a data fusion approach that adaptively leverages commonalities among a\nlarge number of objectives to improve sample efficiency while tackling their\nunknown heterogeneities. We provide sharp statistical guarantees for our\napproach. Numerical experiments on both synthetic and real data demonstrate\nsignificant advantages of our approach over benchmarks.",
    "descriptor": "\nComments: 25 pages\n",
    "authors": [
      "Henry Lam",
      "Kaizheng Wang",
      "Yuhang Wu",
      "Yichen Zhang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.12334"
  },
  {
    "id": "arXiv:2210.12343",
    "title": "Stochastic Qubit Resource Allocation for Quantum Cloud Computing",
    "abstract": "Quantum cloud computing is a promising paradigm for efficiently provisioning\nquantum resources (i.e., qubits) to users. In quantum cloud computing, quantum\ncloud providers provision quantum resources in reservation and on-demand plans\nfor users. Literally, the cost of quantum resources in the reservation plan is\nexpected to be cheaper than the cost of quantum resources in the on-demand\nplan. However, quantum resources in the reservation plan have to be reserved in\nadvance without information about the requirement of quantum circuits\nbeforehand, and consequently, the resources are insufficient, i.e.,\nunder-reservation. Hence, quantum resources in the on-demand plan can be used\nto compensate for the unsatisfied quantum resources required. To end this, we\npropose a quantum resource allocation for the quantum cloud computing system in\nwhich quantum resources and the minimum waiting time of quantum circuits are\njointly optimized. Particularly, the objective is to minimize the total costs\nof quantum circuits under uncertainties regarding qubit requirement and minimum\nwaiting time of quantum circuits. In experiments, practical circuits of quantum\nFourier transform are applied to evaluate the proposed qubit resource\nallocation. The results illustrate that the proposed qubit resource allocation\ncan achieve the optimal total costs.",
    "descriptor": "\nComments: 8 pages, 10 figures, conference\n",
    "authors": [
      "Rakpong Kaewpuang",
      "Minrui Xu",
      "Dusit Niyato",
      "Han Yu",
      "Zehui Xiong",
      "Jiawen Kang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.12343"
  },
  {
    "id": "arXiv:2210.12361",
    "title": "MS-DC-UNeXt: An MLP-based Multi-Scale Feature Learning Framework For  X-ray Images",
    "abstract": "The advancement of deep learning theory and infrastructure is crucial in the\nprogress of automatic segmentation techniques. Compared with traditional\nsegmentation methods, automatic segmentation methods have considerable\nstrengths such as convenience, accuracy, and so on. However, the drawbacks\ncannot be neglected. In the laboratory environment, most of the segmentation\nframeworks are based on deep learning at the cost of sacrificing the\nlightweight network architecture, adding a lot of parameters in the network to\ntrade for excellent segmentation accuracy. In practical clinical applications,\nthe lack of high computing performance (GPU) machines to maintain operational\nefficiency poses a huge challenge for the migration from laboratory to clinic.\nRecently, an alternative to the CNN and Transformer frameworks has been\nenthusiastically touted, with MLP-based network parameters being significantly\ndecreased as all parameters are learned in the linear layer of the MLP and\ngenerate striking outcomes similar to both. Inspired by the MLP-based\nframework, we recommend leveraging the MS-DC-UNeXt as an alternative solution\nfor medical image segmentation, which is mainly composed of Tokenized MLP\nblock, Dual Channel block(DC-block), and Bottleneck (Res-ASPP). Please refer to\nthe paper for the complete abstract",
    "descriptor": "",
    "authors": [
      "Yuanyuan Jia",
      "Xiaoyu Pan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12361"
  },
  {
    "id": "arXiv:2210.12363",
    "title": "Bayesian Convolutional Deep Sets with Task-Dependent Stationary Prior",
    "abstract": "Convolutional deep sets are the architecture of a deep neural network (DNN)\nthat can model stationary stochastic process. This architecture uses the kernel\nsmoother and the DNN to construct the translation equivariant functional\nrepresentations, and thus reflects the inductive bias of the stationarity into\nDNN. However, since this architecture employs the kernel smoother known as the\nnon-parametric model, it may produce ambiguous representations when the number\nof data points is not given sufficiently. To remedy this issue, we introduce\nBayesian convolutional deep sets that construct the random translation\nequivariant functional representations with stationary prior. Furthermore, we\npresent how to impose the task-dependent prior for each dataset because a\nwrongly imposed prior forms an even worse representation than that of the\nkernel smoother. We validate the proposed architecture and its training on\nvarious experiments with time-series and image datasets.",
    "descriptor": "\nComments: 13 pages, 7 figures\n",
    "authors": [
      "Yohan Jung",
      "Jinkyoo Park"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.12363"
  },
  {
    "id": "arXiv:2210.12385",
    "title": "Deep Learning in Single-Cell Analysis",
    "abstract": "Single-cell technologies are revolutionizing the entire field of biology. The\nlarge volumes of data generated by single-cell technologies are\nhigh-dimensional, sparse, heterogeneous, and have complicated dependency\nstructures, making analyses using conventional machine learning approaches\nchallenging and impractical. In tackling these challenges, deep learning often\ndemonstrates superior performance compared to traditional machine learning\nmethods. In this work, we give a comprehensive survey on deep learning in\nsingle-cell analysis. We first introduce background on single-cell technologies\nand their development, as well as fundamental concepts of deep learning\nincluding the most popular deep architectures. We present an overview of the\nsingle-cell analytic pipeline pursued in research applications while noting\ndivergences due to data sources or specific applications. We then review seven\npopular tasks spanning through different stages of the single-cell analysis\npipeline, including multimodal integration, imputation, clustering, spatial\ndomain identification, cell-type deconvolution, cell segmentation, and\ncell-type annotation. Under each task, we describe the most recent developments\nin classical and deep learning methods and discuss their advantages and\ndisadvantages. Deep learning tools and benchmark datasets are also summarized\nfor each task. Finally, we discuss the future directions and the most recent\nchallenges. This survey will serve as a reference for biologists and computer\nscientists, encouraging collaborations.",
    "descriptor": "\nComments: 77 pages, 11 figures, 15 tables, deep learning, single-cell analysis\n",
    "authors": [
      "Dylan Molho",
      "Jiayuan Ding",
      "Zhaoheng Li",
      "Hongzhi Wen",
      "Wenzhuo Tang",
      "Yixin Wang",
      "Julian Venegas",
      "Wei Jin",
      "Renming Liu",
      "Runze Su",
      "Patrick Danaher",
      "Robert Yang",
      "Yu Leo Lei",
      "Yuying Xie",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.12385"
  },
  {
    "id": "arXiv:2210.12388",
    "title": "Diversity-Promoting Ensemble for Medical Image Segmentation",
    "abstract": "Medical image segmentation is an actively studied task in medical imaging,\nwhere the precision of the annotations is of utter importance towards accurate\ndiagnosis and treatment. In recent years, the task has been approached with\nvarious deep learning systems, among the most popular models being U-Net. In\nthis work, we propose a novel strategy to generate ensembles of different\narchitectures for medical image segmentation, by leveraging the diversity\n(decorrelation) of the models forming the ensemble. More specifically, we\nutilize the Dice score among model pairs to estimate the correlation between\nthe outputs of the two models forming each pair. To promote diversity, we\nselect models with low Dice scores among each other. We carry out\ngastro-intestinal tract image segmentation experiments to compare our\ndiversity-promoting ensemble (DiPE) with another strategy to create ensembles\nbased on selecting the top scoring U-Net models. Our empirical results show\nthat DiPE surpasses both individual models as well as the ensemble creation\nstrategy based on selecting the top scoring models.",
    "descriptor": "",
    "authors": [
      "Mariana-Iuliana Georgescu",
      "Radu Tudor Ionescu",
      "Andreea-Iuliana Miron"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12388"
  },
  {
    "id": "arXiv:2210.12392",
    "title": "Testing Independence of Exchangeable Random Variables",
    "abstract": "Given well-shuffled data, can we determine whether the data items are\nstatistically (in)dependent? Formally, we consider the problem of testing\nwhether a set of exchangeable random variables are independent. We will show\nthat this is possible and develop tests that can confidently reject the null\nhypothesis that data is independent and identically distributed and have high\npower for (some) exchangeable distributions. We will make no structural\nassumptions on the underlying sample space. One potential application is in\nDeep Learning, where data is often scraped from the whole internet, with\nduplications abound, which can render data non-iid and test-set evaluation\nprone to give wrong answers.",
    "descriptor": "\nComments: 42 pages, 3 figures\n",
    "authors": [
      "Marcus Hutter"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12392"
  },
  {
    "id": "arXiv:2210.12458",
    "title": "Faster and more diverse de novo molecular optimization with double-loop  reinforcement learning using augmented SMILES",
    "abstract": "Molecular generation via deep learning models in combination with\nreinforcement learning is a powerful way of generating proposed molecules with\ndesirable properties. By defining a multi-objective scoring function, it is\npossible to generate thousands of ideas for molecules that scores well, which\nmakes the approach interesting for drug discovery or material science purposes.\nHowever, if the scoring function is expensive regarding resources, such as time\nor computation, the high number of function evaluations needed for feedback in\nthe reinforcement learning loop becomes a bottleneck. Here we propose to use\ndouble-loop reinforcement learning with simplified molecular line entry system\n(SMILES) augmentation to use scoring calculations more efficiently and arrive\nat well scoring molecules faster. By adding an inner loop where the SMILES\nstrings generated are augmented to alternative non-canonical SMILES and used\nfor additional rounds of reinforcement learning, we can effectively reuse the\nscoring calculations that are done on the molecular level. This approach speeds\nup the learning process regarding scoring function calls, as well as it\nprotects moderately against mode collapse. We find that augmentation repeats\nbetween 5-10x seem safe for most scoring functions and additionally increase\nthe diversity of the generated compounds, as well as making the sampling runs\nof chemical space more reproducible",
    "descriptor": "\nComments: 23 pages and 18 Figures. Supplementary material appended\n",
    "authors": [
      "Esben Jannik Bjerrum",
      "Christian Margreitter",
      "Thomas Blaschke",
      "Raquel Lopez-Rios de Castro"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12458"
  },
  {
    "id": "arXiv:2210.12462",
    "title": "Factor Investing with a Deep Multi-Factor Model",
    "abstract": "Modeling and characterizing multiple factors is perhaps the most important\nstep in achieving excess returns over market benchmarks. Both academia and\nindustry are striving to find new factors that have good explanatory power for\nfuture stock returns and good stability of their predictive power. In practice,\nfactor investing is still largely based on linear multi-factor models, although\nmany deep learning methods show promising results compared to traditional\nmethods in stock trend prediction and portfolio risk management. However, the\nexisting non-linear methods have two drawbacks: 1) there is a lack of\ninterpretation of the newly discovered factors, 2) the financial insights\nbehind the mining process are unclear, making practitioners reluctant to apply\nthe existing methods to factor investing. To address these two shortcomings, we\ndevelop a novel deep multi-factor model that adopts industry neutralization and\nmarket neutralization modules with clear financial insights, which help us\neasily build a dynamic and multi-relational stock graph in a hierarchical\nstructure to learn the graph representation of stock relationships at different\nlevels, e.g., industry level and universal level. Subsequently, graph attention\nmodules are adopted to estimate a series of deep factors that maximize the\ncumulative factor returns. And a factor-attention module is developed to\napproximately compose the estimated deep factors from the input factors, as a\nway to interpret the deep factors explicitly. Extensive experiments on\nreal-world stock market data demonstrate the effectiveness of our deep\nmulti-factor model in the task of factor investing.",
    "descriptor": "\nComments: 8 pages, 4 figures\n",
    "authors": [
      "Zikai Wei",
      "Bo Dai",
      "Dahua Lin"
    ],
    "subjectives": [
      "Computational Finance (q-fin.CP)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12462"
  },
  {
    "id": "arXiv:2210.12497",
    "title": "Deep Linear Networks for Matrix Completion -- An Infinite Depth Limit",
    "abstract": "The deep linear network (DLN) is a model for implicit regularization in\ngradient based optimization of overparametrized learning architectures.\nTraining the DLN corresponds to a Riemannian gradient flow, where the\nRiemannian metric is defined by the architecture of the network and the loss\nfunction is defined by the learning task. We extend this geometric framework,\nobtaining explicit expressions for the volume form, including the case when the\nnetwork has infinite depth. We investigate the link between the Riemannian\ngeometry and the training asymptotics for matrix completion with rigorous\nanalysis and numerics. We propose that implicit regularization is a result of\nbias towards high state space volume.",
    "descriptor": "",
    "authors": [
      "Nadav Cohen",
      "Govind Menon",
      "Zsolt Veraszto"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12497"
  },
  {
    "id": "arXiv:2210.12503",
    "title": "An Axiomatic Characterization of Split Cycle",
    "abstract": "A number of rules for resolving majority cycles in elections have been\nproposed in the literature. Recently, Holliday and Pacuit (Journal of\nTheoretical Politics 33 (2021) 475-524) axiomatically characterized one such\ncycle-resolving rule, dubbed Split Cycle: in each majority cycle, discard the\nmajority preferences with the smallest majority margin. They showed that any\nrule satisfying five standard axioms, plus a weakening of Arrow's Independence\nof Irrelevant Alternatives (IIA) called Coherent IIA, is refined by Split\nCycle. In this paper, we go further and show that Split Cycle is the only rule\nsatisfying the axioms of Holliday and Pacuit together with two additional\naxioms: Coherent Defeat and Positive Involvement in Defeat. Coherent Defeat\nstates that any majority preference not occurring in a cycle is retained, while\nPositive Involvement in Defeat is closely related to the well-known axiom of\nPositive Involvement (as in J. P\\'{e}rez, Social Choice and Welfare 18 (2001)\n601-616). We characterize Split Cycle not only as a collective choice rule but\nalso as a social choice correspondence, over both profiles of linear ballots\nand profiles of ballots allowing ties.",
    "descriptor": "\nComments: 32 pages, 6 figures\n",
    "authors": [
      "Yifeng Ding",
      "Wesley H. Holliday",
      "Eric Pacuit"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.12503"
  },
  {
    "id": "arXiv:2210.12516",
    "title": "Computing Klein-Gordon Spectra",
    "abstract": "We study the computational complexity of the eigenvalue problem for the\nKlein-Gordon equation in the framework of the Solvability Complexity Index\nHierarchy. We prove that the eigenvalue of the Klein-Gordon equation with\nlinearly decaying potential can be computed in a single limit with guaranteed\nerror bounds from above. The proof is constructive, i.e. we obtain a numerical\nalgorithm that can be implemented on a computer. Moreover, we prove abstract\nenclosures for the point spectrum of the Klein-Gordon equation and we compare\nour numerical results to these enclosures. Finally, we apply both the\nimplemented algorithm and our abstract enclosures to several physically\nrelevant potentials such as Sauter and cusp potentials and we provide a\nconvergence and error analysis.",
    "descriptor": "\nComments: 29 pages, 11 figures\n",
    "authors": [
      "Frank R\u00f6sler",
      "Christiane Tretter"
    ],
    "subjectives": [
      "Spectral Theory (math.SP)",
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.12516"
  },
  {
    "id": "arXiv:2210.12532",
    "title": "Deep domain adaptation for polyphonic melody extraction",
    "abstract": "Extraction of the predominant pitch from polyphonic audio is one of the\nfundamental tasks in the field of music information retrieval and computational\nmusicology. To accomplish this task using machine learning, a large amount of\nlabeled audio data is required to train the model that predicts the pitch\ncontour. But a classical model pre-trained on data from one domain (source),\ne.g, songs of a particular singer or genre, may not perform comparatively well\nin extracting melody from other domains (target). The performance of such\nmodels can be boosted by adapting the model using some annotated data in the\ntarget domain. In this work, we study various adaptation techniques applied to\nmachine learning models for polyphonic melody extraction. Experimental results\nshow that meta-learning-based adaptation performs better than simple\nfine-tuning. In addition to this, we find that this method outperforms the\nexisting state-of-the-art non-adaptive polyphonic melody extraction algorithms.",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Kavya Ranjan Saxena",
      "Vipul Arora"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.12532"
  },
  {
    "id": "arXiv:2210.12548",
    "title": "JoJoNet: Joint-contrast and Joint-sampling-and-reconstruction Network  for Multi-contrast MRI",
    "abstract": "Multi-contrast Magnetic Resonance Imaging (MRI) generates multiple medical\nimages with rich and complementary information for routine clinical use;\nhowever, it suffers from a long acquisition time. Recent works for accelerating\nMRI, mainly designed for single contrast, may not be optimal for multi-contrast\nscenario since the inherent correlations among the multi-contrast images are\nnot exploited. In addition, independent reconstruction of each contrast usually\ndoes not translate to optimal performance of downstream tasks. Motivated by\nthese aspects, in this paper we design an end-to-end framework for accelerating\nmulti-contrast MRI which simultaneously optimizes the entire MR imaging\nworkflow including sampling, reconstruction and downstream tasks to achieve the\nbest overall outcomes. The proposed framework consists of a sampling mask\ngenerator for each image contrast and a reconstructor exploiting the\ninter-contrast correlations with a recurrent structure which enables the\ninformation sharing in a holistic way. The sampling mask generator and the\nreconstructor are trained jointly across the multiple image contrasts. The\nacceleration ratio of each image contrast is also learnable and can be driven\nby a downstream task performance. We validate our approach on a multi-contrast\nbrain dataset and a multi-contrast knee dataset. Experiments show that (1) our\nframework consistently outperforms the baselines designed for single contrast\non both datasets; (2) our newly designed recurrent reconstruction network\neffectively improves the reconstruction quality for multi-contrast images; (3)\nthe learnable acceleration ratio improves the downstream task performance\nsignificantly. Overall, this work has potentials to open up new avenues for\noptimizing the entire multi-contrast MR imaging workflow.",
    "descriptor": "",
    "authors": [
      "Lin Zhao",
      "Xiao Chen",
      "Eric Z. Chen",
      "Yikang Liu",
      "Dinggang Shen",
      "Terrence Chen",
      "Shanhui Sun"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12548"
  },
  {
    "id": "arXiv:2210.12578",
    "title": "Feedback Assisted Adversarial Learning to Improve the Quality of  Cone-beam CT Images",
    "abstract": "Unsupervised image translation using adversarial learning has been attracting\nattention to improve the image quality of medical images. However, adversarial\ntraining based on the global evaluation values of discriminators does not\nprovide sufficient translation performance for locally different image\nfeatures. We propose adversarial learning with a feedback mechanism from a\ndiscriminator to improve the quality of CBCT images. This framework employs\nU-net as the discriminator and outputs a probability map representing the local\ndiscrimination results. The probability map is fed back to the generator and\nused for training to improve the image translation. Our experiments using 76\ncorresponding CT-CBCT images confirmed that the proposed framework could\ncapture more diverse image features than conventional adversarial learning\nframeworks and produced synthetic images with pixel values close to the\nreference image and a correlation coefficient of 0.93.",
    "descriptor": "",
    "authors": [
      "Takumi Hase",
      "Megumi Nakao",
      "Mitsuhiro Nakamura",
      "Tetsuya Matsuda"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12578"
  },
  {
    "id": "arXiv:2210.12643",
    "title": "Finding matchings in dense hypergraphs",
    "abstract": "We consider the algorithmic decision problem that takes as input an\n$n$-vertex $k$-uniform hypergraph $H$ with minimum codegree at least $m-c$ and\ndecides whether it has a matching of size $m$. We show that this decision\nproblem is fixed parameter tractable with respect to $c$. Furthermore, our\nalgorithm not only decides the problem, but actually either finds a matching of\nsize $m$ or a certificate that no such matching exists. In particular, when\n$m=n/k$ and $c=O(\\log n)$, this gives a polynomial-time algorithm, that given\nany $n$-vertex $k$-uniform hypergraph $H$ with minimum codegree at least\n$n/k-c$, finds either a perfect matching in $H$ or a certificate that no\nperfect matching exists.",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Jie Han",
      "Peter Keevash"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.12643"
  },
  {
    "id": "arXiv:2210.12675",
    "title": "The geodesic cover problem for butterfly networks",
    "abstract": "A geodesic cover, also known as an isometric path cover, of a graph is a set\nof geodesics which cover the vertex set of the graph. An edge geodesic cover of\na graph is a set of geodesics which cover the edge set of the graph. The\ngeodesic (edge) cover number of a graph is the cardinality of a minimum (edge)\ngeodesic cover. The (edge) geodesic cover problem of a graph is to find the\n(edge) geodesic cover number of the graph. Surprisingly, only partial solutions\nfor these problems are available for most situations. In this paper we\ndemonstrate that the geodesic cover number of the $r$-dimensional butterfly is\n$\\lceil (2/3)2^r\\rceil$ and that its edge geodesic cover number is $2^r$.",
    "descriptor": "",
    "authors": [
      "Paul Manuel",
      "Sandi Klavzar",
      "R. Prabha",
      "Andrew Arokiaraj"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.12675"
  },
  {
    "id": "arXiv:2210.12698",
    "title": "A class of models for random hypergraphs",
    "abstract": "Despite the recently exhibited importance of higher-order interactions for\nvarious processes, few flexible (null) models are available. In particular,\nmost studies on hypergraphs focus on a small set of theoretical models. Here,\nwe introduce a class of models for random hypergraphs which displays a similar\nlevel of flexibility of complex network models and where the main ingredient is\nthe probability that a node belongs to a hyperedge. When this probability is a\nconstant, we obtain a random hypergraph in the same spirit as the Erdos-Renyi\ngraph. This framework also allows us to introduce different ingredients such as\nthe preferential attachment for hypergraphs, or spatial random hypergraphs. In\nparticular, we show that for the Erdos-Renyi case there is a transition\nthreshold scaling as $1/\\sqrt{EN}$ where $N$ is the number of nodes and $E$ the\nnumber of hyperedges. We also discuss a random geometric hypergraph which\ndisplays a percolation transition for a threshold distance scaling as\n$r_c^*\\sim 1/\\sqrt{E}$. For these various models, we provide results for the\nmost interesting measures, and also introduce new ones in the spatial case for\ncharacterizing the geometrical properties of hyperedges. These different models\nmight serve as benchmarks useful for analyzing empirical data.",
    "descriptor": "\nComments: 10 pages, 10 figures\n",
    "authors": [
      "Marc Barthelemy"
    ],
    "subjectives": [
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Social and Information Networks (cs.SI)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.12698"
  },
  {
    "id": "arXiv:2210.12701",
    "title": "Speaker Identification from emotional and noisy speech data using  learned voice segregation and Speech VGG",
    "abstract": "Speech signals are subjected to more acoustic interference and emotional\nfactors than other signals. Noisy emotion-riddled speech data is a challenge\nfor real-time speech processing applications. It is essential to find an\neffective way to segregate the dominant signal from other external influences.\nAn ideal system should have the capacity to accurately recognize required\nauditory events from a complex scene taken in an unfavorable situation. This\npaper proposes a novel approach to speaker identification in unfavorable\nconditions such as emotion and interference using a pre-trained Deep Neural\nNetwork mask and speech VGG. The proposed model obtained superior performance\nover the recent literature in English and Arabic emotional speech data and\nreported an average speaker identification rate of 85.2\\%, 87.0\\%, and 86.6\\%\nusing the Ryerson audio-visual dataset (RAVDESS), speech under simulated and\nactual stress (SUSAS) dataset and Emirati-accented Speech dataset (ESD)\nrespectively.",
    "descriptor": "\nComments: Journal\n",
    "authors": [
      "Shibani Hamsa",
      "Ismail Shahin",
      "Youssef Iraqi",
      "Ernesto Damiani",
      "Naoufel Werghi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.12701"
  },
  {
    "id": "arXiv:2210.12703",
    "title": "Transformations for accelerator-based quantum circuit simulation in  Haskell",
    "abstract": "For efficient hardware-accelerated simulations of quantum circuits, we can\ndefine hardware-specific quantum-circuit transformations. We use a functional\nprogramming approach to create a quantum-circuit analysis and transformation\nmethod implemented in Haskell. This tool forms a key part of our larger\nquantum-computing simulation toolchain. As an example of hardware acceleration,\nwe discuss FPGA-based simulations of selected quantum arithmetic circuits,\nincluding the transformation steps to optimise the hardware utilisation. Future\ndevelopment steps in the Haskell-based analysis and transformation tool are\noutlined. The described toolchain can be found on GitHub:\nhttps://github.com/DevdudeSami/fqt.",
    "descriptor": "",
    "authors": [
      "Youssef Moawad",
      "Wim Vanderbauwhede",
      "Ren\u00e9 Steijl"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.12703"
  },
  {
    "id": "arXiv:2210.12707",
    "title": "Accelerating the training of single-layer binary neural networks using  the HHL quantum algorithm",
    "abstract": "Binary Neural Networks are a promising technique for implementing efficient\ndeep models with reduced storage and computational requirements. The training\nof these is however, still a compute-intensive problem that grows drastically\nwith the layer size and data input. At the core of this calculation is the\nlinear regression problem. The Harrow-Hassidim-Lloyd (HHL) quantum algorithm\nhas gained relevance thanks to its promise of providing a quantum state\ncontaining the solution of a linear system of equations. The solution is\nencoded in superposition at the output of a quantum circuit. Although this\nseems to provide the answer to the linear regression problem for the training\nneural networks, it also comes with multiple, difficult-to-avoid hurdles. This\npaper shows, however, that useful information can be extracted from the\nquantum-mechanical implementation of HHL, and used to reduce the complexity of\nfinding the solution on the classical side.",
    "descriptor": "\nComments: Accepted in the 40th IEEE International Conference on Computer Design (ICCD'22). 9 pages, 8 figures, IEEEtran V1.8b\n",
    "authors": [
      "Sonia Lopez Alarcon",
      "Cory Merkel",
      "Martin Hoffnagle",
      "Sabrina Ly",
      "Alejandro Pozas-Kerstjens"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12707"
  },
  {
    "id": "arXiv:2210.12723",
    "title": "A Faithful Deep Sensitivity Estimation for Accelerated Magnetic  Resonance Imaging",
    "abstract": "Recent deep learning is superior in providing high-quality images and\nultra-fast reconstructions in accelerated magnetic resonance imaging (MRI).\nFaithful coil sensitivity estimation is vital for MRI reconstruction. However,\nmost deep learning methods still rely on pre-estimated sensitivity maps and\nignore their inaccuracy, resulting in the significant quality degradation of\nreconstructed images. In this work, we propose a Joint Deep Sensitivity\nestimation and Image reconstruction network, called JDSI. During the image\nartifacts removal, it gradually provides more faithful sensitivity maps,\nleading to greatly improved image reconstructions. To understand the behavior\nof the network, the mutual promotion of sensitivity estimation and image\nreconstruction is revealed through the visualization of network intermediate\nresults. Results on in vivo datasets and radiologist reader study demonstrate\nthat, the proposed JDSI achieves the state-of-the-art performance visually and\nquantitatively, especially when the accelerated factor is high. Additionally,\nJDSI owns nice robustness to abnormal subjects and different number of\nautocalibration signals.",
    "descriptor": "\nComments: 10 pages, 8 figures, 3 tables\n",
    "authors": [
      "Zi Wang",
      "Haoming Fang",
      "Chen Qian",
      "Boxuan Shi",
      "Lijun Bao",
      "Liuhong Zhu",
      "Jianjun Zhou",
      "Wenping Wei",
      "Jianzhong Lin",
      "Di Guo",
      "Xiaobo Qu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12723"
  },
  {
    "id": "arXiv:2210.12731",
    "title": "Joint Rigid Motion Correction and Sparse-View CT via Self-Calibrating  Neural Field",
    "abstract": "Neural Radiance Field (NeRF) has widely received attention in Sparse-View\n(SV) CT reconstruction problems as a self-supervised deep learning framework.\nNeRF-based SVCT methods model the desired CT image as a continuous function\nthat maps coordinates to intensities and then train a Multi-Layer Perceptron\n(MLP) to learn the function by minimizing loss on the SV measurement. Thanks to\nthe continuous representation provided by NeRF, the function can be\napproximated well and thus the high-quality CT image is reconstructed. However,\nexisting NeRF-based SVCT methods strictly suppose there is completely no\nrelative motion during the CT acquisition because they require accurate\nprojection poses to simulate the X-rays that scan the SV sinogram. Therefore,\nthese methods suffer from severe performance drops for real SVCT imaging with\nmotion. To this end, this work proposes a self-calibrating neural field that\nrecovers the artifacts-free image from the rigid motion-corrupted SV\nmeasurement without using any external data. Specifically, we parametrize the\ncoarse projection poses caused by rigid motion as trainable variables and then\njointly optimize these variables and the MLP. We perform numerical experiments\non a public COVID-19 CT dataset. The results indicate that our model\nsignificantly outperforms two latest NeRF-based methods for SVCT reconstruction\nwith four different levels of rigid motion.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Qing Wu",
      "Xin Li",
      "Hongjiang Wei",
      "Jingyi Yu",
      "Yuyao Zhang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12731"
  },
  {
    "id": "arXiv:2210.12740",
    "title": "HiFi-WaveGAN: Generative Adversarial Network with Auxiliary  Spectrogram-Phase Loss for High-Fidelity Singing Voice Generation",
    "abstract": "Entertainment-oriented singing voice synthesis (SVS) requires a vocoder to\ngenerate high-fidelity (e.g. 48kHz) audio. However, most text-to-speech (TTS)\nvocoders cannot work well in this scenario even if the neural vocoder for TTS\nhas achieved significant progress. In this paper, we propose HiFi-WaveGAN which\nis designed for synthesizing the 48kHz high-quality singing voices from the\nfull-band mel-spectrogram in real-time. Specifically, it consists of a\ngenerator improved from WaveNet, a multi-period discriminator same to HiFiGAN,\nand a multi-resolution spectrogram discriminator borrowed from UnivNet. To\nbetter reconstruct the high-frequency part from the full-band mel-spectrogram,\nwe design a novel auxiliary spectrogram-phase loss to train the neural network,\nwhich can also accelerate the training process. The experimental result shows\nthat our proposed HiFi-WaveGAN significantly outperforms other neural vocoders\nsuch as Parallel WaveGAN (PWG) and HiFiGAN in the mean opinion score (MOS)\nmetric for the 48kHz SVS task. And a comparative study of HiFi-WaveGAN\nwith/without phase loss term proves that phase loss indeed improves the\ntraining speed. Besides, we also compare the spectrogram generated by our\nHiFi-WaveGAN and PWG, which shows our HiFi-WaveGAN has a more powerful ability\nto model the high-frequency parts.",
    "descriptor": "\nComments: submitted to icassp2023\n",
    "authors": [
      "Chunhui Wang",
      "Chang Zeng",
      "Xing He"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.12740"
  },
  {
    "id": "arXiv:2210.12753",
    "title": "Google's 2019 \"Quantum Supremacy'' Claims: Data, Documentation, and  Discussion",
    "abstract": "In October 2019, \"Nature\" published a paper describing an experimental work\nthat took place at Google. The paper claims to demonstrate quantum\n(computational) supremacy on a 53-qubit quantum computer. Since September 2019\nthe authors have been involved in a long-term project to study various\nstatistical aspects of the Google experiment. In particular, we have been\ntrying to gather the relevant data and information, to reconstruct and verify\nthose parts of the Google 2019 supremacy experiments that are based on\nclassical computations (unless they require too heavy computation), and to put\nthe data under statistical analysis. We have now (August 2022) concluded the\npart relating to the gathering of data and information needed for our study of\nthe 2019 Google experiment, and this document describes the available data and\ninformation for the Google 2019 experiment and some of our results and plans.",
    "descriptor": "\nComments: 32 pages, 2 figures\n",
    "authors": [
      "Gil Kalai",
      "Yosef Rinott",
      "Tomer Shoham"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.12753"
  },
  {
    "id": "arXiv:2210.12767",
    "title": "Falsehoods that ML researchers believe about OOD detection",
    "abstract": "Modelling the density $p(x)$ by probabilistic generative models is an\nintuitive way to detect out-of-distribution (OOD) data, but it fails in the\ndeep learning context. In this paper, we list some falsehoods that machine\nlearning researchers believe about density-based OOD detection. Many recent\nworks have proposed likelihood-ratio-based methods to `fix' this issue. We\npropose a framework, the OOD proxy framework, to unify these methods, and we\nargue that likelihood ratio is a principled method for OOD detection and not a\nmere `fix'. Finally, we discuss the relationship between domain detection and\nsemantics.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Andi Zhang",
      "Damon Wischik"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12767"
  },
  {
    "id": "arXiv:2210.12772",
    "title": "Electroanatomic Mapping to determine Scar Regions in patients with  Atrial Fibrillation",
    "abstract": "Left atrial voltage maps are routinely acquired during electroanatomic\nmapping in patients undergoing catheter ablation for atrial fibrillation. For\npatients, who have prior catheter ablation when they are in sinus rhythm, the\nvoltage map can be used to identify low voltage areas using a threshold of 0.2\n- 0.45 mV.\nHowever, such a voltage threshold for maps acquired during atrial\nfibrillation has not been well established. A prerequisite for defining a\nvoltage threshold is to maximize the topologically matched low voltage areas\nbetween the electroanatomic mapping acquired during atrial fibrillation and\nsinus rhythm. This paper demonstrates a new technique to improve the\nsensitivity and specificity of the matched low voltage areas. This is achieved\nby computing omni-directional bipolar voltages and applying Gaussian Process\nRegression based interpolation to derive the AF map. The proposed method is\nevaluated on a test cohort of 7 male patients, and a total of 46,589 data\npoints were included in analysis. The low voltage areas in the posterior left\natrium and pulmonary vein junction are determined using the standard method and\nthe proposed method. Overall, the proposed method showed patient-specific\nsensitivity and specificity in matching low voltage areas of 75.70% and 65.55%\nfor a geometric mean of 70.69%. On average, there was an improvement of 3.00%\nin the geometric mean, 7.88% improvement in sensitivity, 0.30% improvement in\nspecificity compared to the standard method. The results show that the proposed\nmethod is an improvement in matching low voltage areas. This may help develop\nthe voltage threshold to better identify low voltage areas in the left atrium\nfor patients in atrial fibrillation.",
    "descriptor": "",
    "authors": [
      "Jiyue He",
      "Kuk Jin Jang",
      "Katie Walsh",
      "Jackson Liang",
      "Sanjay Dixit",
      "Rahul Mangharam"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12772"
  },
  {
    "id": "arXiv:2210.12774",
    "title": "Manifold Alignment with Label Information",
    "abstract": "Multi-domain data is becoming increasingly common and presents both\nchallenges and opportunities in the data science community. The integration of\ndistinct data-views can be used for exploratory data analysis, and benefit\ndownstream analysis including machine learning related tasks. With this in\nmind, we present a novel manifold alignment method called MALI (Manifold\nalignment with label information) that learns a correspondence between two\ndistinct domains. MALI can be considered as belonging to a middle ground\nbetween the more commonly addressed semi-supervised manifold alignment problem\nwith some known correspondences between the two domains, and the purely\nunsupervised case, where no known correspondences are provided. To do this,\nMALI learns the manifold structure in both domains via a diffusion process and\nthen leverages discrete class labels to guide the alignment. By aligning two\ndistinct domains, MALI recovers a pairing and a common representation that\nreveals related samples in both domains. Additionally, MALI can be used for the\ntransfer learning problem known as domain adaptation. We show that MALI\noutperforms the current state-of-the-art manifold alignment methods across\nmultiple datasets.",
    "descriptor": "",
    "authors": [
      "Andres F. Duque",
      "Myriam Lizotte",
      "Guy Wolf",
      "Kevin R. Moon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12774"
  },
  {
    "id": "arXiv:2210.12791",
    "title": "O-type Stars Stellar Parameter Estimation Using Recurrent Neural  Networks",
    "abstract": "In this paper, we present a deep learning system approach to estimating\nluminosity, effective temperature, and surface gravity of O-type stars using\nthe optical region of the stellar spectra. In previous work, we compare a set\nof machine learning and deep learning algorithms in order to establish a\nreliable way to fit a stellar model using two methods: the classification of\nthe stellar spectra models and the estimation of the physical parameters in a\nregression-type task. Here we present the process to estimate individual\nphysical parameters from an artificial neural network perspective with the\ncapacity to handle stellar spectra with a low signal-to-noise ratio (S/N), in\nthe $<$20 S/N boundaries. The development of three different recurrent neural\nnetwork systems, the training process using stellar spectra models, the test\nover nine different observed stellar spectra, and the comparison with\nestimations in previous works are presented. Additionally, characterization\nmethods for stellar spectra in order to reduce the dimensionality of the input\ndata for the system and optimize the computational resources are discussed.",
    "descriptor": "",
    "authors": [
      "Miguel Flores R.",
      "Luis J. Corral",
      "Celia R. Fierro-Santill\u00e1n",
      "Silvana G. Navarro"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12791"
  },
  {
    "id": "arXiv:2210.12812",
    "title": "Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning  with Parameter Convergence",
    "abstract": "Multi-agent interactions are increasingly important in the context of\nreinforcement learning, and the theoretical foundations of policy gradient\nmethods have attracted surging research interest. We investigate the global\nconvergence of natural policy gradient (NPG) algorithms in multi-agent\nlearning. We first show that vanilla NPG may not have parameter convergence,\ni.e., the convergence of the vector that parameterizes the policy, even when\nthe costs are regularized (which enabled strong convergence guarantees in the\npolicy space in the literature). This non-convergence of parameters leads to\nstability issues in learning, which becomes especially relevant in the function\napproximation setting, where we can only operate on low-dimensional parameters,\ninstead of the high-dimensional policy. We then propose variants of the NPG\nalgorithm, for several standard multi-agent learning scenarios: two-player\nzero-sum matrix and Markov games, and multi-player monotone games, with global\nlast-iterate parameter convergence guarantees. We also generalize the results\nto certain function approximation settings. Note that in our algorithms, the\nagents take symmetric roles. Our results might also be of independent interest\nfor solving nonconvex-nonconcave minimax optimization problems with certain\nstructures. Simulations are also provided to corroborate our theoretical\nfindings.",
    "descriptor": "\nComments: Initially submitted for publication in January 2022\n",
    "authors": [
      "Sarath Pattathil",
      "Kaiqing Zhang",
      "Asuman Ozdaglar"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.12812"
  },
  {
    "id": "arXiv:2210.12825",
    "title": "Patient-Specific Heart Model Towards Atrial Fibrillation",
    "abstract": "Atrial fibrillation is a heart rhythm disorder that affects tens of millions\npeople worldwide. The most effective treatment is catheter ablation. This\ninvolves irreversible heating of abnormal cardiac tissue facilitated by\nelectroanatomical mapping. However, it is difficult to consistently identify\nthe triggers and sources that may initiate or perpetuate atrial fibrillation\ndue to its chaotic behavior. We developed a patient-specific computational\nheart model that can accurately reproduce the activation patterns to help in\nlocalizing these triggers and sources. Our model has high spatial resolution,\nwith whole-atrium temporal synchronous activity, and has patient-specific\naccurate electrophysiological activation patterns. A total of 15 patients data\nwere processed: 8 in sinus rhythm, 6 in atrial flutter and 1 in atrial\ntachycardia. For resolution, the average simulation geometry voxel is a cube of\n2.47 mm length. For synchrony, the model takes in about 1,500 local electrogram\nrecordings, optimally fits parameters to the individual's atrium geometry and\nthen generates whole-atrium activation patterns. For accuracy, the average\nlocal activation time error is 5.47 ms for sinus rhythm, 10.97 ms for flutter\nand tachycardia; and the average correlation is 0.95 for sinus rhythm, 0.81 for\nflutter and tachycardia. This promising result demonstrates our model is an\neffective building block in capturing more complex rhythms such as atrial\nfibrillation to guide physicians for effective ablation therapy.",
    "descriptor": "",
    "authors": [
      "Jiyue He",
      "Arkady Pertsov",
      "Sanjay Dixit",
      "Katie Walsh",
      "Eric Toolan",
      "Rahul Mangharam"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Image and Video Processing (eess.IV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12825"
  },
  {
    "id": "arXiv:2210.12839",
    "title": "Decentralized Stochastic Bilevel Optimization with Improved  Per-Iteration Complexity",
    "abstract": "Bilevel optimization recently has received tremendous attention due to its\ngreat success in solving important machine learning problems like meta\nlearning, reinforcement learning, and hyperparameter optimization. Extending\nsingle-agent training on bilevel problems to the decentralized setting is a\nnatural generalization, and there has been a flurry of work studying\ndecentralized bilevel optimization algorithms. However, it remains unknown how\nto design the distributed algorithm with sample complexity and convergence rate\ncomparable to SGD for stochastic optimization, and at the same time without\ndirectly computing the exact Hessian or Jacobian matrices. In this paper we\npropose such an algorithm. More specifically, we propose a novel decentralized\nstochastic bilevel optimization (DSBO) algorithm that only requires first order\nstochastic oracle, Hessian-vector product and Jacobian-vector product oracle.\nThe sample complexity of our algorithm matches the currently best known results\nfor DSBO, and the advantage of our algorithm is that it does not require\nestimating the full Hessian and Jacobian matrices, thereby having improved\nper-iteration complexity.",
    "descriptor": "",
    "authors": [
      "Xuxing Chen",
      "Minhui Huang",
      "Shiqian Ma",
      "Krishnakumar Balasubramanian"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12839"
  },
  {
    "id": "arXiv:2210.12840",
    "title": "Radical isogenies and modular curves",
    "abstract": "This article explores the connection between radical isogenies and modular\ncurves. Radical isogenies are formulas introduced by Castryck, Decru, and\nVercauteren at Asiacrypt 2020, designed for the computation of chains of\nisogenies of fixed small degree $N.$ An important advantage of radical isogeny\nformulas over other formulas with a similar purpose, is that there is no need\nto generate a point of order $N$ that generates the kernel of the isogeny.\nRadical isogeny formulas were originally developed using elliptic curves in\nTate normal form, while Onuki and Moriya have proposed radical isogenies\nformulas of degrees $3$ and $4$ on Montgomery curves. Furthermore, they\nattempted to obtain a simpler form of radical isogenies using enhanced elliptic\nand modular curves. In this article, we translate the original setup of radical\nisogenies (using Tate normal form) to the language of modular curves. In\naddition, we solve an open problem introduced by Onuki and Moriya regarding\nradical isogeny formulas on $X_0(N).$",
    "descriptor": "\nComments: 17 pages, comments welcome\n",
    "authors": [
      "Valentina Pribani\u0107"
    ],
    "subjectives": [
      "Number Theory (math.NT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.12840"
  },
  {
    "id": "arXiv:2210.12842",
    "title": "Entropic exercises around the Kneser-Poulsen conjecture",
    "abstract": "We develop an information-theoretic approach to study the Kneser--Poulsen\nconjecture in discrete geometry. This leads us to a broad question regarding\nwhether R\\'enyi entropies of independent sums decrease when one of the summands\nis contracted by a $1$-Lipschitz map. We answer this question affirmatively in\nvarious cases.",
    "descriptor": "\nComments: 23 pages, comments welcome!\n",
    "authors": [
      "Gautam Aishwarya",
      "Irfan Alam",
      "Dongbin Li",
      "Sergii Myroshnychenko",
      "Oscar Zatarain-Vera"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.12842"
  },
  {
    "id": "arXiv:2210.12853",
    "title": "Deep-Learning-Based Precipitation Nowcasting with Ground Weather Station  Data and Radar Data",
    "abstract": "Recently, many deep-learning techniques have been applied to various\nweather-related prediction tasks, including precipitation nowcasting (i.e.,\npredicting precipitation levels and locations in the near future). Most\nexisting deep-learning-based approaches for precipitation nowcasting, however,\nconsider only radar and/or satellite images as inputs, and meteorological\nobservations collected from ground weather stations, which are sparsely\nlocated, are relatively unexplored. In this paper, we propose ASOC, a novel\nattentive method for effectively exploiting ground-based meteorological\nobservations from multiple weather stations. ASOC is designed to capture\ntemporal dynamics of the observations and also contextual relationships between\nthem. ASOC is easily combined with existing image-based precipitation\nnowcasting models without changing their architectures. We show that such a\ncombination improves the average critical success index (CSI) of predicting\nheavy (at least 10 mm/hr) and light (at least 1 mm/hr) rainfall events at 1-6\nhr lead times by 5.7%, compared to the original image-based model, using the\nradar images and ground-based observations around South Korea collected from\n2014 to 2020.",
    "descriptor": "\nComments: to appear at the 17th International Workshop on Spatial and Spatiotemporal Data Mining (SSTDM-22)\n",
    "authors": [
      "Jihoon Ko",
      "Kyuhan Lee",
      "Hyunjin Hwang",
      "Kijung Shin"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12853"
  },
  {
    "id": "arXiv:2210.12860",
    "title": "Explicit Second-Order Min-Max Optimization Methods with Optimal  Convergence Guarantee",
    "abstract": "We propose and analyze exact and inexact regularized Newton-type methods for\nfinding a global saddle point of a \\textit{convex-concave} unconstrained\nmin-max optimization problem. Compared to their first-order counterparts,\ninvestigations of second-order methods for min-max optimization are relatively\nlimited, as obtaining global rates of convergence with second-order information\nis much more involved. In this paper, we highlight how second-order information\ncan be used to speed up the dynamics of dual extrapolation methods {despite\ninexactness}. Specifically, we show that the proposed algorithms generate\niterates that remain within a bounded set and the averaged iterates converge to\nan $\\epsilon$-saddle point within $O(\\epsilon^{-2/3})$ iterations in terms of a\ngap function. Our algorithms match the theoretically established lower bound in\nthis context and our analysis provides a simple and intuitive convergence\nanalysis for second-order methods without requiring any compactness\nassumptions. Finally, we present a series of numerical experiments on synthetic\nand real data that demonstrate the efficiency of the proposed algorithms.",
    "descriptor": "\nComments: 31 pages, 12 figures\n",
    "authors": [
      "Tianyi Lin",
      "Panayotis Mertikopoulos",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Complexity (cs.CC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12860"
  },
  {
    "id": "arXiv:2210.12868",
    "title": "Computing the Matching Distance of 2-Parameter Persistence Modules from  Critical Values",
    "abstract": "The exact computation of the matching distance for multi-parameter\npersistence modules is an active area of research in computational topology.\nAchieving an easily obtainable exact computation of this distance would allow\nmulti-parameter persistent homology to be a viable option for data analysis. In\nthis paper, we provide theoretical results for the computation of the matching\ndistance in two dimensions along with a geometric interpretation of the lines\nthrough parameter space realizing this distance. The crucial point of the\nmethod we propose is that it can be easily implemented.",
    "descriptor": "\nComments: 28 pages, 6 figures. Comments welcome\n",
    "authors": [
      "Asilata Bapat",
      "Robyn Brooks",
      "Celia Hacker",
      "Claudia Landi",
      "Barbara I. Mahler",
      "Elizabeth R. Stephenson"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.12868"
  },
  {
    "id": "arXiv:2210.12869",
    "title": "Robust Hypothesis Testing with Moment Constrained Uncertainty Sets",
    "abstract": "The problem of robust binary hypothesis testing is studied. Under both\nhypotheses, the data-generating distributions are assumed to belong to\nuncertainty sets constructed through moments; in particular, the sets contain\ndistributions whose moments are centered around the empirical moments obtained\nfrom training samples. The goal is to design a test that performs well under\nall distributions in the uncertainty sets, i.e., minimize the worst-case error\nprobability over the uncertainty sets. In the finite-alphabet case, the optimal\ntest is obtained. In the infinite-alphabet case, a tractable approximation to\nthe worst-case error is derived that converges to the optimal value using\nfinite samples from the alphabet. A test is further constructed to generalize\nto the entire alphabet. An exponentially consistent test for testing batch\nsamples is also proposed. Numerical results are provided to demonstrate the\nperformance of the proposed robust tests.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2203.12777\n",
    "authors": [
      "Akshayaa Magesh",
      "Zhongchang Sun",
      "Venugopal V. Veeravalli",
      "Shaofeng Zou"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.12869"
  },
  {
    "id": "arXiv:2210.12882",
    "title": "Stochastic Mirror Descent for Large-Scale Sparse Recovery",
    "abstract": "In this paper we discuss an application of Stochastic Approximation to\nstatistical estimation of high-dimensional sparse parameters. The proposed\nsolution reduces to resolving a penalized stochastic optimization problem on\neach stage of a multistage algorithm; each problem being solved to a prescribed\naccuracy by the non-Euclidean Composite Stochastic Mirror Descent (CSMD)\nalgorithm. Assuming that the problem objective is smooth and quadratically\nminorated and stochastic perturbations are sub-Gaussian, our analysis\nprescribes the method parameters which ensure fast convergence of the\nestimation error (the radius of a confidence ball of a given norm around the\napproximate solution). This convergence is linear during the first\n\"preliminary\" phase of the routine and is sublinear during the second\n\"asymptotic\" phase. We consider an application of the proposed approach to\nsparse Generalized Linear Regression problem. In this setting, we show that the\nproposed algorithm attains the optimal convergence of the estimation error\nunder weak assumptions on the regressor distribution. We also present a\nnumerical study illustrating the performance of the algorithm on\nhigh-dimensional simulation data.",
    "descriptor": "",
    "authors": [
      "Sasila Ilandarideva",
      "Yannis Bekri",
      "Anatoli Juditsky",
      "Vianney Perchet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2210.12882"
  },
  {
    "id": "arXiv:2210.12884",
    "title": "Computing the minimum distance of the $C(\\mathbb{O}_{3,6})$ polar  Orthogonal Grassmann code with elementary methods",
    "abstract": "The polar orthogonal Grassmann code $C(\\mathbb{O}_{3,6})$ is the linear code\nassociated to the Grassmann embedding of the Dual Polar space of $Q^+(5,q)$. In\nthis manuscript we study the minimum distance of this embedding. We prove that\nthe minimum distance of the polar orthogonal Grassmann code\n$C(\\mathbb{O}_{3,6})$ is $q^3-q^3$ for $q$ odd and $q^3$ for $q$ even. Our\ntechnique is based on partitioning the orthogonal space into different sets\nsuch that on each partition the code $C(\\mathbb{O}_{3,6})$ is identified with\nevaluations of determinants of skew--symmetric matrices. Our bounds come from\nelementary algebraic methods counting the zeroes of particular classes of\npolynomials. We expect our techniques may be applied to other polar Grassmann\ncodes.",
    "descriptor": "",
    "authors": [
      "Sarah Gregory",
      "Fernando Pi\u00f1ero-Gonz\u00e1lez",
      "Doel Rivera-Laboy",
      "Lani Southern"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Information Theory (cs.IT)",
      "Algebraic Geometry (math.AG)"
    ],
    "url": "https://arxiv.org/abs/2210.12884"
  },
  {
    "id": "arXiv:2210.12931",
    "title": "Removing Radio Frequency Interference from Auroral Kilometric Radiation  with Stacked Autoencoders",
    "abstract": "Radio frequency data in astronomy enable scientists to analyze astrophysical\nphenomena. However, these data can be corrupted by a host of radio frequency\ninterference (RFI) sources that limit the ability to observe underlying natural\nprocesses. In this study, we extended recent work in image processing to remove\nRFI from time-frequency spectrograms containing auroral kilometric radiation\n(AKR), a coherent radio emission originating from the Earth's auroral zones\nthat is used to study astrophysical plasmas. We present a Denoising Autoencoder\nfor Auroral Radio Emissions (DAARE) trained with synthetic spectrograms to\ndenoise AKR spectrograms collected at the South Pole Station. DAARE achieved\n42.2 peak-signal-to-noise ratio (PSNR) and 0.981 structural similarity (SSIM)\non synthesized AKR observations, improving PSNR by 3.9 and SSIM by 0.064\ncompared to state-of-the-art filtering and denoising networks. Qualitative\ncomparisons demonstrate DAARE's denoising capability to effectively remove RFI\nfrom real AKR observations, despite being trained completely on a dataset of\nsimulated AKR. The framework for simulating AKR, training DAARE, and employing\nDAARE can be accessed at https://github.com/Cylumn/daare.",
    "descriptor": "\nComments: 5 pages, 3 figures, 48th International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)\n",
    "authors": [
      "Allen Chang",
      "Mary Knapp",
      "James LaBelle",
      "John Swoboda",
      "Ryan Volz",
      "Philip J. Erickson"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12931"
  },
  {
    "id": "arXiv:2210.12938",
    "title": "GradMix for nuclei segmentation and classification in imbalanced  pathology image datasets",
    "abstract": "An automated segmentation and classification of nuclei is an essential task\nin digital pathology. The current deep learning-based approaches require a vast\namount of annotated datasets by pathologists. However, the existing datasets\nare imbalanced among different types of nuclei in general, leading to a\nsubstantial performance degradation. In this paper, we propose a simple but\neffective data augmentation technique, termed GradMix, that is specifically\ndesigned for nuclei segmentation and classification. GradMix takes a pair of a\nmajor-class nucleus and a rare-class nucleus, creates a customized mixing mask,\nand combines them using the mask to generate a new rare-class nucleus. As it\ncombines two nuclei, GradMix considers both nuclei and the neighboring\nenvironment by using the customized mixing mask. This allows us to generate\nrealistic rare-class nuclei with varying environments. We employed two datasets\nto evaluate the effectiveness of GradMix. The experimental results suggest that\nGradMix is able to improve the performance of nuclei segmentation and\nclassification in imbalanced pathology image datasets.",
    "descriptor": "\nComments: submitted to MICCAI2022\n",
    "authors": [
      "Tan Nhu Nhat Doan",
      "Kyungeun Kim",
      "Boram Song",
      "Jin Tae Kwak"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.12938"
  },
  {
    "id": "arXiv:2210.12953",
    "title": "Implementation of Trained Factorization Machine Recommendation System on  Quantum Annealer",
    "abstract": "Factorization Machine (FM) is the most commonly used model to build a\nrecommendation system since it can incorporate side information to improve\nperformance. However, producing item suggestions for a given user with a\ntrained FM is time-consuming. It requires a run-time of $O((N_m \\log N_m)^2)$,\nwhere $N_m$ is the number of items in the dataset. To address this problem, we\npropose a quadratic unconstrained binary optimization (QUBO) scheme to combine\nwith FM and apply quantum annealing (QA) computation. Compared to classical\nmethods, this hybrid algorithm provides a faster than quadratic speedup in\nfinding good user suggestions. We then demonstrate the aforementioned\ncomputational advantage on current NISQ hardware by experimenting with a real\nexample on a D-Wave annealer.",
    "descriptor": "\nComments: 9 pages, 4 figures\n",
    "authors": [
      "Chen-Yu Liu",
      "Hsin-Yu Wang",
      "Pei-Yen Liao",
      "Ching-Jui Lai",
      "Min-Hsiu Hsieh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12953"
  },
  {
    "id": "arXiv:2210.12983",
    "title": "Poisson multi-Bernoulli mixture filter with general target-generated  measurements and arbitrary clutter",
    "abstract": "This paper shows that the Poisson multi-Bernoulli mixture (PMBM) density is a\nmulti-target conjugate prior for general target-generated measurement\ndistributions and an arbitrary clutter distribution. That is, for this\nmulti-target measurement model and the standard multi-target dynamic model with\nPoisson birth model, the predicted and filtering densities are PMBMs. We derive\nthe corresponding PMBM filtering recursion. Based on this result, we implement\na PMBM filter for point-target measurement models and negative binomial clutter\ndensity in which data association hypotheses with high weights are chosen via\nGibbs sampling. We also implement an extended target PMBM filter with clutter\nthat is the union of Poisson-distributed clutter and a finite number of\nindependent clutter sources. Simulation results show the benefits of the\nproposed filters.",
    "descriptor": "",
    "authors": [
      "\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez",
      "Yuxuan Xia",
      "Lennart Svensson"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.12983"
  },
  {
    "id": "arXiv:2210.12995",
    "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
    "abstract": "In this paper, we present TridentSE, a novel architecture for speech\nenhancement, which is capable of efficiently capturing both global information\nand local details. TridentSE maintains T-F bin level representation to capture\ndetails, and uses a small number of global tokens to process the global\ninformation. Information is propagated between the local and the global\nrepresentations through cross attention modules. To capture both inter- and\nintra-frame information, the global tokens are divided into two groups to\nprocess along the time and the frequency axis respectively. A metric\ndiscriminator is further employed to guide our model to achieve higher\nperceptual quality. Even with significantly lower computational cost, TridentSE\noutperforms a variety of previous speech enhancement methods, achieving a PESQ\nof 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test\nset. Visualization shows that the global tokens learn diverse and interpretable\nglobal patterns.",
    "descriptor": "\nComments: 5 pages, 2 figures, 3 tables\n",
    "authors": [
      "Dacheng Yin",
      "Zhiyuan Zhao",
      "Chuanxin Tang",
      "Zhiwei Xiong",
      "Chong Luo"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.12995"
  },
  {
    "id": "arXiv:2210.13008",
    "title": "Inference for diffusions from low frequency measurements",
    "abstract": "Let $(X_t)$ be a reflected diffusion process in a bounded convex domain in\n$\\mathbb R^d$, solving the stochastic differential equation $$dX_t = \\nabla\nf(X_t) dt + \\sqrt{2f (X_t)} dW_t, ~t \\ge 0,$$ with $W_t$ a $d$-dimensional\nBrownian motion. The data $X_0, X_D, \\dots, X_{ND}$ consist of discrete\nmeasurements and the time interval $D$ between consecutive observations is\nfixed so that one cannot `zoom' into the observed path of the process. The goal\nis to solve the non-linear inverse problem of inferring the diffusivity $f$ and\nthe associated transition operator $P_{t,f}$. We prove injectivity theorems and\nstability estimates for the maps $f \\mapsto P_{t,f} \\mapsto P_{D,f}, t<D$.\nUsing these estimates we then establish the statistical consistency of a class\nof Bayesian algorithms based on Gaussian process priors for the\ninfinite-dimensional parameter $f$, and show optimality of some of the\nconvergence rates obtained. We discuss an underlying relationship between `fast\nconvergence' and the `hot spots' conjecture from spectral geometry.",
    "descriptor": "\nComments: 36 pages, 2 figures\n",
    "authors": [
      "Richard Nickl"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.13008"
  },
  {
    "id": "arXiv:2210.13012",
    "title": "CMU-Net: A Strong ConvMixer-based Medical Ultrasound Image Segmentation  Network",
    "abstract": "U-Net and its extended segmentation model have achieved great success in\nmedical image segmentation tasks. However, due to the inherent local\ncharacteristics of ordinary convolution operations, the encoder cannot\neffectively extract the global context information. In addition, simple skip\nconnection cannot capture salient features. In this work, we propose a full\nconvolutional segmentation network (CMU-Net) which incorporate hybrid\nconvolution and multi-scale attention gate. The ConvMixer module is to mix\ndistant spatial locations for extracting the global context information.\nMoreover, the multi-scale attention gate can help to emphasize valuable\nfeatures and achieve efficient skip connections. Evaluations on open-source\nbreast ultrasound images and private thyroid ultrasound image datasets show\nthat CMU-Net achieves an average IOU of 73.27% and 84.75%, F1-value is 84.16%\nand 91.71%. The code is available at https://github.com/FengheTan9/CMU-Net.",
    "descriptor": "\nComments: 5 pages, 13 figures, conference\n",
    "authors": [
      "Fenghe Tang",
      "Lingtao Wang",
      "Chunping Ning",
      "Min Xian",
      "Jianrui Ding"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.13012"
  },
  {
    "id": "arXiv:2210.13027",
    "title": "E-Valuating Classifier Two-Sample Tests",
    "abstract": "We propose E-C2ST, a classifier two-sample test for high-dimensional data\nbased on E-values. Compared to $p$-values-based tests, tests with E-values have\nfinite sample guarantees for the type I error. E-C2ST combines ideas from\nexisting work on split likelihood ratio tests and predictive independence\ntesting. The resulting E-values incorporate information about the alternative\nhypothesis. We demonstrate the utility of E-C2ST on simulated and real-life\ndata. In all experiments, we observe that when going from small to large sample\nsizes, as expected, E-C2ST starts with lower power compared to other methods\nbut eventually converges towards one. Simultaneously, E-C2ST's type I error\nstays substantially below the chosen significance level, which is not always\nthe case for the baseline methods. Finally, we use an MRI dataset to\ndemonstrate that multiplying E-values from multiple independently conducted\nstudies leads to a combined E-value that retains the finite sample type I error\nguarantees while increasing the power.",
    "descriptor": "",
    "authors": [
      "Teodora Pandeva",
      "Tim Bakker",
      "Christian A. Naesseth",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13027"
  },
  {
    "id": "arXiv:2210.13036",
    "title": "Non-Crossing Shortest Paths are Covered with Exactly Four Forests",
    "abstract": "Given a set of paths $P$ we define the \\emph{Path Covering with Forest\nNumber} of $P$} (PCFN($P$)) as the minimum size of a set $F$ of forests\nsatisfying that every path in $P$ is contained in at least one forest in $F$.\nWe show that PCFN($P$) is treatable when $P$ is a set of non-crossing shortest\npaths in a plane graph or subclasses. We prove that if $P$ is a set of\nnon-crossing shortest paths of a planar graph $G$ whose extremal vertices lie\non the same face of $G$, then PCFN($P$)\\leq 4$, and this bound is tight.",
    "descriptor": "\nComments: 25 pages, 15 figures\n",
    "authors": [
      "Lorenzo Balzotti"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2210.13036"
  },
  {
    "id": "arXiv:2210.13121",
    "title": "The Entropy Method in Large Deviation Theory",
    "abstract": "This paper illustrates the power of the entropy method in addressing problems\nfrom large deviation theory. We provide the entropy proofs for Cramer's theorem\nand Sanov's theorem which are the most fundamental results in large deviation\ntheory. Moreover, by the entropy method, we also strengthen Sanov's theorem to\nthe strong version.",
    "descriptor": "\nComments: 23 pages. A preliminary version. Any comments are welcome\n",
    "authors": [
      "Lei Yu"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.13121"
  },
  {
    "id": "arXiv:2210.13127",
    "title": "A Novel Frame Structure for Cloud-Based Audio-Visual Speech Enhancement  in Multimodal Hearing-aids",
    "abstract": "In this paper, we design a first of its kind transceiver (PHY layer)\nprototype for cloud-based audio-visual (AV) speech enhancement (SE) complying\nwith high data rate and low latency requirements of future multimodal hearing\nassistive technology. The innovative design needs to meet multiple challenging\nconstraints including up/down link communications, delay of transmission and\nsignal processing, and real-time AV SE models processing. The transceiver\nincludes device detection, frame detection, frequency offset estimation, and\nchannel estimation capabilities. We develop both uplink (hearing aid to the\ncloud) and downlink (cloud to hearing aid) frame structures based on the data\nrate and latency requirements. Due to the varying nature of uplink information\n(audio and lip-reading), the uplink channel supports multiple data rate frame\nstructure, while the downlink channel has a fixed data rate frame structure. In\naddition, we evaluate the latency of different PHY layer blocks of the\ntransceiver for developed frame structures using LabVIEW NXG. This can be used\nwith software defined radio (such as Universal Software Radio Peripheral) for\nreal-time demonstration scenarios.",
    "descriptor": "",
    "authors": [
      "Abhijeet Bishnu",
      "Ankit Gupta",
      "Mandar Gogate",
      "Kia Dashtipour",
      "Ahsan Adeel",
      "Amir Hussain",
      "Mathini Sellathurai",
      "Tharmalingam Ratnarajah"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13127"
  },
  {
    "id": "arXiv:2210.13132",
    "title": "PAC-Bayesian Offline Contextual Bandits With Guarantees",
    "abstract": "This paper introduces a new principled approach for offline policy\noptimisation in contextual bandits. For two well-established risk estimators,\nwe propose novel generalisation bounds able to confidently improve upon the\nlogging policy offline. Unlike previous work, our approach does not require\ntuning hyperparameters on held-out sets, and enables deployment with no prior\nA/B testing. This is achieved by analysing the problem through the PAC-Bayesian\nlens; mainly, we let go of traditional policy parametrisation (e.g. softmax)\nand instead interpret the policies as mixtures of deterministic strategies. We\ndemonstrate through extensive experiments evidence of our bounds tightness and\nthe effectiveness of our approach in practical scenarios.",
    "descriptor": "\nComments: Paper under review\n",
    "authors": [
      "Otmane Sakhi",
      "Nicolas Chopin",
      "Pierre Alquier"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13132"
  },
  {
    "id": "arXiv:2210.13144",
    "title": "Weak-Supervised Dysarthria-invariant Features for Spoken Language  Understanding using an FHVAE and Adversarial Training",
    "abstract": "The scarcity of training data and the large speaker variation in dysarthric\nspeech lead to poor accuracy and poor speaker generalization of spoken language\nunderstanding systems for dysarthric speech. Through work on the speech\nfeatures, we focus on improving the model generalization ability with limited\ndysarthric data. Factorized Hierarchical Variational Auto-Encoders (FHVAE)\ntrained unsupervisedly have shown their advantage in disentangling content and\nspeaker representations. Earlier work showed that the dysarthria shows in both\nfeature vectors. Here, we add adversarial training to bridge the gap between\nthe control and dysarthric speech data domains. We extract dysarthric and\nspeaker invariant features using weak supervision. The extracted features are\nevaluated on a Spoken Language Understanding task and yield a higher accuracy\non unseen speakers with more severe dysarthria compared to features from the\nbasic FHVAE model or plain filterbanks.",
    "descriptor": "",
    "authors": [
      "Jinzi Qi",
      "Hugo Van hamme"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13144"
  },
  {
    "id": "arXiv:2210.13179",
    "title": "Occam learning",
    "abstract": "We discuss probabilistic neural network models for unsupervised learning\nwhere the distribution of the hidden layer is fixed. We argue that learning\nmachines with this architecture enjoy a number of desirable properties. For\nexample, the model can be chosen as a simple and interpretable one, it does not\nneed to be over-parametrised and training is argued to be efficient in a\nthermodynamic sense.\nWhen hidden units are binary variables, these models have a natural\ninterpretation in terms of features. We show that the featureless state\ncorresponds to a state of maximal ignorance about the features and that\nlearning the first feature depends on non-Gaussian statistical properties of\nthe data. We suggest that the distribution of hidden variables should be chosen\naccording to the principle of maximal relevance. We introduce the Hierarchical\nFeature Model as an example of a model that satisfies this principle, and that\nencodes an a priori organisation of the feature space.\nWe present extensive numerical experiments in order i) to test that the\ninternal representation of learning machines can indeed be independent of the\ndata with which they are trained and ii) that only a finite number of features\nare needed to describe a datasets.",
    "descriptor": "\nComments: 28 pages, 9 figures\n",
    "authors": [
      "Rongrong Xie",
      "Matteo Marsili"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13179"
  },
  {
    "id": "arXiv:2210.13189",
    "title": "Can Visual Context Improve Automatic Speech Recognition for an Embodied  Agent?",
    "abstract": "The usage of automatic speech recognition (ASR) systems are becoming\nomnipresent ranging from personal assistant to chatbots, home, and industrial\nautomation systems, etc. Modern robots are also equipped with ASR capabilities\nfor interacting with humans as speech is the most natural interaction modality.\nHowever, ASR in robots faces additional challenges as compared to a personal\nassistant. Being an embodied agent, a robot must recognize the physical\nentities around it and therefore reliably recognize the speech containing the\ndescription of such entities. However, current ASR systems are often unable to\ndo so due to limitations in ASR training, such as generic datasets and\nopen-vocabulary modeling. Also, adverse conditions during inference, such as\nnoise, accented, and far-field speech makes the transcription inaccurate. In\nthis work, we present a method to incorporate a robot's visual information into\nan ASR system and improve the recognition of a spoken utterance containing a\nvisible entity. Specifically, we propose a new decoder biasing technique to\nincorporate the visual context while ensuring the ASR output does not degrade\nfor incorrect context. We achieve a 59% relative reduction in WER from an\nunmodified ASR system.",
    "descriptor": "\nComments: Accepted in EMNLP '22\n",
    "authors": [
      "Pradip Pramanick",
      "Chayan Sarkar"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.13189"
  },
  {
    "id": "arXiv:2210.13193",
    "title": "Langevin dynamics based algorithm e-TH$\\varepsilon$O POULA for  stochastic optimization problems with discontinuous stochastic gradient",
    "abstract": "We introduce a new Langevin dynamics based algorithm, called\ne-TH$\\varepsilon$O POULA, to solve optimization problems with discontinuous\nstochastic gradients which naturally appear in real-world applications such as\nquantile estimation, vector quantization, CVaR minimization, and regularized\noptimization problems involving ReLU neural networks. We demonstrate both\ntheoretically and numerically the applicability of the e-TH$\\varepsilon$O POULA\nalgorithm. More precisely, under the conditions that the stochastic gradient is\nlocally Lipschitz in average and satisfies a certain convexity at infinity\ncondition, we establish non-asymptotic error bounds for e-TH$\\varepsilon$O\nPOULA in Wasserstein distances and provide a non-asymptotic estimate for the\nexpected excess risk, which can be controlled to be arbitrarily small. Three\nkey applications in finance and insurance are provided, namely, multi-period\nportfolio optimization, transfer learning in multi-period portfolio\noptimization, and insurance claim prediction, which involve neural networks\nwith (Leaky)-ReLU activation functions. Numerical experiments conducted using\nreal-world datasets illustrate the superior empirical performance of\ne-TH$\\varepsilon$O POULA compared to SGLD, ADAM, and AMSGrad in terms of model\naccuracy.",
    "descriptor": "",
    "authors": [
      "Dong-Young Lim",
      "Ariel Neufeld",
      "Sotirios Sabanis",
      "Ying Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13193"
  },
  {
    "id": "arXiv:2210.13206",
    "title": "Multiplicity-adjusted bootstrap tilting lower confidence bounds for  conditional prediction performance measures",
    "abstract": "In machine learning, the selection of a promising model from a potentially\nlarge number of competing models and the assessment of its generalization\nperformance are critical tasks that need careful consideration. Typically,\nmodel selection and evaluation are strictly separated endeavors, splitting the\nsample at hand into a training, validation, and evaluation set, and only\ncompute a single confidence interval for the prediction performance of the\nfinal selected model. We however propose an algorithm how to compute valid\nlower confidence bounds for multiple models that have been selected based on\ntheir prediction performances in the evaluation set by interpreting the\nselection problem as a simultaneous inference problem. We use bootstrap tilting\nand a maxT-type multiplicity correction. The approach is universally applicable\nfor any combination of prediction models, any model selection strategy, and any\nprediction performance measure that accepts weights. We conducted various\nsimulation experiments which show that our proposed approach yields lower\nconfidence bounds that are at least comparably good as bounds from standard\napproaches, and that reliably reach the nominal coverage probability. In\naddition, especially when sample size is small, our proposed approach yields\nbetter performing prediction models than the default selection of only one\nmodel for evaluation does.",
    "descriptor": "",
    "authors": [
      "Pascal Rink",
      "Werner Brannath"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13206"
  },
  {
    "id": "arXiv:2210.13209",
    "title": "Response to \"Comment on 'Origin of the Curie--von Schweidler law and the  fractional capacitor from time-varying capacitance [J. Pow. Sources 532  (2022) 231309]' \"",
    "abstract": "We welcome Allagui et al.'s discussions about our recent paper that has\nproposed revisions to the existing theory of capacitors. It gives us an\nopportunity to emphasize on the physical underpinnings of the mathematical\nexpressions that are relevant for modeling using fractional derivatives. The\nconcerns raised by Allagui et al. are found to be quite questionable when\nexamined in light of the established standard results of fractional calculus.\nConsequently, the inferences that they have drawn are not true. Finally, we\nwould like to thank Allagui et al. because this subsequent Response to their\nComment has actually led to a further consolidation of our results that are\nsupposed to be significant for materials science as well as for fractional\ncontrol systems and engineering.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Vikash Pandey"
    ],
    "subjectives": [
      "Materials Science (cond-mat.mtrl-sci)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13209"
  },
  {
    "id": "arXiv:2210.13238",
    "title": "Multimodal Model with Text and Drug Embeddings for Adverse Drug Reaction  Classification",
    "abstract": "In this paper, we focus on the classification of tweets as sources of\npotential signals for adverse drug effects (ADEs) or drug reactions (ADRs).\nFollowing the intuition that text and drug structure representations are\ncomplementary, we introduce a multimodal model with two components. These\ncomponents are state-of-the-art BERT-based models for language understanding\nand molecular property prediction. Experiments were carried out on multilingual\nbenchmarks of the Social Media Mining for Health Research and Applications\n(#SMM4H) initiative. Our models obtained state-of-the-art results of 0.61 F1\nand 0.57 F1 on #SMM4H 2021 Shared Tasks 1a and 2 in English and Russian,\nrespectively. On the classification of French tweets from SMM4H 2020 Task 1,\nour approach pushes the state of the art by an absolute gain of 8% F1. Our\nexperiments show that the molecular information obtained from neural networks\nis more beneficial for ADE classification than traditional molecular\ndescriptors. The source code for our models is freely available at\nhttps://github.com/Andoree/smm4h_2021_classification.",
    "descriptor": "\nComments: This paper is accepted to Journal of Biomedical Informatics\n",
    "authors": [
      "Andrey Sakhovskiy",
      "Elena Tutubalina"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13238"
  },
  {
    "id": "arXiv:2210.13247",
    "title": "Containing the spread of a contagion on a tree",
    "abstract": "Contact tracing can be thought of as a race between two processes: an\ninfection process and a tracing process. In this paper, we study a simple model\nof infection spreading on a tree, and a tracer who stabilizes one node at a\ntime. We focus on the question, how should the tracer choose nodes to stabilize\nso as to prevent the infection from spreading further? We study simple\npolicies, which prioritize nodes based on time, infectiousness, or probability\nof generating new contacts.",
    "descriptor": "\nComments: 27 pages, 7 figures\n",
    "authors": [
      "Michela Meister",
      "Jon Kleinberg"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Social and Information Networks (cs.SI)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2210.13247"
  },
  {
    "id": "arXiv:2210.13248",
    "title": "Brouhaha: multi-task training for voice activity detection,  speech-to-noise ratio, and C50 room acoustics estimation",
    "abstract": "Most automatic speech processing systems are sensitive to the acoustic\nenvironment, with degraded performance when applied to noisy or reverberant\nspeech. But how can one tell whether speech is noisy or reverberant? We propose\nBrouhaha, a pipeline to simulate audio segments recorded in noisy and\nreverberant conditions. We then use the simulated audio to jointly train the\nBrouhaha model for voice activity detection, signal-to-noise ratio estimation,\nand C50 room acoustics prediction. We show how the predicted SNR and C50 values\ncan be used to investigate and help diagnose errors made by automatic speech\nprocessing tools (such as pyannote.audio for speaker diarization or OpenAI's\nWhisper for automatic speech recognition). Both our pipeline and a pretrained\nmodel are open source and shared with the speech community.",
    "descriptor": "",
    "authors": [
      "Marvin Lavechin",
      "Marianne M\u00e9tais",
      "Hadrien Titeux",
      "Alodie Boissonnet",
      "Jade Copet",
      "Morgane Rivi\u00e8re",
      "Elika Bergelson",
      "Alejandrina Cristia",
      "Emmanuel Dupoux",
      "Herv\u00e9 Bredin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13248"
  },
  {
    "id": "arXiv:2210.13271",
    "title": "ECG Artifact Removal from Single-Channel Surface EMG Using Fully  Convolutional Networks",
    "abstract": "Electrocardiogram (ECG) artifact contamination often occurs in surface\nelectromyography (sEMG) applications when the measured muscles are in proximity\nto the heart. Previous studies have developed and proposed various methods,\nsuch as high-pass filtering, template subtraction and so forth. However, these\nmethods remain limited by the requirement of reference signals and distortion\nof original sEMG. This study proposed a novel denoising method to eliminate ECG\nartifacts from the single-channel sEMG signals using fully convolutional\nnetworks (FCN). The proposed method adopts a denoise autoencoder structure and\npowerful nonlinear mapping capability of neural networks for sEMG denoising. We\ncompared the proposed approach with conventional approaches, including\nhigh-pass filters and template subtraction, on open datasets called the\nNon-Invasive Adaptive Prosthetics database and MIT-BIH normal sinus rhythm\ndatabase. The experimental results demonstrate that the FCN outperforms\nconventional methods in sEMG reconstruction quality under a wide range of\nsignal-to-noise ratio inputs.",
    "descriptor": "\nComments: 5 pages, 5 figures\n",
    "authors": [
      "Kuan-Chen Wang",
      "Kai-Chun Liu",
      "Sheng-Yu Peng",
      "Yu Tsao"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13271"
  },
  {
    "id": "arXiv:2210.13273",
    "title": "An Acoustical Machine Learning Approach to Determine Abrasive Belt Wear  of Wide Belt Sanders",
    "abstract": "This paper describes a machine learning approach to determine the abrasive\nbelt wear of wide belt sanders used in industrial processes based on acoustic\ndata, regardless of the sanding process-related parameters, Feed speed, Grit\nSize, and Type of material. Our approach utilizes Decision Tree, Random Forest,\nk-nearest Neighbors, and Neural network Classifiers to detect the belt wear\nfrom Spectrograms, Mel Spectrograms, MFCC, IMFCC, and LFCC, yielding an\naccuracy of up to 86.1% on five levels of belt wear. A 96% accuracy could be\nachieved with different Decision Tree Classifiers specialized in different\nsanding parameter configurations. The classifiers could also determine with an\naccuracy of 97% if the machine is currently sanding or is idle and with an\naccuracy of 98.4% and 98.8% detect the sanding parameters Feed speed and Grit\nSize. We can show that low-dimensional mappings of high-dimensional features\ncan be used to visualize belt wear and sanding parameters meaningfully.",
    "descriptor": "\nComments: Conference IEEE SENSORS 2022\n",
    "authors": [
      "Maximilian Bundscherer",
      "Thomas H. Schmitt",
      "Sebastian Bayerl",
      "Thomas Auerbach",
      "Tobias Bocklet"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13273"
  },
  {
    "id": "arXiv:2210.13278",
    "title": "Unconditional Proofs-of-Work and Other Possibilities of Thermodynamic  Cryptography",
    "abstract": "In line with advances in recent years about realizing cryptographic\nfunctionalities in an information-theoretically secure way from physical\nphenomena and laws, we propose here to obtain useful tasks from the sole\nassumption of limited free energy. Specifically, based on that assumption --\nresulting in a setting loosely related to Maurer's bounded-storage model -- we\nderive protocols for unconditional proofs-of-thermodynamical-work, secret\nsharing of free energy, unforgeable money, and proofs-of-position. While our\nschemes can be considered classical and not quantum per se, they are resistant\nagainst both classes of adversaries.",
    "descriptor": "\nComments: 6 pages\n",
    "authors": [
      "Xavier Coiteux-Roy",
      "Stefan Wolf"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.13278"
  },
  {
    "id": "arXiv:2210.13300",
    "title": "Designing Universal Causal Deep Learning Models: The Case of  Infinite-Dimensional Dynamical Systems from Stochastic Analysis",
    "abstract": "Deep learning (DL) is becoming indispensable to contemporary stochastic\nanalysis and finance; nevertheless, it is still unclear how to design a\nprincipled DL framework for approximating infinite-dimensional causal\noperators. This paper proposes a \"geometry-aware\" solution to this open problem\nby introducing a DL model-design framework that takes a suitable\ninfinite-dimensional linear metric spaces as inputs and returns a universal\nsequential DL models adapted to these linear geometries: we call these models\nCausal Neural Operators (CNO). Our main result states that the models produced\nby our framework can uniformly approximate on compact sets and across\narbitrarily finite-time horizons H\\\"older or smooth trace class operators which\ncausally map sequences between given linear metric spaces. Consequentially, we\ndeduce that a single CNO can efficiently approximate the solution operator to a\nbroad range of SDEs, thus allowing us to simultaneously approximate predictions\nfrom families of SDE models, which is vital to computational robust finance. We\ndeduce that the CNO can approximate the solution operator to most stochastic\nfiltering problems, implying that a single CNO can simultaneously filter a\nfamily of partially observed stochastic volatility models.",
    "descriptor": "",
    "authors": [
      "Luca Galimberti",
      "Giulia Livieri",
      "Anastasis Kratsios"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Machine Learning (cs.LG)",
      "Computational Finance (q-fin.CP)"
    ],
    "url": "https://arxiv.org/abs/2210.13300"
  },
  {
    "id": "arXiv:2210.13318",
    "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
    "abstract": "It has been shown that the intelligibility of noisy speech can be improved by\nspeech enhancement algorithms. However, speech enhancement has not been\nestablished as an effective front-end for robust automatic speech recognition\n(ASR) in comparison with an ASR model trained on noisy speech directly. The\ndivide between speech enhancement and ASR impedes the progress of robust ASR\nsystems especially as speech enhancement has made big strides in recent years.\nIn this work, we focus on eliminating such divide with an ARN (attentive\nrecurrent network) based time-domain enhancement model. The proposed system\nfully decouples speech enhancement and an acoustic model trained only on clean\nspeech. Results on the CHiME-2 corpus show that ARN enhanced speech translates\nto improved ASR results. The proposed system achieves $6.28\\%$ average word\nerror rate, outperforming the previous best by $19.3\\%$.",
    "descriptor": "\nComments: 5 pages, 2 figures, submitted to ICASSP 2023\n",
    "authors": [
      "Yufeng Yang",
      "Ashutosh Pandey",
      "DeLiang Wang"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.13318"
  },
  {
    "id": "arXiv:2210.13320",
    "title": "Edge-Cuts and Rooted Spanning Trees",
    "abstract": "We give a closed form formula to determine the size of a k-respecting cut.\nFurther, we show that for any k, the size of the k-respecting cut can be found\nonly using the size of 2-respecting cuts.",
    "descriptor": "",
    "authors": [
      "Mohit Daga"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.13320"
  },
  {
    "id": "arXiv:2210.13327",
    "title": "Deep Kronecker Network",
    "abstract": "We propose Deep Kronecker Network (DKN), a novel framework designed for\nanalyzing medical imaging data, such as MRI, fMRI, CT, etc. Medical imaging\ndata is different from general images in at least two aspects: i) sample size\nis usually much more limited, ii) model interpretation is more of a concern\ncompared to outcome prediction. Due to its unique nature, general methods, such\nas convolutional neural network (CNN), are difficult to be directly applied. As\nsuch, we propose DKN, that is able to i) adapt to low sample size limitation,\nii) provide desired model interpretation, and iii) achieve the prediction power\nas CNN. The DKN is general in the sense that it not only works for both matrix\nand (high-order) tensor represented image data, but also could be applied to\nboth discrete and continuous outcomes. The DKN is built on a Kronecker product\nstructure and implicitly imposes a piecewise smooth property on coefficients.\nMoreover, the Kronecker structure can be written into a convolutional form, so\nDKN also resembles a CNN, particularly, a fully convolutional network (FCN).\nFurthermore, we prove that with an alternating minimization algorithm, the\nsolutions of DKN are guaranteed to converge to the truth geometrically even if\nthe objective function is highly nonconvex. Interestingly, the DKN is also\nhighly connected to the tensor regression framework proposed by Zhou et al.\n(2010), where a CANDECOMP/PARAFAC (CP) low-rank structure is imposed on tensor\ncoefficients. Finally, we conduct both classification and regression analyses\nusing real MRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI)\nto demonstrate the effectiveness of DKN.",
    "descriptor": "",
    "authors": [
      "Long Feng",
      "Guang Yang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13327"
  },
  {
    "id": "arXiv:2210.13331",
    "title": "Theoretical Guarantees for Domain Adaptation with Hierarchical Optimal  Transport",
    "abstract": "Domain adaptation arises as an important problem in statistical learning\ntheory when the data-generating processes differ between training and test\nsamples, respectively called source and target domains. Recent theoretical\nadvances show that the success of domain adaptation algorithms heavily relies\non their ability to minimize the divergence between the probability\ndistributions of the source and target domains. However, minimizing this\ndivergence cannot be done independently of the minimization of other key\ningredients such as the source risk or the combined error of the ideal joint\nhypothesis. The trade-off between these terms is often ensured by algorithmic\nsolutions that remain implicit and not directly reflected by the theoretical\nguarantees. To get to the bottom of this issue, we propose in this paper a new\ntheoretical framework for domain adaptation through hierarchical optimal\ntransport. This framework provides more explicit generalization bounds and\nallows us to consider the natural hierarchical organization of samples in both\ndomains into classes or clusters. Additionally, we provide a new divergence\nmeasure between the source and target domains called Hierarchical Wasserstein\ndistance that indicates under mild assumptions, which structures have to be\naligned to lead to a successful adaptation.",
    "descriptor": "",
    "authors": [
      "Mourad El Hamri",
      "Youn\u00e8s Bennani",
      "Issam Falih"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13331"
  },
  {
    "id": "arXiv:2210.13336",
    "title": "Brain Tumor Segmentation using Enhanced U-Net Model with Empirical  Analysis",
    "abstract": "Cancer of the brain is deadly and requires careful surgical segmentation. The\nbrain tumors were segmented using U-Net using a Convolutional Neural Network\n(CNN). When looking for overlaps of necrotic, edematous, growing, and healthy\ntissue, it might be hard to get relevant information from the images. The 2D\nU-Net network was improved and trained with the BraTS datasets to find these\nfour areas. U-Net can set up many encoder and decoder routes that can be used\nto get information from images that can be used in different ways. To reduce\ncomputational time, we use image segmentation to exclude insignificant\nbackground details. Experiments on the BraTS datasets show that our proposed\nmodel for segmenting brain tumors from MRI (MRI) works well. In this study, we\ndemonstrate that the BraTS datasets for 2017, 2018, 2019, and 2020 do not\nsignificantly differ from the BraTS 2019 dataset's attained dice scores of\n0.8717 (necrotic), 0.9506 (edema), and 0.9427 (enhancing).",
    "descriptor": "\nComments: 5 tables, 4 figures, 5 equations\n",
    "authors": [
      "MD Abdullah Al Nasim",
      "Abdullah Al Munem",
      "Maksuda Islam",
      "Md Aminul Haque Palash",
      "MD. Mahim Anjum Haque",
      "Faisal Muhammad Shah"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13336"
  },
  {
    "id": "arXiv:2210.13354",
    "title": "Matching Map Recovery with an Unknown Number of Outliers",
    "abstract": "We consider the problem of finding the matching map between two sets of\n$d$-dimensional noisy feature-vectors. The distinctive feature of our setting\nis that we do not assume that all the vectors of the first set have their\ncorresponding vector in the second set. If $n$ and $m$ are the sizes of these\ntwo sets, we assume that the matching map that should be recovered is defined\non a subset of unknown cardinality $k^*\\le \\min(n,m)$. We show that, in the\nhigh-dimensional setting, if the signal-to-noise ratio is larger than\n$5(d\\log(4nm/\\alpha))^{1/4}$, then the true matching map can be recovered with\nprobability $1-\\alpha$. Interestingly, this threshold does not depend on $k^*$\nand is the same as the one obtained in prior work in the case of $k =\n\\min(n,m)$. The procedure for which the aforementioned property is proved is\nobtained by a data-driven selection among candidate mappings\n$\\{\\hat\\pi_k:k\\in[\\min(n,m)]\\}$. Each $\\hat\\pi_k$ minimizes the sum of squares\nof distances between two sets of size $k$. The resulting optimization problem\ncan be formulated as a minimum-cost flow problem, and thus solved efficiently.\nFinally, we report the results of numerical experiments on both synthetic and\nreal-world data that illustrate our theoretical results and provide further\ninsight into the properties of the algorithms studied in this work.",
    "descriptor": "\nComments: 15 pages, 8 figures\n",
    "authors": [
      "Arshak Minasyan",
      "Tigran Galstyan",
      "Sona Hunanyan",
      "Arnak Dalalyan"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13354"
  },
  {
    "id": "arXiv:2210.13355",
    "title": "Calibration tests beyond classification",
    "abstract": "Most supervised machine learning tasks are subject to irreducible prediction\nerrors. Probabilistic predictive models address this limitation by providing\nprobability distributions that represent a belief over plausible targets,\nrather than point estimates. Such models can be a valuable tool in\ndecision-making under uncertainty, provided that the model output is meaningful\nand interpretable. Calibrated models guarantee that the probabilistic\npredictions are neither over- nor under-confident. In the machine learning\nliterature, different measures and statistical tests have been proposed and\nstudied for evaluating the calibration of classification models. For regression\nproblems, however, research has been focused on a weaker condition of\ncalibration based on predicted quantiles for real-valued targets. In this\npaper, we propose the first framework that unifies calibration evaluation and\ntests for general probabilistic predictive models. It applies to any such\nmodel, including classification and regression models of arbitrary dimension.\nFurthermore, the framework generalizes existing measures and provides a more\nintuitive reformulation of a recently proposed framework for calibration in\nmulti-class classification. In particular, we reformulate and generalize the\nkernel calibration error, its estimators, and hypothesis tests using\nscalar-valued kernels, and evaluate the calibration of real-valued regression\nproblems.",
    "descriptor": "\nComments: 37 pages, 12 figures. Fixes some comments about the kernel choice in the original paper: this https URL\n",
    "authors": [
      "David Widmann",
      "Fredrik Lindsten",
      "Dave Zachariah"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13355"
  },
  {
    "id": "arXiv:2210.13364",
    "title": "Large Batch and Patch Size Training for Medical Image Segmentation",
    "abstract": "Multi-organ segmentation enables organ evaluation, accounts the relationship\nbetween multiple organs, and facilitates accurate diagnosis and treatment\ndecisions. However, only few models can perform segmentation accurately because\nof the lack of datasets and computational resources. On AMOS2022 challenge,\nwhich is a large-scale, clinical, and diverse abdominal multiorgan segmentation\nbenchmark, we trained a 3D-UNet model with large batch and patch sizes using\nmulti-GPU distributed training. Segmentation performance tended to increase for\nmodels with large batch and patch sizes compared with the baseline settings.\nThe accuracy was further improved by using ensemble models that were trained\nwith different settings. These results provide a reference for parameter\nselection in organ segmentation.",
    "descriptor": "",
    "authors": [
      "Junya Sato",
      "Shoji Kido"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13364"
  },
  {
    "id": "arXiv:2210.13379",
    "title": "Ranking nodes in directed networks via continuous-time quantum walks",
    "abstract": "Four new centrality measures for directed networks based on unitary,\ncontinuous-time quantum walks (CTQW) in $n$ dimensions -- where $n$ is the\nnumber of nodes -- are presented, tested and discussed. The main idea behind\nthese methods consists in re-casting the classical HITS and PageRank algorithms\nas eigenvector problems for symmetric matrices, and using these symmetric\nmatrices as Hamiltonians for CTQWs, in order to obtain a unitary evolution\noperator. The choice of the initial state is also crucial. Two options were\ntested: a vector with uniform occupation and a vector weighted w.r.t.~in- or\nout-degrees (for authority and hub centrality, respectively). Two methods are\nbased on a HITS-derived Hamiltonian, and two use a PageRank-derived\nHamiltonian. Centrality scores for the nodes are defined as the average\noccupation values. All the methods have been tested on a set of small, simple\ngraphs in order to spot possible evident drawbacks, and then on a larger number\nof artificially generated larger-sized graphs, in order to draw a comparison\nwith classical HITS and PageRank. Numerical results show that, despite some\npathologies found in three of the methods when analyzing small graphs, all the\nmethods are effective in finding the first and top ten nodes in larger graphs.\nWe comment on the results and offer some insight into the good accordance\nbetween classical and quantum approaches.",
    "descriptor": "",
    "authors": [
      "Paola Boito",
      "Roberto Grena"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.13379"
  },
  {
    "id": "arXiv:2210.13390",
    "title": "On the failure of variational score matching for VAE models",
    "abstract": "Score matching (SM) is a convenient method for training flexible\nprobabilistic models, which is often preferred over the traditional\nmaximum-likelihood (ML) approach. However, these models are less interpretable\nthan normalized models; as such, training robustness is in general difficult to\nassess. We present a critical study of existing variational SM objectives,\nshowing catastrophic failure on a wide range of datasets and network\narchitectures. Our theoretical insights on the objectives emerge directly from\ntheir equivalent autoencoding losses when optimizing variational autoencoder\n(VAE) models. First, we show that in the Fisher autoencoder, SM produces far\nworse models than maximum-likelihood, and approximate inference by Fisher\ndivergence can lead to low-density local optima. However, with important\nmodifications, this objective reduces to a regularized autoencoding loss that\nresembles the evidence lower bound (ELBO). This analysis predicts that the\nmodified SM algorithm should behave very similarly to ELBO on Gaussian VAEs. We\nthen review two other FD-based objectives from the literature and show that\nthey reduce to uninterpretable autoencoding losses, likely leading to poor\nperformance. The experiments verify our theoretical predictions and suggest\nthat only ELBO and the baseline objective robustly produce expected results,\nwhile previously proposed SM methods do not.",
    "descriptor": "",
    "authors": [
      "Li Kevin Wenliang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13390"
  },
  {
    "id": "arXiv:2210.13400",
    "title": "Sampling with Mollified Interaction Energy Descent",
    "abstract": "Sampling from a target measure whose density is only known up to a\nnormalization constant is a fundamental problem in computational statistics and\nmachine learning. In this paper, we present a new optimization-based method for\nsampling called mollified interaction energy descent (MIED). MIED minimizes a\nnew class of energies on probability measures called mollified interaction\nenergies (MIEs). These energies rely on mollifier functions -- smooth\napproximations of the Dirac delta originated from PDE theory. We show that as\nthe mollifier approaches the Dirac delta, the MIE converges to the chi-square\ndivergence with respect to the target measure and the gradient flow of the MIE\nagrees with that of the chi-square divergence. Optimizing this energy with\nproper discretization yields a practical first-order particle-based algorithm\nfor sampling in both unconstrained and constrained domains. We show\nexperimentally that for unconstrained sampling problems our algorithm performs\non par with existing particle-based algorithms like SVGD, while for constrained\nsampling problems our method readily incorporates constrained optimization\ntechniques to handle more flexible constraints with strong performance compared\nto alternatives.",
    "descriptor": "",
    "authors": [
      "Lingxiao Li",
      "Qiang Liu",
      "Anna Korba",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.13400"
  },
  {
    "id": "arXiv:2210.13415",
    "title": "Deep Learning Approach for Dynamic Sampling for Multichannel Mass  Spectrometry Imaging",
    "abstract": "Mass Spectrometry Imaging (MSI), using traditional rectilinear scanning,\ntakes hours to days for high spatial resolution acquisitions. Given that most\npixels within a sample's field of view are often neither relevant to underlying\nbiological structures nor chemically informative, MSI presents as a prime\ncandidate for integration with sparse and dynamic sampling algorithms. During a\nscan, stochastic models determine which locations probabilistically contain\ninformation critical to the generation of low-error reconstructions. Decreasing\nthe number of required physical measurements thereby minimizes overall\nacquisition times. A Deep Learning Approach for Dynamic Sampling (DLADS),\nutilizing a Convolutional Neural Network (CNN) and encapsulating molecular mass\nintensity distributions within a third dimension, demonstrates a simulated 70%\nthroughput improvement for Nanospray Desorption Electrospray Ionization\n(nano-DESI) MSI tissues. Evaluations are conducted between DLADS and a\nSupervised Learning Approach for Dynamic Sampling, with Least-Squares\nregression (SLADS-LS) and a Multi-Layer Perceptron (MLP) network (SLADS-Net).\nWhen compared with SLADS-LS, limited to a single m/z channel, as well as\nmultichannel SLADS-LS and SLADS-Net, DLADS respectively improves regression\nperformance by 36.7%, 7.0%, and 6.2%, resulting in gains to reconstruction\nquality of 6.0%, 2.1%, and 3.4% for acquisition of targeted m/z.",
    "descriptor": "",
    "authors": [
      "David Helminiak",
      "Hang Hu",
      "Julia Laskin",
      "Dong Hye Ye"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.13415"
  },
  {
    "id": "arXiv:2210.13430",
    "title": "Data-Driven Stabilizing and Robust Control of Discrete-Time Linear  Systems with Error in Variables",
    "abstract": "This work presents a sum-of-squares (SOS) based framework to perform\ndata-driven stabilization and robust control tasks on discrete-time linear\nsystems where the full-state observations are corrupted by L-infinity bounded\nmeasurement noise (error in variable setting). Certificates of state-feedback\nsuperstability or quadratic stability of all plants in a consistency set are\nprovided by solving a feasibility program formed by polynomial nonnegativity\nconstraints. Under mild compactness and data-collection assumptions, SOS\ntightenings in rising degree will converge to recover the true superstabilizing\ncontroller, with slight conservatism introduced for quadratic stabilizability.\nThe performance of this SOS method is improved through the application of a\ntheorem of alternatives while retaining tightness, in which the unknown noise\nvariables are eliminated from the consistency set description. This SOS\nfeasibility method is extended to provide worst-case-optimal robust controllers\nunder H2 control costs. The consistency set description may be broadened to\ninclude cases where the data and process are affected by a combination of\nL-infinity bounded measurement, process, and input noise. Further\ngeneralizations include varying noise sets, non-uniform sampling, and switched\nsystems stabilization.",
    "descriptor": "\nComments: 27 pages, 1 figure, 9 tables\n",
    "authors": [
      "Jared Miller",
      "Tianyu Dai",
      "Mario Sznaier"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.13430"
  },
  {
    "id": "arXiv:2210.13438",
    "title": "High Fidelity Neural Audio Compression",
    "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec\nleveraging neural networks. It consists in a streaming encoder-decoder\narchitecture with quantized latent space trained in an end-to-end fashion. We\nsimplify and speed-up the training by using a single multiscale spectrogram\nadversary that efficiently reduces artifacts and produce high-quality samples.\nWe introduce a novel loss balancer mechanism to stabilize training: the weight\nof a loss now defines the fraction of the overall gradient it should represent,\nthus decoupling the choice of this hyper-parameter from the typical scale of\nthe loss. Finally, we study how lightweight Transformer models can be used to\nfurther compress the obtained representation by up to 40%, while staying faster\nthan real time. We provide a detailed description of the key design choices of\nthe proposed model including: training objective, architectural changes and a\nstudy of various perceptual loss functions. We present an extensive subjective\nevaluation (MUSHRA tests) together with an ablation study for a range of\nbandwidths and audio domains, including speech, noisy-reverberant speech, and\nmusic. Our approach is superior to the baselines methods across all evaluated\nsettings, considering both 24 kHz monophonic and 48 kHz stereophonic audio.\nCode and models are available at github.com/facebookresearch/encodec.",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Alexandre D\u00e9fossez",
      "Jade Copet",
      "Gabriel Synnaeve",
      "Yossi Adi"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.13438"
  },
  {
    "id": "arXiv:2210.13441",
    "title": "Bridging Machine Learning and Sciences: Opportunities and Challenges",
    "abstract": "The application of machine learning in sciences has seen exciting advances in\nrecent years. As a widely-applicable technique, anomaly detection has been long\nstudied in the machine learning community. Especially, deep neural nets-based\nout-of-distribution detection has made great progress for high-dimensional\ndata. Recently, these techniques have been showing their potential in\nscientific disciplines. We take a critical look at their applicative prospects\nincluding data universality, experimental protocols, model robustness, etc. We\ndiscuss examples that display transferable practices and domain-specific\nchallenges simultaneously, providing a starting point for establishing a novel\ninterdisciplinary research paradigm in the near future.",
    "descriptor": "\nComments: 8 pages, 3 figures\n",
    "authors": [
      "Taoli Cheng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "High Energy Physics - Phenomenology (hep-ph)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2210.13441"
  },
  {
    "id": "arXiv:1310.7423",
    "title": "Infinite Probabilistic Secret Sharing",
    "abstract": "Infinite Probabilistic Secret Sharing",
    "descriptor": "",
    "authors": [
      "L\u00e1szl\u00f3 Csirmaz"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/1310.7423"
  },
  {
    "id": "arXiv:1606.04671",
    "title": "Progressive Neural Networks",
    "abstract": "Progressive Neural Networks",
    "descriptor": "",
    "authors": [
      "Andrei A. Rusu",
      "Neil C. Rabinowitz",
      "Guillaume Desjardins",
      "Hubert Soyer",
      "James Kirkpatrick",
      "Koray Kavukcuoglu",
      "Razvan Pascanu",
      "Raia Hadsell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1606.04671"
  },
  {
    "id": "arXiv:1612.05832",
    "title": "Implementations and the independent set polynomial below the Shearer  threshold",
    "abstract": "Comments: To appear in TCS",
    "descriptor": "\nComments: To appear in TCS\n",
    "authors": [
      "Andreas Galanis",
      "Leslie Ann Goldberg",
      "Daniel Stefankovic"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/1612.05832"
  },
  {
    "id": "arXiv:1702.00374",
    "title": "A Semantic Account of Metric Preservation",
    "abstract": "A Semantic Account of Metric Preservation",
    "descriptor": "",
    "authors": [
      "Arthur Azevedo de Amorim",
      "Marco Gaboardi",
      "Justin Hsu",
      "Shin-ya Katsumata",
      "Ikram Cherigui"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/1702.00374"
  },
  {
    "id": "arXiv:1802.02219",
    "title": "Practical Transfer Learning for Bayesian Optimization",
    "abstract": "Comments: This version fixes a minor error in the equation in Section 3.2 of V3",
    "descriptor": "\nComments: This version fixes a minor error in the equation in Section 3.2 of V3\n",
    "authors": [
      "Matthias Feurer",
      "Benjamin Letham",
      "Frank Hutter",
      "Eytan Bakshy"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/1802.02219"
  },
  {
    "id": "arXiv:1903.09668",
    "title": "Data Augmentation for Bayesian Deep Learning",
    "abstract": "Data Augmentation for Bayesian Deep Learning",
    "descriptor": "",
    "authors": [
      "Yuexi Wang",
      "Nicholas G. Polson",
      "Vadim O. Sokolov"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/1903.09668"
  },
  {
    "id": "arXiv:1904.10554",
    "title": "Deep Q-Learning for Nash Equilibria: Nash-DQN",
    "abstract": "Comments: 15 pages, 3 figures",
    "descriptor": "\nComments: 15 pages, 3 figures\n",
    "authors": [
      "Philippe Casgrain",
      "Brian Ning",
      "Sebastian Jaimungal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Computational Finance (q-fin.CP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1904.10554"
  },
  {
    "id": "arXiv:1906.03133",
    "title": "Convergence analysis of approximation formulas for analytic functions  via duality for potential energy minimization",
    "abstract": "Comments: 17 pages",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "Satoshi Hayakawa",
      "Ken'ichiro Tanaka"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/1906.03133"
  },
  {
    "id": "arXiv:1911.03867",
    "title": "A Modular Deep Learning Pipeline for Galaxy-Scale Strong Gravitational  Lens Detection and Modeling",
    "abstract": "A Modular Deep Learning Pipeline for Galaxy-Scale Strong Gravitational  Lens Detection and Modeling",
    "descriptor": "",
    "authors": [
      "Sandeep Madireddy",
      "Nesar Ramachandra",
      "Nan Li",
      "James Butler",
      "Prasanna Balaprakash",
      "Salman Habib",
      "Katrin Heitmann",
      "LSST Dark Energy Science Collaboration"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Cosmology and Nongalactic Astrophysics (astro-ph.CO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1911.03867"
  },
  {
    "id": "arXiv:1912.07483",
    "title": "Analyticity and hp discontinuous Galerkin approximation of nonlinear  Schr\u00f6dinger eigenproblems",
    "abstract": "Analyticity and hp discontinuous Galerkin approximation of nonlinear  Schr\u00f6dinger eigenproblems",
    "descriptor": "",
    "authors": [
      "Yvon Maday",
      "Carlo Marcati"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Analysis of PDEs (math.AP)"
    ],
    "url": "https://arxiv.org/abs/1912.07483"
  },
  {
    "id": "arXiv:2001.05787",
    "title": "Weight Enumerators and Cardinalities for Number-Theoretic Codes",
    "abstract": "Comments: 9 pages, accepted to IEEE Transactions on Information Theory",
    "descriptor": "\nComments: 9 pages, accepted to IEEE Transactions on Information Theory\n",
    "authors": [
      "Takayuki Nozaki"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2001.05787"
  },
  {
    "id": "arXiv:2004.05258",
    "title": "Exploring Optimal Deep Learning Models for Image-based Malware Variant  Classification",
    "abstract": "Comments: 11 pages with 10figures. This is the accepted version of the paper published in IEEE COMPSAC 2022. The published version is available at this http URL",
    "descriptor": "\nComments: 11 pages with 10figures. This is the accepted version of the paper published in IEEE COMPSAC 2022. The published version is available at this http URL\n",
    "authors": [
      "Rikima Mitsuhashi",
      "Takahiro Shinagawa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2004.05258"
  },
  {
    "id": "arXiv:2005.11589",
    "title": "MaxSAT Resolution and Subcube Sums",
    "abstract": "MaxSAT Resolution and Subcube Sums",
    "descriptor": "",
    "authors": [
      "Yuval Filmus",
      "Meena Mahajan",
      "Gaurav Sood",
      "Marc Vinyals"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2005.11589"
  },
  {
    "id": "arXiv:2007.09855",
    "title": "Wide Boosting",
    "abstract": "Comments: Gradient Boosting, Wide Neural Networks",
    "descriptor": "\nComments: Gradient Boosting, Wide Neural Networks\n",
    "authors": [
      "Michael T. Horrell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2007.09855"
  },
  {
    "id": "arXiv:2007.16126",
    "title": "Functional Tucker approximation using Chebyshev interpolation",
    "abstract": "Functional Tucker approximation using Chebyshev interpolation",
    "descriptor": "",
    "authors": [
      "Sergey Dolgov",
      "Daniel Kressner",
      "Christoph Str\u00f6ssner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2007.16126"
  },
  {
    "id": "arXiv:2008.07292",
    "title": "A classical-logic view of a paraconsistent logic",
    "abstract": "Comments: 17 pages, definition of structures corrected",
    "descriptor": "\nComments: 17 pages, definition of structures corrected\n",
    "authors": [
      "C. A. Middelburg"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2008.07292"
  },
  {
    "id": "arXiv:2009.01947",
    "title": "Practical and Parallelizable Algorithms for Non-Monotone Submodular  Maximization with Size Constraint",
    "abstract": "Comments: 36 pages",
    "descriptor": "\nComments: 36 pages\n",
    "authors": [
      "Yixin Chen",
      "Alan Kuhnle"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.01947"
  },
  {
    "id": "arXiv:2010.08233",
    "title": "Concurrent Process Histories and Resource Transducers",
    "abstract": "Comments: 22 pages. Many Figures. Extended version of this https URL",
    "descriptor": "\nComments: 22 pages. Many Figures. Extended version of this https URL\n",
    "authors": [
      "Chad Nester"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2010.08233"
  },
  {
    "id": "arXiv:2011.11576",
    "title": "Conjecturing-Based Computational Discovery of Patterns in Data",
    "abstract": "Comments: 40 pages, 4 figures",
    "descriptor": "\nComments: 40 pages, 4 figures\n",
    "authors": [
      "J.P. Brooks",
      "D.J. Edwards",
      "C.E. Larson",
      "N. Van Cleemput"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2011.11576"
  },
  {
    "id": "arXiv:2012.02113",
    "title": "Entropy and Diversity: The Axiomatic Approach",
    "abstract": "Comments: Book, viii + 442 pages. Version 3: small number of minor corrections",
    "descriptor": "\nComments: Book, viii + 442 pages. Version 3: small number of minor corrections\n",
    "authors": [
      "Tom Leinster"
    ],
    "subjectives": [
      "Populations and Evolution (q-bio.PE)",
      "Information Theory (cs.IT)",
      "Classical Analysis and ODEs (math.CA)",
      "Category Theory (math.CT)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2012.02113"
  },
  {
    "id": "arXiv:2012.15375",
    "title": "Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion  Dialogues via Reinforcement Learning and Human Demonstration",
    "abstract": "Comments: EMNLP 2021 Findings",
    "descriptor": "\nComments: EMNLP 2021 Findings\n",
    "authors": [
      "Weiyan Shi",
      "Yu Li",
      "Saurav Sahay",
      "Zhou Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2012.15375"
  },
  {
    "id": "arXiv:2101.05149",
    "title": "On the Computational Properties of Obviously Strategy-Proof Mechanisms",
    "abstract": "Comments: working paper. Updates: added results for anonymous choice rules",
    "descriptor": "\nComments: working paper. Updates: added results for anonymous choice rules\n",
    "authors": [
      "Louis Golowich",
      "Shengwu Li"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2101.05149"
  },
  {
    "id": "arXiv:2101.07354",
    "title": "Exact Recovery of Community Structures Using DeepWalk and Node2vec",
    "abstract": "Exact Recovery of Community Structures Using DeepWalk and Node2vec",
    "descriptor": "",
    "authors": [
      "Yichi Zhang",
      "Minh Tang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2101.07354"
  },
  {
    "id": "arXiv:2102.00627",
    "title": "On the Relationship between Explanation and Recommendation: Learning to  Rank Explanations for Improved Performance",
    "abstract": "Comments: Published as a journal at ACM TIST",
    "descriptor": "\nComments: Published as a journal at ACM TIST\n",
    "authors": [
      "Lei Li",
      "Yongfeng Zhang",
      "Li Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2102.00627"
  },
  {
    "id": "arXiv:2102.09731",
    "title": "Efficient Riccati recursion for optimal control problems with pure-state  equality constraints",
    "abstract": "Comments: 8 pages, 3 figures. This paper has been accepted to be presented at 2022 Americal Control Conference (ACC2022)",
    "descriptor": "\nComments: 8 pages, 3 figures. This paper has been accepted to be presented at 2022 Americal Control Conference (ACC2022)\n",
    "authors": [
      "Sotaro Katayama",
      "Toshiyuki Ohtsuka"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2102.09731"
  },
  {
    "id": "arXiv:2102.11887",
    "title": "Quantum Cross Entropy and Maximum Likelihood Principle",
    "abstract": "Comments: Added discussion on extensivity and parallel processing. Acknowledgement modified to reflect truth",
    "descriptor": "\nComments: Added discussion on extensivity and parallel processing. Acknowledgement modified to reflect truth\n",
    "authors": [
      "Zhou Shangnan",
      "Yixu Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.11887"
  },
  {
    "id": "arXiv:2103.07942",
    "title": "Do open citations give insights on the qualitative peer-review  evaluation in research assessments? An analysis of the Italian National  Scientific Qualification",
    "abstract": "Do open citations give insights on the qualitative peer-review  evaluation in research assessments? An analysis of the Italian National  Scientific Qualification",
    "descriptor": "",
    "authors": [
      "Federica Bologna",
      "Angelo Di Iorio",
      "Silvio Peroni",
      "Francesco Poggi"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2103.07942"
  },
  {
    "id": "arXiv:2104.11893",
    "title": "LGD-GCN: Local and Global Disentangled Graph Convolutional Networks",
    "abstract": "LGD-GCN: Local and Global Disentangled Graph Convolutional Networks",
    "descriptor": "",
    "authors": [
      "Jingwei Guo",
      "Kaizhu Huang",
      "Xinping Yi",
      "Rui Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2104.11893"
  },
  {
    "id": "arXiv:2105.04371",
    "title": "Poolingformer: Long Document Modeling with Pooling Attention",
    "abstract": "Comments: Accepted by ICML 2021",
    "descriptor": "\nComments: Accepted by ICML 2021\n",
    "authors": [
      "Hang Zhang",
      "Yeyun Gong",
      "Yelong Shen",
      "Weisheng Li",
      "Jiancheng Lv",
      "Nan Duan",
      "Weizhu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2105.04371"
  },
  {
    "id": "arXiv:2105.06942",
    "title": "VICEROY: GDPR-/CCPA-compliant Enforcement of Verifiable Accountless  Consumer Requests",
    "abstract": "VICEROY: GDPR-/CCPA-compliant Enforcement of Verifiable Accountless  Consumer Requests",
    "descriptor": "",
    "authors": [
      "Scott Jordan",
      "Yoshimichi Nakatsuka",
      "Ercan Ozturk",
      "Andrew Paverd",
      "Gene Tsudik"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2105.06942"
  },
  {
    "id": "arXiv:2105.15197",
    "title": "A Simple and General Debiased Machine Learning Theorem with Finite  Sample Guarantees",
    "abstract": "Comments: Biometrika 2022",
    "descriptor": "\nComments: Biometrika 2022\n",
    "authors": [
      "Victor Chernozhukov",
      "Whitney K. Newey",
      "Rahul Singh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2105.15197"
  },
  {
    "id": "arXiv:2106.03157",
    "title": "Self-Supervision is All You Need for Solving Rubik's Cube",
    "abstract": "Comments: 9 pages, 6 figures",
    "descriptor": "\nComments: 9 pages, 6 figures\n",
    "authors": [
      "Kyo Takano"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.03157"
  },
  {
    "id": "arXiv:2106.04176",
    "title": "Efficient solution method based on inverse dynamics for optimal control  problems of rigid body systems",
    "abstract": "Comments: 7 pages, 3 figures. This paper has been accepted to be presented 2021 IEEE International Conference on Robotics and Automation (ICRA2021)",
    "descriptor": "\nComments: 7 pages, 3 figures. This paper has been accepted to be presented 2021 IEEE International Conference on Robotics and Automation (ICRA2021)\n",
    "authors": [
      "Sotaro Katayama",
      "Toshiyuki Ohtsuka"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2106.04176"
  },
  {
    "id": "arXiv:2106.05568",
    "title": "Explainable AI, but explainable to whom?",
    "abstract": "Comments: Book chapter for AI in Healthcare",
    "descriptor": "\nComments: Book chapter for AI in Healthcare\n",
    "authors": [
      "Julie Gerlings",
      "Millie S\u00f8ndergaard Jensen",
      "Arisa Shollo"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2106.05568"
  },
  {
    "id": "arXiv:2106.07704",
    "title": "Efficient (Soft) Q-Learning for Text Generation with Limited Good Data",
    "abstract": "Comments: Code available at this https URL",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Han Guo",
      "Bowen Tan",
      "Zhengzhong Liu",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.07704"
  },
  {
    "id": "arXiv:2106.10898",
    "title": "BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender  System",
    "abstract": "Comments: Master dissertation",
    "descriptor": "\nComments: Master dissertation\n",
    "authors": [
      "Shenghao Xu"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2106.10898"
  },
  {
    "id": "arXiv:2106.11696",
    "title": "Diversity-aware $k$-median : Clustering with fair center representation",
    "abstract": "Comments: To appear in ECML-PKDD 2021",
    "descriptor": "\nComments: To appear in ECML-PKDD 2021\n",
    "authors": [
      "Suhas Thejaswi",
      "Bruno Ordozgoiti",
      "Aristides Gionis"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2106.11696"
  },
  {
    "id": "arXiv:2106.12936",
    "title": "Fundamental limits for learning hidden Markov model parameters",
    "abstract": "Comments: To appear in IEEE Transactions on Information Theory Print ISSN: 0018-9448 Online ISSN: 1557-9654",
    "descriptor": "\nComments: To appear in IEEE Transactions on Information Theory Print ISSN: 0018-9448 Online ISSN: 1557-9654\n",
    "authors": [
      "Kweku Abraham",
      "Zacharie Naulet",
      "Elisabeth Gassiat"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2106.12936"
  },
  {
    "id": "arXiv:2106.13823",
    "title": "Quantum Data Compression and Quantum Cross Entropy",
    "abstract": "Comments: Acknowledgement modified to reflect truth",
    "descriptor": "\nComments: Acknowledgement modified to reflect truth\n",
    "authors": [
      "Zhou Shangnan"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2106.13823"
  },
  {
    "id": "arXiv:2107.01131",
    "title": "Tight Mutual Information Estimation With Contrastive Fenchel-Legendre  Optimization",
    "abstract": "Tight Mutual Information Estimation With Contrastive Fenchel-Legendre  Optimization",
    "descriptor": "",
    "authors": [
      "Qing Guo",
      "Junya Chen",
      "Dong Wang",
      "Yuewei Yang",
      "Xinwei Deng",
      "Lawrence Carin",
      "Fan Li",
      "Jing Huang",
      "Chenyang Tao"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.01131"
  },
  {
    "id": "arXiv:2107.03129",
    "title": "A stochastic first-order trust-region method with inexact restoration  for finite-sum minimization",
    "abstract": "A stochastic first-order trust-region method with inexact restoration  for finite-sum minimization",
    "descriptor": "",
    "authors": [
      "Stefania Bellavia",
      "Natasa Krejic",
      "Benedetta Morini",
      "Simone Rebegoldi"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.03129"
  },
  {
    "id": "arXiv:2107.04892",
    "title": "A multi-orthogonal polynomials' approach to bulk queueing theory",
    "abstract": "A multi-orthogonal polynomials' approach to bulk queueing theory",
    "descriptor": "",
    "authors": [
      "Ulises Fidalgo"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Complex Variables (math.CV)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2107.04892"
  },
  {
    "id": "arXiv:2107.05018",
    "title": "CLAP: A New Algorithm for Promise CSPs",
    "abstract": "Comments: Full version of a SODA 2022 paper",
    "descriptor": "\nComments: Full version of a SODA 2022 paper\n",
    "authors": [
      "Lorenzo Ciardo",
      "Stanislav \u017divn\u00fd"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2107.05018"
  },
  {
    "id": "arXiv:2107.06093",
    "title": "A generalized hypothesis test for community structure in networks",
    "abstract": "A generalized hypothesis test for community structure in networks",
    "descriptor": "",
    "authors": [
      "Eric Yanchenko",
      "Srijan Sengupta"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2107.06093"
  },
  {
    "id": "arXiv:2107.07110",
    "title": "Recurrent Parameter Generators",
    "abstract": "Recurrent Parameter Generators",
    "descriptor": "",
    "authors": [
      "Jiayun Wang",
      "Yubei Chen",
      "Stella X. Yu",
      "Brian Cheung",
      "Yann LeCun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.07110"
  },
  {
    "id": "arXiv:2108.01781",
    "title": "Lifted contact dynamics for efficient optimal control of rigid body  systems with contacts",
    "abstract": "Comments: 8 pages, 4 figures. This work has been accepted to be presented at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)",
    "descriptor": "\nComments: 8 pages, 4 figures. This work has been accepted to be presented at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)\n",
    "authors": [
      "Sotaro Katayama",
      "Toshiyuki Ohtsuka"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2108.01781"
  },
  {
    "id": "arXiv:2108.08067",
    "title": "Exact enumeration of satisfiable 2-SAT formulae",
    "abstract": "Comments: 29 pages, 6 tables, 10 figures. For associated python code, see this https URL",
    "descriptor": "\nComments: 29 pages, 6 tables, 10 figures. For associated python code, see this https URL\n",
    "authors": [
      "Sergey Dovgal",
      "\u00c9lie de Panafieu",
      "Vlady Ravelomanana"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Discrete Mathematics (cs.DM)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2108.08067"
  },
  {
    "id": "arXiv:2108.09306",
    "title": "D-DARTS: Distributed Differentiable Architecture Search",
    "abstract": "D-DARTS: Distributed Differentiable Architecture Search",
    "descriptor": "",
    "authors": [
      "Alexandre Heuillet",
      "Hedi Tabia",
      "Hichem Arioui",
      "Kamal Youcef-Toumi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2108.09306"
  },
  {
    "id": "arXiv:2108.11820",
    "title": "Large Large deviations for spatial telecommunication systems: The  boolean model",
    "abstract": "Comments: 17 pages",
    "descriptor": "\nComments: 17 pages\n",
    "authors": [
      "A.K. Boahen",
      "T. Katsekpor",
      "K. Doku-Amponsah"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2108.11820"
  },
  {
    "id": "arXiv:2109.01372",
    "title": "Sample Noise Impact on Active Learning",
    "abstract": "Comments: 9 pages, 3 figure, for the code, see this https URL",
    "descriptor": "\nComments: 9 pages, 3 figure, for the code, see this https URL\n",
    "authors": [
      "Alexandre Abraham",
      "L\u00e9o Dreyfus-Schmidt"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2109.01372"
  },
  {
    "id": "arXiv:2109.04381",
    "title": "Copy-Move Image Forgery Detection Based on Evolving Circular Domains  Coverage",
    "abstract": "Comments: Accepted by Multimedia Tools and Applications",
    "descriptor": "\nComments: Accepted by Multimedia Tools and Applications\n",
    "authors": [
      "Shilin Lu",
      "Xinghong Hu",
      "Chengyou Wang",
      "Lu Chen",
      "Shulu Han",
      "Yuejia Han"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.04381"
  },
  {
    "id": "arXiv:2109.05067",
    "title": "The Flaws of Policies Requiring Human Oversight of Government Algorithms",
    "abstract": "The Flaws of Policies Requiring Human Oversight of Government Algorithms",
    "descriptor": "",
    "authors": [
      "Ben Green"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2109.05067"
  },
  {
    "id": "arXiv:2109.05569",
    "title": "MovieCuts: A New Dataset and Benchmark for Cut Type Recognition",
    "abstract": "Comments: Paper's website: this https URL",
    "descriptor": "\nComments: Paper's website: this https URL\n",
    "authors": [
      "Alejandro Pardo",
      "Fabian Caba Heilbron",
      "Juan Le\u00f3n Alc\u00e1zar",
      "Ali Thabet",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.05569"
  },
  {
    "id": "arXiv:2109.07429",
    "title": "Towards a Game-Theoretic Security Analysis of Off-Chain Protocols",
    "abstract": "Comments: This submission is the extended version of our CSF 2023 paper \"Towards a Game-Theoretic Security Analysis of Off-Chain Protocols\"",
    "descriptor": "\nComments: This submission is the extended version of our CSF 2023 paper \"Towards a Game-Theoretic Security Analysis of Off-Chain Protocols\"\n",
    "authors": [
      "Sophie Rain",
      "Georgia Avarikioti",
      "Laura Kov\u00e1cs",
      "Matteo Maffei"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2109.07429"
  },
  {
    "id": "arXiv:2109.12941",
    "title": "PicTalky: Augmentative and Alternative Communication Software for  Language Developmental Disabilities",
    "abstract": "Comments: Accepted in AACL 2022 Demo Track",
    "descriptor": "\nComments: Accepted in AACL 2022 Demo Track\n",
    "authors": [
      "Chanjun Park",
      "Yoonna Jang",
      "Seolhwa Lee",
      "Jaehyung Seo",
      "Kisu Yang",
      "Heuiseok Lim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2109.12941"
  },
  {
    "id": "arXiv:2110.02343",
    "title": "Quantum Semi-Supervised Learning with Quantum Supremacy",
    "abstract": "Comments: Acknowledgement modified to reflect truth",
    "descriptor": "\nComments: Acknowledgement modified to reflect truth\n",
    "authors": [
      "Zhou Shangnan"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2110.02343"
  },
  {
    "id": "arXiv:2110.02474",
    "title": "Can an AI agent hit a moving target?",
    "abstract": "Can an AI agent hit a moving target?",
    "descriptor": "",
    "authors": [],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.02474"
  },
  {
    "id": "arXiv:2110.04591",
    "title": "Zig-Zag Modules: Cosheaves and K-Theory",
    "abstract": "Comments: v4: final, section 4 rewritten to included pointed set valued cosheaves",
    "descriptor": "\nComments: v4: final, section 4 rewritten to included pointed set valued cosheaves\n",
    "authors": [
      "Ryan E. Grady",
      "Anna Schenfisch"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)",
      "Category Theory (math.CT)"
    ],
    "url": "https://arxiv.org/abs/2110.04591"
  },
  {
    "id": "arXiv:2110.05169",
    "title": "Learning a subspace of policies for online adaptation in Reinforcement  Learning",
    "abstract": "Learning a subspace of policies for online adaptation in Reinforcement  Learning",
    "descriptor": "",
    "authors": [
      "Jean-Baptiste Gaya",
      "Laure Soulier",
      "Ludovic Denoyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.05169"
  },
  {
    "id": "arXiv:2110.07084",
    "title": "Online Bipartite Matching with Reusable Resources",
    "abstract": "Comments: This paper is a merged version of two ACM EC 2022 papers: (i) \"Online Bipartite Matching of Reusable Resources\" by Steven Delong, Alireza Farhadi, Rad Niazadeh and Balu Sivan (ii) \"Periodic Reranking for Online Matching of Reusable Resources\" By Rajan Udwani Journal version: Under submission in Mathematics of Operations Research",
    "descriptor": "\nComments: This paper is a merged version of two ACM EC 2022 papers: (i) \"Online Bipartite Matching of Reusable Resources\" by Steven Delong, Alireza Farhadi, Rad Niazadeh and Balu Sivan (ii) \"Periodic Reranking for Online Matching of Reusable Resources\" By Rajan Udwani Journal version: Under submission in Mathematics of Operations Research\n",
    "authors": [
      "Steven Delong",
      "Alireza Farhadi",
      "Rad Niazadeh",
      "Balasubramanian Sivan",
      "Rajan Udwani"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2110.07084"
  },
  {
    "id": "arXiv:2110.08426",
    "title": "EncT5: A Framework for Fine-tuning T5 as Non-autoregressive Models",
    "abstract": "Comments: Update multi-label and structured prediction results",
    "descriptor": "\nComments: Update multi-label and structured prediction results\n",
    "authors": [
      "Frederick Liu",
      "Terry Huang",
      "Shihang Lyu",
      "Siamak Shakeri",
      "Hongkun Yu",
      "Jing Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.08426"
  },
  {
    "id": "arXiv:2110.08432",
    "title": "Meta-Learning with Adjoint Methods",
    "abstract": "Meta-Learning with Adjoint Methods",
    "descriptor": "",
    "authors": [
      "Shibo Li",
      "Zheng Wang",
      "Akil Narayan",
      "Robert Kirby",
      "Shandian Zhe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.08432"
  },
  {
    "id": "arXiv:2110.09902",
    "title": "Toward Understanding Convolutional Neural Networks from Volterra  Convolution Perspective",
    "abstract": "Toward Understanding Convolutional Neural Networks from Volterra  Convolution Perspective",
    "descriptor": "",
    "authors": [
      "Tenghui Li",
      "Guoxu Zhou",
      "Yuning Qiu",
      "Qibin Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.09902"
  },
  {
    "id": "arXiv:2110.10603",
    "title": "Uncovering In-DRAM RowHammer Protection Mechanisms: A New Methodology,  Custom RowHammer Patterns, and Implications",
    "abstract": "Comments: This work is to appear at the 54th IEEE/ACM International Symposium on Microarchitecture (MICRO 2021)",
    "descriptor": "\nComments: This work is to appear at the 54th IEEE/ACM International Symposium on Microarchitecture (MICRO 2021)\n",
    "authors": [
      "Hasan Hassan",
      "Yahya Can Tugrul",
      "Jeremie S. Kim",
      "Victor van der Veen",
      "Kaveh Razavi",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2110.10603"
  },
  {
    "id": "arXiv:2111.02997",
    "title": "Global Optimality and Finite Sample Analysis of Softmax Off-Policy Actor  Critic under State Distribution Mismatch",
    "abstract": "Comments: Journal of Machine Learning Research 2022",
    "descriptor": "\nComments: Journal of Machine Learning Research 2022\n",
    "authors": [
      "Shangtong Zhang",
      "Remi Tachet",
      "Romain Laroche"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.02997"
  },
  {
    "id": "arXiv:2111.03664",
    "title": "Oracle Teacher: Leveraging Target Information for Better Knowledge  Distillation of CTC Models",
    "abstract": "Comments: Under review",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Ji Won Yoon",
      "Hyung Yong Kim",
      "Hyeonseung Lee",
      "Sunghwan Ahn",
      "Nam Soo Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2111.03664"
  },
  {
    "id": "arXiv:2111.03709",
    "title": "Positivity-Preserving Lax-Wendroff Discontinuous Galerkin Schemes for  Quadrature-Based Moment-Closure Approximations of Kinetic Models",
    "abstract": "Comments: 54 pages, 9 figures, 4 tables",
    "descriptor": "\nComments: 54 pages, 9 figures, 4 tables\n",
    "authors": [
      "Erica R. Johnson",
      "James A. Rossmanith",
      "Christine Vaughan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.03709"
  },
  {
    "id": "arXiv:2111.04585",
    "title": "Fast global spectral methods for three-dimensional partial differential  equations",
    "abstract": "Fast global spectral methods for three-dimensional partial differential  equations",
    "descriptor": "",
    "authors": [
      "Christoph Str\u00f6ssner",
      "Daniel Kressner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.04585"
  },
  {
    "id": "arXiv:2111.04600",
    "title": "Rational Framing Motions and Spatial Rational Pythagorean Hodograph  Curves",
    "abstract": "Rational Framing Motions and Spatial Rational Pythagorean Hodograph  Curves",
    "descriptor": "",
    "authors": [
      "Bahar Kalkan",
      "Daniel F. Scharler",
      "Hans-Peter Schr\u00f6cker",
      "Zbyn\u011bk \u0160\u00edr"
    ],
    "subjectives": [
      "Metric Geometry (math.MG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.04600"
  },
  {
    "id": "arXiv:2111.08356",
    "title": "On-Demand Unlabeled Personalized Federated Learning",
    "abstract": "On-Demand Unlabeled Personalized Federated Learning",
    "descriptor": "",
    "authors": [
      "Ohad Amosy",
      "Gal Eyal",
      "Gal Chechik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.08356"
  },
  {
    "id": "arXiv:2111.11349",
    "title": "Computation of the self-diffusion coefficient with low-rank tensor  methods: application to the simulation of a cross-diffusion system",
    "abstract": "Computation of the self-diffusion coefficient with low-rank tensor  methods: application to the simulation of a cross-diffusion system",
    "descriptor": "",
    "authors": [
      "Jad Dabaghi",
      "Virginie Ehrlacher",
      "Christoph Str\u00f6ssner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2111.11349"
  },
  {
    "id": "arXiv:2111.11926",
    "title": "An Educated Warm Start For Deep Image Prior-Based Micro CT  Reconstruction",
    "abstract": "An Educated Warm Start For Deep Image Prior-Based Micro CT  Reconstruction",
    "descriptor": "",
    "authors": [
      "Riccardo Barbano",
      "Johannes Leuschner",
      "Maximilian Schmidt",
      "Alexander Denker",
      "Andreas Hauptmann",
      "Peter Maa\u00df",
      "Bangti Jin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.11926"
  },
  {
    "id": "arXiv:2112.01476",
    "title": "KPDrop: Improving Absent Keyphrase Generation",
    "abstract": "Comments: Accepted in EMNLP Findings 2022",
    "descriptor": "\nComments: Accepted in EMNLP Findings 2022\n",
    "authors": [
      "Jishnu Ray Chowdhury",
      "Seoyeon Park",
      "Tuhin Kundu",
      "Cornelia Caragea"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.01476"
  },
  {
    "id": "arXiv:2112.04871",
    "title": "KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge  Graph Embeddings",
    "abstract": "KGE-CL: Contrastive Learning of Tensor Decomposition Based Knowledge  Graph Embeddings",
    "descriptor": "",
    "authors": [
      "Zhiping Luo",
      "Wentao Xu",
      "Weiqing Liu",
      "Jiang Bian",
      "Jian Yin",
      "Tie-Yan Liu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.04871"
  },
  {
    "id": "arXiv:2112.05483",
    "title": "Latency-Aware Multi-antenna SWIPT System with Battery-Constrained  Receivers",
    "abstract": "Comments: 34 pages, 12 figures",
    "descriptor": "\nComments: 34 pages, 12 figures\n",
    "authors": [
      "Dileep Kumar",
      "Onel L. Alcaraz L\u00f3pez",
      "Satya Krishna Joshi",
      "Antti T\u00f6lli"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2112.05483"
  },
  {
    "id": "arXiv:2112.05567",
    "title": "An Annotation-based Approach for Finding Bugs in Neural Network Programs",
    "abstract": "Comments: New experiments added (lots of new content)",
    "descriptor": "\nComments: New experiments added (lots of new content)\n",
    "authors": [
      "Mohammad Rezaalipour",
      "Carlo A. Furia"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2112.05567"
  },
  {
    "id": "arXiv:2112.06204",
    "title": "Few-Shot Out-of-Domain Transfer Learning of Natural Language  Explanations in a Label-Abundant Setup",
    "abstract": "Comments: Accepted to the EMNLP Findings 2022",
    "descriptor": "\nComments: Accepted to the EMNLP Findings 2022\n",
    "authors": [
      "Yordan Yordanov",
      "Vid Kocijan",
      "Thomas Lukasiewicz",
      "Oana-Maria Camburu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.06204"
  },
  {
    "id": "arXiv:2112.07030",
    "title": "Clustering with fair-center representation: parameterized approximation  algorithms and heuristics",
    "abstract": "Clustering with fair-center representation: parameterized approximation  algorithms and heuristics",
    "descriptor": "",
    "authors": [
      "Suhas Thejaswi",
      "Ameet Gadekar",
      "Bruno Ordozgoiti",
      "Michal Osadnik"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2112.07030"
  },
  {
    "id": "arXiv:2112.07377",
    "title": "A note on calculi for non-deterministic many-valued logics",
    "abstract": "A note on calculi for non-deterministic many-valued logics",
    "descriptor": "",
    "authors": [
      "Michael Kaminski"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2112.07377"
  },
  {
    "id": "arXiv:2112.08549",
    "title": "A prediction-based approach for online dynamic patient scheduling: a  case study in radiotherapy treatment",
    "abstract": "A prediction-based approach for online dynamic patient scheduling: a  case study in radiotherapy treatment",
    "descriptor": "",
    "authors": [
      "Tu-San Pham",
      "Antoine Legrain",
      "Patrick De Causmaecker",
      "Louis-Martin Rousseau"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.08549"
  },
  {
    "id": "arXiv:2112.08808",
    "title": "Simple Questions Generate Named Entity Recognition Datasets",
    "abstract": "Comments: EMNLP 2022. Code and datasets available at this https URL",
    "descriptor": "\nComments: EMNLP 2022. Code and datasets available at this https URL\n",
    "authors": [
      "Hyunjae Kim",
      "Jaehyo Yoo",
      "Seunghyun Yoon",
      "Jinhyuk Lee",
      "Jaewoo Kang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.08808"
  },
  {
    "id": "arXiv:2112.10668",
    "title": "Few-shot Learning with Multilingual Language Models",
    "abstract": "Comments: Accepted to EMNLP 2022; 34 pages",
    "descriptor": "\nComments: Accepted to EMNLP 2022; 34 pages\n",
    "authors": [
      "Xi Victoria Lin",
      "Todor Mihaylov",
      "Mikel Artetxe",
      "Tianlu Wang",
      "Shuohui Chen",
      "Daniel Simig",
      "Myle Ott",
      "Naman Goyal",
      "Shruti Bhosale",
      "Jingfei Du",
      "Ramakanth Pasunuru",
      "Sam Shleifer",
      "Punit Singh Koura",
      "Vishrav Chaudhary",
      "Brian O'Horo",
      "Jeff Wang",
      "Luke Zettlemoyer",
      "Zornitsa Kozareva",
      "Mona Diab",
      "Veselin Stoyanov",
      "Xian Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2112.10668"
  },
  {
    "id": "arXiv:2112.11122",
    "title": "Generating Chords from Melody with Flexible Harmonic Rhythm and  Controllable Harmonic Density",
    "abstract": "Comments: 5 pages, 3 figures, 1 table",
    "descriptor": "\nComments: 5 pages, 3 figures, 1 table\n",
    "authors": [
      "Shangda Wu",
      "Yue Yang",
      "Zhaowen Wang",
      "Xiaobing Li",
      "Maosong Sun"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2112.11122"
  },
  {
    "id": "arXiv:2112.12612",
    "title": "Towards Disturbance-Free Visual Mobile Manipulation",
    "abstract": "Comments: WACV 2023",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Tianwei Ni",
      "Kiana Ehsani",
      "Luca Weihs",
      "Jordi Salvador"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.12612"
  },
  {
    "id": "arXiv:2112.14432",
    "title": "Variational-Based Nonlinear Bayesian Filtering with Biased Observations",
    "abstract": "Variational-Based Nonlinear Bayesian Filtering with Biased Observations",
    "descriptor": "",
    "authors": [
      "Aamir Hussain Chughtai",
      "Arslan Majal",
      "Muhammad Tahir",
      "Momin Uppal"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2112.14432"
  },
  {
    "id": "arXiv:2112.14499",
    "title": "Decidable problems in substitution shifts",
    "abstract": "Decidable problems in substitution shifts",
    "descriptor": "",
    "authors": [
      "Marie-Pierre B\u00e9al",
      "Dominique Perrin",
      "Antonio Restivo"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.14499"
  },
  {
    "id": "arXiv:2201.01443",
    "title": "Neural KEM: A Kernel Method with Deep Coefficient Prior for PET Image  Reconstruction",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2110.01174",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2110.01174\n",
    "authors": [
      "Siqi Li",
      "Kuang Gong",
      "Ramsey D. Badawi",
      "Edward J. Kim",
      "Jinyi Qi",
      "Guobao Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2201.01443"
  },
  {
    "id": "arXiv:2201.01961",
    "title": "Diversity-boosted Generalization-Specialization Balancing for Zero-shot  Learning",
    "abstract": "Diversity-boosted Generalization-Specialization Balancing for Zero-shot  Learning",
    "descriptor": "",
    "authors": [
      "Yun Li",
      "Zhe Liu",
      "Xiaojun Chang",
      "Julian McAuley",
      "Lina Yao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.01961"
  },
  {
    "id": "arXiv:2201.02009",
    "title": "PAEG: Phrase-level Adversarial Example Generation for Neural Machine  Translation",
    "abstract": "Comments: 13 pages",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "Juncheng Wan",
      "Jian Yang",
      "Shuming Ma",
      "Dongdong Zhang",
      "Weinan Zhang",
      "Yong Yu",
      "Zhoujun Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.02009"
  },
  {
    "id": "arXiv:2201.02850",
    "title": "Image-based Automatic Dial Meter Reading in Unconstrained Scenarios",
    "abstract": "Image-based Automatic Dial Meter Reading in Unconstrained Scenarios",
    "descriptor": "",
    "authors": [
      "Gabriel Salomon",
      "Rayson Laroca",
      "David Menotti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.02850"
  },
  {
    "id": "arXiv:2201.05899",
    "title": "Unobserved Local Structures Make Compositional Generalization Hard",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Ben Bogin",
      "Shivanshu Gupta",
      "Jonathan Berant"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.05899"
  },
  {
    "id": "arXiv:2201.05955",
    "title": "WANLI: Worker and AI Collaboration for Natural Language Inference  Dataset Creation",
    "abstract": "Comments: EMNLP Findings camera-ready",
    "descriptor": "\nComments: EMNLP Findings camera-ready\n",
    "authors": [
      "Alisa Liu",
      "Swabha Swayamdipta",
      "Noah A. Smith",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.05955"
  },
  {
    "id": "arXiv:2201.06009",
    "title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Aman Madaan",
      "Niket Tandon",
      "Peter Clark",
      "Yiming Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.06009"
  },
  {
    "id": "arXiv:2201.08081",
    "title": "LEMON: Language-Based Environment Manipulation via Execution-Guided  Pre-training",
    "abstract": "Comments: EMNLP 2022 Findings",
    "descriptor": "\nComments: EMNLP 2022 Findings\n",
    "authors": [
      "Qi Shi",
      "Qian Liu",
      "Bei Chen",
      "Yu Zhang",
      "Ting Liu",
      "Jian-Guang Lou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.08081"
  },
  {
    "id": "arXiv:2201.08193",
    "title": "TextHacker: Learning based Hybrid Local Search Algorithm for Text  Hard-label Adversarial Attack",
    "abstract": "Comments: Accepted by EMNLP 2022 Findings, Code is available at this https URL",
    "descriptor": "\nComments: Accepted by EMNLP 2022 Findings, Code is available at this https URL\n",
    "authors": [
      "Zhen Yu",
      "Xiaosen Wang",
      "Wanxiang Che",
      "Kun He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.08193"
  },
  {
    "id": "arXiv:2201.11473",
    "title": "Reasoning Like Program Executors",
    "abstract": "Comments: To appear in EMNLP 2022 main conference. The first two authors contributed equally",
    "descriptor": "\nComments: To appear in EMNLP 2022 main conference. The first two authors contributed equally\n",
    "authors": [
      "Xinyu Pi",
      "Qian Liu",
      "Bei Chen",
      "Morteza Ziyadi",
      "Zeqi Lin",
      "Qiang Fu",
      "Yan Gao",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2201.11473"
  },
  {
    "id": "arXiv:2201.11945",
    "title": "Learning Proximal Operators to Discover Multiple Optima",
    "abstract": "Learning Proximal Operators to Discover Multiple Optima",
    "descriptor": "",
    "authors": [
      "Lingxiao Li",
      "Noam Aigerman",
      "Vladimir G. Kim",
      "Jiajin Li",
      "Kristjan Greenewald",
      "Mikhail Yurochkin",
      "Justin Solomon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.11945"
  },
  {
    "id": "arXiv:2201.12502",
    "title": "Unsupervised Multi-Granularity Summarization",
    "abstract": "Comments: EMNLP 2022 Findings",
    "descriptor": "\nComments: EMNLP 2022 Findings\n",
    "authors": [
      "Ming Zhong",
      "Yang Liu",
      "Suyu Ge",
      "Yuning Mao",
      "Yizhu Jiao",
      "Xingxing Zhang",
      "Yichong Xu",
      "Chenguang Zhu",
      "Michael Zeng",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2201.12502"
  },
  {
    "id": "arXiv:2201.12719",
    "title": "Faster Convergence of Local SGD for Over-Parameterized Models",
    "abstract": "Faster Convergence of Local SGD for Over-Parameterized Models",
    "descriptor": "",
    "authors": [
      "Tiancheng Qin",
      "S. Rasoul Etesami",
      "C\u00e9sar A. Uribe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2201.12719"
  },
  {
    "id": "arXiv:2201.13086",
    "title": "Securing Federated Sensitive Topic Classification against Poisoning  Attacks",
    "abstract": "Securing Federated Sensitive Topic Classification against Poisoning  Attacks",
    "descriptor": "",
    "authors": [
      "Tianyue Chu",
      "Alvaro Garcia-Recuero",
      "Costas Iordanou",
      "Georgios Smaragdakis",
      "Nikolaos Laoutaris"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2201.13086"
  },
  {
    "id": "arXiv:2201.13279",
    "title": "UQGAN: A Unified Model for Uncertainty Quantification of Deep  Classifiers trained via Conditional GANs",
    "abstract": "UQGAN: A Unified Model for Uncertainty Quantification of Deep  Classifiers trained via Conditional GANs",
    "descriptor": "",
    "authors": [
      "Philipp Oberdiek",
      "Gernot A. Fink",
      "Matthias Rottmann"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.13279"
  },
  {
    "id": "arXiv:2202.00293",
    "title": "Phase diagram of Stochastic Gradient Descent in high-dimensional  two-layer neural networks",
    "abstract": "Comments: 28 pages",
    "descriptor": "\nComments: 28 pages\n",
    "authors": [
      "Rodrigo Veiga",
      "Ludovic Stephan",
      "Bruno Loureiro",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.00293"
  },
  {
    "id": "arXiv:2202.00299",
    "title": "Learning Physics-Consistent Particle Interactions",
    "abstract": "Comments: Under review. 19 pages main content + 18 pages SI. Links of supporting code and data can be found at the end of main content",
    "descriptor": "\nComments: Under review. 19 pages main content + 18 pages SI. Links of supporting code and data can be found at the end of main content\n",
    "authors": [
      "Zhichao Han",
      "David S. Kammer",
      "Olga Fink"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2202.00299"
  },
  {
    "id": "arXiv:2202.02947",
    "title": "Parallel Successive Learning for Dynamic Distributed Model Training over  Heterogeneous Wireless Networks",
    "abstract": "Parallel Successive Learning for Dynamic Distributed Model Training over  Heterogeneous Wireless Networks",
    "descriptor": "",
    "authors": [
      "Seyyedali Hosseinalipour",
      "Su Wang",
      "Nicolo Michelusi",
      "Vaneet Aggarwal",
      "Christopher G. Brinton",
      "David J. Love",
      "Mung Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2202.02947"
  },
  {
    "id": "arXiv:2202.04136",
    "title": "Generative multitask learning mitigates target-causing confounding",
    "abstract": "Generative multitask learning mitigates target-causing confounding",
    "descriptor": "",
    "authors": [
      "Taro Makino",
      "Krzysztof J. Geras",
      "Kyunghyun Cho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.04136"
  },
  {
    "id": "arXiv:2202.04173",
    "title": "Exploring the Limits of Domain-Adaptive Training for Detoxifying  Large-Scale Language Models",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Boxin Wang",
      "Wei Ping",
      "Chaowei Xiao",
      "Peng Xu",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bo Li",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04173"
  },
  {
    "id": "arXiv:2202.04377",
    "title": "Constant Approximating Parameterized $k$-SetCover is W[2]-hard",
    "abstract": "Constant Approximating Parameterized $k$-SetCover is W[2]-hard",
    "descriptor": "",
    "authors": [
      "Bingkai Lin",
      "Xuandi Ren",
      "Yican Sun",
      "Xiuhan Wang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2202.04377"
  },
  {
    "id": "arXiv:2202.05262",
    "title": "Locating and Editing Factual Associations in GPT",
    "abstract": "Comments: 35 pages, 30 figures. Code and data at this https URL",
    "descriptor": "\nComments: 35 pages, 30 figures. Code and data at this https URL\n",
    "authors": [
      "Kevin Meng",
      "David Bau",
      "Alex Andonian",
      "Yonatan Belinkov"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.05262"
  },
  {
    "id": "arXiv:2202.07340",
    "title": "Low-rank tensor approximations for solving multi-marginal optimal  transport problems",
    "abstract": "Low-rank tensor approximations for solving multi-marginal optimal  transport problems",
    "descriptor": "",
    "authors": [
      "Christoph Str\u00f6ssner",
      "Daniel Kressner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2202.07340"
  },
  {
    "id": "arXiv:2202.07922",
    "title": "ZeroGen: Efficient Zero-shot Learning via Dataset Generation",
    "abstract": "Comments: Accepted by EMNLP 2022 (Main Conference)",
    "descriptor": "\nComments: Accepted by EMNLP 2022 (Main Conference)\n",
    "authors": [
      "Jiacheng Ye",
      "Jiahui Gao",
      "Qintong Li",
      "Hang Xu",
      "Jiangtao Feng",
      "Zhiyong Wu",
      "Tao Yu",
      "Lingpeng Kong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.07922"
  },
  {
    "id": "arXiv:2202.07962",
    "title": "Revisiting Parameter-Efficient Tuning: Are We Really There Yet?",
    "abstract": "Comments: EMNLP 2022 main conference",
    "descriptor": "\nComments: EMNLP 2022 main conference\n",
    "authors": [
      "Guanzheng Chen",
      "Fangyu Liu",
      "Zaiqiao Meng",
      "Shangsong Liang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.07962"
  },
  {
    "id": "arXiv:2202.08423",
    "title": "Chord-Conditioned Melody Harmonization with Controllable Harmonicity",
    "abstract": "Comments: 5 pages, 7 figures",
    "descriptor": "\nComments: 5 pages, 7 figures\n",
    "authors": [
      "Shangda Wu",
      "Xiaobing Li",
      "Maosong Sun"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2202.08423"
  },
  {
    "id": "arXiv:2202.08682",
    "title": "A General Deep Learning framework for Neuron Instance Segmentation based  on Efficient UNet and Morphological Post-processing",
    "abstract": "A General Deep Learning framework for Neuron Instance Segmentation based  on Efficient UNet and Morphological Post-processing",
    "descriptor": "",
    "authors": [
      "Huaqian Wu",
      "Nicolas Souedet",
      "Caroline Jan",
      "C\u00e9dric Clouchoux",
      "Thierry Delzescaux"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.08682"
  },
  {
    "id": "arXiv:2202.10574",
    "title": "A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation  in Two-sided Markets",
    "abstract": "A Multi-Agent Reinforcement Learning Framework for Off-Policy Evaluation  in Two-sided Markets",
    "descriptor": "",
    "authors": [
      "Chengchun Shi",
      "Runzhe Wan",
      "Ge Song",
      "Shikai Luo",
      "Rui Song",
      "Hongtu Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10574"
  },
  {
    "id": "arXiv:2202.11479",
    "title": "Listen to Interpret: Post-hoc Interpretability for Audio Networks with  NMF",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Jayneel Parekh",
      "Sanjeel Parekh",
      "Pavlo Mozharovskyi",
      "Florence d'Alch\u00e9-Buc",
      "Ga\u00ebl Richard"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2202.11479"
  },
  {
    "id": "arXiv:2202.12459",
    "title": "APEACH: Attacking Pejorative Expressions with Analysis on  Crowd-Generated Hate Speech Evaluation Datasets",
    "abstract": "Comments: 11pages, 6 figures",
    "descriptor": "\nComments: 11pages, 6 figures\n",
    "authors": [
      "Kichang Yang",
      "Wonjun Jang",
      "Won Ik Cho"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2202.12459"
  },
  {
    "id": "arXiv:2202.13826",
    "title": "Magnitude-aware Probabilistic Speaker Embeddings",
    "abstract": "Comments: Accepted to Odyssey 2022: The Speaker and Language Recognition Workshop, camera-ready version",
    "descriptor": "\nComments: Accepted to Odyssey 2022: The Speaker and Language Recognition Workshop, camera-ready version\n",
    "authors": [
      "Nikita Kuzmin",
      "Igor Fedorov",
      "Alexey Sholokhov"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2202.13826"
  },
  {
    "id": "arXiv:2203.00241",
    "title": "Pond: CXL-Based Memory Pooling Systems for Cloud Platforms",
    "abstract": "Comments: Update affiliations",
    "descriptor": "\nComments: Update affiliations\n",
    "authors": [
      "Huaicheng Li",
      "Daniel S. Berger",
      "Stanko Novakovic",
      "Lisa Hsu",
      "Dan Ernst",
      "Pantea Zardoshti",
      "Monish Shah",
      "Samir Rajadnya",
      "Scott Lee",
      "Ishwar Agarwal",
      "Mark D. Hill",
      "Marcus Fontoura",
      "Ricardo Bianchini"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2203.00241"
  },
  {
    "id": "arXiv:2203.00997",
    "title": "Whole-body model predictive control with rigid contacts via online  switching time optimization",
    "abstract": "Comments: 8 pages, 10 figures. This work has been accepted to be presented at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)",
    "descriptor": "\nComments: 8 pages, 10 figures. This work has been accepted to be presented at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022)\n",
    "authors": [
      "Sotaro Katayama",
      "Toshiyuki Ohtsuka"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2203.00997"
  },
  {
    "id": "arXiv:2203.01382",
    "title": "Nemo: Guiding and Contextualizing Weak Supervision for Interactive Data  Programming",
    "abstract": "Comments: To appear in PVLDB 2022",
    "descriptor": "\nComments: To appear in PVLDB 2022\n",
    "authors": [
      "Cheng-Yu Hsieh",
      "Jieyu Zhang",
      "Alexander Ratner"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.01382"
  },
  {
    "id": "arXiv:2203.02700",
    "title": "RACE: Retrieval-Augmented Commit Message Generation",
    "abstract": "Comments: Accepted by EMNLP 2022 (The 2022 Conference on Empirical Methods in Natural Language Processing)",
    "descriptor": "\nComments: Accepted by EMNLP 2022 (The 2022 Conference on Empirical Methods in Natural Language Processing)\n",
    "authors": [
      "Ensheng Shi",
      "Yanlin Wang",
      "Wei Tao",
      "Lun Du",
      "Hongyu Zhang",
      "Shi Han",
      "Dongmei Zhang",
      "Hongbin Sun"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.02700"
  },
  {
    "id": "arXiv:2203.04115",
    "title": "Biological Sequence Design with GFlowNets",
    "abstract": "Comments: ICML 2022. 15 pages, 3 figures. Code available at: this https URL",
    "descriptor": "\nComments: ICML 2022. 15 pages, 3 figures. Code available at: this https URL\n",
    "authors": [
      "Moksh Jain",
      "Emmanuel Bengio",
      "Alex-Hernandez Garcia",
      "Jarrid Rector-Brooks",
      "Bonaventure F. P. Dossou",
      "Chanakya Ekbote",
      "Jie Fu",
      "Tianyu Zhang",
      "Micheal Kilgour",
      "Dinghuai Zhang",
      "Lena Simine",
      "Payel Das",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.04115"
  },
  {
    "id": "arXiv:2203.04212",
    "title": "Measuring the Mixing of Contextual Information in the Transformer",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Javier Ferrando",
      "Gerard I. G\u00e1llego",
      "Marta R. Costa-juss\u00e0"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.04212"
  },
  {
    "id": "arXiv:2203.04427",
    "title": "Experimental Security Analysis of the App Model in Business  Collaboration Platforms",
    "abstract": "Experimental Security Analysis of the App Model in Business  Collaboration Platforms",
    "descriptor": "",
    "authors": [
      "Yunang Chen",
      "Yue Gao",
      "Nick Ceccio",
      "Rahul Chatterjee",
      "Kassem Fawaz",
      "Earlence Fernandes"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2203.04427"
  },
  {
    "id": "arXiv:2203.06313",
    "title": "Performance Analysis of Intelligent Reflecting Surface Assisted  Opportunistic Communications",
    "abstract": "Comments: 17 pages, 9 figures",
    "descriptor": "\nComments: 17 pages, 9 figures\n",
    "authors": [
      "L. Yashvanth",
      "Chandra R. Murthy"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2203.06313"
  },
  {
    "id": "arXiv:2203.07527",
    "title": "A Linearly Convergent Douglas-Rachford Splitting Solver for Markovian  Information-Theoretic Optimization Problems",
    "abstract": "A Linearly Convergent Douglas-Rachford Splitting Solver for Markovian  Information-Theoretic Optimization Problems",
    "descriptor": "",
    "authors": [
      "Teng-Hui Huang",
      "Aly El Gamal",
      "Hesham El Gamal"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2203.07527"
  },
  {
    "id": "arXiv:2203.08118",
    "title": "Representation Learning for Resource-Constrained Keyphrase Generation",
    "abstract": "Comments: EMNLP 2022 (Findings)",
    "descriptor": "\nComments: EMNLP 2022 (Findings)\n",
    "authors": [
      "Di Wu",
      "Wasi Uddin Ahmad",
      "Sunipa Dev",
      "Kai-Wei Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08118"
  },
  {
    "id": "arXiv:2203.08252",
    "title": "Wind energy forecasting with missing values within a fully conditional  specification framework",
    "abstract": "Comments: revision to International Journal of Forecasting",
    "descriptor": "\nComments: revision to International Journal of Forecasting\n",
    "authors": [
      "Honglin Wen",
      "Pierre Pinson",
      "Jie Gu",
      "Zhijian Jin"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.08252"
  },
  {
    "id": "arXiv:2203.08377",
    "title": "RIS Partitioning Based Scalable Beamforming Design for Large-Scale MIMO:  Asymptotic Analysis and Optimization",
    "abstract": "Comments: under major revision with IEEE Transactions on Wireless Communications",
    "descriptor": "\nComments: under major revision with IEEE Transactions on Wireless Communications\n",
    "authors": [
      "Chang Cai",
      "Xiaojun Yuan",
      "Ying-Jun Angela Zhang"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2203.08377"
  },
  {
    "id": "arXiv:2203.08383",
    "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought",
    "abstract": "Comments: EMNLP-2022 Camera-Ready. Code will be available at this https URL",
    "descriptor": "\nComments: EMNLP-2022 Camera-Ready. Code will be available at this https URL\n",
    "authors": [
      "Boshi Wang",
      "Xiang Deng",
      "Huan Sun"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.08383"
  },
  {
    "id": "arXiv:2203.08597",
    "title": "Less is More: Summary of Long Instructions is Better for Program  Synthesis",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Kirby Kuznia",
      "Swaroop Mishra",
      "Mihir Parmar",
      "Chitta Baral"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.08597"
  },
  {
    "id": "arXiv:2203.09251",
    "title": "Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs",
    "abstract": "Near Instance-Optimal PAC Reinforcement Learning for Deterministic MDPs",
    "descriptor": "",
    "authors": [
      "Andrea Tirinzoni",
      "Aymen Al-Marjani",
      "Emilie Kaufmann"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.09251"
  },
  {
    "id": "arXiv:2203.09590",
    "title": "Enhanced Temporal Knowledge Embeddings with Contextualized Language  Representations",
    "abstract": "Comments: 10 pages",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Zhen Han",
      "Ruotong Liao",
      "Beiyan Liu",
      "Yao Zhang",
      "Zifeng Ding",
      "Jindong Gu",
      "Heinz K\u00f6ppl",
      "Hinrich Sch\u00fctze",
      "Volker Tresp"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.09590"
  },
  {
    "id": "arXiv:2203.10117",
    "title": "On the role of Lip Articulation in Visual Speech Perception",
    "abstract": "Comments: Submitted to ICASSP 2023",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Zakaria Aldeneh",
      "Masha Fedzechkina",
      "Skyler Seto",
      "Katherine Metcalf",
      "Miguel Sarabia",
      "Nicholas Apostoloff",
      "Barry-John Theobald"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2203.10117"
  },
  {
    "id": "arXiv:2203.13530",
    "title": "Multimodal Pre-training Based on Graph Attention Network for Document  Understanding",
    "abstract": "Multimodal Pre-training Based on Graph Attention Network for Document  Understanding",
    "descriptor": "",
    "authors": [
      "Zhenrong Zhang",
      "Jiefeng Ma",
      "Jun Du",
      "Licheng Wang",
      "Jianshu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.13530"
  },
  {
    "id": "arXiv:2203.13950",
    "title": "Multi-Edge Server-Assisted Dynamic Federated Learning with an Optimized  Floating Aggregation Point",
    "abstract": "Multi-Edge Server-Assisted Dynamic Federated Learning with an Optimized  Floating Aggregation Point",
    "descriptor": "",
    "authors": [
      "Bhargav Ganguly",
      "Seyyedali Hosseinalipour",
      "Kwang Taik Kim",
      "Christopher G. Brinton",
      "Vaneet Aggarwal",
      "David J. Love",
      "Mung Chiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2203.13950"
  },
  {
    "id": "arXiv:2203.14603",
    "title": "The SAME score: Improved cosine based bias score for word embeddings",
    "abstract": "Comments: 17 pages, 5 figures. arXiv admin note: substantial text overlap with arXiv:2111.07864",
    "descriptor": "\nComments: 17 pages, 5 figures. arXiv admin note: substantial text overlap with arXiv:2111.07864\n",
    "authors": [
      "Sarah Schr\u00f6der",
      "Alexander Schulz",
      "Philip Kenneweg",
      "Robert Feldhans",
      "Fabian Hinder",
      "Barbara Hammer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2203.14603"
  },
  {
    "id": "arXiv:2203.14838",
    "title": "Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition",
    "abstract": "Comments: 5 pages, 3 figures",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Yuchen Hu",
      "Nana Hou",
      "Chen Chen",
      "Eng Siong Chng"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2203.14838"
  },
  {
    "id": "arXiv:2203.15219",
    "title": "AR Point&Click: An Interface for Setting Robot Navigation Goals",
    "abstract": "Comments: Accepted at ICSR 2022 \"14th International Conference on Social Robotics\", 6 Pages, 5 Figures, 4 Tables",
    "descriptor": "\nComments: Accepted at ICSR 2022 \"14th International Conference on Social Robotics\", 6 Pages, 5 Figures, 4 Tables\n",
    "authors": [
      "Morris Gu",
      "Elizabeth Croft",
      "Akansel Cosgun"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2203.15219"
  },
  {
    "id": "arXiv:2203.15312",
    "title": "In-N-Out Generative Learning for Dense Unsupervised Video Segmentation",
    "abstract": "Comments: Accepted by ACM MM 2022",
    "descriptor": "\nComments: Accepted by ACM MM 2022\n",
    "authors": [
      "Xiao Pan",
      "Peike Li",
      "Zongxin Yang",
      "Huiling Zhou",
      "Chang Zhou",
      "Hongxia Yang",
      "Jingren Zhou",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.15312"
  },
  {
    "id": "arXiv:2203.16214",
    "title": "Adaptive Divergence-based Non-negative Latent Factor Analysis",
    "abstract": "Adaptive Divergence-based Non-negative Latent Factor Analysis",
    "descriptor": "",
    "authors": [
      "Ye Yuan",
      "Guangxiao Yuan",
      "Renfang Wang",
      "Xin Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.16214"
  },
  {
    "id": "arXiv:2203.17114",
    "title": "A Methodology for Abstracting the Physical Layer of Direct V2X  Communications Technologies",
    "abstract": "A Methodology for Abstracting the Physical Layer of Direct V2X  Communications Technologies",
    "descriptor": "",
    "authors": [
      "Wu Zhuofei",
      "Stefania Bartoletti",
      "Vincent Martinez",
      "Alessandro Bazzi"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2203.17114"
  },
  {
    "id": "arXiv:2203.17144",
    "title": "Instability of backoff protocols with arbitrary arrival rates",
    "abstract": "Instability of backoff protocols with arbitrary arrival rates",
    "descriptor": "",
    "authors": [
      "Leslie Ann Goldberg",
      "John Lapinskas"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Discrete Mathematics (cs.DM)",
      "Networking and Internet Architecture (cs.NI)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2203.17144"
  },
  {
    "id": "arXiv:2204.00004",
    "title": "Reproducibility Issues for BERT-based Evaluation Metrics",
    "abstract": "Comments: EMNLP 2022 Camera-ready",
    "descriptor": "\nComments: EMNLP 2022 Camera-ready\n",
    "authors": [
      "Yanran Chen",
      "Jonas Belouadi",
      "Steffen Eger"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.00004"
  },
  {
    "id": "arXiv:2204.02592",
    "title": "Thinking inside The Box: Learning Hypercube Representations for Group  Recommendation",
    "abstract": "Comments: SIGIR'22",
    "descriptor": "\nComments: SIGIR'22\n",
    "authors": [
      "Tong Chen",
      "Hongzhi Yin",
      "Jing Long",
      "Quoc Viet Hung Nguyen",
      "Yang Wang",
      "Meng Wang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2204.02592"
  },
  {
    "id": "arXiv:2204.03051",
    "title": "Perceive, Represent, Generate: Translating Multimodal Information to  Robotic Motion Trajectories",
    "abstract": "Comments: 14 pages, 4 figures, 8 tables, 1 algorithm",
    "descriptor": "\nComments: 14 pages, 4 figures, 8 tables, 1 algorithm\n",
    "authors": [
      "F\u00e1bio Vital",
      "Miguel Vasco",
      "Alberto Sardinha",
      "Francisco Melo"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.03051"
  },
  {
    "id": "arXiv:2204.03897",
    "title": "Sim-to-Real Learning of Compliant Bipedal Locomotion on Torque  Sensor-Less Gear-Driven Humanoid",
    "abstract": "Comments: An accompanying video is available at the following link: this https URL",
    "descriptor": "\nComments: An accompanying video is available at the following link: this https URL\n",
    "authors": [
      "Shimpei Masuda",
      "Kuniyuki Takahashi"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.03897"
  },
  {
    "id": "arXiv:2204.03943",
    "title": "Tensor approximation of the self-diffusion matrix of tagged particle  processes",
    "abstract": "Tensor approximation of the self-diffusion matrix of tagged particle  processes",
    "descriptor": "",
    "authors": [
      "Jad Dabaghi",
      "Virginie Ehrlacher",
      "Christoph Str\u00f6ssner"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2204.03943"
  },
  {
    "id": "arXiv:2204.04058",
    "title": "Improving Tokenisation by Alternative Treatment of Spaces",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Edward Gow-Smith",
      "Harish Tayyar Madabushi",
      "Carolina Scarton",
      "Aline Villavicencio"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.04058"
  },
  {
    "id": "arXiv:2204.05158",
    "title": "Gaining Insights into Unrecognized User Utterances in Task-Oriented  Dialog Systems",
    "abstract": "Comments: Accepted at EMNLP 2022 (industry track), 8 pages",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (industry track), 8 pages\n",
    "authors": [
      "Ella Rabinovich",
      "Matan Vetzler",
      "David Boaz",
      "Vineet Kumar",
      "Gaurav Pandey",
      "Ateret Anaby-Tavor"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.05158"
  },
  {
    "id": "arXiv:2204.05289",
    "title": "Towards Online Domain Adaptive Object Detection",
    "abstract": "Comments: Accepted to WACV 2023",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Vibashan VS",
      "Poojan Oza",
      "Vishal M. Patel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.05289"
  },
  {
    "id": "arXiv:2204.07496",
    "title": "Improving Passage Retrieval with Zero-Shot Question Generation",
    "abstract": "Comments: EMNLP 2022 camera-ready version",
    "descriptor": "\nComments: EMNLP 2022 camera-ready version\n",
    "authors": [
      "Devendra Singh Sachan",
      "Mike Lewis",
      "Mandar Joshi",
      "Armen Aghajanyan",
      "Wen-tau Yih",
      "Joelle Pineau",
      "Luke Zettlemoyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2204.07496"
  },
  {
    "id": "arXiv:2204.07667",
    "title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language  Models",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Weiyan Shi",
      "Ryan Shea",
      "Si Chen",
      "Chiyuan Zhang",
      "Ruoxi Jia",
      "Zhou Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2204.07667"
  },
  {
    "id": "arXiv:2204.07705",
    "title": "Super-NaturalInstructions: Generalization via Declarative Instructions  on 1600+ NLP Tasks",
    "abstract": "Comments: Accepted to EMNLP 2022, 25 pages",
    "descriptor": "\nComments: Accepted to EMNLP 2022, 25 pages\n",
    "authors": [
      "Yizhong Wang",
      "Swaroop Mishra",
      "Pegah Alipoormolabashi",
      "Yeganeh Kordi",
      "Amirreza Mirzaei",
      "Anjana Arunkumar",
      "Arjun Ashok",
      "Arut Selvan Dhanasekaran",
      "Atharva Naik",
      "David Stap",
      "Eshaan Pathak",
      "Giannis Karamanolakis",
      "Haizhi Gary Lai",
      "Ishan Purohit",
      "Ishani Mondal",
      "Jacob Anderson",
      "Kirby Kuznia",
      "Krima Doshi",
      "Maitreya Patel",
      "Kuntal Kumar Pal",
      "Mehrad Moradshahi",
      "Mihir Parmar",
      "Mirali Purohit",
      "Neeraj Varshney",
      "Phani Rohitha Kaza",
      "Pulkit Verma",
      "Ravsehaj Singh Puri",
      "Rushang Karia",
      "Shailaja Keyur Sampat",
      "Savan Doshi",
      "Siddhartha Mishra",
      "Sujan Reddy",
      "Sumanta Patro",
      "Tanay Dixit",
      "Xudong Shen",
      "Chitta Baral",
      "Yejin Choi",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Daniel Khashabi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.07705"
  },
  {
    "id": "arXiv:2204.08109",
    "title": "ArcaneQA: Dynamic Program Induction and Contextualized Encoding for  Knowledge Base Question Answering",
    "abstract": "Comments: 14 pages; COLING'22 Outstanding Paper Award; numbers on WebQSP fixed",
    "descriptor": "\nComments: 14 pages; COLING'22 Outstanding Paper Award; numbers on WebQSP fixed\n",
    "authors": [
      "Yu Gu",
      "Yu Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.08109"
  },
  {
    "id": "arXiv:2204.08142",
    "title": "Dynamic Position Encoding for Transformers",
    "abstract": "Dynamic Position Encoding for Transformers",
    "descriptor": "",
    "authors": [
      "Joyce Zheng",
      "Mehdi Rezagholizadeh",
      "Peyman Passban"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.08142"
  },
  {
    "id": "arXiv:2204.08189",
    "title": "Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile  Edge",
    "abstract": "Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile  Edge",
    "descriptor": "",
    "authors": [
      "Qun Song",
      "Zhenyu Yan",
      "Wenjie Luo",
      "Rui Tan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.08189"
  },
  {
    "id": "arXiv:2204.08516",
    "title": "LwHBench: A low-level hardware component benchmark and dataset for  Single Board Computers",
    "abstract": "LwHBench: A low-level hardware component benchmark and dataset for  Single Board Computers",
    "descriptor": "",
    "authors": [
      "Pedro Miguel S\u00e1nchez S\u00e1nchez",
      "Jos\u00e9 Mar\u00eda Jorquera Valero",
      "Alberto Huertas Celdr\u00e1n",
      "G\u00e9r\u00f4me Bovet",
      "Manuel Gil P\u00e9rez",
      "Gregorio Mart\u00ednez P\u00e9rez"
    ],
    "subjectives": [
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2204.08516"
  },
  {
    "id": "arXiv:2204.08858",
    "title": "An Investigation of Monotonic Transducers for Large-Scale Automatic  Speech Recognition",
    "abstract": "Comments: Accepted to SLT 2022",
    "descriptor": "\nComments: Accepted to SLT 2022\n",
    "authors": [
      "Niko Moritz",
      "Frank Seide",
      "Duc Le",
      "Jay Mahadeokar",
      "Christian Fuegen"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2204.08858"
  },
  {
    "id": "arXiv:2204.08915",
    "title": "Bots, Disinformation, and the First Trump Impeachment",
    "abstract": "Bots, Disinformation, and the First Trump Impeachment",
    "descriptor": "",
    "authors": [
      "Michael Rossetti",
      "Tauhid Zaman"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2204.08915"
  },
  {
    "id": "arXiv:2204.09222",
    "title": "K-LITE: Learning Transferable Visual Models with External Knowledge",
    "abstract": "Comments: NeurIPS 2022 camera ready",
    "descriptor": "\nComments: NeurIPS 2022 camera ready\n",
    "authors": [
      "Sheng Shen",
      "Chunyuan Li",
      "Xiaowei Hu",
      "Jianwei Yang",
      "Yujia Xie",
      "Pengchuan Zhang",
      "Zhe Gan",
      "Lijuan Wang",
      "Lu Yuan",
      "Ce Liu",
      "Kurt Keutzer",
      "Trevor Darrell",
      "Anna Rohrbach",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.09222"
  },
  {
    "id": "arXiv:2204.09649",
    "title": "BliMe: Verifiably Secure Outsourced Computation with Hardware-Enforced  Taint Tracking",
    "abstract": "BliMe: Verifiably Secure Outsourced Computation with Hardware-Enforced  Taint Tracking",
    "descriptor": "",
    "authors": [
      "Hossam ElAtali",
      "Lachlan J. Gunn",
      "Hans Liljestrand",
      "N. Asokan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2204.09649"
  },
  {
    "id": "arXiv:2204.10293",
    "title": "A Hierarchical N-Gram Framework for Zero-Shot Link Prediction",
    "abstract": "Comments: Published as a conference paper at EMNLP Findings 2022",
    "descriptor": "\nComments: Published as a conference paper at EMNLP Findings 2022\n",
    "authors": [
      "Mingchen Li",
      "Junfan Chen",
      "Samuel Mensah",
      "Nikolaos Aletras",
      "Xiulong Yang",
      "Yang Ye"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.10293"
  },
  {
    "id": "arXiv:2204.10757",
    "title": "FaithDial: A Faithful Benchmark for Information-Seeking Dialogue",
    "abstract": "Comments: TACL 2022 (20 pages, 3 figures, 10 tables)",
    "descriptor": "\nComments: TACL 2022 (20 pages, 3 figures, 10 tables)\n",
    "authors": [
      "Nouha Dziri",
      "Ehsan Kamalloo",
      "Sivan Milton",
      "Osmar Zaiane",
      "Mo Yu",
      "Edoardo M. Ponti",
      "Siva Reddy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.10757"
  },
  {
    "id": "arXiv:2204.11621",
    "title": "Semantic Geometric Fusion Multi-object Tracking and Lidar Odometry in  Dynamic Environment",
    "abstract": "Semantic Geometric Fusion Multi-object Tracking and Lidar Odometry in  Dynamic Environment",
    "descriptor": "",
    "authors": [
      "Tingchen Ma",
      "Yongsheng Ou"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2204.11621"
  },
  {
    "id": "arXiv:2204.11824",
    "title": "Semi-Parametric Neural Image Synthesis",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Andreas Blattmann",
      "Robin Rombach",
      "Kaan Oktay",
      "Jonas M\u00fcller",
      "Bj\u00f6rn Ommer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.11824"
  },
  {
    "id": "arXiv:2204.13074",
    "title": "Towards Teachable Reasoning Systems: Using a Dynamic Memory of User  Feedback for Continual System Improvement",
    "abstract": "Comments: accepted at EMNLP 2022",
    "descriptor": "\nComments: accepted at EMNLP 2022\n",
    "authors": [
      "Bhavana Dalvi Mishra",
      "Oyvind Tafjord",
      "Peter Clark"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.13074"
  },
  {
    "id": "arXiv:2204.13221",
    "title": "TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal  Restriction",
    "abstract": "Comments: EMNLP 2022. v2 updated for EMNLP camera-ready",
    "descriptor": "\nComments: EMNLP 2022. v2 updated for EMNLP camera-ready\n",
    "authors": [
      "Yizhi Li",
      "Wei Fan",
      "Chao Liu",
      "Chenghua Lin",
      "Jiang Qian"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13221"
  },
  {
    "id": "arXiv:2204.14017",
    "title": "Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient  Ensembling",
    "abstract": "Comments: Accepted to EMNLP 2022, 9 pages and Appendix",
    "descriptor": "\nComments: Accepted to EMNLP 2022, 9 pages and Appendix\n",
    "authors": [
      "KiYoon Yoo",
      "Nojun Kwak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.14017"
  },
  {
    "id": "arXiv:2205.00165",
    "title": "NeuralEF: Deconstructing Kernels by Deep Neural Networks",
    "abstract": "Comments: International Conference on Machine Learning (ICML), 2022",
    "descriptor": "\nComments: International Conference on Machine Learning (ICML), 2022\n",
    "authors": [
      "Zhijie Deng",
      "Jiaxin Shi",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.00165"
  },
  {
    "id": "arXiv:2205.00968",
    "title": "Detection Recovery in Online Multi-Object Tracking with Sparse Graph  Tracker",
    "abstract": "Comments: Accepted to WACV 2023",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Jeongseok Hyun",
      "Myunggu Kang",
      "Dongyoon Wee",
      "Dit-Yan Yeung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.00968"
  },
  {
    "id": "arXiv:2205.02591",
    "title": "PI-NLF: A Proportional-Integral Approach for Non-negative Latent Factor  Analysis",
    "abstract": "PI-NLF: A Proportional-Integral Approach for Non-negative Latent Factor  Analysis",
    "descriptor": "",
    "authors": [
      "Ye Yuan",
      "Xin Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02591"
  },
  {
    "id": "arXiv:2205.03116",
    "title": "Longitudinal cardio-respiratory fitness prediction through wearables in  free-living environments",
    "abstract": "Comments: Accepted in Nature Digital Medicine, 16 pages",
    "descriptor": "\nComments: Accepted in Nature Digital Medicine, 16 pages\n",
    "authors": [
      "Dimitris Spathis",
      "Ignacio Perez-Pozuelo",
      "Tomas I. Gonzales",
      "Yu Wu",
      "Soren Brage",
      "Nicholas Wareham",
      "Cecilia Mascolo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.03116"
  },
  {
    "id": "arXiv:2205.03284",
    "title": "Dimension Reduction for Efficient Dense Retrieval via Conditional  Autoencoder",
    "abstract": "Comments: Accepted by EMNLP 2022",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Zhenghao Liu",
      "Han Zhang",
      "Chenyan Xiong",
      "Zhiyuan Liu",
      "Yu Gu",
      "Xiaohua Li"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2205.03284"
  },
  {
    "id": "arXiv:2205.04180",
    "title": "EF-BV: A Unified Theory of Error Feedback and Variance Reduction  Mechanisms for Biased and Unbiased Compression in Distributed Optimization",
    "abstract": "Comments: Conference NeurIPS 2022",
    "descriptor": "\nComments: Conference NeurIPS 2022\n",
    "authors": [
      "Laurent Condat",
      "Kai Yi",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.04180"
  },
  {
    "id": "arXiv:2205.05831",
    "title": "Feature Extractor Stacking for Cross-domain Few-shot Meta-learning",
    "abstract": "Feature Extractor Stacking for Cross-domain Few-shot Meta-learning",
    "descriptor": "",
    "authors": [
      "Hongyu Wang",
      "Eibe Frank",
      "Bernhard Pfahringer",
      "Michael Mayo",
      "Geoffrey Holmes"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.05831"
  },
  {
    "id": "arXiv:2205.07085",
    "title": "Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging",
    "abstract": "Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging",
    "descriptor": "",
    "authors": [
      "David Ahmedt-Aristizabal",
      "Chuong Nguyen",
      "Lachlan Tychsen-Smith",
      "Ashley Stacey",
      "Shenghong Li",
      "Joseph Pathikulangara",
      "Lars Petersson",
      "Dadong Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.07085"
  },
  {
    "id": "arXiv:2205.07257",
    "title": "Not to Overfit or Underfit the Source Domains? An Empirical Study of  Domain Generalization in Question Answering",
    "abstract": "Not to Overfit or Underfit the Source Domains? An Empirical Study of  Domain Generalization in Question Answering",
    "descriptor": "",
    "authors": [
      "Md Arafat Sultan",
      "Avirup Sil",
      "Radu Florian"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.07257"
  },
  {
    "id": "arXiv:2205.07593",
    "title": "A Parallel Algorithm for $(3 + \\varepsilon)$-Approximate Correlation  Clustering",
    "abstract": "A Parallel Algorithm for $(3 + \\varepsilon)$-Approximate Correlation  Clustering",
    "descriptor": "",
    "authors": [
      "M\u00e9lanie Cambus",
      "Shreyas Pai",
      "Jara Uitto"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.07593"
  },
  {
    "id": "arXiv:2205.07686",
    "title": "CQR-SQL: Conversational Question Reformulation Enhanced  Context-Dependent Text-to-SQL Parsers",
    "abstract": "Comments: Accepted at EMNLP 2022 (findings)",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (findings)\n",
    "authors": [
      "Dongling Xiao",
      "Linzheng Chai",
      "Qian-Wen Zhang",
      "Zhao Yan",
      "Zhoujun Li",
      "Yunbo Cao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.07686"
  },
  {
    "id": "arXiv:2205.08232",
    "title": "LogicSolver: Towards Interpretable Math Word Problem Solving with  Logical Prompt-enhanced Learning",
    "abstract": "LogicSolver: Towards Interpretable Math Word Problem Solving with  Logical Prompt-enhanced Learning",
    "descriptor": "",
    "authors": [
      "Zhicheng Yang",
      "Jinghui Qin",
      "Jiaqi Chen",
      "Liang Lin",
      "Xiaodan Liang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.08232"
  },
  {
    "id": "arXiv:2205.08534",
    "title": "Vision Transformer Adapter for Dense Predictions",
    "abstract": "Vision Transformer Adapter for Dense Predictions",
    "descriptor": "",
    "authors": [
      "Zhe Chen",
      "Yuchen Duan",
      "Wenhai Wang",
      "Junjun He",
      "Tong Lu",
      "Jifeng Dai",
      "Yu Qiao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.08534"
  },
  {
    "id": "arXiv:2205.09076",
    "title": "On the complexity of recognizing Stick, BipHook and Max Point-Tolerance  graphs",
    "abstract": "Comments: 24 pages, 12 figures",
    "descriptor": "\nComments: 24 pages, 12 figures\n",
    "authors": [
      "Irena Rusu"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2205.09076"
  },
  {
    "id": "arXiv:2205.09548",
    "title": "ODBO: Bayesian Optimization with Search Space Prescreening for Directed  Protein Evolution",
    "abstract": "Comments: 27 pages, 13 figures",
    "descriptor": "\nComments: 27 pages, 13 figures\n",
    "authors": [
      "Lixue Cheng",
      "Ziyi Yang",
      "Changyu Hsieh",
      "Benben Liao",
      "Shengyu Zhang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2205.09548"
  },
  {
    "id": "arXiv:2205.09612",
    "title": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence  Network",
    "abstract": "CLCNet: Rethinking of Ensemble Modeling with Classification Confidence  Network",
    "descriptor": "",
    "authors": [
      "Yao-Ching Yu",
      "Shi-Jinn Horng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.09612"
  },
  {
    "id": "arXiv:2205.09630",
    "title": "Acceptability Judgements via Examining the Topology of Attention Maps",
    "abstract": "Comments: Accepted to EMNLP 2022 Findings",
    "descriptor": "\nComments: Accepted to EMNLP 2022 Findings\n",
    "authors": [
      "Daniil Cherniavskii",
      "Eduard Tulchinskii",
      "Vladislav Mikhailov",
      "Irina Proskurina",
      "Laida Kushnareva",
      "Ekaterina Artemova",
      "Serguei Barannikov",
      "Irina Piontkovskaya",
      "Dmitri Piontkovski",
      "Evgeny Burnaev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09630"
  },
  {
    "id": "arXiv:2205.10772",
    "title": "Fast Instrument Learning with Faster Rates",
    "abstract": "Comments: NeurIPS camera ready. Code available at this https URL",
    "descriptor": "\nComments: NeurIPS camera ready. Code available at this https URL\n",
    "authors": [
      "Ziyu Wang",
      "Yuhao Zhou",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Econometrics (econ.EM)"
    ],
    "url": "https://arxiv.org/abs/2205.10772"
  },
  {
    "id": "arXiv:2205.10843",
    "title": "Commonsense Knowledge Salience Evaluation with a Benchmark Dataset in  E-commerce",
    "abstract": "Comments: Accepted to EMNLP 2022 (Findings)",
    "descriptor": "\nComments: Accepted to EMNLP 2022 (Findings)\n",
    "authors": [
      "Yincen Qu",
      "Ningyu Zhang",
      "Hui Chen",
      "Zelin Dai",
      "Zezhong Xu",
      "Chengming Wang",
      "Xiaoyu Wang",
      "Qiang Chen",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10843"
  },
  {
    "id": "arXiv:2205.10936",
    "title": "On Elimination Strategies for Bandit Fixed-Confidence Identification",
    "abstract": "On Elimination Strategies for Bandit Fixed-Confidence Identification",
    "descriptor": "",
    "authors": [
      "Andrea Tirinzoni",
      "R\u00e9my Degenne"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.10936"
  },
  {
    "id": "arXiv:2205.10964",
    "title": "The Geometry of Multilingual Language Model Representations",
    "abstract": "Comments: Accepted to EMNLP 2022",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "Tyler A. Chang",
      "Zhuowen Tu",
      "Benjamin K. Bergen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10964"
  },
  {
    "id": "arXiv:2205.11024",
    "title": "Vector-Quantized Input-Contextualized Soft Prompts for Natural Language  Understanding",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Rishabh Bhardwaj",
      "Amrita Saha",
      "Steven C.H. Hoi",
      "Soujanya Poria"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11024"
  },
  {
    "id": "arXiv:2205.11196",
    "title": "Zero-Sum Games and Linear Programming Duality",
    "abstract": "Comments: v2: Adler and Brooks/Reny discussed, stronger Theorem 7 with new proof",
    "descriptor": "\nComments: v2: Adler and Brooks/Reny discussed, stronger Theorem 7 with new proof\n",
    "authors": [
      "Bernhard von Stengel"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2205.11196"
  },
  {
    "id": "arXiv:2205.11277",
    "title": "When does Parameter-Efficient Transfer Learning Work for Machine  Translation?",
    "abstract": "Comments: Accepted at EMNLP 2022 (Main Conference)",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (Main Conference)\n",
    "authors": [
      "Ahmet \u00dcst\u00fcn",
      "Asa Cooper Stickland"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11277"
  },
  {
    "id": "arXiv:2205.11361",
    "title": "Chaotic Regularization and Heavy-Tailed Limits for Deterministic  Gradient Descent",
    "abstract": "Comments: 24 pages, accepted at NeurIPS 2022",
    "descriptor": "\nComments: 24 pages, accepted at NeurIPS 2022\n",
    "authors": [
      "Soon Hoe Lim",
      "Yijun Wan",
      "Umut \u015eim\u015fekli"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.11361"
  },
  {
    "id": "arXiv:2205.11380",
    "title": "Outliers Dimensions that Disrupt Transformers Are Driven by Frequency",
    "abstract": "Comments: To appear in Findings of EMNLP 2022",
    "descriptor": "\nComments: To appear in Findings of EMNLP 2022\n",
    "authors": [
      "Giovanni Puccetti",
      "Anna Rogers",
      "Aleksandr Drozd",
      "Felice Dell'Orletta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11380"
  },
  {
    "id": "arXiv:2205.11416",
    "title": "The Importance of Being Parameters: An Intra-Distillation Method for  Serious Gains",
    "abstract": "Comments: Accepted at EMNLP 2022",
    "descriptor": "\nComments: Accepted at EMNLP 2022\n",
    "authors": [
      "Haoran Xu",
      "Philipp Koehn",
      "Kenton Murray"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11416"
  },
  {
    "id": "arXiv:2205.11686",
    "title": "On Advances in Text Generation from Images Beyond Captioning: A Case  Study in Self-Rationalization",
    "abstract": "Comments: v2: EMNLP Findings 2022 accepted paper camera-ready version. 9 pages main, 2 pages appendix",
    "descriptor": "\nComments: v2: EMNLP Findings 2022 accepted paper camera-ready version. 9 pages main, 2 pages appendix\n",
    "authors": [
      "Shruti Palaskar",
      "Akshita Bhagia",
      "Yonatan Bisk",
      "Florian Metze",
      "Alan W Black",
      "Ana Marasovi\u0107"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.11686"
  },
  {
    "id": "arXiv:2205.11758",
    "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of  Multilingual Language Models",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Terra Blevins",
      "Hila Gonen",
      "Luke Zettlemoyer"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11758"
  },
  {
    "id": "arXiv:2205.11764",
    "title": "D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat",
    "abstract": "D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat",
    "descriptor": "",
    "authors": [
      "Binwei Yao",
      "Chao Shi",
      "Likai Zou",
      "Lingfeng Dai",
      "Mengyue Wu",
      "Lu Chen",
      "Zhen Wang",
      "Kai Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.11764"
  },
  {
    "id": "arXiv:2205.12148",
    "title": "Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer",
    "abstract": "Comments: Accepted at EMNLP 2022 (Main Conference)",
    "descriptor": "\nComments: Accepted at EMNLP 2022 (Main Conference)\n",
    "authors": [
      "Ahmet \u00dcst\u00fcn",
      "Arianna Bisazza",
      "Gosse Bouma",
      "Gertjan van Noord",
      "Sebastian Ruder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12148"
  },
  {
    "id": "arXiv:2205.12443",
    "title": "Generating Natural Language Proofs with Verifier-Guided Search",
    "abstract": "Comments: EMNLP 2022. Code and models are available at this https URL v3 added evaluation of GPT-3 and Codex",
    "descriptor": "\nComments: EMNLP 2022. Code and models are available at this https URL v3 added evaluation of GPT-3 and Codex\n",
    "authors": [
      "Kaiyu Yang",
      "Jia Deng",
      "Danqi Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2205.12443"
  },
  {
    "id": "arXiv:2205.12485",
    "title": "Conditional set generation using Seq2seq models",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Aman Madaan",
      "Dheeraj Rajagopal",
      "Niket Tandon",
      "Yiming Yang",
      "Antoine Bosselut"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12485"
  },
  {
    "id": "arXiv:2205.12507",
    "title": "Re-Examining Calibration: The Case of Question Answering",
    "abstract": "Comments: EMNLP 2022 Findings",
    "descriptor": "\nComments: EMNLP 2022 Findings\n",
    "authors": [
      "Chenglei Si",
      "Chen Zhao",
      "Sewon Min",
      "Jordan Boyd-Graber"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12507"
  },
  {
    "id": "arXiv:2205.12548",
    "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
    "abstract": "Comments: EMNLP 2022 Camera Ready. Code available at this https URL",
    "descriptor": "\nComments: EMNLP 2022 Camera Ready. Code available at this https URL\n",
    "authors": [
      "Mingkai Deng",
      "Jianyu Wang",
      "Cheng-Ping Hsieh",
      "Yihan Wang",
      "Han Guo",
      "Tianmin Shu",
      "Meng Song",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12548"
  },
  {
    "id": "arXiv:2205.12609",
    "title": "Generating Information-Seeking Conversations from Unlabeled Documents",
    "abstract": "Comments: Accepted to EMNLP 2022 main conference",
    "descriptor": "\nComments: Accepted to EMNLP 2022 main conference\n",
    "authors": [
      "Gangwoo Kim",
      "Sungdong Kim",
      "Kang Min Yoo",
      "Jaewoo Kang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12609"
  },
  {
    "id": "arXiv:2205.12647",
    "title": "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation",
    "abstract": "Comments: Accepted as a main conference paper at EMNLP 2022, 22 pages, 8 figures, 11 tables",
    "descriptor": "\nComments: Accepted as a main conference paper at EMNLP 2022, 22 pages, 8 figures, 11 tables\n",
    "authors": [
      "Tu Vu",
      "Aditya Barua",
      "Brian Lester",
      "Daniel Cer",
      "Mohit Iyyer",
      "Noah Constant"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12647"
  },
  {
    "id": "arXiv:2205.12685",
    "title": "Ground-Truth Labels Matter: A Deeper Look into Input-Label  Demonstrations",
    "abstract": "Comments: Accepted to EMNLP Long. Kang Min Yoo and Junyeob Kim contributed equally. Kang Min Yoo and Taeuk Kim are the corresponding authors",
    "descriptor": "\nComments: Accepted to EMNLP Long. Kang Min Yoo and Junyeob Kim contributed equally. Kang Min Yoo and Taeuk Kim are the corresponding authors\n",
    "authors": [
      "Kang Min Yoo",
      "Junyeob Kim",
      "Hyuhng Joon Kim",
      "Hyunsoo Cho",
      "Hwiyeol Jo",
      "Sang-Woo Lee",
      "Sang-goo Lee",
      "Taeuk Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12685"
  },
  {
    "id": "arXiv:2205.12694",
    "title": "Train Flat, Then Compress: Sharpness-Aware Minimization Learns More  Compressible Models",
    "abstract": "Comments: EMNLP 2022 Findings, 28 pages",
    "descriptor": "\nComments: EMNLP 2022 Findings, 28 pages\n",
    "authors": [
      "Clara Na",
      "Sanket Vaibhav Mehta",
      "Emma Strubell"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12694"
  },
  {
    "id": "arXiv:2205.12900",
    "title": "Differentially Private Data Generation Needs Better Features",
    "abstract": "Differentially Private Data Generation Needs Better Features",
    "descriptor": "",
    "authors": [
      "Fredrik Harder",
      "Milad Jalali Asadabadi",
      "Danica J. Sutherland",
      "Mijung Park"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12900"
  },
  {
    "id": "arXiv:2205.13054",
    "title": "Scalable and Low-Latency Federated Learning with Cooperative Mobile Edge  Networking",
    "abstract": "Comments: accepted for publication in IEEE Transactions on Mobile Computing",
    "descriptor": "\nComments: accepted for publication in IEEE Transactions on Mobile Computing\n",
    "authors": [
      "Zhenxiao Zhang",
      "Zhidong Gao",
      "Yuanxiong Guo",
      "Yanmin Gong"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13054"
  },
  {
    "id": "arXiv:2205.13496",
    "title": "Censored Quantile Regression Neural Networks for Distribution-Free  Survival Analysis",
    "abstract": "Censored Quantile Regression Neural Networks for Distribution-Free  Survival Analysis",
    "descriptor": "",
    "authors": [
      "Tim Pearce",
      "Jong-Hyeon Jeong",
      "Yichen Jia",
      "Jun Zhu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13496"
  },
  {
    "id": "arXiv:2205.13554",
    "title": "Training and Inference on Any-Order Autoregressive Models the Right Way",
    "abstract": "Comments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Andy Shih",
      "Dorsa Sadigh",
      "Stefano Ermon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13554"
  },
  {
    "id": "arXiv:2205.14027",
    "title": "Learning Dynamical Systems via Koopman Operator Regression in  Reproducing Kernel Hilbert Spaces",
    "abstract": "Comments: Main text: 10 pages, 2 figures, 1 table. Supplementary informations: 18 pages, 5 figures, 2 tables",
    "descriptor": "\nComments: Main text: 10 pages, 2 figures, 1 table. Supplementary informations: 18 pages, 5 figures, 2 tables\n",
    "authors": [
      "Vladimir Kostic",
      "Pietro Novelli",
      "Andreas Maurer",
      "Carlo Ciliberto",
      "Lorenzo Rosasco",
      "Massimiliano Pontil"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14027"
  },
  {
    "id": "arXiv:2205.14094",
    "title": "Failure Detection in Medical Image Classification: A Reality Check and  Benchmarking Testbed",
    "abstract": "Comments: Published in Transactions on Machine Learning Research (10/2022)",
    "descriptor": "\nComments: Published in Transactions on Machine Learning Research (10/2022)\n",
    "authors": [
      "Melanie Bernhardt",
      "Fabio De Sousa Ribeiro",
      "Ben Glocker"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.14094"
  },
  {
    "id": "arXiv:2205.15009",
    "title": "Carleman Lifting for Nonlinear System Identification with Guaranteed  Error Bounds",
    "abstract": "Carleman Lifting for Nonlinear System Identification with Guaranteed  Error Bounds",
    "descriptor": "",
    "authors": [
      "Moad Abudia",
      "Joel A. Rosenfeld",
      "Rushikesh Kamalapurkar"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2205.15009"
  },
  {
    "id": "arXiv:2205.15075",
    "title": "Align then Fusion: Generalized Large-scale Multi-view Clustering with  Anchor Matching Correspondences",
    "abstract": "Comments: Accepted to the Conference on the Advances in Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: Accepted to the Conference on the Advances in Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Siwei Wang",
      "Xinwang Liu",
      "Suyuan Liu",
      "Jiaqi Jin",
      "Wenxuan Tu",
      "Xinzhong Zhu",
      "En Zhu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15075"
  },
  {
    "id": "arXiv:2205.15234",
    "title": "Few-Shot Adaptation of Pre-Trained Networks for Domain Shift",
    "abstract": "Comments: Accepted to IJCAI 2022",
    "descriptor": "\nComments: Accepted to IJCAI 2022\n",
    "authors": [
      "Wenyu Zhang",
      "Li Shen",
      "Wanyue Zhang",
      "Chuan-Sheng Foo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15234"
  },
  {
    "id": "arXiv:2205.15397",
    "title": "Minimax Optimal Online Imitation Learning via Replay Estimation",
    "abstract": "Minimax Optimal Online Imitation Learning via Replay Estimation",
    "descriptor": "",
    "authors": [
      "Gokul Swamy",
      "Nived Rajaraman",
      "Matthew Peng",
      "Sanjiban Choudhury",
      "J. Andrew Bagnell",
      "Zhiwei Steven Wu",
      "Jiantao Jiao",
      "Kannan Ramchandran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15397"
  },
  {
    "id": "arXiv:2205.15466",
    "title": "Data Banzhaf: A Robust Data Valuation Framework for Machine Learning",
    "abstract": "Data Banzhaf: A Robust Data Valuation Framework for Machine Learning",
    "descriptor": "",
    "authors": [
      "Jiachen T. Wang",
      "Ruoxi Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15466"
  },
  {
    "id": "arXiv:2205.15827",
    "title": "Robust Anytime Learning of Markov Decision Processes",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Marnix Suilen",
      "Thiago D. Sim\u00e3o",
      "David Parker",
      "Nils Jansen"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.15827"
  },
  {
    "id": "arXiv:2205.15947",
    "title": "Evaluating Robustness to Dataset Shift via Parametric Robustness Sets",
    "abstract": "Comments: NeurIPS 2022; Equal Contribution by Nikolaj/Michael, order determined by coin flip",
    "descriptor": "\nComments: NeurIPS 2022; Equal Contribution by Nikolaj/Michael, order determined by coin flip\n",
    "authors": [
      "Nikolaj Thams",
      "Michael Oberst",
      "David Sontag"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.15947"
  },
  {
    "id": "arXiv:2206.00051",
    "title": "Instance-Specific Augmentation: Capturing Local Invariances",
    "abstract": "Instance-Specific Augmentation: Capturing Local Invariances",
    "descriptor": "",
    "authors": [
      "Ning Miao",
      "Tom Rainforth",
      "Emile Mathieu",
      "Yann Dubois",
      "Yee Whye Teh",
      "Adam Foster",
      "Hyunjik Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00051"
  },
  {
    "id": "arXiv:2206.00274",
    "title": "Point-Teaching: Weakly Semi-Supervised Object Detection with Point  Annotations",
    "abstract": "Point-Teaching: Weakly Semi-Supervised Object Detection with Point  Annotations",
    "descriptor": "",
    "authors": [
      "Yongtao Ge",
      "Qiang Zhou",
      "Xinlong Wang",
      "Zhibin Wang",
      "Hao Li",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00274"
  },
  {
    "id": "arXiv:2206.00719",
    "title": "Dataset Distillation using Neural Feature Regression",
    "abstract": "Comments: NeurIPS 2022 camera-ready version",
    "descriptor": "\nComments: NeurIPS 2022 camera-ready version\n",
    "authors": [
      "Yongchao Zhou",
      "Ehsan Nezhadarya",
      "Jimmy Ba"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00719"
  },
  {
    "id": "arXiv:2206.01382",
    "title": "Falconn++: A Locality-sensitive Filtering Approach for Approximate  Nearest Neighbor Search",
    "abstract": "Comments: To appear in NeurIPS 2022",
    "descriptor": "\nComments: To appear in NeurIPS 2022\n",
    "authors": [
      "Ninh Pham",
      "Tao Liu"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.01382"
  },
  {
    "id": "arXiv:2206.01520",
    "title": "A Survey on Computationally Efficient Neural Architecture Search",
    "abstract": "Comments: 20 pages, 7 figures",
    "descriptor": "\nComments: 20 pages, 7 figures\n",
    "authors": [
      "Shiqing Liu",
      "Haoyu Zhang",
      "Yaochu Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.01520"
  },
  {
    "id": "arXiv:2206.01900",
    "title": "Estimating counterfactual treatment outcomes over time in complex  multi-agent scenarios",
    "abstract": "Comments: 13 pages, 6 figures. Part of this paper will be presented in SIGSPATIAL 2022",
    "descriptor": "\nComments: 13 pages, 6 figures. Part of this paper will be presented in SIGSPATIAL 2022\n",
    "authors": [
      "Keisuke Fujii",
      "Koh Takeuchi",
      "Atsushi Kuribayashi",
      "Naoya Takeishi",
      "Yoshinobu Kawahara",
      "Kazuya Takeda"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.01900"
  },
  {
    "id": "arXiv:2206.02829",
    "title": "RORL: Robust Offline Reinforcement Learning via Conservative Smoothing",
    "abstract": "Comments: Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Rui Yang",
      "Chenjia Bai",
      "Xiaoteng Ma",
      "Zhaoran Wang",
      "Chongjie Zhang",
      "Lei Han"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.02829"
  },
  {
    "id": "arXiv:2206.02874",
    "title": "Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and  Numerical Behaviors",
    "abstract": "Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and  Numerical Behaviors",
    "descriptor": "",
    "authors": [
      "Wei Sun",
      "Ang Li",
      "Tong Geng",
      "Sander Stuijk",
      "Henk Corporaal"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2206.02874"
  },
  {
    "id": "arXiv:2206.04168",
    "title": "Incremental Recursive Ranking Grouping for Large Scale Global  Optimization",
    "abstract": "Incremental Recursive Ranking Grouping for Large Scale Global  Optimization",
    "descriptor": "",
    "authors": [
      "Marcin Michal Komarnicki",
      "Michal Witold Przewozniczek",
      "Halina Kwasnicka",
      "Krzysztof Walkowiak"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.04168"
  },
  {
    "id": "arXiv:2206.04546",
    "title": "Pragmatically Learning from Pedagogical Demonstrations in Multi-Goal  Environments",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Hugo Caselles-Dupr\u00e9",
      "Olivier Sigaud",
      "Mohamed Chetouani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04546"
  },
  {
    "id": "arXiv:2206.04624",
    "title": "Factuality Enhanced Language Models for Open-Ended Text Generation",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Nayeon Lee",
      "Wei Ping",
      "Peng Xu",
      "Mostofa Patwary",
      "Pascale Fung",
      "Mohammad Shoeybi",
      "Bryan Catanzaro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04624"
  },
  {
    "id": "arXiv:2206.04771",
    "title": "Joint Entropy Search for Maximally-Informed Bayesian Optimization",
    "abstract": "Comments: 10 pages, 8 figures",
    "descriptor": "\nComments: 10 pages, 8 figures\n",
    "authors": [
      "Carl Hvarfner",
      "Frank Hutter",
      "Luigi Nardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04771"
  },
  {
    "id": "arXiv:2206.04777",
    "title": "Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized  Linear Models",
    "abstract": "Trimmed Maximum Likelihood Estimation for Robust Learning in Generalized  Linear Models",
    "descriptor": "",
    "authors": [
      "Pranjal Awasthi",
      "Abhimanyu Das",
      "Weihao Kong",
      "Rajat Sen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.04777"
  },
  {
    "id": "arXiv:2206.05904",
    "title": "Superiority of GNN over NN in generalizing bandlimited functions",
    "abstract": "Superiority of GNN over NN in generalizing bandlimited functions",
    "descriptor": "",
    "authors": [
      "A. Martina Neuman",
      "Rongrong Wang",
      "Yuying Xie"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.05904"
  },
  {
    "id": "arXiv:2206.07327",
    "title": "Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging  Features For Elderly And Dysarthric Speech Recognition",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2203.10274, add new experiments",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2203.10274, add new experiments\n",
    "authors": [
      "Shujie Hu",
      "Xurong Xie",
      "Mengzhe Geng",
      "Mingyu Cui",
      "Jiajun Deng",
      "Guinan Li",
      "Tianzi Wang",
      "Xunying Liu",
      "Helen Meng"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07327"
  },
  {
    "id": "arXiv:2206.07551",
    "title": "Unknown-Aware Domain Adversarial Learning for Open-Set Domain Adaptation",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "JoonHo Jang",
      "Byeonghu Na",
      "DongHyeok Shin",
      "Mingi Ji",
      "Kyungwoo Song",
      "Il-Chul Moon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.07551"
  },
  {
    "id": "arXiv:2206.08413",
    "title": "Recursion does not always help",
    "abstract": "Comments: stronger results; more uniform organisation; some corrections",
    "descriptor": "\nComments: stronger results; more uniform organisation; some corrections\n",
    "authors": [
      "Gordon Plotkin"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2206.08413"
  },
  {
    "id": "arXiv:2206.08704",
    "title": "Maximum Class Separation as Inductive Bias in One Matrix",
    "abstract": "Maximum Class Separation as Inductive Bias in One Matrix",
    "descriptor": "",
    "authors": [
      "Tejaswi Kasarla",
      "Gertjan J. Burghouts",
      "Max van Spengler",
      "Elise van der Pol",
      "Rita Cucchiara",
      "Pascal Mettes"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.08704"
  },
  {
    "id": "arXiv:2206.08933",
    "title": "A theory of learning with constrained weight-distribution",
    "abstract": "Comments: 38 pages, 13 figures. Updated introduction part and fixed several typos",
    "descriptor": "\nComments: 38 pages, 13 figures. Updated introduction part and fixed several typos\n",
    "authors": [
      "Weishun Zhong",
      "Ben Sorscher",
      "Daniel D Lee",
      "Haim Sompolinsky"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08933"
  },
  {
    "id": "arXiv:2206.10589",
    "title": "EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for  Mobile Vision Applications",
    "abstract": "Comments: Accepted at ECCVW 2022 (Oral, CADL: Computational Aspects of Deep Learning)",
    "descriptor": "\nComments: Accepted at ECCVW 2022 (Oral, CADL: Computational Aspects of Deep Learning)\n",
    "authors": [
      "Muhammad Maaz",
      "Abdelrahman Shaker",
      "Hisham Cholakkal",
      "Salman Khan",
      "Syed Waqas Zamir",
      "Rao Muhammad Anwer",
      "Fahad Shahbaz Khan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.10589"
  },
  {
    "id": "arXiv:2206.10770",
    "title": "On the Statistical Efficiency of Reward-Free Exploration in Non-Linear  RL",
    "abstract": "On the Statistical Efficiency of Reward-Free Exploration in Non-Linear  RL",
    "descriptor": "",
    "authors": [
      "Jinglin Chen",
      "Aditya Modi",
      "Akshay Krishnamurthy",
      "Nan Jiang",
      "Alekh Agarwal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.10770"
  },
  {
    "id": "arXiv:2206.11769",
    "title": "Single-phase deep learning in cortico-cortical networks",
    "abstract": "Comments: Accepted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022). 22 pages, 9 figures, 5 tables",
    "descriptor": "\nComments: Accepted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022). 22 pages, 9 figures, 5 tables\n",
    "authors": [
      "Will Greedy",
      "Heng Wei Zhu",
      "Joseph Pemberton",
      "Jack Mellor",
      "Rui Ponte Costa"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.11769"
  },
  {
    "id": "arXiv:2206.12229",
    "title": "Exact Prosody Cloning in Zero-Shot Multispeaker Text-to-Speech",
    "abstract": "Comments: Accepted to IEEE SLT 2022",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Florian Lux",
      "Julia Koch",
      "Ngoc Thang Vu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2206.12229"
  },
  {
    "id": "arXiv:2206.13307",
    "title": "Robust and Secure Resource Allocation for ISAC Systems: A Novel  Optimization Framework for Variable-Length Snapshots",
    "abstract": "Comments: 38 pages, 12 figures",
    "descriptor": "\nComments: 38 pages, 12 figures\n",
    "authors": [
      "Dongfang Xu",
      "Xianghao Yu",
      "Derrick Wing Kwan Ng",
      "Anke Schmeink",
      "Robert Schober"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2206.13307"
  },
  {
    "id": "arXiv:2206.13627",
    "title": "Learning constitutive models from microstructural simulations via a  non-intrusive reduced basis method: Extension to geometrical  parameterizations",
    "abstract": "Learning constitutive models from microstructural simulations via a  non-intrusive reduced basis method: Extension to geometrical  parameterizations",
    "descriptor": "",
    "authors": [
      "Theron Guo",
      "Francesco A. B. Silva",
      "Ond\u0159ej Roko\u0161",
      "Karen Veroy"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2206.13627"
  },
  {
    "id": "arXiv:2206.15441",
    "title": "Classical and learned MR to pseudo-CT mappings for accurate transcranial  ultrasound simulation",
    "abstract": "Classical and learned MR to pseudo-CT mappings for accurate transcranial  ultrasound simulation",
    "descriptor": "",
    "authors": [
      "Maria Miscouridou",
      "Jos\u00e9 A. Pineda-Pardo",
      "Charlotte J. Stagg",
      "Bradley E. Treeby",
      "Antonio Stanziola"
    ],
    "subjectives": [
      "Medical Physics (physics.med-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.15441"
  },
  {
    "id": "arXiv:2207.00504",
    "title": "The closest vector problem and the zero-temperature p-spin landscape for  lossy compression",
    "abstract": "Comments: 29 pages, 13 figures",
    "descriptor": "\nComments: 29 pages, 13 figures\n",
    "authors": [
      "Alfredo Braunstein",
      "Louise Budzynski",
      "Stefano Crotti",
      "Federico Ricci-Tersenghi"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2207.00504"
  },
  {
    "id": "arXiv:2207.00531",
    "title": "Masked Autoencoder for Self-Supervised Pre-training on Lidar Point  Clouds",
    "abstract": "Masked Autoencoder for Self-Supervised Pre-training on Lidar Point  Clouds",
    "descriptor": "",
    "authors": [
      "Georg Hess",
      "Johan Jaxing",
      "Elias Svensson",
      "David Hagerman",
      "Christoffer Petersson",
      "Lennart Svensson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.00531"
  },
  {
    "id": "arXiv:2207.00678",
    "title": "Infinite-Fidelity Coregionalization for Physical Simulation",
    "abstract": "Infinite-Fidelity Coregionalization for Physical Simulation",
    "descriptor": "",
    "authors": [
      "Shibo Li",
      "Zheng Wang",
      "Robert M. Kirby",
      "Shandian Zhe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.00678"
  },
  {
    "id": "arXiv:2207.01063",
    "title": "DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech",
    "abstract": "Comments: 5 pages, 1 figures, 4 tables. Submitted to ICASSP 2023",
    "descriptor": "\nComments: 5 pages, 1 figures, 4 tables. Submitted to ICASSP 2023\n",
    "authors": [
      "Keon Lee",
      "Kyumin Park",
      "Daeyoung Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2207.01063"
  },
  {
    "id": "arXiv:2207.02505",
    "title": "Pure Transformers are Powerful Graph Learners",
    "abstract": "Comments: 26 pages, 8 figures",
    "descriptor": "\nComments: 26 pages, 8 figures\n",
    "authors": [
      "Jinwoo Kim",
      "Tien Dat Nguyen",
      "Seonwoo Min",
      "Sungjun Cho",
      "Moontae Lee",
      "Honglak Lee",
      "Seunghoon Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.02505"
  },
  {
    "id": "arXiv:2207.02535",
    "title": "On Galois hulls of linear codes and new entanglement-assisted quantum  error-correcting codes",
    "abstract": "Comments: 39 pages, 9 tables. More content is included",
    "descriptor": "\nComments: 39 pages, 9 tables. More content is included\n",
    "authors": [
      "Yang Li",
      "Shixin Zhu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2207.02535"
  },
  {
    "id": "arXiv:2207.03526",
    "title": "Reinforcement Learning-based Joint User Scheduling and Link  Configuration in Millimeter-wave Networks",
    "abstract": "Comments: 17 pages, 10 Figures",
    "descriptor": "\nComments: 17 pages, 10 Figures\n",
    "authors": [
      "Yi Zhang",
      "Robert W. Heath Jr"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2207.03526"
  },
  {
    "id": "arXiv:2207.04027",
    "title": "A Multi-tasking Model of Speaker-Keyword Classification for Keeping  Human in the Loop of Drone-assisted Inspection",
    "abstract": "Comments: Submitted to Engineering Applications of Artificial Intelligence journal in the end of June 2022. Received the 1st review in Sep 2022 and submitted the revision on Oct 2022. Currently it's under 2nd review",
    "descriptor": "\nComments: Submitted to Engineering Applications of Artificial Intelligence journal in the end of June 2022. Received the 1st review in Sep 2022 and submitted the revision on Oct 2022. Currently it's under 2nd review\n",
    "authors": [
      "Yu Li",
      "Anisha Parsan",
      "Bill Wang",
      "Penghao Dong",
      "Shanshan Yao",
      "Ruwen Qin"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2207.04027"
  },
  {
    "id": "arXiv:2207.05219",
    "title": "Grounding Aleatoric Uncertainty for Unsupervised Environment Design",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Minqi Jiang",
      "Michael Dennis",
      "Jack Parker-Holder",
      "Andrei Lupu",
      "Heinrich K\u00fcttler",
      "Edward Grefenstette",
      "Tim Rockt\u00e4schel",
      "Jakob Foerster"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.05219"
  },
  {
    "id": "arXiv:2207.05984",
    "title": "Unsupervised Learning for Combinatorial Optimization with Principled  Objective Relaxation",
    "abstract": "Comments: Code: this https URL",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Haoyu Wang",
      "Nan Wu",
      "Hang Yang",
      "Cong Hao",
      "Pan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2207.05984"
  },
  {
    "id": "arXiv:2207.06489",
    "title": "A Data-Efficient Deep Learning Framework for Segmentation and  Classification of Histopathology Images",
    "abstract": "Comments: Originally published at the ECCV 2022 Medical Computer Vision Workshop (ECCV-MCV 2022)",
    "descriptor": "\nComments: Originally published at the ECCV 2022 Medical Computer Vision Workshop (ECCV-MCV 2022)\n",
    "authors": [
      "Pranav Singh",
      "Jacopo Cirrone"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.06489"
  },
  {
    "id": "arXiv:2207.07260",
    "title": "Accelerated Probabilistic Marching Cubes by Deep Learning for  Time-Varying Scalar Ensembles",
    "abstract": "Comments: 5 pages, IEEE Vis 2022 Short Paper",
    "descriptor": "\nComments: 5 pages, IEEE Vis 2022 Short Paper\n",
    "authors": [
      "Mengjiao Han",
      "Tushar M. Athawale",
      "David Pugmire",
      "Chris R. Johnson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2207.07260"
  },
  {
    "id": "arXiv:2207.09209",
    "title": "FLDetector: Defending Federated Learning Against Model Poisoning Attacks  via Detecting Malicious Clients",
    "abstract": "Comments: Accepted by KDD 2022 (Research Track)",
    "descriptor": "\nComments: Accepted by KDD 2022 (Research Track)\n",
    "authors": [
      "Zaixi Zhang",
      "Xiaoyu Cao",
      "Jinyuan Jia",
      "Neil Zhenqiang Gong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.09209"
  },
  {
    "id": "arXiv:2207.09304",
    "title": "A sharp uniform-in-time error estimate for Stochastic Gradient Langevin  Dynamics",
    "abstract": "A sharp uniform-in-time error estimate for Stochastic Gradient Langevin  Dynamics",
    "descriptor": "",
    "authors": [
      "Lei Li",
      "Yuliang Wang"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.09304"
  },
  {
    "id": "arXiv:2207.11620",
    "title": "Instant Neural Representation for Interactive Volume Rendering",
    "abstract": "Comments: This manuscript was submitted to IEEE VIS 2022 (March 31 2022), then resubmitted to IEEE TVCG (October 21 2022). We have modified our manuscript in response to previous reviews. There is also a supplementary video for this manuscript, which can be accessed via this link: this https URL",
    "descriptor": "\nComments: This manuscript was submitted to IEEE VIS 2022 (March 31 2022), then resubmitted to IEEE TVCG (October 21 2022). We have modified our manuscript in response to previous reviews. There is also a supplementary video for this manuscript, which can be accessed via this link: this https URL\n",
    "authors": [
      "Qi Wu",
      "David Bauer",
      "Michael J. Doyle",
      "Kwan-Liu Ma"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.11620"
  },
  {
    "id": "arXiv:2207.11629",
    "title": "Presenting with Quantitative Inequational Theories",
    "abstract": "Presenting with Quantitative Inequational Theories",
    "descriptor": "",
    "authors": [
      "Todd Schmid"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2207.11629"
  },
  {
    "id": "arXiv:2207.11717",
    "title": "A Priority Map for Vision-and-Language Navigation with Trajectory Plans  and Feature-Location Cues",
    "abstract": "A Priority Map for Vision-and-Language Navigation with Trajectory Plans  and Feature-Location Cues",
    "descriptor": "",
    "authors": [
      "Jason Armitage",
      "Leonardo Impett",
      "Rico Sennrich"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.11717"
  },
  {
    "id": "arXiv:2207.11838",
    "title": "SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning  with Human Object Interactions",
    "abstract": "Comments: 14 pages, 6 figures, 6 tables",
    "descriptor": "\nComments: 14 pages, 6 figures, 6 tables\n",
    "authors": [
      "Ansh Mittal",
      "Shuvam Ghosal",
      "Rishibha Bansal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.11838"
  },
  {
    "id": "arXiv:2207.12214",
    "title": "Laplacian-based Cluster-Contractive t-SNE for High Dimensional Data  Visualization",
    "abstract": "Laplacian-based Cluster-Contractive t-SNE for High Dimensional Data  Visualization",
    "descriptor": "",
    "authors": [
      "Yan Sun",
      "Yi Han",
      "Jicong Fan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2207.12214"
  },
  {
    "id": "arXiv:2207.12399",
    "title": "Color Coding of Large Value Ranges Applied to Meteorological Data",
    "abstract": "Comments: Preprint and Author Version of a Short Paper, accepted to the 2022 IEEE Visualization Conference (VIS)",
    "descriptor": "\nComments: Preprint and Author Version of a Short Paper, accepted to the 2022 IEEE Visualization Conference (VIS)\n",
    "authors": [
      "Daniel Braun",
      "Kerstin Ebell",
      "Vera Schemann",
      "Laura Pelchmann",
      "Susanne Crewell",
      "Rita Borgo",
      "Tatiana von Landesberger"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2207.12399"
  },
  {
    "id": "arXiv:2207.13219",
    "title": "Dalorex: A Data-Local Program Execution and Architecture for  Memory-bound Applications",
    "abstract": "Comments: To Appear in Proceedings of the 29th IEEE Symposium on High-Performance Computer Architecture (HPCA-29)",
    "descriptor": "\nComments: To Appear in Proceedings of the 29th IEEE Symposium on High-Performance Computer Architecture (HPCA-29)\n",
    "authors": [
      "Marcelo Orenes Vera",
      "Esin Tureci",
      "David Wentzlaff",
      "Margaret Martonosi"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2207.13219"
  },
  {
    "id": "arXiv:2207.13358",
    "title": "A Case for Self-Managing DRAM Chips: Improving Performance, Efficiency,  Reliability, and Security via Autonomous in-DRAM Maintenance Operations",
    "abstract": "A Case for Self-Managing DRAM Chips: Improving Performance, Efficiency,  Reliability, and Security via Autonomous in-DRAM Maintenance Operations",
    "descriptor": "",
    "authors": [
      "Hasan Hassan",
      "Ataberk Olgun",
      "A. Giray Yaglikci",
      "Haocong Luo",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2207.13358"
  },
  {
    "id": "arXiv:2208.01320",
    "title": "Compound Density Networks for Risk Prediction using Electronic Health  Records",
    "abstract": "Comments: 8 pages, 6 figures, accepted at IEEE BIBM 2022",
    "descriptor": "\nComments: 8 pages, 6 figures, accepted at IEEE BIBM 2022\n",
    "authors": [
      "Yuxi Liu",
      "Shaowen Qin",
      "Zhenhao Zhang",
      "Wei Shao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.01320"
  },
  {
    "id": "arXiv:2208.02225",
    "title": "Sequence Model Imitation Learning with Unobserved Contexts",
    "abstract": "Sequence Model Imitation Learning with Unobserved Contexts",
    "descriptor": "",
    "authors": [
      "Gokul Swamy",
      "Sanjiban Choudhury",
      "J. Andrew Bagnell",
      "Zhiwei Steven Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.02225"
  },
  {
    "id": "arXiv:2208.02753",
    "title": "Spectral Universality of Regularized Linear Regression with Nearly  Deterministic Sensing Matrices",
    "abstract": "Spectral Universality of Regularized Linear Regression with Nearly  Deterministic Sensing Matrices",
    "descriptor": "",
    "authors": [
      "Rishabh Dudeja",
      "Subhabrata Sen",
      "Yue M. Lu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2208.02753"
  },
  {
    "id": "arXiv:2208.04749",
    "title": "Where's the Learning in Representation Learning for Compositional  Semantics and the Case of Thematic Fit",
    "abstract": "Comments: Published in Blackbox NLP workshop, EMNLP 2022. 12 pages including Appendices, 1 figure (with 6 sub-figures)",
    "descriptor": "\nComments: Published in Blackbox NLP workshop, EMNLP 2022. 12 pages including Appendices, 1 figure (with 6 sub-figures)\n",
    "authors": [
      "Mughilan Muthupari",
      "Samrat Halder",
      "Asad Sayeed",
      "Yuval Marton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2208.04749"
  },
  {
    "id": "arXiv:2208.06120",
    "title": "Bayesian Inference with Latent Hamiltonian Neural Networks",
    "abstract": "Comments: Added code repository (this https URL)",
    "descriptor": "\nComments: Added code repository (this https URL)\n",
    "authors": [
      "Somayajulu L. N. Dhulipala",
      "Yifeng Che",
      "Michael D. Shields"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.06120"
  },
  {
    "id": "arXiv:2208.07522",
    "title": "Reliable Decision from Multiple Subtasks through Threshold Optimization:  Content Moderation in the Wild",
    "abstract": "Comments: WSDM 2023",
    "descriptor": "\nComments: WSDM 2023\n",
    "authors": [
      "Donghyun Son",
      "Byounggyu Lew",
      "Kwanghee Choi",
      "Yongsu Baek",
      "Seungwoo Choi",
      "Beomjun Shin",
      "Sungjoo Ha",
      "Buru Chang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computers and Society (cs.CY)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2208.07522"
  },
  {
    "id": "arXiv:2208.07698",
    "title": "Score-Based Diffusion meets Annealed Importance Sampling",
    "abstract": "Comments: accepted at NeurIPS 2022",
    "descriptor": "\nComments: accepted at NeurIPS 2022\n",
    "authors": [
      "Arnaud Doucet",
      "Will Grathwohl",
      "Alexander G. D. G. Matthews",
      "Heiko Strathmann"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.07698"
  },
  {
    "id": "arXiv:2208.07715",
    "title": "Hyperparameter Optimization of Generative Adversarial Network Models for  High-Energy Physics Simulations",
    "abstract": "Comments: Submitted to Computing and Software for Big Science (October 19, 2022)",
    "descriptor": "\nComments: Submitted to Computing and Software for Big Science (October 19, 2022)\n",
    "authors": [
      "Vincent Dumont",
      "Xiangyang Ju",
      "Juliane Mueller"
    ],
    "subjectives": [
      "High Energy Physics - Experiment (hep-ex)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.07715"
  },
  {
    "id": "arXiv:2208.09027",
    "title": "GraTO: Graph Neural Network Framework Tackling Over-smoothing with  Neural Architecture Search",
    "abstract": "Comments: accepted at CIKM2022",
    "descriptor": "\nComments: accepted at CIKM2022\n",
    "authors": [
      "Xinshun Feng",
      "Herun Wan",
      "Shangbin Feng",
      "Hongrui Wang",
      "Jun Zhou",
      "Qinghua Zheng",
      "Minnan Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.09027"
  },
  {
    "id": "arXiv:2208.09108",
    "title": "Numerical weighted integration of functions having mixed smoothness",
    "abstract": "Numerical weighted integration of functions having mixed smoothness",
    "descriptor": "",
    "authors": [
      "Dinh D\u0169ng"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2208.09108"
  },
  {
    "id": "arXiv:2208.09625",
    "title": "SPOT: Knowledge-Enhanced Language Representations for Information  Extraction",
    "abstract": "Comments: CIKM 2022",
    "descriptor": "\nComments: CIKM 2022\n",
    "authors": [
      "Jiacheng Li",
      "Yannis Katsis",
      "Tyler Baldwin",
      "Ho-Cheol Kim",
      "Andrew Bartko",
      "Julian McAuley",
      "Chun-Nan Hsu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.09625"
  },
  {
    "id": "arXiv:2208.09788",
    "title": "FaceOff: A Video-to-Video Face Swapping System",
    "abstract": "Comments: Accepted at WACV 2023",
    "descriptor": "\nComments: Accepted at WACV 2023\n",
    "authors": [
      "Aditya Agarwal",
      "Bipasha Sen",
      "Rudrabha Mukhopadhyay",
      "Vinay Namboodiri",
      "C.V. Jawahar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.09788"
  },
  {
    "id": "arXiv:2208.10033",
    "title": "Evaluating and Crafting Datasets Effective for Deep Learning With Data  Maps",
    "abstract": "Comments: 5 pages, 3 tables, 1 figure",
    "descriptor": "\nComments: 5 pages, 3 tables, 1 figure\n",
    "authors": [
      "Jay Bishnu",
      "Andrew Gondoputro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.10033"
  },
  {
    "id": "arXiv:2208.10322",
    "title": "Mix-Pooling Strategy for Attention Mechanism",
    "abstract": "Comments: Work in progress",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Shanshan Zhong",
      "Wushao Wen",
      "Jinghui Qin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.10322"
  },
  {
    "id": "arXiv:2208.10947",
    "title": "Towards Natural Language-Based Visualization Authoring",
    "abstract": "Towards Natural Language-Based Visualization Authoring",
    "descriptor": "",
    "authors": [
      "Yun Wang",
      "Zhitao Hou",
      "Leixian Shen",
      "Tongshuang Wu",
      "Jiaqi Wang",
      "He Huang",
      "Haidong Zhang",
      "Dongmei Zhang"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2208.10947"
  },
  {
    "id": "arXiv:2208.11246",
    "title": "Accelerating SGD for Highly Ill-Conditioned Huge-Scale Online Matrix  Completion",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Gavin Zhang",
      "Hong-Ming Chiu",
      "Richard Y. Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.11246"
  },
  {
    "id": "arXiv:2208.12651",
    "title": "DBE-KT22: A Knowledge Tracing Dataset Based on Online Student Evaluation",
    "abstract": "DBE-KT22: A Knowledge Tracing Dataset Based on Online Student Evaluation",
    "descriptor": "",
    "authors": [
      "Ghodai Abdelrahman",
      "Sherif Abdelfattah",
      "Qing Wang",
      "Yu Lin"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.12651"
  },
  {
    "id": "arXiv:2208.12723",
    "title": "Software Performability Analysis Using Fast Parametric Model Checking",
    "abstract": "Software Performability Analysis Using Fast Parametric Model Checking",
    "descriptor": "",
    "authors": [
      "Xinwei Fang",
      "Radu Calinescu",
      "Simos Gerasimou",
      "Faisal Alhwikem"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2208.12723"
  },
  {
    "id": "arXiv:2208.12976",
    "title": "Paraconsistent logic and query answering in inconsistent databases",
    "abstract": "Comments: 19 pages; revision of v2, definitions of structures and relational theories corrected",
    "descriptor": "\nComments: 19 pages; revision of v2, definitions of structures and relational theories corrected\n",
    "authors": [
      "C.A. Middelburg"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2208.12976"
  },
  {
    "id": "arXiv:2208.13471",
    "title": "How to Extend the Abstraction Refinement Model for Systems with Emergent  Behavior ?",
    "abstract": "How to Extend the Abstraction Refinement Model for Systems with Emergent  Behavior ?",
    "descriptor": "",
    "authors": [
      "Mohamed Toufik Ailane",
      "Christoph knieke",
      "Andreas rausch"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2208.13471"
  },
  {
    "id": "arXiv:2208.14508",
    "title": "Swin-transformer-yolov5 For Real-time Wine Grape Bunch Detection",
    "abstract": "Comments: 30 pages; 15 figures;Corresponding author: Xin Zhang Department of Agricultural and Biological Engineering Mississippi State University Mississippi State, MS 39762, USA (xzhang@abe.msstate.edu)",
    "descriptor": "\nComments: 30 pages; 15 figures;Corresponding author: Xin Zhang Department of Agricultural and Biological Engineering Mississippi State University Mississippi State, MS 39762, USA (xzhang@abe.msstate.edu)\n",
    "authors": [
      "Shenglian Lu",
      "Xiaoyu Liu",
      "Zixaun He",
      "Manoj Karkee",
      "Xin Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.14508"
  },
  {
    "id": "arXiv:2208.14542",
    "title": "TCAM: Temporal Class Activation Maps for Object Localization in  Weakly-Labeled Unconstrained Videos",
    "abstract": "Comments: 13 pages, 7 figures",
    "descriptor": "\nComments: 13 pages, 7 figures\n",
    "authors": [
      "Soufiane Belharbi",
      "Ismail Ben Ayed",
      "Luke McCaffrey",
      "Eric Granger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.14542"
  },
  {
    "id": "arXiv:2208.14602",
    "title": "Continuous QA Learning with Structured Prompts",
    "abstract": "Continuous QA Learning with Structured Prompts",
    "descriptor": "",
    "authors": [
      "Yinhe Zheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.14602"
  },
  {
    "id": "arXiv:2208.14656",
    "title": "LawBreaker: An Approach for Specifying Traffic Laws and Fuzzing  Autonomous Vehicles",
    "abstract": "Comments: Accepted by the 37th IEEE/ACM International Conference on Automated Software Engineering (ASE 2022)",
    "descriptor": "\nComments: Accepted by the 37th IEEE/ACM International Conference on Automated Software Engineering (ASE 2022)\n",
    "authors": [
      "Yang Sun",
      "Christopher M. Poskitt",
      "Jun Sun",
      "Yuqi Chen",
      "Zijiang Yang"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2208.14656"
  },
  {
    "id": "arXiv:2209.00089",
    "title": "Analytic solution of the exact Daum-Huang flow equation for particle  filters",
    "abstract": "Analytic solution of the exact Daum-Huang flow equation for particle  filters",
    "descriptor": "",
    "authors": [
      "Oliv\u00e9r T\u00f6r\u0151",
      "Tam\u00e1s B\u00e9csi"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.00089"
  },
  {
    "id": "arXiv:2209.02495",
    "title": "Transfer Learning of Lexical Semantic Families for Argumentative  Discourse Units Identification",
    "abstract": "Transfer Learning of Lexical Semantic Families for Argumentative  Discourse Units Identification",
    "descriptor": "",
    "authors": [
      "Jo\u00e3o Rodrigues",
      "Ruben Branco",
      "Ant\u00f3nio Branco"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.02495"
  },
  {
    "id": "arXiv:2209.03735",
    "title": "Beyond Double Ascent via Recurrent Neural Tangent Kernel in Sequential  Recommendation",
    "abstract": "Beyond Double Ascent via Recurrent Neural Tangent Kernel in Sequential  Recommendation",
    "descriptor": "",
    "authors": [
      "Ruihong Qiu",
      "Zi Huang",
      "Hongzhi Yin"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2209.03735"
  },
  {
    "id": "arXiv:2209.04430",
    "title": "Investigation of a Machine learning methodology for the SKA pulsar  search pipeline",
    "abstract": "Investigation of a Machine learning methodology for the SKA pulsar  search pipeline",
    "descriptor": "",
    "authors": [
      "Shashank Sanjay Bhat",
      "Thiagaraj Prabu",
      "Ben Stappers",
      "Atul Ghalame",
      "Snehanshu Saha",
      "T.S.B Sudarshan",
      "Zafiirah Hosenie"
    ],
    "subjectives": [
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.04430"
  },
  {
    "id": "arXiv:2209.04994",
    "title": "Knowledge Base Question Answering: A Semantic Parsing Perspective",
    "abstract": "Comments: 19 pages, 3 figures; accepted to AKBC'22",
    "descriptor": "\nComments: 19 pages, 3 figures; accepted to AKBC'22\n",
    "authors": [
      "Yu Gu",
      "Vardaan Pahuja",
      "Gong Cheng",
      "Yu Su"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.04994"
  },
  {
    "id": "arXiv:2209.05703",
    "title": "Independent Learning in Mean-Field Games: Satisficing Paths and  Convergence to Subjective Equilibria",
    "abstract": "Independent Learning in Mean-Field Games: Satisficing Paths and  Convergence to Subjective Equilibria",
    "descriptor": "",
    "authors": [
      "Bora Yongacoglu",
      "G\u00fcrdal Arslan",
      "Serdar Y\u00fcksel"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2209.05703"
  },
  {
    "id": "arXiv:2209.06424",
    "title": "COMPASS: A Formal Framework and Aggregate Dataset for Generalized  Surgical Procedure Modeling",
    "abstract": "Comments: 12 pages, 5 figures, 8 tables",
    "descriptor": "\nComments: 12 pages, 5 figures, 8 tables\n",
    "authors": [
      "Kay Hutchinson",
      "Ian Reyes",
      "Zongyu Li",
      "Homa Alemzadeh"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.06424"
  },
  {
    "id": "arXiv:2209.07052",
    "title": "MIPI 2022 Challenge on Under-Display Camera Image Restoration: Methods  and Results",
    "abstract": "Comments: ECCV 2022 Mobile Intelligent Photography and Imaging (MIPI) Workshop--Under-display Camera Image Restoration Challenge Report. MIPI workshop website: this http URL",
    "descriptor": "\nComments: ECCV 2022 Mobile Intelligent Photography and Imaging (MIPI) Workshop--Under-display Camera Image Restoration Challenge Report. MIPI workshop website: this http URL\n",
    "authors": [
      "Ruicheng Feng",
      "Chongyi Li",
      "Shangchen Zhou",
      "Wenxiu Sun",
      "Qingpeng Zhu",
      "Jun Jiang",
      "Qingyu Yang",
      "Chen Change Loy",
      "Jinwei Gu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.07052"
  },
  {
    "id": "arXiv:2209.07235",
    "title": "Sound and Complete Verification of Polynomial Networks",
    "abstract": "Comments: Accepted in NeurIPS 2022",
    "descriptor": "\nComments: Accepted in NeurIPS 2022\n",
    "authors": [
      "Elias Abad Rocamora",
      "Mehmet Fatih Sahin",
      "Fanghui Liu",
      "Grigorios G Chrysos",
      "Volkan Cevher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.07235"
  },
  {
    "id": "arXiv:2209.08312",
    "title": "Base rate neglect in computer science education",
    "abstract": "Comments: 14 pages, 3 figures",
    "descriptor": "\nComments: 14 pages, 3 figures\n",
    "authors": [
      "Koby Mike",
      "Orit Hazzan"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.08312"
  },
  {
    "id": "arXiv:2209.08355",
    "title": "Differentiable Topology-Preserved Distance Transform for Pulmonary  Airway Segmentation",
    "abstract": "Comments: 10 pages, 7 figures",
    "descriptor": "\nComments: 10 pages, 7 figures\n",
    "authors": [
      "Minghui Zhang",
      "Guang-Zhong Yang",
      "Yun Gu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.08355"
  },
  {
    "id": "arXiv:2209.08456",
    "title": "Deep Learning-Based Rate-Splitting Multiple Access for Reconfigurable  Intelligent Surface-Aided Tera-Hertz Massive MIMO",
    "abstract": "Comments: submitted to IEEE Journal on Selected Areas in Communications",
    "descriptor": "\nComments: submitted to IEEE Journal on Selected Areas in Communications\n",
    "authors": [
      "Minghui Wu",
      "Zhen Gao",
      "Yang Huang",
      "Zhenyu Xiao",
      "Derrick Wing Kwan Ng",
      "Zhaoyang Zhang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.08456"
  },
  {
    "id": "arXiv:2209.09478",
    "title": "Guiding vector fields for the distributed motion coordination of mobile  robots",
    "abstract": "Comments: Evolved paper from arXiv:2103.12372. Accepted to IEEE Transactions on Robotics",
    "descriptor": "\nComments: Evolved paper from arXiv:2103.12372. Accepted to IEEE Transactions on Robotics\n",
    "authors": [
      "Weijia Yao",
      "Hector Garcia de Marina",
      "Zhiyong Sun",
      "Ming Cao"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.09478"
  },
  {
    "id": "arXiv:2209.09936",
    "title": "Solving Fredholm Integral Equations of the First Kind via Wasserstein  Gradient Flows",
    "abstract": "Solving Fredholm Integral Equations of the First Kind via Wasserstein  Gradient Flows",
    "descriptor": "",
    "authors": [
      "Francesca R. Crucinio",
      "Valentin De Bortoli",
      "Arnaud Doucet",
      "Adam M. Johansen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Functional Analysis (math.FA)",
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2209.09936"
  },
  {
    "id": "arXiv:2209.10099",
    "title": "On the benefits of self-taught learning for brain decoding",
    "abstract": "On the benefits of self-taught learning for brain decoding",
    "descriptor": "",
    "authors": [
      "Elodie Germani",
      "Elisa Fromont",
      "Camille Maumet"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.10099"
  },
  {
    "id": "arXiv:2209.10715",
    "title": "Batch Bayesian optimisation via density-ratio estimation with guarantees",
    "abstract": "Comments: Extended version of paper accepted at NeurIPS 2022",
    "descriptor": "\nComments: Extended version of paper accepted at NeurIPS 2022\n",
    "authors": [
      "Rafael Oliveira",
      "Louis Tiao",
      "Fabio Ramos"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.10715"
  },
  {
    "id": "arXiv:2209.11827",
    "title": "One-Shot Reachability Analysis of Neural Network Dynamical Systems",
    "abstract": "Comments: The introduction is updated",
    "descriptor": "\nComments: The introduction is updated\n",
    "authors": [
      "Shaoru Chen",
      "Victor M. Preciado",
      "Mahyar Fazlyab"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.11827"
  },
  {
    "id": "arXiv:2209.11981",
    "title": "Universal Densities Exist for Every Finite Reference Measure",
    "abstract": "Comments: 21 pages, no figures",
    "descriptor": "\nComments: 21 pages, no figures\n",
    "authors": [
      "\u0141ukasz D\u0119bowski"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2209.11981"
  },
  {
    "id": "arXiv:2209.12702",
    "title": "End-to-End Lyrics Recognition with Self-supervised Learning",
    "abstract": "Comments: 4 pages, 2 figures, 3 tables",
    "descriptor": "\nComments: 4 pages, 2 figures, 3 tables\n",
    "authors": [
      "Xiangyu Zhang",
      "Shuyue Stella Li",
      "Zhanhong He",
      "Roberto Togneri",
      "Leibny Paola Garcia"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2209.12702"
  },
  {
    "id": "arXiv:2209.12749",
    "title": "Joint Task Offloading and Resource Optimization in NOMA-based Vehicular  Edge Computing: A Game-Theoretic DRL Approach",
    "abstract": "Joint Task Offloading and Resource Optimization in NOMA-based Vehicular  Edge Computing: A Game-Theoretic DRL Approach",
    "descriptor": "",
    "authors": [
      "Xincao Xu",
      "Kai Liu",
      "Penglin Dai",
      "Feiyu Jin",
      "Hualing Ren",
      "Choujun Zhan",
      "Songtao Guo"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.12749"
  },
  {
    "id": "arXiv:2209.13002",
    "title": "Automated Urban Planning aware Spatial Hierarchies and Human  Instructions",
    "abstract": "Comments: Needs to improve and polish",
    "descriptor": "\nComments: Needs to improve and polish\n",
    "authors": [
      "Dongjie Wang",
      "Kunpeng Liu",
      "Yanyong Huang",
      "Leilei Sun",
      "Bowen Du",
      "Yanjie Fu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.13002"
  },
  {
    "id": "arXiv:2209.13232",
    "title": "A Survey on Graph Neural Networks and Graph Transformers in Computer  Vision: A Task-Oriented Perspective",
    "abstract": "Comments: Preprint",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Chaoqi Chen",
      "Yushuang Wu",
      "Qiyuan Dai",
      "Hong-Yu Zhou",
      "Mutian Xu",
      "Sibei Yang",
      "Xiaoguang Han",
      "Yizhou Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13232"
  },
  {
    "id": "arXiv:2209.14688",
    "title": "An Inductive Construction for Many-Valued Coalgebraic Modal Logic",
    "abstract": "Comments: 15 pages,submitted to a conference",
    "descriptor": "\nComments: 15 pages,submitted to a conference\n",
    "authors": [
      "Chun-Yu Lin",
      "Churn-Jung Liau"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2209.14688"
  },
  {
    "id": "arXiv:2209.15200",
    "title": "An efficient encoder-decoder architecture with top-down attention for  speech separation",
    "abstract": "Comments: 13 pages, 4 figures",
    "descriptor": "\nComments: 13 pages, 4 figures\n",
    "authors": [
      "Kai Li",
      "Runxuan Yang",
      "Xiaolin Hu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15200"
  },
  {
    "id": "arXiv:2210.00314",
    "title": "CAST: Concurrent Recognition and Segmentation with Adaptive Segment  Tokens",
    "abstract": "CAST: Concurrent Recognition and Segmentation with Adaptive Segment  Tokens",
    "descriptor": "",
    "authors": [
      "Tsung-Wei Ke",
      "Stella X. Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.00314"
  },
  {
    "id": "arXiv:2210.01765",
    "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
    "abstract": "Comments: Preprint. Work in Progress. Code: this https URL",
    "descriptor": "\nComments: Preprint. Work in Progress. Code: this https URL\n",
    "authors": [
      "Shengjie Luo",
      "Tianlang Chen",
      "Yixian Xu",
      "Shuxin Zheng",
      "Tie-Yan Liu",
      "Di He",
      "Liwei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.01765"
  },
  {
    "id": "arXiv:2210.02202",
    "title": "A new family of Constitutive Artificial Neural Networks towards  automated model discovery",
    "abstract": "Comments: 31 pages, 14 figures",
    "descriptor": "\nComments: 31 pages, 14 figures\n",
    "authors": [
      "Kevin Linka",
      "Ellen Kuhl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)"
    ],
    "url": "https://arxiv.org/abs/2210.02202"
  },
  {
    "id": "arXiv:2210.02621",
    "title": "U3E: Unsupervised and Erasure-based Evidence Extraction for Machine  Reading Comprehension",
    "abstract": "U3E: Unsupervised and Erasure-based Evidence Extraction for Machine  Reading Comprehension",
    "descriptor": "",
    "authors": [
      "Suzhe He",
      "Shumin Shi",
      "Chenghao Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02621"
  },
  {
    "id": "arXiv:2210.02729",
    "title": "Join-Chain Network: A Logical Reasoning View of the Multi-head Attention  in Transformer",
    "abstract": "Comments: Accepted by 2022 IEEE International Conference on Data Mining (ICDM 2022) Workshop on Foundation Models for Vision and Language",
    "descriptor": "\nComments: Accepted by 2022 IEEE International Conference on Data Mining (ICDM 2022) Workshop on Foundation Models for Vision and Language\n",
    "authors": [
      "Jianyi Zhang",
      "Yiran Chen",
      "Jianshu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02729"
  },
  {
    "id": "arXiv:2210.03078",
    "title": "Rainier: Reinforced Knowledge Introspector for Commonsense Question  Answering",
    "abstract": "Comments: EMNLP 2022 main conference",
    "descriptor": "\nComments: EMNLP 2022 main conference\n",
    "authors": [
      "Jiacheng Liu",
      "Skyler Hallinan",
      "Ximing Lu",
      "Pengfei He",
      "Sean Welleck",
      "Hannaneh Hajishirzi",
      "Yejin Choi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.03078"
  },
  {
    "id": "arXiv:2210.03102",
    "title": "Ambiguous Images With Human Judgments for Robust Visual Event  Classification",
    "abstract": "Comments: 10 pages, NeurIPS 2022 Datasets and Benchmarks Track",
    "descriptor": "\nComments: 10 pages, NeurIPS 2022 Datasets and Benchmarks Track\n",
    "authors": [
      "Kate Sanders",
      "Reno Kriz",
      "Anqi Liu",
      "Benjamin Van Durme"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.03102"
  },
  {
    "id": "arXiv:2210.03221",
    "title": "PQLM -- Multilingual Decentralized Portable Quantum Language Model for  Privacy Protection",
    "abstract": "Comments: 5 pages, 3 figures, 3 tables",
    "descriptor": "\nComments: 5 pages, 3 figures, 3 tables\n",
    "authors": [
      "Shuyue Stella Li",
      "Xiangyu Zhang",
      "Shu Zhou",
      "Hongchao Shu",
      "Ruixing Liang",
      "Hexin Liu",
      "Leibny Paola Garcia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.03221"
  },
  {
    "id": "arXiv:2210.03235",
    "title": "Improving Large-scale Paraphrase Acquisition and Generation",
    "abstract": "Comments: The project webpage is at this http URL Accepted at EMNLP 2022",
    "descriptor": "\nComments: The project webpage is at this http URL Accepted at EMNLP 2022\n",
    "authors": [
      "Yao Dou",
      "Chao Jiang",
      "Wei Xu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.03235"
  },
  {
    "id": "arXiv:2210.03312",
    "title": "Distillation-Resistant Watermarking for Model Protection in NLP",
    "abstract": "Distillation-Resistant Watermarking for Model Protection in NLP",
    "descriptor": "",
    "authors": [
      "Xuandong Zhao",
      "Lei Li",
      "Yu-Xiang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.03312"
  },
  {
    "id": "arXiv:2210.03575",
    "title": "Are Representations Built from the Ground Up? An Empirical Examination  of Local Composition in Language Models",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Emmy Liu",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.03575"
  },
  {
    "id": "arXiv:2210.03600",
    "title": "BlanketSet -- A clinical real word action recognition and qualitative  semi-synchronised MoCap dataset",
    "abstract": "Comments: 5 pages, Dataset available at: $\\href{this https URL}{rdm.inesctec.pt/dataset/nis-2022-004}$",
    "descriptor": "\nComments: 5 pages, Dataset available at: $\\href{this https URL}{rdm.inesctec.pt/dataset/nis-2022-004}$\n",
    "authors": [
      "Jo\u00e3o Carmona",
      "Tam\u00e1s Kar\u00e1csony",
      "Jo\u00e3o Paulo Silva Cunha"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.03600"
  },
  {
    "id": "arXiv:2210.03640",
    "title": "Artificial Intelligence and Natural Language Processing and  Understanding in Space: A Methodological Framework and Four ESA Case Studies",
    "abstract": "Artificial Intelligence and Natural Language Processing and  Understanding in Space: A Methodological Framework and Four ESA Case Studies",
    "descriptor": "",
    "authors": [
      "Jos\u00e9 Manuel G\u00f3mez-P\u00e9rez",
      "Andr\u00e9s Garc\u00eda-Silva",
      "Rosemarie Leone",
      "Mirko Albani",
      "Moritz Fontaine",
      "Charles Poncet",
      "Leopold Summerer",
      "Alessandro Donati",
      "Ilaria Roma",
      "Stefano Scaglioni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.03640"
  },
  {
    "id": "arXiv:2210.03739",
    "title": "Dual-Stage Deeply Supervised Attention-based Convolutional Neural  Networks for Mandibular Canal Segmentation in CBCT Scans",
    "abstract": "Comments: 7 Pages",
    "descriptor": "\nComments: 7 Pages\n",
    "authors": [
      "Azka Rehman",
      "Muhammad Usman",
      "Rabeea Jawaid",
      "Shi Sub Byon",
      "Sung Hyun Kim",
      "Byoung Dai Lee",
      "Byung il Lee",
      "Yeong Gil Shin"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.03739"
  },
  {
    "id": "arXiv:2210.03956",
    "title": "Robust Graph Structure Learning over Images via Multiple Statistical  Tests",
    "abstract": "Comments: Accepted by the NeurIPS 2022. Homepage: this https URL",
    "descriptor": "\nComments: Accepted by the NeurIPS 2022. Homepage: this https URL\n",
    "authors": [
      "Yaohua Wang",
      "FangYi Zhang",
      "Ming Lin",
      "Senzhang Wang",
      "Xiuyu Sun",
      "Rong Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.03956"
  },
  {
    "id": "arXiv:2210.04041",
    "title": "Almost-lossless compression of a low-rank random tensor",
    "abstract": "Comments: This version fixes typos and adds some remarks",
    "descriptor": "\nComments: This version fixes typos and adds some remarks\n",
    "authors": [
      "Minh Thanh Vu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.04041"
  },
  {
    "id": "arXiv:2210.04325",
    "title": "ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Jiannan Xiang",
      "Zhengzhong Liu",
      "Yucheng Zhou",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04325"
  },
  {
    "id": "arXiv:2210.04561",
    "title": "A Comprehensive Survey of Data Augmentation in Visual Reinforcement  Learning",
    "abstract": "Comments: A well-classified paper list that will be continuously updated can be found at this https URL",
    "descriptor": "\nComments: A well-classified paper list that will be continuously updated can be found at this https URL\n",
    "authors": [
      "Guozheng Ma",
      "Zhen Wang",
      "Zhecheng Yuan",
      "Xueqian Wang",
      "Bo Yuan",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04561"
  },
  {
    "id": "arXiv:2210.04610",
    "title": "Red-Teaming the Stable Diffusion Safety Filter",
    "abstract": "Red-Teaming the Stable Diffusion Safety Filter",
    "descriptor": "",
    "authors": [
      "Javier Rando",
      "Daniel Paleka",
      "David Lindner",
      "Lennart Heim",
      "Florian Tram\u00e8r"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04610"
  },
  {
    "id": "arXiv:2210.04633",
    "title": "CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models  for Programming Language Attend Code Structure",
    "abstract": "Comments: Accepted by EMNLP 2022",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Nuo Chen",
      "Qiushi Sun",
      "Renyu Zhu",
      "Xiang Li",
      "Xuesong Lu",
      "Ming Gao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.04633"
  },
  {
    "id": "arXiv:2210.04700",
    "title": "Bio-inspired Algorithms in the Optimisation of Wireless Sensor Networks",
    "abstract": "Bio-inspired Algorithms in the Optimisation of Wireless Sensor Networks",
    "descriptor": "",
    "authors": [
      "Joana Matos",
      "Carine M. Rebello",
      "Erbet A. Costa",
      "Luana P. Queiroz",
      "Maria Joao B. Regufe",
      "Idelfonso B.R. Nogueira"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.04700"
  },
  {
    "id": "arXiv:2210.04870",
    "title": "SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge  Graph Link Prediction",
    "abstract": "Comments: Accepted to Findings of EMNLP 2022",
    "descriptor": "\nComments: Accepted to Findings of EMNLP 2022\n",
    "authors": [
      "Miao Peng",
      "Ben Liu",
      "Qianqian Xie",
      "Wenjie Xu",
      "Hua Wang",
      "Min Peng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04870"
  },
  {
    "id": "arXiv:2210.05150",
    "title": "DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical  Reinforcement Learning",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Seungjae Lee",
      "Jigang Kim",
      "Inkyu Jang",
      "H. Jin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05150"
  },
  {
    "id": "arXiv:2210.05177",
    "title": "Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation  Approach",
    "abstract": "Comments: 20 pages, 5figures, accepted by NeurIPS 2022",
    "descriptor": "\nComments: 20 pages, 5figures, accepted by NeurIPS 2022\n",
    "authors": [
      "Peng Mi",
      "Li Shen",
      "Tianhe Ren",
      "Yiyi Zhou",
      "Xiaoshuai Sun",
      "Rongrong Ji",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05177"
  },
  {
    "id": "arXiv:2210.05674",
    "title": "Semi-supervised detection of structural damage using Variational  Autoencoder and a One-Class Support Vector Machine",
    "abstract": "Semi-supervised detection of structural damage using Variational  Autoencoder and a One-Class Support Vector Machine",
    "descriptor": "",
    "authors": [
      "Andrea Pollastro",
      "Giusiana Testa",
      "Antonio Bilotta",
      "Roberto Prevete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05674"
  },
  {
    "id": "arXiv:2210.05725",
    "title": "Measuring and Improving Semantic Diversity of Dialogue Generation",
    "abstract": "Comments: EMNLP22 Findings",
    "descriptor": "\nComments: EMNLP22 Findings\n",
    "authors": [
      "Seungju Han",
      "Beomsu Kim",
      "Buru Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05725"
  },
  {
    "id": "arXiv:2210.06565",
    "title": "That's the Wrong Lung! Evaluating and Improving the Interpretability of  Unsupervised Multimodal Encoders for Medical Data",
    "abstract": "That's the Wrong Lung! Evaluating and Improving the Interpretability of  Unsupervised Multimodal Encoders for Medical Data",
    "descriptor": "",
    "authors": [
      "Denis Jered McInerney",
      "Geoffrey Young",
      "Jan-Willem van de Meent",
      "Byron C. Wallace"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.06565"
  },
  {
    "id": "arXiv:2210.06774",
    "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Kevin Yang",
      "Yuandong Tian",
      "Nanyun Peng",
      "Dan Klein"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06774"
  },
  {
    "id": "arXiv:2210.06983",
    "title": "Denoising Masked AutoEncoders are Certifiable Robust Vision Learners",
    "abstract": "Denoising Masked AutoEncoders are Certifiable Robust Vision Learners",
    "descriptor": "",
    "authors": [
      "Quanlin Wu",
      "Hang Ye",
      "Yuntian Gu",
      "Huishuai Zhang",
      "Di He",
      "Liwei Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06983"
  },
  {
    "id": "arXiv:2210.07128",
    "title": "Language Models of Code are Few-Shot Commonsense Learners",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Aman Madaan",
      "Shuyan Zhou",
      "Uri Alon",
      "Yiming Yang",
      "Graham Neubig"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07128"
  },
  {
    "id": "arXiv:2210.07428",
    "title": "A comparative study of the performance of different search algorithms on  FOON graphs",
    "abstract": "A comparative study of the performance of different search algorithms on  FOON graphs",
    "descriptor": "",
    "authors": [
      "Kumar Shashwat"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.07428"
  },
  {
    "id": "arXiv:2210.07455",
    "title": "Controlling Bias Exposure for Fair Interpretable Predictions",
    "abstract": "Comments: Accepted to EMNLP-2022 Findings",
    "descriptor": "\nComments: Accepted to EMNLP-2022 Findings\n",
    "authors": [
      "Zexue He",
      "Yu Wang",
      "Julian McAuley",
      "Bodhisattwa Prasad Majumder"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07455"
  },
  {
    "id": "arXiv:2210.07474",
    "title": "SQA3D: Situated Question Answering in 3D Scenes",
    "abstract": "Comments: First two authors contributed equally",
    "descriptor": "\nComments: First two authors contributed equally\n",
    "authors": [
      "Xiaojian Ma",
      "Silong Yong",
      "Zilong Zheng",
      "Qing Li",
      "Yitao Liang",
      "Song-Chun Zhu",
      "Siyuan Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.07474"
  },
  {
    "id": "arXiv:2210.07565",
    "title": "Multi-Task Pre-Training of Modular Prompt for Few-Shot Learning",
    "abstract": "Comments: Code and data are publicly available at this https URL",
    "descriptor": "\nComments: Code and data are publicly available at this https URL\n",
    "authors": [
      "Tianxiang Sun",
      "Zhengfu He",
      "Qin Zhu",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.07565"
  },
  {
    "id": "arXiv:2210.07884",
    "title": "SealClub: Computer-aided Paper Document Authentication",
    "abstract": "SealClub: Computer-aided Paper Document Authentication",
    "descriptor": "",
    "authors": [
      "Mart\u00edn Ochoa",
      "Jorge Toro-Pozo",
      "David Basin"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.07884"
  },
  {
    "id": "arXiv:2210.08016",
    "title": "Prediction of drug effectiveness in rheumatoid arthritis patients based  on machine learning algorithms",
    "abstract": "Comments: 13 pages, 5 figures, to be published in ICBBE 2022",
    "descriptor": "\nComments: 13 pages, 5 figures, to be published in ICBBE 2022\n",
    "authors": [
      "Shengjia Chen",
      "Nikunj Gupta",
      "Woodward B. Galbraith",
      "Valay Shah",
      "Jacopo Cirrone"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2210.08016"
  },
  {
    "id": "arXiv:2210.08027",
    "title": "Predicting Good Quantum Circuit Compilation Options",
    "abstract": "Comments: 10 pages, 6 figures, v2: slightly revised manuscript",
    "descriptor": "\nComments: 10 pages, 6 figures, v2: slightly revised manuscript\n",
    "authors": [
      "Nils Quetschlich",
      "Lukas Burgholzer",
      "Robert Wille"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2210.08027"
  },
  {
    "id": "arXiv:2210.08220",
    "title": "Min max method, shape, topological derivatives, averaged Lagrangian,  homogenization, two scale convergence, Helmholtz equation",
    "abstract": "Min max method, shape, topological derivatives, averaged Lagrangian,  homogenization, two scale convergence, Helmholtz equation",
    "descriptor": "",
    "authors": [
      "Mame Gor Ngom",
      "Ibrahima Faye",
      "Diaraf Seck"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.08220"
  },
  {
    "id": "arXiv:2210.08291",
    "title": "Bidirectional Semi-supervised Dual-branch CNN for Robust 3D  Reconstruction of Stereo Endoscopic Images via Adaptive Cross and Parallel  Supervisions",
    "abstract": "Comments: 11 pages",
    "descriptor": "\nComments: 11 pages\n",
    "authors": [
      "Hongkuan Shi",
      "Zhiwei Wang",
      "Ying Zhou",
      "Dun Li",
      "Xin Yang",
      "Qiang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.08291"
  },
  {
    "id": "arXiv:2210.08435",
    "title": "On the User Behavior Leakage from Recommender System Exposure",
    "abstract": "Comments: Submitted to the Special Section on Trustworthy Recommendation and Search of ACM TOIS on May 31, 2022 and accepted on October 4",
    "descriptor": "\nComments: Submitted to the Special Section on Trustworthy Recommendation and Search of ACM TOIS on May 31, 2022 and accepted on October 4\n",
    "authors": [
      "Xin Xin",
      "Jiyuan Yang",
      "Hanbing Wang",
      "Jun Ma",
      "Pengjie Ren",
      "Hengliang Luo",
      "Xinlei Shi",
      "Zhumin Chen",
      "Zhaochun Ren"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.08435"
  },
  {
    "id": "arXiv:2210.08465",
    "title": "Character-Centric Story Visualization via Visual Planning and Token  Alignment",
    "abstract": "Comments: accepted by EMNLP2022",
    "descriptor": "\nComments: accepted by EMNLP2022\n",
    "authors": [
      "Hong Chen",
      "Rujun Han",
      "Te-Lin Wu",
      "Hideki Nakayama",
      "Nanyun Peng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.08465"
  },
  {
    "id": "arXiv:2210.08530",
    "title": "Logical Relations for Partial Features and Automatic Differentiation  Correctness",
    "abstract": "Comments: 25 pages (18 pages + references and appendices), conference paper (the corresponding extended work can be found at arXiv:2210.07724), submitted to FoSSaCS. arXiv admin note: substantial text overlap with arXiv:2210.07724",
    "descriptor": "\nComments: 25 pages (18 pages + references and appendices), conference paper (the corresponding extended work can be found at arXiv:2210.07724), submitted to FoSSaCS. arXiv admin note: substantial text overlap with arXiv:2210.07724\n",
    "authors": [
      "Fernando Lucatelli Nunes",
      "Matthijs V\u00e1k\u00e1r"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)",
      "Logic in Computer Science (cs.LO)",
      "Category Theory (math.CT)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.08530"
  },
  {
    "id": "arXiv:2210.08559",
    "title": "Coordinated Topic Modeling",
    "abstract": "Coordinated Topic Modeling",
    "descriptor": "",
    "authors": [
      "Pritom Saha Akash",
      "Jie Huang",
      "Kevin Chen-Chuan Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.08559"
  },
  {
    "id": "arXiv:2210.08709",
    "title": "A Unified Positive-Unlabeled Learning Framework for Document-Level  Relation Extraction with Different Levels of Labeling",
    "abstract": "Comments: Accepted to EMNLP 2022 Main Conference",
    "descriptor": "\nComments: Accepted to EMNLP 2022 Main Conference\n",
    "authors": [
      "Ye Wang",
      "Xinxin Liu",
      "Wenxin Hu",
      "Tao Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.08709"
  },
  {
    "id": "arXiv:2210.08749",
    "title": "A Transformer-based Generative Model for De Novo Molecular Design",
    "abstract": "A Transformer-based Generative Model for De Novo Molecular Design",
    "descriptor": "",
    "authors": [
      "Wenlu Wang",
      "Ye Wang",
      "Honggang Zhao",
      "Simone Sciabola"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2210.08749"
  },
  {
    "id": "arXiv:2210.08998",
    "title": "A Symbolic Representation of Human Posture for Interpretable Learning  and Reasoning",
    "abstract": "Comments: Accepted for presentation at the AAAI 2022 Fall Symposium Series, in the symposium for Artificial Intelligence for Human-Robot Interaction",
    "descriptor": "\nComments: Accepted for presentation at the AAAI 2022 Fall Symposium Series, in the symposium for Artificial Intelligence for Human-Robot Interaction\n",
    "authors": [
      "Richard G. Freedman",
      "Joseph B. Mueller",
      "Jack Ladwig",
      "Steven Johnston",
      "David McDonald",
      "Helen Wauck",
      "Ruta Wheelock",
      "Hayley Borck"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.08998"
  },
  {
    "id": "arXiv:2210.09059",
    "title": "Space Trusted Autonomy Readiness Levels",
    "abstract": "Space Trusted Autonomy Readiness Levels",
    "descriptor": "",
    "authors": [
      "Kerianne L. Hobbs",
      "Joseph B. Lyons",
      "Martin S. Feather",
      "Benjamen P Bycroft",
      "Sean Phillips",
      "Michelle Simon",
      "Mark Harter",
      "Kenneth Costello",
      "Yuri Gawdiak",
      "Stephen Paine"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.09059"
  },
  {
    "id": "arXiv:2210.09364",
    "title": "Probabilistic Categorical Adversarial Attack & Adversarial Training",
    "abstract": "Probabilistic Categorical Adversarial Attack & Adversarial Training",
    "descriptor": "",
    "authors": [
      "Pengfei He",
      "Han Xu",
      "Jie Ren",
      "Yuxuan Wan",
      "Zitao Liu",
      "Jiliang Tang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.09364"
  },
  {
    "id": "arXiv:2210.09496",
    "title": "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning  with Demonstrations",
    "abstract": "Comments: 27 pages; published as NeurIPS 2022 poster paper",
    "descriptor": "\nComments: 27 pages; published as NeurIPS 2022 poster paper\n",
    "authors": [
      "Kai Yan",
      "Alexander G. Schwing",
      "Yu-Xiong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.09496"
  },
  {
    "id": "arXiv:2210.09784",
    "title": "Generalized Many-Body Dispersion Correction through Random-phase  Approximation for Chemically Accurate Density Functional Theory",
    "abstract": "Generalized Many-Body Dispersion Correction through Random-phase  Approximation for Chemically Accurate Density Functional Theory",
    "descriptor": "",
    "authors": [
      "Pier Paolo Poier",
      "Louis Lagard\u00e8re",
      "Jean-Philip Piquemal"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09784"
  },
  {
    "id": "arXiv:2210.09850",
    "title": "Near Real-time CO$_2$ Emissions Based on Carbon Satellite and Artificial  Intelligence",
    "abstract": "Near Real-time CO$_2$ Emissions Based on Carbon Satellite and Artificial  Intelligence",
    "descriptor": "",
    "authors": [
      "Zhengwen Zhang",
      "Jinjin Gu",
      "Junhua Zhao",
      "Jianwei Huang",
      "Haifeng Wu"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.09850"
  },
  {
    "id": "arXiv:2210.09904",
    "title": "MaSS: Multi-attribute Selective Suppression",
    "abstract": "MaSS: Multi-attribute Selective Suppression",
    "descriptor": "",
    "authors": [
      "Chun-Fu Chen",
      "Shaohan Hu",
      "Zhonghao Shi",
      "Prateek Gulati",
      "Bill Moriarty",
      "Marco Pistoia",
      "Vincenzo Piuri",
      "Pierangela Samarati"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.09904"
  },
  {
    "id": "arXiv:2210.09953",
    "title": "Randomized Cholesky QR factorizations",
    "abstract": "Randomized Cholesky QR factorizations",
    "descriptor": "",
    "authors": [
      "Oleg Balabanov"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.09953"
  },
  {
    "id": "arXiv:2210.10033",
    "title": "Edge-based Monocular Thermal-Inertial Odometry in Visually Degraded  Environments",
    "abstract": "Comments: 8 pages, 10 figures,",
    "descriptor": "\nComments: 8 pages, 10 figures,\n",
    "authors": [
      "Yu Wang",
      "Haoyao Chen",
      "Yufeng Liu",
      "Shiwu Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.10033"
  },
  {
    "id": "arXiv:2210.10138",
    "title": "Class-Level Confidence Based 3D Semi-Supervised Learning",
    "abstract": "Class-Level Confidence Based 3D Semi-Supervised Learning",
    "descriptor": "",
    "authors": [
      "Zhimin Chen",
      "Longlong Jing",
      "Liang Yang",
      "Yingwei Li",
      "Bing Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.10138"
  },
  {
    "id": "arXiv:2210.10358",
    "title": "Leveraging a New Spanish Corpus for Multilingual and Crosslingual  Metaphor Detection",
    "abstract": "Comments: To be published in CoNLL 2022",
    "descriptor": "\nComments: To be published in CoNLL 2022\n",
    "authors": [
      "Elisa Sanchez-Bayona",
      "Rodrigo Agerri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.10358"
  },
  {
    "id": "arXiv:2210.10362",
    "title": "CPL: Counterfactual Prompt Learning for Vision and Language Models",
    "abstract": "CPL: Counterfactual Prompt Learning for Vision and Language Models",
    "descriptor": "",
    "authors": [
      "Xuehai He",
      "Diji Yang",
      "Weixi Feng",
      "Tsu-Jui Fu",
      "Arjun Akula",
      "Varun Jampani",
      "Pradyumna Narayana",
      "Sugato Basu",
      "William Yang Wang",
      "Xin Eric Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.10362"
  },
  {
    "id": "arXiv:2210.10392",
    "title": "Spatio-channel Attention Blocks for Cross-modal Crowd Counting",
    "abstract": "Comments: Accepted to ACCV 2022 (Oral). Code is available at this https URL",
    "descriptor": "\nComments: Accepted to ACCV 2022 (Oral). Code is available at this https URL\n",
    "authors": [
      "Youjia Zhang",
      "Soyun Choi",
      "Sungeun Hong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.10392"
  },
  {
    "id": "arXiv:2210.10488",
    "title": "Attribution and Obfuscation of Neural Text Authorship: A Data Mining  Perspective",
    "abstract": "Attribution and Obfuscation of Neural Text Authorship: A Data Mining  Perspective",
    "descriptor": "",
    "authors": [
      "Adaku Uchendu",
      "Thai Le",
      "Dongwon Lee"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.10488"
  },
  {
    "id": "arXiv:2210.10491",
    "title": "Visual SLAM: What are the Current Trends and What to Expect?",
    "abstract": "Comments: 18 pages, 4 figures, 1 table",
    "descriptor": "\nComments: 18 pages, 4 figures, 1 table\n",
    "authors": [
      "Ali Tourani",
      "Hriday Bavle",
      "Jose Luis Sanchez-Lopez",
      "Holger Voos"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.10491"
  },
  {
    "id": "arXiv:2210.10562",
    "title": "Research on Hermitian self-dual codes, GRS codes and EGRS codes",
    "abstract": "Comments: 15 pages",
    "descriptor": "\nComments: 15 pages\n",
    "authors": [
      "Ruhao Wan",
      "Shixin Zhu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.10562"
  },
  {
    "id": "arXiv:2210.10709",
    "title": "Schema-aware Reference as Prompt Improves Data-Efficient Relational  Triple and Event Extraction",
    "abstract": "Comments: Work in progress",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Yunzhi Yao",
      "Shengyu Mao",
      "Xiang Chen",
      "Ningyu Zhang",
      "Shumin Deng",
      "Huajun Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.10709"
  },
  {
    "id": "arXiv:2210.10913",
    "title": "Palm up: Playing in the Latent Manifold for Unsupervised Pretraining",
    "abstract": "Comments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Hao Liu",
      "Tom Zahavy",
      "Volodymyr Mnih",
      "Satinder Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.10913"
  },
  {
    "id": "arXiv:2210.10972",
    "title": "A Multimodal Sensor Fusion Framework Robust to Missing Modalities for  Person Recognition",
    "abstract": "Comments: Accepted for ACM Multimedia Asia, 2022",
    "descriptor": "\nComments: Accepted for ACM Multimedia Asia, 2022\n",
    "authors": [
      "Vijay John",
      "Yasutomo Kawanishi"
    ],
    "subjectives": [
      "Multimedia (cs.MM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.10972"
  },
  {
    "id": "arXiv:2210.11018",
    "title": "An Attention-Guided and Wavelet-Constrained Generative Adversarial  Network for Infrared and Visible Image Fusion",
    "abstract": "An Attention-Guided and Wavelet-Constrained Generative Adversarial  Network for Infrared and Visible Image Fusion",
    "descriptor": "",
    "authors": [
      "Xiaowen Liu",
      "Renhua Wang",
      "Hongtao Huo",
      "Xin Yang",
      "Jing Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.11018"
  },
  {
    "id": "arXiv:2210.11035",
    "title": "PointTAD: Multi-Label Temporal Action Detection with Learnable Query  Points",
    "abstract": "Comments: NeurIPS 2022 camera ready version",
    "descriptor": "\nComments: NeurIPS 2022 camera ready version\n",
    "authors": [
      "Jing Tan",
      "Xiaotong Zhao",
      "Xintian Shi",
      "Bin Kang",
      "Limin Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.11035"
  },
  {
    "id": "arXiv:2210.11060",
    "title": "Doc2Bot: Accessing Heterogeneous Documents via Conversational Bots",
    "abstract": "Comments: 17 pages, 14 figures. Accepted by Findings of EMNLP 2022",
    "descriptor": "\nComments: 17 pages, 14 figures. Accepted by Findings of EMNLP 2022\n",
    "authors": [
      "Haomin Fu",
      "Yeqin Zhang",
      "Haiyang Yu",
      "Jian Sun",
      "Fei Huang",
      "Luo Si",
      "Yongbin Li",
      "Cam-Tu Nguyen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.11060"
  },
  {
    "id": "arXiv:2210.11065",
    "title": "MovieCLIP: Visual Scene Recognition in Movies",
    "abstract": "Comments: Accepted to 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2023). Project website with supplemental material: this https URL Revised version with updated author affiliations",
    "descriptor": "\nComments: Accepted to 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2023). Project website with supplemental material: this https URL Revised version with updated author affiliations\n",
    "authors": [
      "Digbalay Bose",
      "Rajat Hebbar",
      "Krishna Somandepalli",
      "Haoyang Zhang",
      "Yin Cui",
      "Kree Cole-McLaughlin",
      "Huisheng Wang",
      "Shrikanth Narayanan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.11065"
  },
  {
    "id": "arXiv:2210.11269",
    "title": "Accurate Extrinsic Prediction of Physical Systems Using Transformers",
    "abstract": "Comments: 13 pages, 10 figures, submitted at SIAM Data Mining 23 (SDM23)",
    "descriptor": "\nComments: 13 pages, 10 figures, submitted at SIAM Data Mining 23 (SDM23)\n",
    "authors": [
      "Arnaud Pannatier",
      "Kyle Matoba",
      "Fran\u00e7ois Fleuret"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Fluid Dynamics (physics.flu-dyn)"
    ],
    "url": "https://arxiv.org/abs/2210.11269"
  },
  {
    "id": "arXiv:2210.11773",
    "title": "SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval",
    "abstract": "Comments: 12 pages, accepted by EMNLP 2022",
    "descriptor": "\nComments: 12 pages, accepted by EMNLP 2022\n",
    "authors": [
      "Kun Zhou",
      "Yeyun Gong",
      "Xiao Liu",
      "Wayne Xin Zhao",
      "Yelong Shen",
      "Anlei Dong",
      "Jingwen Lu",
      "Rangan Majumder",
      "Ji-Rong Wen",
      "Nan Duan",
      "Weizhu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.11773"
  },
  {
    "id": "arXiv:2210.11874",
    "title": "Blind Polynomial Regression",
    "abstract": "Comments: Submitted to ICASSP 2023",
    "descriptor": "\nComments: Submitted to ICASSP 2023\n",
    "authors": [
      "Alberto Natali",
      "Geert Leus"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.11874"
  },
  {
    "id": "arXiv:2210.12023",
    "title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning  with Language Models",
    "abstract": "Comments: A shorter version of the paper was accepted at the MATH-AI Workshop at NeurIPS 2022",
    "descriptor": "\nComments: A shorter version of the paper was accepted at the MATH-AI Workshop at NeurIPS 2022\n",
    "authors": [
      "Alessandro Stolfo",
      "Zhijing Jin",
      "Kumar Shridhar",
      "Bernhard Sch\u00f6lkopf",
      "Mrinmaya Sachan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.12023"
  },
  {
    "id": "arXiv:2210.12034",
    "title": "Proceedings of the Dialogue Robot Competition 2022",
    "abstract": "Comments: Proceedings of the Dialogue Robot Competition 2022",
    "descriptor": "\nComments: Proceedings of the Dialogue Robot Competition 2022\n",
    "authors": [
      "Ryuichiro Higashinaka",
      "Takashi Minato",
      "Hiromitsu Nishizaki",
      "Takayuki Nagai"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.12034"
  }
]