[
  {
    "id": "arXiv:2210.05671",
    "title": "iMedBot: A Web-based Intelligent Agent for Healthcare Related Prediction  and Deep Learning",
    "abstract": "Background: Breast cancer is a multifactorial disease, genetic and\nenvironmental factors will affect its incidence probability. Breast cancer\nmetastasis is one of the main cause of breast cancer related deaths reported by\nthe American Cancer Society (ACS). Method: the iMedBot is a web application\nthat we developed using the python Flask web framework and deployed on Amazon\nWeb Services. It contains a frontend and a backend. The backend is supported by\na python program we developed using the python Keras and scikit-learn packages,\nwhich can be used to learn deep feedforward neural network (DFNN) models.\nResult: the iMedBot can provide two main services: 1. it can predict 5-, 10-,\nor 15-year breast cancer metastasis based on a set of clinical information\nprovided by a user. The prediction is done by using a set of DFNN models that\nwere pretrained, and 2. It can train DFNN models for a user using user-provided\ndataset. The model trained will be evaluated using AUC and both the AUC value\nand the AUC ROC curve will be provided. Conclusion: The iMedBot web application\nprovides a user-friendly interface for user-agent interaction in conducting\npersonalized prediction and model training. It is an initial attempt to convert\nresults of deep learning research into an online tool that may stir further\nresearch interests in this direction. Keywords: Deep learning, Breast Cancer,\nWeb application, Model training.",
    "descriptor": "",
    "authors": [
      "Chuhan Xu",
      "Xia Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.05671"
  },
  {
    "id": "arXiv:2210.05674",
    "title": "Unsupervised detection of structural damage using Variational  Autoencoder and a One-Class Support Vector Machine",
    "abstract": "In recent years, Artificial Neural Networks (ANNs) have been introduced in\nStructural Health Monitoring (SHM) systems. An unsupervised method with a\ndata-driven approach allows the ANN training on data acquired from an undamaged\nstructural condition to detect structural damages. In standard approaches,\nafter the training stage, a decision rule is manually defined to detect\nanomalous data. However, this process could be made automatic using machine\nlearning methods, whom performances are maximised using hyperparameter\noptimization techniques. The paper proposes an unsupervised method with a\ndata-driven approach to detect structural anomalies. The methodology consists\nof: (i) a Variational Autoencoder (VAE) to approximate undamaged data\ndistribution and (ii) a One-Class Support Vector Machine (OC-SVM) to\ndiscriminate different health conditions using damage sensitive features\nextracted from VAE's signal reconstruction. The method is applied to a scale\nsteel structure that was tested in nine damage's scenarios by IASC-ASCE\nStructural Health Monitoring Task Group.",
    "descriptor": "",
    "authors": [
      "Andrea Pollastro",
      "Giusiana Testa",
      "Antonio Bilotta",
      "Roberto Prevete"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05674"
  },
  {
    "id": "arXiv:2210.05675",
    "title": "Transformers generalize differently from information stored in context  vs in weights",
    "abstract": "Transformer models can use two fundamentally different kinds of information:\ninformation stored in weights during training, and information provided\n``in-context'' at inference time. In this work, we show that transformers\nexhibit different inductive biases in how they represent and generalize from\nthe information in these two sources. In particular, we characterize whether\nthey generalize via parsimonious rules (rule-based generalization) or via\ndirect comparison with observed examples (exemplar-based generalization). This\nis of important practical consequence, as it informs whether to encode\ninformation in weights or in context, depending on how we want models to use\nthat information. In transformers trained on controlled stimuli, we find that\ngeneralization from weights is more rule-based whereas generalization from\ncontext is largely exemplar-based. In contrast, we find that in transformers\npre-trained on natural language, in-context learning is significantly\nrule-based, with larger models showing more rule-basedness. We hypothesise that\nrule-based generalization from in-context information might be an emergent\nconsequence of large-scale training on language, which has sparse rule-like\nstructure. Using controlled stimuli, we verify that transformers pretrained on\ndata containing sparse rule-like structure exhibit more rule-based\ngeneralization.",
    "descriptor": "",
    "authors": [
      "Stephanie C.Y. Chan",
      "Ishita Dasgupta",
      "Junkyung Kim",
      "Dharshan Kumaran",
      "Andrew K. Lampinen",
      "Felix Hill"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05675"
  },
  {
    "id": "arXiv:2210.05676",
    "title": "Towards Consistency and Complementarity: A Multiview Graph Information  Bottleneck Approach",
    "abstract": "The empirical studies of Graph Neural Networks (GNNs) broadly take the\noriginal node feature and adjacency relationship as singleview input, ignoring\nthe rich information of multiple graph views. To circumvent this issue, the\nmultiview graph analysis framework has been developed to fuse graph information\nacross views. How to model and integrate shared (i.e. consistency) and\nview-specific (i.e. complementarity) information is a key issue in multiview\ngraph analysis. In this paper, we propose a novel Multiview Variational Graph\nInformation Bottleneck (MVGIB) principle to maximize the agreement for common\nrepresentations and the disagreement for view-specific representations. Under\nthis principle, we formulate the common and view-specific information\nbottleneck objectives across multiviews by using constraints from mutual\ninformation. However, these objectives are hard to directly optimize since the\nmutual information is computationally intractable. To tackle this challenge, we\nderive variational lower and upper bounds of mutual information terms, and then\ninstead optimize variational bounds to find the approximate solutions for the\ninformation objectives. Extensive experiments on graph benchmark datasets\ndemonstrate the superior effectiveness of the proposed method.",
    "descriptor": "",
    "authors": [
      "Xiaolong Fan",
      "Maoguo Gong",
      "Yue Wu",
      "Mingyang Zhang",
      "Hao Li",
      "Xiangming Jiang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05676"
  },
  {
    "id": "arXiv:2210.05704",
    "title": "Dynamic Ensemble Size Adjustment for Memory Constrained Mondrian Forest",
    "abstract": "Supervised learning algorithms generally assume the availability of enough\nmemory to store data models during the training and test phases. However, this\nassumption is unrealistic when data comes in the form of infinite data streams,\nor when learning algorithms are deployed on devices with reduced amounts of\nmemory. Such memory constraints impact the model behavior and assumptions. In\nthis paper, we show that under memory constraints, increasing the size of a\ntree-based ensemble classifier can worsen its performance. In particular, we\nexperimentally show the existence of an optimal ensemble size for a\nmemory-bounded Mondrian forest on data streams and we design an algorithm to\nguide the forest toward that optimal number by using an estimation of\noverfitting. We tested different variations for this algorithm on a variety of\nreal and simulated datasets, and we conclude that our method can achieve up to\n95% of the performance of an optimally-sized Mondrian forest for stable\ndatasets, and can even outperform it for datasets with concept drifts. All our\nmethods are implemented in the OrpailleCC open-source library and are ready to\nbe used on embedded systems and connected objects.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.07871\n",
    "authors": [
      "Martin Khannouz",
      "Tristan Glatard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05704"
  },
  {
    "id": "arXiv:2210.05708",
    "title": "Perspectives on Negative Research Results in Pervasive Computing",
    "abstract": "Not all research leads to fruitful results; trying new ways or methods may\nsurpass the state of the art, but sometimes the hypothesis is not proven or the\nimprovement is insignificant. In a systems discipline like pervasive computing,\nthere are many sources of errors, from hardware issues over communication\nchannels to heterogeneous software environments. However, failure to succeed is\nnot a failure to progress. It is essential to create platforms for sharing\ninsights, experiences, and lessons learned when conducting research in\npervasive computing so that the same mistakes are not repeated. And sometimes,\na problem is a symptom of discovering new research challenges. Based on the\ncollective input of the First International Workshop on Negative Results in\nPervasive Computing (PerFail 2022), co-located with the 20th International\nConference on Pervasive Computing and Communications (PerCom 2022), this paper\npresents a comprehensive discussion on perspectives on publishing negative\nresults and lessons learned in pervasive computing.",
    "descriptor": "",
    "authors": [
      "Ella Peltonen",
      "Nitinder Mohan",
      "Peter Zdankin",
      "Tanya Shreedhar",
      "Tri Nguyen",
      "Suzan Bayhan",
      "Jon Crowcroft",
      "Jussi Kangasharju",
      "Daniela Nicklas"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.05708"
  },
  {
    "id": "arXiv:2210.05709",
    "title": "Shapley Head Pruning: Identifying and Removing Interference in  Multilingual Transformers",
    "abstract": "Multilingual transformer-based models demonstrate remarkable zero and\nfew-shot transfer across languages by learning and reusing language-agnostic\nfeatures. However, as a fixed-size model acquires more languages, its\nperformance across all languages degrades, a phenomenon termed interference.\nOften attributed to limited model capacity, interference is commonly addressed\nby adding additional parameters despite evidence that transformer-based models\nare overparameterized. In this work, we show that it is possible to reduce\ninterference by instead identifying and pruning language-specific parameters.\nFirst, we use Shapley Values, a credit allocation metric from coalitional game\ntheory, to identify attention heads that introduce interference. Then, we show\nthat removing identified attention heads from a fixed model improves\nperformance for a target language on both sentence classification and\nstructural prediction, seeing gains as large as 24.7\\%. Finally, we provide\ninsights on language-agnostic and language-specific attention heads using\nattention visualization.",
    "descriptor": "\nComments: 8 Pages, 9 Figures\n",
    "authors": [
      "William Held",
      "Diyi Yang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05709"
  },
  {
    "id": "arXiv:2210.05714",
    "title": "Visual Language Maps for Robot Navigation",
    "abstract": "Grounding language to the visual observations of a navigating agent can be\nperformed using off-the-shelf visual-language models pretrained on\nInternet-scale data (e.g., image captions). While this is useful for matching\nimages to natural language descriptions of object goals, it remains disjoint\nfrom the process of mapping the environment, so that it lacks the spatial\nprecision of classic geometric maps. To address this problem, we propose\nVLMaps, a spatial map representation that directly fuses pretrained\nvisual-language features with a 3D reconstruction of the physical world. VLMaps\ncan be autonomously built from video feed on robots using standard exploration\napproaches and enables natural language indexing of the map without additional\nlabeled data. Specifically, when combined with large language models (LLMs),\nVLMaps can be used to (i) translate natural language commands into a sequence\nof open-vocabulary navigation goals (which, beyond prior work, can be spatial\nby construction, e.g., \"in between the sofa and TV\" or \"three meters to the\nright of the chair\") directly localized in the map, and (ii) can be shared\namong multiple robots with different embodiments to generate new obstacle maps\non-the-fly (by using a list of obstacle categories). Extensive experiments\ncarried out in simulated and real world environments show that VLMaps enable\nnavigation according to more complex language instructions than existing\nmethods. Videos are available at https:vlmaps.github.io.",
    "descriptor": "\nComments: Project page: https:vlmaps.github.io\n",
    "authors": [
      "Chenguang Huang",
      "Oier Mees",
      "Andy Zeng",
      "Wolfram Burgard"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05714"
  },
  {
    "id": "arXiv:2210.05715",
    "title": "Relational Embeddings for Language Independent Stance Detection",
    "abstract": "The large majority of the research performed on stance detection has been\nfocused on developing more or less sophisticated text classification systems,\neven when many benchmarks are based on social network data such as Twitter.\nThis paper aims to take on the stance detection task by placing the emphasis\nnot so much on the text itself but on the interaction data available on social\nnetworks. More specifically, we propose a new method to leverage social\ninformation such as friends and retweets by generating relational embeddings,\nnamely, dense vector representations of interaction pairs. Our method can be\napplied to any language and target without any manual tuning. Our experiments\non seven publicly available datasets and four different languages show that\ncombining our relational embeddings with textual methods helps to substantially\nimprove performance, obtaining best results for six out of seven evaluation\nsettings, outperforming strong baselines based on large pre-trained language\nmodels.",
    "descriptor": "",
    "authors": [
      "Joseba Fernandez de Landa",
      "Rodrigo Agerri"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05715"
  },
  {
    "id": "arXiv:2210.05721",
    "title": "Analyzing Text Representations under Tight Annotation Budgets: Measuring  Structural Alignment",
    "abstract": "Annotating large collections of textual data can be time consuming and\nexpensive. That is why the ability to train models with limited annotation\nbudgets is of great importance. In this context, it has been shown that under\ntight annotation budgets the choice of data representation is key. The goal of\nthis paper is to better understand why this is so. With this goal in mind, we\npropose a metric that measures the extent to which a given representation is\nstructurally aligned with a task. We conduct experiments on several text\nclassification datasets testing a variety of models and representations. Using\nour proposed metric we show that an efficient representation for a task (i.e.\none that enables learning from few samples) is a representation that induces a\ngood alignment between latent input structure and class structure.",
    "descriptor": "",
    "authors": [
      "C\u00e9sar Gonz\u00e1lez-Guti\u00e9rrez",
      "Audi Primadhanty",
      "Francesco Cazzaro",
      "Ariadna Quattoni"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05721"
  },
  {
    "id": "arXiv:2210.05723",
    "title": "Embeddings as Epistemic States: Limitations on the Use of Pooling  Operators for Accumulating Knowledge",
    "abstract": "Various neural network architectures rely on pooling operators to aggregate\ninformation coming from different sources. It is often implicitly assumed in\nsuch contexts that vectors encode epistemic states, i.e. that vectors capture\nthe evidence that has been obtained about some properties of interest, and that\npooling these vectors yields a vector that combines this evidence. We study,\nfor a number of standard pooling operators, under what conditions they are\ncompatible with this idea, which we call the epistemic pooling principle. While\nwe find that all the considered pooling operators can satisfy the epistemic\npooling principle, this only holds when embeddings are sufficiently\nhigh-dimensional and, for most pooling operators, when the embeddings satisfy\nparticular constraints (e.g. having non-negative coordinates). We then study\nthe implications of these constraints, starting from the idea that we should be\nable to verify whether an arbitrary propositional formula is satisfied in the\nepistemic state encoded by a given vector. We find that when the epistemic\npooling principle is satisfied, in most cases it is impossible to verify the\nsatisfaction of propositional formulas using linear scoring functions, with two\nexceptions: (i) max-pooling with embeddings that are upper-bounded and (ii)\nHadamard pooling with non-negative embeddings. Finally, we also study an\nextension of the epistemic pooling principle to weighted epistemic states,\nwhere max-pooling emerges as the most suitable operator.",
    "descriptor": "",
    "authors": [
      "Steven Schockaert"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05723"
  },
  {
    "id": "arXiv:2210.05725",
    "title": "Measuring and Improving Semantic Diversity of Dialogue Generation",
    "abstract": "Response diversity has become an important criterion for evaluating the\nquality of open-domain dialogue generation models. However, current evaluation\nmetrics for response diversity often fail to capture the semantic diversity of\ngenerated responses, as they mainly consider lexical aspects of the generated\nresponses. In this paper, we introduce a new automatic evaluation metric to\nmeasure the semantic diversity of generated responses. Through human\nevaluation, we demonstrate that our proposed metric captures human judgments on\nresponse diversity better than existing lexical-level diversity metrics.\nFurthermore, motivated by analyzing an existing dialogue dataset, we propose a\nsimple yet effective learning method that improves the semantic diversity of\ngenerated responses. Our learning method weights training samples based on the\nsemantic distribution of the training set. We show that our learning method\nimproves response diversity and coherency better than other baseline methods\nthrough automatic and human evaluation.",
    "descriptor": "\nComments: EMNLP22 Findings\n",
    "authors": [
      "Seungju Han",
      "Beomsu Kim",
      "Buru Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05725"
  },
  {
    "id": "arXiv:2210.05726",
    "title": "Automatic Speech Recognition of Low-Resource Languages Based on Chukchi",
    "abstract": "The following paper presents a project focused on the research and creation\nof a new Automatic Speech Recognition (ASR) based in the Chukchi language.\nThere is no one complete corpus of the Chukchi language, so most of the work\nconsisted in collecting audio and texts in the Chukchi language from open\nsources and processing them. We managed to collect 21:34:23 hours of audio\nrecordings and 112,719 sentences (or 2,068,273 words) of text in the Chukchi\nlanguage. The XLSR model was trained on the obtained data, which showed good\nresults even with a small amount of data. Besides the fact that the Chukchi\nlanguage is a low-resource language, it is also polysynthetic, which\nsignificantly complicates any automatic processing. Thus, the usual WER metric\nfor evaluating ASR becomes less indicative for a polysynthetic language.\nHowever, the CER metric showed good results. The question of metrics for\npolysynthetic languages remains open.",
    "descriptor": "",
    "authors": [
      "Anastasia Safonova",
      "Tatiana Yudina",
      "Emil Nadimanov",
      "Cydnie Davenport"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05726"
  },
  {
    "id": "arXiv:2210.05728",
    "title": "DeepMend: Learning Occupancy Functions to Represent Shape for Repair",
    "abstract": "We present DeepMend, a novel approach to reconstruct restorations to\nfractured shapes using learned occupancy functions. Existing shape repair\napproaches predict low-resolution voxelized restorations, or require symmetries\nor access to a pre-existing complete oracle. We represent the occupancy of a\nfractured shape as the conjunction of the occupancy of an underlying complete\nshape and the fracture surface, which we model as functions of latent codes\nusing neural networks. Given occupancy samples from an input fractured shape,\nwe estimate latent codes using an inference loss augmented with novel penalty\nterms that avoid empty or voluminous restorations. We use inferred codes to\nreconstruct the restoration shape. We show results with simulated fractures on\nsynthetic and real-world scanned objects, and with scanned real fractured mugs.\nCompared to the existing voxel approach and two baseline methods, our work\nshows state-of-the-art results in accuracy and avoiding restoration artifacts\nover non-fracture regions of the fractured shape.",
    "descriptor": "\nComments: To be published at ECCV 2020 (poster)\n",
    "authors": [
      "Nikolas Lamb",
      "Sean Banerjee",
      "Natasha Kholgade Banerjee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05728"
  },
  {
    "id": "arXiv:2210.05735",
    "title": "TetGAN: A Convolutional Neural Network for Tetrahedral Mesh Generation",
    "abstract": "We present TetGAN, a convolutional neural network designed to generate\ntetrahedral meshes. We represent shapes using an irregular tetrahedral grid\nwhich encodes an occupancy and displacement field. Our formulation enables\ndefining tetrahedral convolution, pooling, and upsampling operations to\nsynthesize explicit mesh connectivity with variable topological genus. The\nproposed neural network layers learn deep features over each tetrahedron and\nlearn to extract patterns within spatial regions across multiple scales. We\nillustrate the capabilities of our technique to encode tetrahedral meshes into\na semantically meaningful latent-space which can be used for shape editing and\nsynthesis. Our project page is at https://threedle.github.io/tetGAN/.",
    "descriptor": "\nComments: Accepted to BMVC2022\n",
    "authors": [
      "William Gao",
      "April Wang",
      "Gal Metzer",
      "Raymond A. Yeh",
      "Rana Hanocka"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05735"
  },
  {
    "id": "arXiv:2210.05738",
    "title": "Distance Map Supervised Landmark Localization for MR-TRUS Registration",
    "abstract": "In this work, we propose to explicitly use the landmarks of prostate to guide\nthe MR-TRUS image registration. We first train a deep neural network to\nautomatically localize a set of meaningful landmarks, and then directly\ngenerate the affine registration matrix from the location of these landmarks.\nFor landmark localization, instead of directly training a network to predict\nthe landmark coordinates, we propose to regress a full-resolution distance map\nof the landmark, which is demonstrated effective in avoiding statistical bias\nto unsatisfactory performance and thus improving performance. We then use the\npredicted landmarks to generate the affine transformation matrix, which\noutperforms the clinicians' manual rigid registration by a significant margin\nin terms of TRE.",
    "descriptor": "\nComments: Submitted to SPIE Medical Imaging 2023\n",
    "authors": [
      "Xinrui Song",
      "Xuanang Xu",
      "Sheng Xu",
      "Baris Turkbey",
      "Bradford J. Wood",
      "Thomas Sanford",
      "Pingkun Yan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05738"
  },
  {
    "id": "arXiv:2210.05740",
    "title": "Stochastic Constrained DRO with a Complexity Independent of Sample Size",
    "abstract": "Distributionally Robust Optimization (DRO), as a popular method to train\nrobust models against distribution shift between training and test sets, has\nreceived tremendous attention in recent years. In this paper, we propose and\nanalyze stochastic algorithms that apply to both non-convex and convex losses\nfor solving Kullback Leibler divergence constrained DRO problem. Compared with\nexisting methods solving this problem, our stochastic algorithms not only enjoy\ncompetitive if not better complexity independent of sample size but also just\nrequire a constant batch size at every iteration, which is more practical for\nbroad applications. We establish a nearly optimal complexity bound for finding\nan $\\epsilon$ stationary solution for non-convex losses and an optimal\ncomplexity for finding an $\\epsilon$ optimal solution for convex losses.\nEmpirical studies demonstrate the effectiveness of the proposed algorithms for\nsolving non-convex and convex constrained DRO problems.",
    "descriptor": "\nComments: 37 pages, 16 figures\n",
    "authors": [
      "Qi Qi",
      "Jiameng Lyu",
      "Kung sik Chan",
      "Er Wei Bai",
      "Tianbao Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05740"
  },
  {
    "id": "arXiv:2210.05741",
    "title": "Road Slope Prediction and Vehicle Dynamics Control for Autonomous  Vehicles",
    "abstract": "Autonomous vehicles can enhance overall performance and implement safety\nmeasures in ways that are impossible with conventional automobiles. These\nfunctions are executed through vehicle control systems, which have been the\nsubject of considerable research. Autonomous cars have a distinct advantage as\nthey possess various perception sensors that can predict road surface\nconditions and other phenomena ahead of time. Many modern automotive control\nsystems treat the road slope as a constant and do not account for changes in\nthe road profile in their vehicle models. As a result, vehicle states may be\nmiscalculated, which, in the worst-case scenario, may result in accidents. This\nis particularly true for high center-of-gravity vehicles like trailers and\ndelivery trucks. With the help of perception sensors in autonomous vehicles, a\nroad slope estimation system can be developed to aid these control systems in\nmaking informed decisions regarding the vehicle's state. The current review is\ndivided into three logical steps that can be discussed in the following manner:\nthe first section describes and reviews the individual steps for developing a\nroad slope estimation system. The second one provides a detailed review of\nprevious investigations that implemented different methods that employ this\nprediction system to improve overall vehicle performance. Finally, a roll\ncontrol system is presented as an innovative idea that builds on the whole\ndiscussion. A rollover prevention system with prediction abilities is presented\nbecause (1) it proves to be a critical safety feature, especially for heavy\nvehicles like buses, trucks, delivery trailers, etc., and (2) not enough\nresearch has been conducted on technologies that integrate a roll stability\ncontroller with a slope estimation system.",
    "descriptor": "\nComments: 16 pages, 15 figures\n",
    "authors": [
      "Gautam Shetty",
      "Sabir Hossain",
      "Chuan Hu",
      "Xianke Lin"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05741"
  },
  {
    "id": "arXiv:2210.05742",
    "title": "Curved Representation Space of Vision Transformers",
    "abstract": "Neural networks with self-attention (a.k.a. Transformers) like ViT and Swin\nhave emerged as a better alternative to traditional convolutional neural\nnetworks (CNNs) for computer vision tasks. However, our understanding of how\nthe new architecture works is still limited. In this paper, we focus on the\nphenomenon that Transformers show higher robustness against corruptions than\nCNNs, while not being overconfident (in fact, we find Transformers are actually\nunderconfident). This is contrary to the intuition that robustness increases\nwith confidence. We resolve this contradiction by investigating how the output\nof the penultimate layer moves in the representation space as the input data\nmoves within a small area. In particular, we show the following. (1) While CNNs\nexhibit fairly linear relationship between the input and output movements,\nTransformers show nonlinear relationship for some data. For those data, the\noutput of Transformers moves in a curved trajectory as the input moves\nlinearly. (2) When a data is located in a curved region, it is hard to move it\nout of the decision region since the output moves along a curved trajectory\ninstead of a straight line to the decision boundary, resulting in high\nrobustness of Transformers. (3) If a data is slightly modified to jump out of\nthe curved region, the movements afterwards become linear and the output goes\nto the decision boundary directly. Thus, Transformers can be attacked easily\nafter a small random jump and the perturbation in the final attacked data\nremains imperceptible, i.e., there does exist a decision boundary near the\ndata. This also explains the underconfident prediction of Transformers. (4) The\ncurved regions in the representation space start to form at an early training\nstage and grow throughout the training course. Some data are trapped in the\nregions, obstructing Transformers from reducing the training loss.",
    "descriptor": "",
    "authors": [
      "Juyeop Kim",
      "Junha Park",
      "Songkuk Kim",
      "Jong-Seok Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05742"
  },
  {
    "id": "arXiv:2210.05751",
    "title": "Toward Sustainable Continual Learning: Detection and Knowledge  Repurposing of Similar Tasks",
    "abstract": "Most existing works on continual learning (CL) focus on overcoming the\ncatastrophic forgetting (CF) problem, with dynamic models and replay methods\nperforming exceptionally well. However, since current works tend to assume\nexclusivity or dissimilarity among learning tasks, these methods require\nconstantly accumulating task-specific knowledge in memory for each task. This\nresults in the eventual prohibitive expansion of the knowledge repository if we\nconsider learning from a long sequence of tasks. In this work, we introduce a\nparadigm where the continual learner gets a sequence of mixed similar and\ndissimilar tasks. We propose a new continual learning framework that uses a\ntask similarity detection function that does not require additional learning,\nwith which we analyze whether there is a specific task in the past that is\nsimilar to the current task. We can then reuse previous task knowledge to slow\ndown parameter expansion, ensuring that the CL system expands the knowledge\nrepository sublinearly to the number of learned tasks. Our experiments show\nthat the proposed framework performs competitively on widely used computer\nvision benchmarks such as CIFAR10, CIFAR100, and EMNIST.",
    "descriptor": "",
    "authors": [
      "Sijia Wang",
      "Yoojin Choi",
      "Junya Chen",
      "Mostafa El-Khamy",
      "Ricardo Henao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05751"
  },
  {
    "id": "arXiv:2210.05756",
    "title": "Streaming Punctuation for Long-form Dictation with Transformers",
    "abstract": "While speech recognition Word Error Rate (WER) has reached human parity for\nEnglish, long-form dictation scenarios still suffer from segmentation and\npunctuation problems resulting from irregular pausing patterns or slow\nspeakers. Transformer sequence tagging models are effective at capturing long\nbi-directional context, which is crucial for automatic punctuation. A typical\nAutomatic Speech Recognition (ASR) production system, however, is constrained\nby real-time requirements, making it hard to incorporate the right context when\nmaking punctuation decisions. In this paper, we propose a streaming approach\nfor punctuation or re-punctuation of ASR output using dynamic decoding windows\nand measure its impact on punctuation and segmentation accuracy in a variety of\nscenarios. The new system tackles over-segmentation issues, improving\nsegmentation F0.5-score by 13.9%. Streaming punctuation achieves an average\nBLEU-score gain of 0.66 for the downstream task of Machine Translation (MT).",
    "descriptor": "",
    "authors": [
      "Piyush Behre",
      "Sharman Tan",
      "Padma Varadharajan",
      "Shuangyu Chang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05756"
  },
  {
    "id": "arXiv:2210.05758",
    "title": "Decoupled Context Processing for Context Augmented Language Modeling",
    "abstract": "Language models can be augmented with a context retriever to incorporate\nknowledge from large external databases. By leveraging retrieved context, the\nneural network does not have to memorize the massive amount of world knowledge\nwithin its internal parameters, leading to better parameter efficiency,\ninterpretability and modularity. In this paper we examined a simple yet\neffective architecture for incorporating external context into language models\nbased on decoupled Encoder Decoder architecture. We showed that such a simple\narchitecture achieves competitive results on auto-regressive language modeling\nand open domain question answering tasks. We also analyzed the behavior of the\nproposed model which performs grounded context transfer. Finally we discussed\nthe computational implications of such retrieval augmented models.",
    "descriptor": "",
    "authors": [
      "Zonglin Li",
      "Ruiqi Guo",
      "Sanjiv Kumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05758"
  },
  {
    "id": "arXiv:2210.05760",
    "title": "Detecting Propagators of Disinformation on Twitter Using Quantitative  Discursive Analysis",
    "abstract": "Efforts by foreign actors to influence public opinion have gained\nconsiderable attention because of their potential to impact democratic\nelections. Thus, the ability to identify and counter sources of disinformation\nis increasingly becoming a top priority for government entities in order to\nprotect the integrity of democratic processes. This study presents a method of\nidentifying Russian disinformation bots on Twitter using centering resonance\nanalysis and Clauset-Newman-Moore community detection. The data reflect a\nsignificant degree of discursive dissimilarity between known Russian\ndisinformation bots and a control set of Twitter users during the timeframe of\nthe 2016 U.S. Presidential Election. The data also demonstrate statistically\nsignificant classification capabilities (MCC = 0.9070) based on community\nclustering. The prediction algorithm is very effective at identifying true\npositives (bots), but is not able to resolve true negatives (non-bots) because\nof the lack of discursive similarity between control users. This leads to a\nhighly sensitive means of identifying propagators of disinformation with a high\ndegree of discursive similarity on Twitter, with implications for limiting the\nspread of disinformation that could impact democratic processes.",
    "descriptor": "\nComments: 12 pages\n",
    "authors": [
      "Mark M. Bailey"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05760"
  },
  {
    "id": "arXiv:2210.05765",
    "title": "A Bimodal Hydrostatic Actuator for Robotic Legs with Compliant Fast  Motion and High Lifting Force",
    "abstract": "Robotic legs have bimodal operations: swing phases when the leg needs to move\nquickly in the air (high-speed, low-force) and stance phases when the leg bears\nthe weight of the system (low-speed, high-force). Sizing a traditional\nsingle-ratio actuation system for such extremum operations leads to oversized\nheavy electric motor and poor energy efficiency, which hinder the capability of\nlegged systems that bear the mass of their actuators and energy source. This\npaper explores an actuation concept where a hydrostatic transmission is\ndynamically reconfigured using valves to suit the requirements of each phase of\na robotic leg. An analysis of the mass-delay-flow trade-off for the switching\nvalve is presented. Then, a custom actuation system is built and integrated on\na robotic leg test bench to evaluate the concept. Experimental results show\nthat 1) small motorized ball valves can make fast transitions between operating\nmodes when designed for this task, 2) the proposed operating principle and\ncontrol schemes allow for seamless transitions, even during an impact with the\nground and 3) the actuator characteristics address the needs of a leg bimodal\noperation in terms of force, speed and compliance.",
    "descriptor": "\nComments: 7 pages, 15 figures\n",
    "authors": [
      "Alex Lecavalier",
      "Jeff Denis",
      "Jean-S\u00e9bastien Plante",
      "Alexandre Girard"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05765"
  },
  {
    "id": "arXiv:2210.05766",
    "title": "Match Cutting: Finding Cuts with Smooth Visual Transitions",
    "abstract": "A match cut is a transition between a pair of shots that uses similar\nframing, composition, or action to fluidly bring the viewer from one scene to\nthe next. Match cuts are frequently used in film, television, and advertising.\nHowever, finding shots that work together is a highly manual and time-consuming\nprocess that can take days. We propose a modular and flexible system to\nefficiently find high-quality match cut candidates starting from millions of\nshot pairs. We annotate and release a dataset of approximately 20k labeled\npairs that we use to evaluate our system, using both classification and metric\nlearning approaches that leverage a variety of image, video, audio, and\naudio-visual feature extractors. In addition, we release code and embeddings\nfor reproducing our experiments at github.com/netflix/matchcut.",
    "descriptor": "",
    "authors": [
      "Boris Chen",
      "Amir Ziai",
      "Rebecca Tucker",
      "Yuchen Xie"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.05766"
  },
  {
    "id": "arXiv:2210.05767",
    "title": "A Multi-Agent Approach for Adaptive Finger Cooperation in Learning-based  In-Hand Manipulation",
    "abstract": "In-hand manipulation is challenging for a multi-finger robotic hand due to\nits high degrees of freedom and the complex interaction with the object. To\nenable in-hand manipulation, existing deep reinforcement learning based\napproaches mainly focus on training a single robot-structure-specific policy\nthrough the centralized learning mechanism, lacking adaptability to changes\nlike robot malfunction. To solve this limitation, this work treats each finger\nas an individual agent and trains multiple agents to control their assigned\nfingers to complete the in-hand manipulation task cooperatively. We propose the\nMulti-Agent Global-Observation Critic and Local-Observation Actor (MAGCLA)\nmethod, where the critic can observe all agents' actions globally, and the\nactor only locally observes its neighbors' actions. Besides, conventional\nindividual experience replay may cause unstable cooperation due to the\nasynchronous performance increment of each agent, which is critical for in-hand\nmanipulation tasks. To solve this issue, we propose the Synchronized Hindsight\nExperience Replay (SHER) method to synchronize and efficiently reuse the\nreplayed experience across all agents. The methods are evaluated in two in-hand\nmanipulation tasks on the Shadow dexterous hand. The results show that SHER\nhelps MAGCLA achieve comparable learning efficiency to a single policy, and the\nMAGCLA approach is more generalizable in different tasks. The trained policies\nhave higher adaptability in the robot malfunction test compared to the baseline\nmulti-agent and single-agent approaches.",
    "descriptor": "\nComments: Submitted to ICRA 2023\n",
    "authors": [
      "Lingfeng Tao",
      "Jiucai Zhang",
      "Michael Bowman",
      "Xiaoli Zhang"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05767"
  },
  {
    "id": "arXiv:2210.05769",
    "title": "Vote'n'Rank: Revision of Benchmarking with Social Choice Theory",
    "abstract": "The development of state-of-the-art systems in different applied areas of\nmachine learning (ML) is driven by benchmarks, which have shaped the paradigm\nof evaluating generalisation capabilities from multiple perspectives. Although\nthe paradigm is shifting towards more fine-grained evaluation across diverse\ntasks, the delicate question of how to aggregate the performances has received\nparticular interest in the community. In general, benchmarks follow the\nunspoken utilitarian principles, where the systems are ranked based on their\nmean average score over task-specific metrics. Such aggregation procedure has\nbeen viewed as a sub-optimal evaluation protocol, which may have created the\nillusion of progress. This paper proposes Vote'n'Rank, a framework for ranking\nsystems in multi-task benchmarks under the principles of the social choice\ntheory. We demonstrate that our approach can be efficiently utilised to draw\nnew insights on benchmarking in several ML sub-fields and identify the\nbest-performing systems in research and development case studies. The\nVote'n'Rank's procedures are more robust than the mean average while being able\nto handle missing performance scores and determine conditions under which the\nsystem becomes the winner.",
    "descriptor": "",
    "authors": [
      "Mark Rofin",
      "Vladislav Mikhailov",
      "Mikhail Florinskiy",
      "Andrey Kravchenko",
      "Elena Tutubalina",
      "Tatiana Shavrina",
      "Daniel Karabekyan",
      "Ekaterina Artemova"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05769"
  },
  {
    "id": "arXiv:2210.05770",
    "title": "Deep Active Ensemble Sampling For Image Classification",
    "abstract": "Conventional active learning (AL) frameworks aim to reduce the cost of data\nannotation by actively requesting the labeling for the most informative data\npoints. However, introducing AL to data hungry deep learning algorithms has\nbeen a challenge. Some proposed approaches include uncertainty-based\ntechniques, geometric methods, implicit combination of uncertainty-based and\ngeometric approaches, and more recently, frameworks based on semi/self\nsupervised techniques. In this paper, we address two specific problems in this\narea. The first is the need for efficient exploitation/exploration trade-off in\nsample selection in AL. For this, we present an innovative integration of\nrecent progress in both uncertainty-based and geometric frameworks to enable an\nefficient exploration/exploitation trade-off in sample selection strategy. To\nthis end, we build on a computationally efficient approximate of Thompson\nsampling with key changes as a posterior estimator for uncertainty\nrepresentation. Our framework provides two advantages: (1) accurate posterior\nestimation, and (2) tune-able trade-off between computational overhead and\nhigher accuracy. The second problem is the need for improved training protocols\nin deep AL. For this, we use ideas from semi/self supervised learning to\npropose a general approach that is independent of the specific AL technique\nbeing used. Taken these together, our framework shows a significant improvement\nover the state-of-the-art, with results that are comparable to the performance\nof supervised-learning under the same setting. We show empirical results of our\nframework, and comparative performance with the state-of-the-art on four\ndatasets, namely, MNIST, CIFAR10, CIFAR100 and ImageNet to establish a new\nbaseline in two different settings.",
    "descriptor": "\nComments: ACCV 2022\n",
    "authors": [
      "Salman Mohamadi",
      "Gianfranco Doretto",
      "Donald A. Adjeroh"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05770"
  },
  {
    "id": "arXiv:2210.05772",
    "title": "Applying FrameNet to Chinese(Poetry)",
    "abstract": "FrameNet( Fillmore and Baker [2009] ) is well-known for its wide use for\nknowledge representation in the form of inheritance-based ontologies and\nlexica( Trott et al. [2020] ). Although FrameNet is usually applied to\nlanguages like English, Spanish and Italian, there are still plenty of FrameNet\ndata sets available for other languages like Chinese, which differs\nsignificantly from those languages based on Latin alphabets. In this paper, the\ntranslation from ancient Chinese Poetry to modern Chinese will be first\nconducted to further apply the Chinese FrameNet(CFN, provided by Shanxi\nUniversity). Afterwards, the translation from modern Chinese will be conducted\nas well for the comparison between the applications of CFN and English\nFrameNet. Finally, the overall comparison will be draw between CFN to modern\nChinese and English FrameNet.",
    "descriptor": "",
    "authors": [
      "Zirong Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.05772"
  },
  {
    "id": "arXiv:2210.05773",
    "title": "Bil-DOS: A Bi-lingual Dialogue Ordering System (for Subway)",
    "abstract": "Due to the unfamiliarity to particular words(or proper nouns) for\ningredients, non-native English speakers can be extremely confused about the\nordering process in restaurants like Subway. Thus, We developed a dialogue\nsystem, which supports Chinese(Mandarin)1 and English2 at the same time. In\nother words, users can switch arbitrarily between Chinese(Mandarin) and English\nas the conversation is being conducted. This system is specifically designed\nfor Subway ordering3. In BilDOS, we designed a Discriminator module to tell the\nlanguage is being used in inputted user utterance, a Translator module to\ntranslate used language into English if it is not English, and a Dialogue\nManager module to detect the intention within inputted user utterances, handle\noutlier inputs by throwing clarification requests, map detected Intention and\ndetailed Keyword4 into a particular intention class, locate the current\nordering process, continue to give queries to finish the order, conclude the\norder details once the order is completed, activate the evaluation process when\nthe conversation is done.",
    "descriptor": "",
    "authors": [
      "Zirong Chen",
      "Haotian Xue"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05773"
  },
  {
    "id": "arXiv:2210.05775",
    "title": "C-Mixup: Improving Generalization in Regression",
    "abstract": "Improving the generalization of deep networks is an important open challenge,\nparticularly in domains without plentiful data. The mixup algorithm improves\ngeneralization by linearly interpolating a pair of examples and their\ncorresponding labels. These interpolated examples augment the original training\nset. Mixup has shown promising results in various classification tasks, but\nsystematic analysis of mixup in regression remains underexplored. Using mixup\ndirectly on regression labels can result in arbitrarily incorrect labels. In\nthis paper, we propose a simple yet powerful algorithm, C-Mixup, to improve\ngeneralization on regression tasks. In contrast with vanilla mixup, which picks\ntraining examples for mixing with uniform probability, C-Mixup adjusts the\nsampling probability based on the similarity of the labels. Our theoretical\nanalysis confirms that C-Mixup with label similarity obtains a smaller mean\nsquare error in supervised regression and meta-regression than vanilla mixup\nand using feature similarity. Another benefit of C-Mixup is that it can improve\nout-of-distribution robustness, where the test distribution is different from\nthe training distribution. By selectively interpolating examples with similar\nlabels, it mitigates the effects of domain-associated information and yields\ndomain-invariant representations. We evaluate C-Mixup on eleven datasets,\nranging from tabular to video data. Compared to the best prior approach,\nC-Mixup achieves 6.56%, 4.76%, 5.82% improvements in in-distribution\ngeneralization, task generalization, and out-of-distribution robustness,\nrespectively. Code is released at https://github.com/huaxiuyao/C-Mixup.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Huaxiu Yao",
      "Yiping Wang",
      "Linjun Zhang",
      "James Zou",
      "Chelsea Finn"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05775"
  },
  {
    "id": "arXiv:2210.05780",
    "title": "Cross-Lingual Speaker Identification Using Distant Supervision",
    "abstract": "Speaker identification, determining which character said each utterance in\nliterary text, benefits many downstream tasks. Most existing approaches use\nexpert-defined rules or rule-based features to directly approach this task, but\nthese approaches come with significant drawbacks, such as lack of contextual\nreasoning and poor cross-lingual generalization. In this work, we propose a\nspeaker identification framework that addresses these issues. We first extract\nlarge-scale distant supervision signals in English via general-purpose tools\nand heuristics, and then apply these weakly-labeled instances with a focus on\nencouraging contextual reasoning to train a cross-lingual language model. We\nshow that the resulting model outperforms previous state-of-the-art methods on\ntwo English speaker identification benchmarks by up to 9% in accuracy and 5%\nwith only distant supervision, as well as two Chinese speaker identification\ndatasets by up to 4.7%.",
    "descriptor": "",
    "authors": [
      "Ben Zhou",
      "Dian Yu",
      "Dong Yu",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05780"
  },
  {
    "id": "arXiv:2210.05781",
    "title": "Transforming RDF-star to Property Graphs: A Preliminary Analysis of  Transformation Approaches -- extended version",
    "abstract": "RDF and property graph models have many similarities, such as using basic\ngraph concepts like nodes and edges. However, such models differ in their\nmodeling approach, expressivity, serialization, and the nature of applications.\nRDF is the de-facto standard model for knowledge graphs on the Semantic Web and\nsupported by a rich ecosystem for inference and processing. The property graph\nmodel, in contrast, provides advantages in scalable graph analytical tasks,\nsuch as graph matching, path analysis, and graph traversal. RDF-star extends\nRDF and allows capturing metadata as a first-class citizen. To tap on the\nadvantages of alternative models, the literature proposes different ways of\ntransforming knowledge graphs between property graphs and RDF. However, most of\nthese approaches cannot provide complete transformations for RDF-star graphs.\nHence, this paper provides a step towards transforming RDF-star graphs into\nproperty graphs. In particular, we identify different cases to evaluate\ntransformation approaches from RDF-star to property graphs. Specifically, we\ncategorize two classes of transformation approaches and analyze them based on\nthe test cases. The obtained insights will form the foundation for building\ncomplete transformation approaches in the future.",
    "descriptor": "",
    "authors": [
      "Ghadeer Abuoda",
      "Daniele Dell'Aglio",
      "Arthur Keen",
      "Katja Hose"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.05781"
  },
  {
    "id": "arXiv:2210.05782",
    "title": "Gradient-Guided Importance Sampling for Learning Binary Energy-Based  Models",
    "abstract": "Learning energy-based models (EBMs) is known to be difficult especially on\ndiscrete data where gradient-based learning strategies cannot be applied\ndirectly. Although ratio matching is a sound method to learn discrete EBMs, it\nsuffers from expensive computation and excessive memory requirement, thereby\nresulting in difficulties for learning EBMs on high-dimensional data. Motivated\nfrom these limitations, in this study, we propose ratio matching with\ngradient-guided importance sampling (RMwGGIS). Particularly, we use the\ngradient of the energy function w.r.t. the discrete data space to approximately\nconstruct the provably optimal proposal distribution, which is subsequently\nused by importance sampling to efficiently estimate the original ratio matching\nobjective. We perform experiments on density modeling over synthetic discrete\ndata, graph generation, and training Ising models to evaluate our proposed\nmethod. The experimental results demonstrate that our method can significantly\nalleviate the limitations of ratio matching, perform more effectively in\npractice, and scale to high-dimensional problems. Our implementation is\navailable at {https://github.com/divelab/RMwGGIS.",
    "descriptor": "",
    "authors": [
      "Meng Liu",
      "Haoran Liu",
      "Shuiwang Ji"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05782"
  },
  {
    "id": "arXiv:2210.05783",
    "title": "Towards Discriminative and Transferable One-Stage Few-Shot Object  Detectors",
    "abstract": "Recent object detection models require large amounts of annotated data for\ntraining a new classes of objects. Few-shot object detection (FSOD) aims to\naddress this problem by learning novel classes given only a few samples. While\ncompetitive results have been achieved using two-stage FSOD detectors,\ntypically one-stage FSODs underperform compared to them. We make the\nobservation that the large gap in performance between two-stage and one-stage\nFSODs are mainly due to their weak discriminability, which is explained by a\nsmall post-fusion receptive field and a small number of foreground samples in\nthe loss function. To address these limitations, we propose the Few-shot\nRetinaNet (FSRN) that consists of: a multi-way support training strategy to\naugment the number of foreground samples for dense meta-detectors, an early\nmulti-level feature fusion providing a wide receptive field that covers the\nwhole anchor area and two augmentation techniques on query and source images to\nenhance transferability. Extensive experiments show that the proposed approach\naddresses the limitations and boosts both discriminability and transferability.\nFSRN is almost two times faster than two-stage FSODs while remaining\ncompetitive in accuracy, and it outperforms the state-of-the-art of one-stage\nmeta-detectors and also some two-stage FSODs on the MS-COCO and PASCAL VOC\nbenchmarks.",
    "descriptor": "",
    "authors": [
      "Karim Guirguis",
      "Mohamed Abdelsamad",
      "George Eskandar",
      "Ahmed Hendawy",
      "Matthias Kayser",
      "Bin Yang",
      "Juergen Beyerer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05783"
  },
  {
    "id": "arXiv:2210.05784",
    "title": "REMS: Middleware for Robotics Education and Development",
    "abstract": "This paper introduces REMS, a robotics middleware and control framework that\nis designed to introduce the Zen of Python to robotics and to improve robotics\neducation and development flow. Although existing middleware can serve hardware\nabstraction and modularity, setting up environments and learning\nmiddleware-specific syntax and procedures are less viable in education. They\ncan curb opportunities to understand robotics concepts, theories, and\nalgorithms. Robotics is a field of integration; students and developers from\nvarious backgrounds will be involved in programming. Establishing Pythonic and\nobject-oriented robotic framework in a natural way can enhance modular and\nabstracted programming for better readability, reusability, and simplicity, but\nalso supports useful and practical skills generally in coding. REMS is to be a\nvaluable robot educational medium not just as a tool and to be a platform from\none robot to multi-agent across hardware, simulation, and analytical model\nimplementations.",
    "descriptor": "\nComments: Submission to ICRA2023\n",
    "authors": [
      "Yusuke Tanaka",
      "Ankur Mehta"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Multiagent Systems (cs.MA)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05784"
  },
  {
    "id": "arXiv:2210.05785",
    "title": "Scaling Up Deliberation for Multilingual ASR",
    "abstract": "Multilingual end-to-end automatic speech recognition models are attractive\ndue to its simplicity in training and deployment. Recent work on large-scale\ntraining of such models has shown promising results compared to monolingual\nmodels. However, the work often focuses on multilingual models themselves in a\nsingle-pass setup. In this work, we investigate second-pass deliberation for\nmultilingual speech recognition. Our proposed deliberation is multilingual,\ni.e., the text encoder encodes hypothesis text from multiple languages, and the\ndecoder attends to multilingual text and audio. We investigate scaling the\ndeliberation text encoder and decoder, and compare scaling the deliberation\ndecoder and the first-pass cascaded encoder. We show that deliberation improves\nthe average WER on 9 languages by 4% relative compared to the single-pass\nmodel. By increasing the size of the deliberation up to 1B parameters, the\naverage WER improvement increases to 9%, with up to 14% for certain languages.\nOur deliberation rescorer is based on transformer layers and can be\nparallelized during rescoring.",
    "descriptor": "",
    "authors": [
      "Ke Hu",
      "Bo Li",
      "Tara N. Sainath"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05785"
  },
  {
    "id": "arXiv:2210.05788",
    "title": "A Note on Reachability and Distance Oracles for Transmission Graphs",
    "abstract": "Let $P$ be a set of $n$ points in the plane, where each point $p\\in P$ has a\ntransmission radius $r(p)>0$. The transmission graph defined by $P$ and the\ngiven radii, denoted by $\\mathcal{G}_{\\mathrm{tr}}(P)$, is the directed graph\nwhose nodes are the points in $P$ and that contains the arcs $(p,q)$ such that\n$|pq|\\leq r(p)$.\nAn and Oh [Algorithmica 2022] presented a reachability oracle for\ntransmission graphs. Their oracle uses $O(n^{5/3})$ storage and, given two\nquery points $s,t\\in P$, can decide in $O(n^{2/3})$ time if there is a path\nfrom $s$ to $t$ in $\\mathcal{G}_{\\mathrm{tr}}(P)$. We show that the\nclique-based separators introduced by De Berg \\emph{et al.} [SICOMP 2020] can\nbe used to improve the storage of the oracle to $O(n\\sqrt{n})$ and the query\ntime to $O(\\sqrt{n})$. Our oracle can be extended to approximate distance\nqueries: we can construct, for a given parameter $\\varepsilon>0$, an oracle\nthat uses $O((n/\\varepsilon)\\sqrt{n}\\log n)$ storage and that can report in\n$O((\\sqrt{n}/\\varepsilon)\\log n)$ time a value $d_{\\mathrm{hop}}^*(s,t)$\nsatisfying $d_{\\mathrm{hop}}(s,t) \\leq d_{\\mathrm{hop}}^*(s,t) <\n(1+\\varepsilon)\\cdot d_{\\mathrm{hop}}(s,t) + 1$, where $d_{\\mathrm{hop}}(s,t)$\nis the hop-distance from $s$ to $t$. We also show how to extend the oracle to\nso-called continuous queries, where the target point $t$ can be any point in\nthe plane.\nTo obtain an efficient preprocessing algorithm, we show that a clique-based\nseparator of a set~$F$ of convex fat objects in $\\Bbb{R}^d$ can be constructed\nin $O(n\\log n)$ time.",
    "descriptor": "",
    "authors": [
      "Mark de Berg"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.05788"
  },
  {
    "id": "arXiv:2210.05789",
    "title": "Trading Off Resource Budgets for Improved Regret Bounds",
    "abstract": "In this work we consider a variant of adversarial online learning where in\neach round one picks $B$ out of $N$ arms and incurs cost equal to the\n$\\textit{minimum}$ of the costs of each arm chosen. We propose an algorithm\ncalled Follow the Perturbed Multiple Leaders (FPML) for this problem, which we\nshow (by adapting the techniques of Kalai and Vempala [2005]) achieves expected\nregret $\\mathcal{O}(T^{\\frac{1}{B+1}}\\ln(N)^{\\frac{B}{B+1}})$ over time horizon\n$T$ relative to the $\\textit{single}$ best arm in hindsight. This introduces a\ntrade-off between the budget $B$ and the single-best-arm regret, and we proceed\nto investigate several applications of this trade-off. First, we observe that\nalgorithms which use standard regret minimizers as subroutines can sometimes be\nadapted by replacing these subroutines with FPML, and we use this to generalize\nexisting algorithms for Online Submodular Function Maximization [Streeter and\nGolovin, 2008] in both the full feedback and semi-bandit feedback settings.\nNext, we empirically evaluate our new algorithms on an online black-box\nhyperparameter optimization problem. Finally, we show how FPML can lead to new\nalgorithms for Linear Programming which require stronger oracles at the benefit\nof fewer oracle calls.",
    "descriptor": "\nComments: 27 pages, 2 figures, accepted to NeurIPS 2022\n",
    "authors": [
      "Damon Falck",
      "Thomas Orton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05789"
  },
  {
    "id": "arXiv:2210.05790",
    "title": "Transfer Learning with Joint Fine-Tuning for Multimodal Sentiment  Analysis",
    "abstract": "Most existing methods focus on sentiment analysis of textual data. However,\nrecently there has been a massive use of images and videos on social platforms,\nmotivating sentiment analysis from other modalities. Current studies show that\nexploring other modalities (e.g., images) increases sentiment analysis\nperformance. State-of-the-art multimodal models, such as CLIP and VisualBERT,\nare pre-trained on datasets with the text paired with images. Although the\nresults obtained by these models are promising, pre-training and sentiment\nanalysis fine-tuning tasks of these models are computationally expensive. This\npaper introduces a transfer learning approach using joint fine-tuning for\nsentiment analysis. Our proposal achieved competitive results using a more\nstraightforward alternative fine-tuning strategy that leverages different\npre-trained unimodal models and efficiently combines them in a multimodal\nspace. Moreover, our proposal allows flexibility when incorporating any\npre-trained model for texts and images during the joint fine-tuning stage,\nbeing especially interesting for sentiment classification in low-resource\nscenarios.",
    "descriptor": "\nComments: Talk: this https URL\n",
    "authors": [
      "Guilherme Louren\u00e7o de Toledo",
      "Ricardo Marcondes Marcacini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05790"
  },
  {
    "id": "arXiv:2210.05791",
    "title": "Sociotechnical Harms: Scoping a Taxonomy for Harm Reduction",
    "abstract": "Understanding the landscape of potential harms from algorithmic systems\nenables practitioners to better anticipate consequences of the systems they\nbuild. It also supports the prospect of incorporating controls to help minimize\nharms that emerge from the interplay of technologies and social and cultural\ndynamics. A growing body of scholarship has identified a wide range of harms\nacross different algorithmic technologies. However, computing research and\npractitioners lack a high level and synthesized overview of harms from\nalgorithmic systems arising at the micro-, meso-, and macro-levels of society.\nWe present an applied taxonomy of sociotechnical harms to support more\nsystematic surfacing of potential harms in algorithmic systems. Based on a\nscoping review of computing research ($n=172$), we identified five major themes\nrelated to sociotechnical harms - representational, allocative,\nquality-of-service, interpersonal harms, and social system/societal harms - and\nsub-themes. We describe these categories and conclude with a discussion of\nchallenges and opportunities for future research.",
    "descriptor": "",
    "authors": [
      "Renee Shelby",
      "Shalaleh Rismani",
      "Kathryn Henne",
      "AJung Moon",
      "Negar Rostamzadeh",
      "Paul Nicholas",
      "N'Mah Yilla",
      "Jess Gallegos",
      "Andrew Smart",
      "Emilio Garcia",
      "Gurleen Virk"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "General Literature (cs.GL)"
    ],
    "url": "https://arxiv.org/abs/2210.05791"
  },
  {
    "id": "arXiv:2210.05793",
    "title": "Comparison of Soft and Hard Target RNN-T Distillation for Large-scale  ASR",
    "abstract": "Knowledge distillation is an effective machine learning technique to transfer\nknowledge from a teacher model to a smaller student model, especially with\nunlabeled data. In this paper, we focus on knowledge distillation for the RNN-T\nmodel, which is widely used in state-of-the-art (SoTA) automatic speech\nrecognition (ASR). Specifically, we compared using soft and hard target\ndistillation to train large-scaleRNN-T models on the LibriSpeech/LibriLight\npublic dataset (60k hours) and our in-house data (600k hours). We found that\nhard tar-gets are more effective when the teacher and student have different\narchitecture, such as large teacher and small streaming student. On the other\nhand, soft target distillation works better in self-training scenario like\niterative large teacher training. For a large model with0.6B weights, we\nachieve a new SoTA word error rate (WER) on LibriSpeech (8% relative\nimprovement on dev-other) using Noisy Student Training with soft target\ndistillation. It also allows our production teacher to adapt new data domain\ncontinuously.",
    "descriptor": "\nComments: 8 pages, 1 figure\n",
    "authors": [
      "Dongseong Hwang",
      "Khe Chai Sim",
      "Yu Zhang",
      "Trevor Strohman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05793"
  },
  {
    "id": "arXiv:2210.05794",
    "title": "Robustify Transformers with Robust Kernel Density Estimation",
    "abstract": "Recent advances in Transformer architecture have empowered its empirical\nsuccess in various tasks across different domains. However, existing works\nmainly focus on improving the standard accuracy and computational cost, without\nconsidering the robustness of contaminated samples. Existing work has shown\nthat the self-attention mechanism, which is the center of the Transformer\narchitecture, can be viewed as a non-parametric estimator based on the\nwell-known kernel density estimation (KDE). This motivates us to leverage the\nrobust kernel density estimation (RKDE) in the self-attention mechanism, to\nalleviate the issue of the contamination of data by down-weighting the weight\nof bad samples in the estimation process. The modified self-attention mechanism\ncan be incorporated into different Transformer variants. Empirical results on\nlanguage modeling and image classification tasks demonstrate the effectiveness\nof this approach.",
    "descriptor": "\nComments: 17 pages, 2 figures, 2 tables\n",
    "authors": [
      "Xing Han",
      "Tongzheng Ren",
      "Tan Minh Nguyen",
      "Khai Nguyen",
      "Joydeep Ghosh",
      "Nhat Ho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05794"
  },
  {
    "id": "arXiv:2210.05795",
    "title": "Online Team Formation under Different Synergies",
    "abstract": "Team formation is ubiquitous in many sectors: education, labor markets,\nsports, etc. A team's success depends on its members' latent types, which are\nnot directly observable but can be (partially) inferred from past performances.\nFrom the viewpoint of a principal trying to select teams, this leads to a\nnatural exploration-exploitation trade-off: retain successful teams that are\ndiscovered early, or reassign agents to learn more about their types? We study\na natural model for online team formation, where a principal repeatedly\npartitions a group of agents into teams. Agents have binary latent types, each\nteam comprises two members, and a team's performance is a symmetric function of\nits members' types. Over multiple rounds, the principal selects matchings over\nagents and incurs regret equal to the deficit in the number of successful teams\nversus the optimal matching for the given function. Our work provides a\ncomplete characterization of the regret landscape for all symmetric functions\nof two binary inputs. In particular, we develop team-selection policies that,\ndespite being agnostic of model parameters, achieve optimal or near-optimal\nregret against an adaptive adversary.",
    "descriptor": "\nComments: 37 pages, extended version of WINE 2022 accepted paper including all proofs\n",
    "authors": [
      "Matthew Eichhorn",
      "Siddhartha Banerjee",
      "David Kempe"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.05795"
  },
  {
    "id": "arXiv:2210.05801",
    "title": "Linkless Link Prediction via Relational Distillation",
    "abstract": "Graph Neural Networks (GNNs) have been widely used on graph data and have\nshown exceptional performance in the task of link prediction. Despite their\neffectiveness, GNNs often suffer from high latency due to non-trivial\nneighborhood data dependency in practical deployments. To address this issue,\nresearchers have proposed methods based on knowledge distillation (KD) to\ntransfer the knowledge from teacher GNNs to student MLPs, which are known to be\nefficient even with industrial scale data, and have shown promising results on\nnode classification. Nonetheless, using KD to accelerate link prediction is\nstill unexplored. In this work, we start with exploring two direct analogs of\ntraditional KD for link prediction, i.e., predicted logit-based matching and\nnode representation-based matching. Upon observing direct KD analogs do not\nperform well for link prediction, we propose a relational KD framework,\nLinkless Link Prediction (LLP). Unlike simple KD methods that match independent\nlink logits or node representations, LLP distills relational knowledge that is\ncentered around each (anchor) node to the student MLP. Specifically, we propose\ntwo matching strategies that complement each other: rank-based matching and\ndistribution-based matching. Extensive experiments demonstrate that LLP boosts\nthe link prediction performance of MLPs with significant margins, and even\noutperforms the teacher GNNs on 6 out of 9 benchmarks. LLP also achieves a\n776.37x speedup in link prediction inference compared to GNNs on the large\nscale OGB-Citation2 dataset.",
    "descriptor": "",
    "authors": [
      "Zhichun Guo",
      "William Shiao",
      "Shichang Zhang",
      "Yozen Liu",
      "Nitesh Chawla",
      "Neil Shah",
      "Tong Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05801"
  },
  {
    "id": "arXiv:2210.05805",
    "title": "Exploration via Elliptical Episodic Bonuses",
    "abstract": "In recent years, a number of reinforcement learning (RL) methods have been\nproposed to explore complex environments which differ across episodes. In this\nwork, we show that the effectiveness of these methods critically relies on a\ncount-based episodic term in their exploration bonus. As a result, despite\ntheir success in relatively simple, noise-free settings, these methods fall\nshort in more realistic scenarios where the state space is vast and prone to\nnoise. To address this limitation, we introduce Exploration via Elliptical\nEpisodic Bonuses (E3B), a new method which extends count-based episodic bonuses\nto continuous state spaces and encourages an agent to explore states that are\ndiverse under a learned embedding within each episode. The embedding is learned\nusing an inverse dynamics model in order to capture controllable aspects of the\nenvironment. Our method sets a new state-of-the-art across 16 challenging tasks\nfrom the MiniHack suite, without requiring task-specific inductive biases. E3B\nalso matches existing methods on sparse reward, pixel-based VizDoom\nenvironments, and outperforms existing methods in reward-free exploration on\nHabitat, demonstrating that it can scale to high-dimensional pixel-based\nobservations and realistic environments.",
    "descriptor": "",
    "authors": [
      "Mikael Henaff",
      "Roberta Raileanu",
      "Minqi Jiang",
      "Tim Rockt\u00e4schel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05805"
  },
  {
    "id": "arXiv:2210.05806",
    "title": "Little or no equalization is needed in energy-efficient sub-THz mobile  access",
    "abstract": "By trading coverage and hardware complexity for abundance of spectrum,\nsub-THz mobile access networks are expected to operate under highly directive\nand relatively spectrally inefficient transmission regimes, while still\noffering enormous capacity gains over current sub-6GHz alternatives. Building\non this assumption, and supported by extensive indoor directional channel\nmeasurements at 160 GHz, this study advocates the use of very simple modulation\nand equalization techniques for sub-THz mobile access. Specifically, we\ndemonstrate that, under the aforementioned transmission regimes, little or no\nequalization is needed for scoring significant capacity gain targets. In\nparticular, we show that single-carrier or low-number-of-subcarriers\nmodulations are very attractive competitors to the dramatically more complex\nand energy inefficient traditional multi-carrier designs.",
    "descriptor": "",
    "authors": [
      "Lorenzo Miretti",
      "Thomas K\u00fchne",
      "Alper Schultze",
      "Wilhelm Keusgen",
      "Giuseppe Caire",
      "Michael Peter",
      "Slawomir Sta\u0144czak",
      "Taro Eichler"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05806"
  },
  {
    "id": "arXiv:2210.05810",
    "title": "Continuous conditional video synthesis by neural processes",
    "abstract": "We propose a unified model for multiple conditional video synthesis tasks,\nincluding video prediction and video frame interpolation. We show that\nconditional video synthesis can be formulated as a neural process, which maps\ninput spatio-temporal coordinates to target pixel values given context\nspatio-temporal coordinates and pixels values. Specifically, we feed an\nimplicit neural representations of coordinates into a Transformer-based\nnon-autoregressive conditional video synthesis model. Our task-specific models\noutperform previous work for video interpolation on multiple datasets and reach\na competitive performance with the state-of-the-art models for video\nprediction. Importantly, the model is able to interpolate or predict with an\narbitrary high frame rate, i.e., continuous synthesis. Our source code is\navailable at \\url{https://github.com/NPVS/NPVS}.",
    "descriptor": "",
    "authors": [
      "Xi Ye",
      "Guillaume-Alexandre Bilodeau"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05810"
  },
  {
    "id": "arXiv:2210.05811",
    "title": "Deep Counterfactual Estimation with Categorical Background Variables",
    "abstract": "Referred to as the third rung of the causal inference ladder, counterfactual\nqueries typically ask the \"What if ?\" question retrospectively. The standard\napproach to estimate counterfactuals resides in using a structural equation\nmodel that accurately reflects the underlying data generating process. However,\nsuch models are seldom available in practice and one usually wishes to infer\nthem from observational data alone. Unfortunately, the correct structural\nequation model is in general not identifiable from the observed factual\ndistribution. Nevertheless, in this work, we show that under the assumption\nthat the main latent contributors to the treatment responses are categorical,\nthe counterfactuals can be still reliably predicted. Building upon this\nassumption, we introduce CounterFactual Query Prediction (CFQP), a novel method\nto infer counterfactuals from continuous observations when the background\nvariables are categorical. We show that our method significantly outperforms\npreviously available deep-learning-based counterfactual methods, both\ntheoretically and empirically on time series and image data. Our code is\navailable at https://github.com/edebrouwer/cfqp.",
    "descriptor": "\nComments: Proceedings of the NeurIPS 2022 Conference\n",
    "authors": [
      "Edward De Brouwer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.05811"
  },
  {
    "id": "arXiv:2210.05813",
    "title": "Software Supply Chain Attribute Integrity (SCAI)",
    "abstract": "The Software Supply Chain Attribute Integrity, or SCAI (pronounced \"sky\"),\nspecification proposes a data format for capturing functional attribute and\nintegrity information about software artifacts and their supply chain. SCAI\ndata can be associated with executable binaries, statically- or\ndynamically-linked libraries, software packages, container images, software\ntoolchains, and compute environments.\nAs such, SCAI is intended to be implemented as part of an existing software\nsupply chain attestation framework by software development tools or services\n(e.g., builders, CI/CD pipelines, software analysis tools) seeking to capture\nmore granular information about the attributes and behavior of the software\nartifacts they produce. That is, SCAI assumes that implementers will have\nappropriate processes and tooling in place for capturing other types of\nsoftware supply chain metadata, which can be extended to add support for SCAI.",
    "descriptor": "",
    "authors": [
      "Marcela S. Melara"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05813"
  },
  {
    "id": "arXiv:2210.05815",
    "title": "Underspecification in Scene Description-to-Depiction Tasks",
    "abstract": "Questions regarding implicitness, ambiguity and underspecification are\ncrucial for understanding the task validity and ethical concerns of multimodal\nimage+text systems, yet have received little attention to date. This position\npaper maps out a conceptual framework to address this gap, focusing on systems\nwhich generate images depicting scenes from scene descriptions. In doing so, we\naccount for how texts and images convey meaning differently. We outline a set\nof core challenges concerning textual and visual ambiguity, as well as risks\nthat may be amplified by ambiguous and underspecified elements. We propose and\ndiscuss strategies for addressing these challenges, including generating\nvisually ambiguous images, and generating a set of diverse images.",
    "descriptor": "",
    "authors": [
      "Ben Hutchinson",
      "Jason Baldridge",
      "Vinodkumar Prabhakaran"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05815"
  },
  {
    "id": "arXiv:2210.05825",
    "title": "Controllable Radiance Fields for Dynamic Face Synthesis",
    "abstract": "Recent work on 3D-aware image synthesis has achieved compelling results using\nadvances in neural rendering. However, 3D-aware synthesis of face dynamics\nhasn't received much attention. Here, we study how to explicitly control\ngenerative model synthesis of face dynamics exhibiting non-rigid motion (e.g.,\nfacial expression change), while simultaneously ensuring 3D-awareness. For this\nwe propose a Controllable Radiance Field (CoRF): 1) Motion control is achieved\nby embedding motion features within the layered latent motion space of a\nstyle-based generator; 2) To ensure consistency of background, motion features\nand subject-specific attributes such as lighting, texture, shapes, albedo, and\nidentity, a face parsing net, a head regressor and an identity encoder are\nincorporated. On head image/video data we show that CoRFs are 3D-aware while\nenabling editing of identity, viewing directions, and motion.",
    "descriptor": "\nComments: Accepted to 3DV 2022. 13 pages, 15 figures\n",
    "authors": [
      "Peiye Zhuang",
      "Liqian Ma",
      "Oluwasanmi Koyejo",
      "Alexander G. Schwing"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05825"
  },
  {
    "id": "arXiv:2210.05828",
    "title": "AMICO: Amodal Instance Composition",
    "abstract": "Image composition aims to blend multiple objects to form a harmonized image.\nExisting approaches often assume precisely segmented and intact objects. Such\nassumptions, however, are hard to satisfy in unconstrained scenarios. We\npresent Amodal Instance Composition for compositing imperfect -- potentially\nincomplete and/or coarsely segmented -- objects onto a target image. We first\ndevelop object shape prediction and content completion modules to synthesize\nthe amodal contents. We then propose a neural composition model to blend the\nobjects seamlessly. Our primary technical novelty lies in using separate\nforeground/background representations and blending mask prediction to alleviate\nsegmentation errors. Our results show state-of-the-art performance on public\nCOCOA and KINS benchmarks and attain favorable visual results across diverse\nscenes. We demonstrate various image composition applications such as object\ninsertion and de-occlusion.",
    "descriptor": "\nComments: Accepted to BMVC 2021, 20 oages, 12 figures\n",
    "authors": [
      "Peiye Zhuang",
      "Jia-bin Huang",
      "Ayush Saraf",
      "Xuejian Rong",
      "Changil Kim",
      "Denis Demandolx"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05828"
  },
  {
    "id": "arXiv:2210.05831",
    "title": "Social-Group-Agnostic Word Embedding Debiasing via the Stereotype  Content Model",
    "abstract": "Existing word embedding debiasing methods require social-group-specific word\npairs (e.g., \"man\"-\"woman\") for each social attribute (e.g., gender), which\ncannot be used to mitigate bias for other social groups, making these methods\nimpractical or costly to incorporate understudied social groups in debiasing.\nWe propose that the Stereotype Content Model (SCM), a theoretical framework\ndeveloped in social psychology for understanding the content of stereotypes,\nwhich structures stereotype content along two psychological dimensions -\n\"warmth\" and \"competence\" - can help debiasing efforts to become\nsocial-group-agnostic by capturing the underlying connection between bias and\nstereotypes. Using only pairs of terms for warmth (e.g., \"genuine\"-\"fake\") and\ncompetence (e.g.,\"smart\"-\"stupid\"), we perform debiasing with established\nmethods and find that, across gender, race, and age, SCM-based debiasing\nperforms comparably to group-specific debiasing",
    "descriptor": "\nComments: short paper\n",
    "authors": [
      "Ali Omrani",
      "Brendan Kennedy",
      "Mohammad Atari",
      "Morteza Dehghani"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05831"
  },
  {
    "id": "arXiv:2210.05832",
    "title": "SaiT: Sparse Vision Transformers through Adaptive Token Pruning",
    "abstract": "While vision transformers have achieved impressive results, effectively and\nefficiently accelerating these models can further boost performances. In this\nwork, we propose a dense/sparse training framework to obtain a unified model,\nenabling weight sharing across various token densities. Thus one model offers a\nrange of accuracy and throughput tradeoffs for different applications. Besides,\nwe introduce adaptive token pruning to optimize the patch token sparsity based\non the input image. In addition, we investigate knowledge distillation to\nenhance token selection capability in early transformer modules. Sparse\nadaptive image Transformer (SaiT) offers varying levels of model acceleration\nby merely changing the token sparsity on the fly. Specifically, SaiT reduces\nthe computation complexity (FLOPs) by 39% - 43% and increases the throughput by\n67% - 91% with less than 0.5% accuracy loss for various vision transformer\nmodels. Meanwhile, the same model also provides the zero accuracy drop option\nby skipping the sparsification step. SaiT achieves better accuracy and\ncomputation tradeoffs than state-of-the-art transformer and convolutional\nmodels.",
    "descriptor": "",
    "authors": [
      "Ling Li",
      "David Thorsley",
      "Joseph Hassoun"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05832"
  },
  {
    "id": "arXiv:2210.05833",
    "title": "Parameter estimation of the homodyned K distribution based on neural  networks and trainable fractional-order moments",
    "abstract": "Homodyned K (HK) distribution has been widely used to describe the scattering\nphenomena arising in various research fields, such as ultrasound imaging or\noptics. In this work, we propose a machine learning based approach to the\nestimation of the HK distribution parameters. We develop neural networks that\ncan estimate the HK distribution parameters based on the signal-to-noise ratio,\nskewness and kurtosis calculated using fractional-order moments. Compared to\nthe previous approaches, we consider the orders of the moments as trainable\nvariables that can be optimized along with the network weights using the\nback-propagation algorithm. Networks are trained based on samples generated\nfrom the HK distribution. Obtained results demonstrate that the proposed method\ncan be used to accurately estimate the HK distribution parameters.",
    "descriptor": "\nComments: 5 pages, 3 figures\n",
    "authors": [
      "Michal Byra",
      "Ziemowit Klimonda",
      "Piotr Jarosik"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.05833"
  },
  {
    "id": "arXiv:2210.05834",
    "title": "Effectiveness of the Recent Advances in Capsule Networks",
    "abstract": "Convolutional neural networks (CNNs) have revolutionized the field of deep\nneural networks. However, recent research has shown that CNNs fail to\ngeneralize under various conditions and hence the idea of capsules was\nintroduced in 2011, though the real surge of research started from 2017. In\nthis paper, we present an overview of the recent advances in capsule\narchitecture and routing mechanisms. In addition, we find that the relative\nfocus in recent literature is on modifying routing procedure or architecture as\na whole but the study of other finer components, specifically, squash function\nis wanting. Thus, we also present some new insights regarding the effect of\nsquash functions in performance of the capsule networks. Finally, we conclude\nby discussing and proposing possible opportunities in the field of capsule\nnetworks.",
    "descriptor": "",
    "authors": [
      "Nidhin Harilal",
      "Rohan Patil"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05834"
  },
  {
    "id": "arXiv:2210.05835",
    "title": "Synthetic Power Analyses: Empirical Evaluation and Application to  Cognitive Neuroimaging",
    "abstract": "In the experimental sciences, statistical power analyses are often used\nbefore data collection to determine the required sample size. However,\ntraditional power analyses can be costly when data are difficult or expensive\nto collect. We propose synthetic power analyses; a framework for estimating\nstatistical power at various sample sizes, and empirically explore the\nperformance of synthetic power analysis for sample size selection in cognitive\nneuroscience experiments. To this end, brain imaging data is synthesized using\nan implicit generative model conditioned on observed cognitive processes.\nFurther, we propose a simple procedure to modify the statistical tests which\nresult in conservative statistics. Our empirical results suggest that synthetic\npower analysis could be a low-cost alternative to pilot data collection when\nthe proposed experiments share cognitive processes with previously conducted\nexperiments.",
    "descriptor": "\nComments: Accepted to Asilomar 2019\n",
    "authors": [
      "Peiye Zhuang",
      "Bliss Chapman",
      "Ran Li",
      "Oluwasanmi Koyejo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05835"
  },
  {
    "id": "arXiv:2210.05836",
    "title": "CLIP also Understands Text: Prompting CLIP for Phrase Understanding",
    "abstract": "Contrastive Language-Image Pretraining (CLIP) efficiently learns visual\nconcepts by pre-training with natural language supervision. CLIP and its visual\nencoder have been explored on various vision and language tasks and achieve\nstrong zero-shot or transfer learning performance. However, the application of\nits text encoder solely for text understanding has been less explored. In this\npaper, we find that the text encoder of CLIP actually demonstrates strong\nability for phrase understanding, and can even significantly outperform popular\nlanguage models such as BERT with a properly designed prompt. Extensive\nexperiments validate the effectiveness of our method across different datasets\nand domains on entity clustering and entity set expansion tasks.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "An Yan",
      "Jiacheng Li",
      "Wanrong Zhu",
      "Yujie Lu",
      "William Yang Wang",
      "Julian McAuley"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05836"
  },
  {
    "id": "arXiv:2210.05837",
    "title": "A composable machine-learning approach for steady-state simulations on  high-resolution grids",
    "abstract": "In this paper we show that our Machine Learning (ML) approach, CoMLSim\n(Composable Machine Learning Simulator), can simulate PDEs on highly-resolved\ngrids with higher accuracy and generalization to out-of-distribution source\nterms and geometries than traditional ML baselines. Our unique approach\ncombines key principles of traditional PDE solvers with local-learning and\nlow-dimensional manifold techniques to iteratively simulate PDEs on large\ncomputational domains. The proposed approach is validated on more than 5\nsteady-state PDEs across different PDE conditions on highly-resolved grids and\ncomparisons are made with the commercial solver, Ansys Fluent as well as 4\nother state-of-the-art ML methods. The numerical experiments show that our\napproach outperforms ML baselines in terms of 1) accuracy across quantitative\nmetrics and 2) generalization to out-of-distribution conditions as well as\ndomain sizes. Additionally, we provide results for a large number of ablations\nexperiments conducted to highlight components of our approach that strongly\ninfluence the results. We conclude that our local-learning and\niterative-inferencing approach reduces the challenge of generalization that\nmost ML models face.",
    "descriptor": "",
    "authors": [
      "Rishikesh Ranade",
      "Chris Hill",
      "Lalit Ghule",
      "Jay Pathak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05837"
  },
  {
    "id": "arXiv:2210.05839",
    "title": "SEAL : Interactive Tool for Systematic Error Analysis and Labeling",
    "abstract": "With the advent of Transformers, large language models (LLMs) have saturated\nwell-known NLP benchmarks and leaderboards with high aggregate performance.\nHowever, many times these models systematically fail on tail data or rare\ngroups not obvious in aggregate evaluation. Identifying such problematic data\ngroups is even more challenging when there are no explicit labels (e.g.,\nethnicity, gender, etc.) and further compounded for NLP datasets due to the\nlack of visual features to characterize failure modes (e.g., Asian males,\nanimals indoors, waterbirds on land, etc.). This paper introduces an\ninteractive Systematic Error Analysis and Labeling (\\seal) tool that uses a\ntwo-step approach to first identify high error slices of data and then, in the\nsecond step, introduce methods to give human-understandable semantics to those\nunderperforming slices. We explore a variety of methods for coming up with\ncoherent semantics for the error groups using language models for semantic\nlabeling and a text-to-image model for generating visual features. SEAL toolkit\nand demo screencast is available at https://huggingface.co/spaces/nazneen/seal.",
    "descriptor": "\nComments: Accepted at EMNLP 2022 demo track\n",
    "authors": [
      "Nazneen Rajani",
      "Weixin Liang",
      "Lingjiao Chen",
      "Meg Mitchell",
      "James Zou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.05839"
  },
  {
    "id": "arXiv:2210.05840",
    "title": "LiveSeg: Unsupervised Multimodal Temporal Segmentation of Long  Livestream Videos",
    "abstract": "Livestream videos have become a significant part of online learning, where\ndesign, digital marketing, creative painting, and other skills are taught by\nexperienced experts in the sessions, making them valuable materials. However,\nLivestream tutorial videos are usually hours long, recorded, and uploaded to\nthe Internet directly after the live sessions, making it hard for other people\nto catch up quickly. An outline will be a beneficial solution, which requires\nthe video to be temporally segmented according to topics. In this work, we\nintroduced a large Livestream video dataset named MultiLive, and formulated the\ntemporal segmentation of the long Livestream videos (TSLLV) task. We propose\nLiveSeg, an unsupervised Livestream video temporal Segmentation solution, which\ntakes advantage of multimodal features from different domains. Our method\nachieved a $16.8\\%$ F1-score performance improvement compared with the\nstate-of-the-art method.",
    "descriptor": "",
    "authors": [
      "Jielin Qiu",
      "Franck Dernoncourt",
      "Trung Bui",
      "Zhaowen Wang",
      "Ding Zhao",
      "Hailin Jin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05840"
  },
  {
    "id": "arXiv:2210.05844",
    "title": "SegViT: Semantic Segmentation with Plain Vision Transformers",
    "abstract": "We explore the capability of plain Vision Transformers (ViTs) for semantic\nsegmentation and propose the SegVit. Previous ViT-based segmentation networks\nusually learn a pixel-level representation from the output of the ViT.\nDifferently, we make use of the fundamental component -- attention mechanism,\nto generate masks for semantic segmentation. Specifically, we propose the\nAttention-to-Mask (ATM) module, in which the similarity maps between a set of\nlearnable class tokens and the spatial feature maps are transferred to the\nsegmentation masks. Experiments show that our proposed SegVit using the ATM\nmodule outperforms its counterparts using the plain ViT backbone on the ADE20K\ndataset and achieves new state-of-the-art performance on COCO-Stuff-10K and\nPASCAL-Context datasets. Furthermore, to reduce the computational cost of the\nViT backbone, we propose query-based down-sampling (QD) and query-based\nup-sampling (QU) to build a Shrunk structure. With the proposed Shrunk\nstructure, the model can save up to $40\\%$ computations while maintaining\ncompetitive performance.",
    "descriptor": "\nComments: 9 Pages, NeurIPS 2022\n",
    "authors": [
      "Bowen Zhang",
      "Zhi Tian",
      "Quan Tang",
      "Xiangxiang Chu",
      "Xiaolin Wei",
      "Chunhua Shen",
      "Yifan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05844"
  },
  {
    "id": "arXiv:2210.05845",
    "title": "Contrastive introspection (ConSpec) to rapidly identify invariant steps  for success",
    "abstract": "Reinforcement learning (RL) algorithms have achieved notable success in\nrecent years, but still struggle with fundamental issues in long-term credit\nassignment. It remains difficult to learn in situations where success is\ncontingent upon multiple critical steps that are distant in time from each\nother and from a sparse reward; as is often the case in real life. Moreover,\nhow RL algorithms assign credit in these difficult situations is typically not\ncoded in a way that can rapidly generalize to new situations. Here, we present\nan approach using offline contrastive learning, which we call contrastive\nintrospection (ConSpec), that can be added to any existing RL algorithm and\naddresses both issues. In ConSpec, a contrastive loss is used during offline\nreplay to identify invariances among successful episodes. This takes advantage\nof the fact that it is easier to retrospectively identify the small set of\nsteps that success is contingent upon than it is to prospectively predict\nreward at every step taken in the environment. ConSpec stores this knowledge in\na collection of prototypes summarizing the intermediate states required for\nsuccess. During training, arrival at any state that matches these prototypes\ngenerates an intrinsic reward that is added to any external rewards. As well,\nthe reward shaping provided by ConSpec can be made to preserve the optimal\npolicy of the underlying RL agent. The prototypes in ConSpec provide two key\nbenefits for credit assignment: (1) They enable rapid identification of all the\ncritical states. (2) They do so in a readily interpretable manner, enabling out\nof distribution generalization when sensory features are altered. In summary,\nConSpec is a modular system that can be added to any existing RL algorithm to\nimprove its long-term credit assignment.",
    "descriptor": "",
    "authors": [
      "Chen Sun",
      "Wannan Yang",
      "Benjamin Alsbury-Nealy",
      "Yoshua Bengio",
      "Blake Richards"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05845"
  },
  {
    "id": "arXiv:2210.05846",
    "title": "FasterRisk: Fast and Accurate Interpretable Risk Scores",
    "abstract": "Over the last century, risk scores have been the most popular form of\npredictive model used in healthcare and criminal justice. Risk scores are\nsparse linear models with integer coefficients; often these models can be\nmemorized or placed on an index card. Typically, risk scores have been created\neither without data or by rounding logistic regression coefficients, but these\nmethods do not reliably produce high-quality risk scores. Recent work used\nmathematical programming, which is computationally slow. We introduce an\napproach for efficiently producing a collection of high-quality risk scores\nlearned from data. Specifically, our approach produces a pool of almost-optimal\nsparse continuous solutions, each with a different support set, using a\nbeam-search algorithm. Each of these continuous solutions is transformed into a\nseparate risk score through a \"star ray\" search, where a range of multipliers\nare considered before rounding the coefficients sequentially to maintain low\nlogistic loss. Our algorithm returns all of these high-quality risk scores for\nthe user to consider. This method completes within minutes and can be valuable\nin a broad variety of applications.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Jiachang Liu",
      "Chudi Zhong",
      "Boxuan Li",
      "Margo Seltzer",
      "Cynthia Rudin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05846"
  },
  {
    "id": "arXiv:2210.05847",
    "title": "Experimental Evaluation of Baselines for Forecasting Social Media  Timeseries",
    "abstract": "Forecasting social media activity can be of practical use in many scenarios,\nfrom understanding trends, such as which topics are likely to engage more users\nin the coming week, to identifying unusual behavior, such as coordinated\ninformation operations or PumpNDump efforts. To evaluate a new approach to\nforecasting, it is important to have baselines against which to assess\nperformance gains. We experimentally evaluate the performance of four baselines\nfor forecasting activity in several social media datasets that record\ndiscussions related to three different geo-political contexts synchronously\ntaking place on two different platforms, Twitter and YouTube. Experiments are\ndone over hourly time periods. Our evaluation identifies the baselines which\nare most accurate for particular metrics and thus provide guidance for future\nwork in social media modeling.",
    "descriptor": "",
    "authors": [
      "Kin Wai Ng",
      "Frederick Mubang",
      "Lawrence O. Hall",
      "John Skvoretz",
      "Adriana Iamnitchi"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.05847"
  },
  {
    "id": "arXiv:2210.05852",
    "title": "Quantifying hierarchy in scientific teams",
    "abstract": "This paper provides a detailed description of the data collection and machine\nlearning model used in our recent PNAS paper \"Flat Teams Drive Scientific\nInnovation\" Xu et al. [2022a]. Here, we discuss how the features of scientific\npublication can be used to estimate the implicit hierarchy in the corresponding\nauthor teams. Besides, we also describe the method of evaluating the impact of\nteam hierarchy on scientific outputs. More details will be updated in this\narticle continuously. Raw data and Readme document can be accessed in this\nGitHub repository Xu et al. [2022b].",
    "descriptor": "",
    "authors": [
      "Fengli Xu",
      "Lingfei Wu",
      "James A. Evans"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.05852"
  },
  {
    "id": "arXiv:2210.05857",
    "title": "FlowDrone: Wind Estimation and Gust Rejection on UAVs Using  Fast-Response Hot-Wire Flow Sensors",
    "abstract": "Unmanned aerial vehicles (UAVs) are finding use in applications that place\nincreasing emphasis on robustness to external disturbances including extreme\nwind. However, traditional multirotor UAV platforms do not directly sense wind;\nconventional flow sensors are too slow, insensitive, or bulky for widespread\nintegration on UAVs. Instead, drones typically observe the effects of wind\nindirectly through accumulated errors in position or trajectory tracking. In\nthis work, we integrate a novel flow sensor based on micro-electro-mechanical\nsystems (MEMS) hot-wire technology developed in our prior work onto a\nmultirotor UAV for wind estimation. These sensors are omnidirectional,\nlightweight, fast, and accurate. In order to achieve superior tracking\nperformance in windy conditions, we train a `wind-aware' residual-based\ncontroller via reinforcement learning using simulated wind gusts and their\naerodynamic effects on the drone. In extensive hardware experiments, we\ndemonstrate the wind-aware controller outperforming two strong `wind-unaware'\nbaseline controllers in challenging windy conditions.",
    "descriptor": "\nComments: submitted to ICRA 2023\n",
    "authors": [
      "Nathaniel Simon",
      "Allen Z. Ren",
      "Alexander Piqu\u00e9",
      "David Snyder",
      "Daphne Barretto",
      "Marcus Hultmark",
      "Anirudha Majumdar"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05857"
  },
  {
    "id": "arXiv:2210.05858",
    "title": "On abstract homogeneous interval-valued functions",
    "abstract": "In this paper we develop the idea of abstract homogeneity in the context of\ninterval-valued (IV) functions endowed with admissible orders. We investigate\nsome of its properties and the notion of self-homogeneity. We show how this\nframework behaves with respect to some fuzzy connectives. We show that some\nproperties of the usual notions of $\\min$ and $\\max$ are not more available\nwhen we deal with admissible orders.\nKeywords: Interval-Valued Functions; Abstract Homogeneity; Monotonicity.",
    "descriptor": "\nComments: 18 pages\n",
    "authors": [
      "Ana Shirley Monteiro",
      "Regivan Santiago",
      "Radko Mesiar",
      "Marisol Gomez",
      "Martin Papco",
      "Mikel Ferrero-Jaurrieta",
      "Humberto Bustince"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05858"
  },
  {
    "id": "arXiv:2210.05861",
    "title": "SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric  Models",
    "abstract": "Understanding dynamics from visual observations is a challenging problem that\nrequires disentangling individual objects from the scene and learning their\ninteractions. While recent object-centric models can successfully decompose a\nscene into objects, modeling their dynamics effectively still remains a\nchallenge. We address this problem by introducing SlotFormer -- a\nTransformer-based autoregressive model operating on learned object-centric\nrepresentations. Given a video clip, our approach reasons over object features\nto model spatio-temporal relationships and predicts accurate future object\nstates. In this paper, we successfully apply SlotFormer to perform video\nprediction on datasets with complex object interactions. Moreover, the\nunsupervised SlotFormer's dynamics model can be used to improve the performance\non supervised downstream tasks, such as Visual Question Answering (VQA), and\ngoal-conditioned planning. Compared to past works on dynamics modeling, our\nmethod achieves significantly better long-term synthesis of object dynamics,\nwhile retaining high quality visual generation. Besides, SlotFormer enables VQA\nmodels to reason about the future without object-level labels, even\noutperforming counterparts that use ground-truth annotations. Finally, we show\nits ability to serve as a world model for model-based planning, which is\ncompetitive with methods designed specifically for such tasks.",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Ziyi Wu",
      "Nikita Dvornik",
      "Klaus Greff",
      "Thomas Kipf",
      "Animesh Garg"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05861"
  },
  {
    "id": "arXiv:2210.05863",
    "title": "Decentralized Planning for Car-Like Robotic Swarm in Unstructured  Environments",
    "abstract": "Robot swarm is a hot spot in robotic research community. In this paper, we\npropose a decentralized framework for car-like robotic swarm which is capable\nof real-time planning in unstructured environments. In this system, path\nfinding is guided by environmental topology information to avoid frequent\ntopological change, and search-based speed planning is leveraged to escape from\ninfeasible initial value's local minima. Then spatial-temporal optimization is\nemployed to generate a safe, smooth and dynamically feasible trajectory. During\noptimization, penalty is imposed on signed distance between agents to realize\ncollision avoidance, and differential flatness cooperated with limitation on\nfront steer angle satisfies the non-holonomic constraints. With trajectories\nbroadcast to the wireless network, agents are able to check and prevent from\npotential collisions. We validate the robustness of our system in simulation\nand real-world experiments. Code will be released as open-source packages.",
    "descriptor": "\nComments: Submitted to ICRA2023\n",
    "authors": [
      "Changjia Ma",
      "Zhichao Han",
      "Tingrui Zhang",
      "Jingping Wang",
      "Long Xu",
      "Chengyang Li",
      "Chao Xu",
      "Fei Gao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05863"
  },
  {
    "id": "arXiv:2210.05866",
    "title": "Deep Learning for Iris Recognition: A Survey",
    "abstract": "In this survey, we provide a comprehensive review of more than 200 papers,\ntechnical reports, and GitHub repositories published over the last 10 years on\nthe recent developments of deep learning techniques for iris recognition,\ncovering broad topics on algorithm designs, open-source tools, open challenges,\nand emerging research. First, we conduct a comprehensive analysis of deep\nlearning techniques developed for two main sub-tasks in iris biometrics:\nsegmentation and recognition. Second, we focus on deep learning techniques for\nthe robustness of iris recognition systems against presentation attacks and via\nhuman-machine pairing. Third, we delve deep into deep learning techniques for\nforensic application, especially in post-mortem iris recognition. Fourth, we\nreview open-source resources and tools in deep learning techniques for iris\nrecognition. Finally, we highlight the technical challenges, emerging research\ntrends, and outlook for the future of deep learning in iris recognition.",
    "descriptor": "",
    "authors": [
      "Kien Nguyen",
      "Hugo Proen\u00e7a",
      "Fernando Alonso-Fernandez"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05866"
  },
  {
    "id": "arXiv:2210.05870",
    "title": "LACV-Net: Semantic Segmentation of Large-Scale Point Cloud Scene via  Local Adaptive and Comprehensive VLAD",
    "abstract": "Large-scale point cloud semantic segmentation is an important task in 3D\ncomputer vision, which is widely applied in autonomous driving, robotics, and\nvirtual reality. Current large-scale point cloud semantic segmentation methods\nusually use down-sampling operations to improve computation efficiency and\nacquire point clouds with multi-resolution. However, this may cause the problem\nof missing local information. Meanwhile, it is difficult for networks to\ncapture global information in large-scale distributed contexts. To capture\nlocal and global information effectively, we propose an end-to-end deep neural\nnetwork called LACV-Net for large-scale point cloud semantic segmentation. The\nproposed network contains three main components: 1) a local adaptive feature\naugmentation module (LAFA) to adaptively learn the similarity of centroids and\nneighboring points to augment the local context; 2) a comprehensive VLAD module\n(C-VLAD) that fuses local features with multi-layer, multi-scale, and\nmulti-resolution to represent a comprehensive global description vector; and 3)\nan aggregation loss function to effectively optimize the segmentation\nboundaries by constraining the adaptive weight from the LAFA module. Compared\nto state-of-the-art networks on several large-scale benchmark datasets,\nincluding S3DIS, Toronto3D, and SensatUrban, we demonstrated the effectiveness\nof the proposed network.",
    "descriptor": "",
    "authors": [
      "Ziyin Zeng",
      "Yongyang Xu",
      "Zhong Xie",
      "Wei Tang",
      "Jie Wan",
      "Weichao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05870"
  },
  {
    "id": "arXiv:2210.05872",
    "title": "Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion  Image Manipulation",
    "abstract": "Fashion attribute editing is a task that aims to convert the semantic\nattributes of a given fashion image while preserving the irrelevant regions.\nPrevious works typically employ conditional GANs where the generator explicitly\nlearns the target attributes and directly execute the conversion. These\napproaches, however, are neither scalable nor generic as they operate only with\nfew limited attributes and a separate generator is required for each dataset or\nattribute set. Inspired by the recent advancement of diffusion models, we\nexplore the classifier-guided diffusion that leverages the off-the-shelf\ndiffusion model pretrained on general visual semantics such as Imagenet. In\norder to achieve a generic editing pipeline, we pose this as multi-attribute\nimage manipulation task, where the attribute ranges from item category, fabric,\npattern to collar and neckline. We empirically show that conventional methods\nfail in our challenging setting, and study efficient adaptation scheme that\ninvolves recently introduced attention-pooling technique to obtain a\nmulti-attribute classifier guidance. Based on this, we present a mask-free\nfashion attribute editing framework that leverages the classifier logits and\nthe cross-attention map for manipulation. We empirically demonstrate that our\nframework achieves convincing sample quality and attribute alignments.",
    "descriptor": "\nComments: Accepted to WACV 2023\n",
    "authors": [
      "Chaerin Kong",
      "DongHyeon Jeon",
      "Ohjoon Kwon",
      "Nojun Kwak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05872"
  },
  {
    "id": "arXiv:2210.05874",
    "title": "Multi-Content Time-Series Popularity Prediction with Multiple-Model  Transformers in MEC Networks",
    "abstract": "Coded/uncoded content placement in Mobile Edge Caching (MEC) has evolved as\nan efficient solution to meet the significant growth of global mobile data\ntraffic by boosting the content diversity in the storage of caching nodes. To\nmeet the dynamic nature of the historical request pattern of multimedia\ncontents, the main focus of recent researches has been shifted to develop\ndata-driven and real-time caching schemes. In this regard and with the\nassumption that users' preferences remain unchanged over a short horizon, the\nTop-K popular contents are identified as the output of the learning model. Most\nexisting datadriven popularity prediction models, however, are not suitable for\nthe coded/uncoded content placement frameworks. On the one hand, in\ncoded/uncoded content placement, in addition to classifying contents into two\ngroups, i.e., popular and nonpopular, the probability of content request is\nrequired to identify which content should be stored partially/completely, where\nthis information is not provided by existing data-driven popularity prediction\nmodels. On the other hand, the assumption that users' preferences remain\nunchanged over a short horizon only works for content with a smooth request\npattern. To tackle these challenges, we develop a Multiple-model (hybrid)\nTransformer-based Edge Caching (MTEC) framework with higher generalization\nability, suitable for various types of content with different time-varying\nbehavior, that can be adapted with coded/uncoded content placement frameworks.\nSimulation results corroborate the effectiveness of the proposed MTEC caching\nframework in comparison to its counterparts in terms of the cache-hit ratio,\nclassification accuracy, and the transferred byte volume.",
    "descriptor": "",
    "authors": [
      "Zohreh HajiAkhondi-Meybodi",
      "Arash Mohammadi",
      "Ming Hou",
      "Elahe Rahimian",
      "Shahin Heidarian",
      "Jamshid Abouei",
      "Konstantinos N. Plataniotis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05874"
  },
  {
    "id": "arXiv:2210.05875",
    "title": "MedJEx: A Medical Jargon Extraction Model with Wiki's Hyperlink Span and  Contextualized Masked Language Model Score",
    "abstract": "This paper proposes a new natural language processing (NLP) application for\nidentifying medical jargon terms potentially difficult for patients to\ncomprehend from electronic health record (EHR) notes. We first present a novel\nand publicly available dataset with expert-annotated medical jargon terms from\n18K+ EHR note sentences ($MedJ$). Then, we introduce a novel medical jargon\nextraction ($MedJEx$) model which has been shown to outperform existing\nstate-of-the-art NLP models. First, MedJEx improved the overall performance\nwhen it was trained on an auxiliary Wikipedia hyperlink span dataset, where\nhyperlink spans provide additional Wikipedia articles to explain the spans (or\nterms), and then fine-tuned on the annotated MedJ data. Secondly, we found that\na contextualized masked language model score was beneficial for detecting\ndomain-specific unfamiliar jargon terms. Moreover, our results show that\ntraining on the auxiliary Wikipedia hyperlink span datasets improved six out of\neight biomedical named entity recognition benchmark datasets. Both MedJ and\nMedJEx are publicly available.",
    "descriptor": "\nComments: Accepted to EMNLP 22\n",
    "authors": [
      "Sunjae Kwon",
      "Zonghai Yao",
      "Harmon S. Jordan",
      "David A. Levy",
      "Brian Corner",
      "Hong Yu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05875"
  },
  {
    "id": "arXiv:2210.05876",
    "title": "Statistical Modeling of Soft Error Influence on Neural Networks",
    "abstract": "Soft errors in large VLSI circuits pose dramatic influence on computing- and\nmemory-intensive neural network (NN) processing. Understanding the influence of\nsoft errors on NNs is critical to protect against soft errors for reliable NN\nprocessing. Prior work mainly rely on fault simulation to analyze the influence\nof soft errors on NN processing. They are accurate but usually specific to\nlimited configurations of errors and NN models due to the prohibitively slow\nsimulation speed especially for large NN models and datasets. With the\nobservation that the influence of soft errors propagates across a large number\nof neurons and accumulates as well, we propose to characterize the soft error\ninduced data disturbance on each neuron with normal distribution model\naccording to central limit theorem and develop a series of statistical models\nto analyze the behavior of NN models under soft errors in general. The\nstatistical models reveal not only the correlation between soft errors and NN\nmodel accuracy, but also how NN parameters such as quantization and\narchitecture affect the reliability of NNs. The proposed models are compared\nwith fault simulation and verified comprehensively. In addition, we observe\nthat the statistical models that characterize the soft error influence can also\nbe utilized to predict fault simulation results in many cases and we explore\nthe use of the proposed statistical models to accelerate fault simulations of\nNNs. According to our experiments, the accelerated fault simulation shows\nalmost two orders of magnitude speedup with negligible simulation accuracy loss\nover the baseline fault simulations.",
    "descriptor": "",
    "authors": [
      "Haitong Huang",
      "Xinghua Xue",
      "Cheng Liu",
      "Ying Wang",
      "Tao Luo",
      "Long Cheng",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.05876"
  },
  {
    "id": "arXiv:2210.05879",
    "title": "Learning by Asking Questions for Knowledge-based Novel Object  Recognition",
    "abstract": "In real-world object recognition, there are numerous object classes to be\nrecognized. Conventional image recognition based on supervised learning can\nonly recognize object classes that exist in the training data, and thus has\nlimited applicability in the real world. On the other hand, humans can\nrecognize novel objects by asking questions and acquiring knowledge about them.\nInspired by this, we study a framework for acquiring external knowledge through\nquestion generation that would help the model instantly recognize novel\nobjects. Our pipeline consists of two components: the Object Classifier, which\nperforms knowledge-based object recognition, and the Question Generator, which\ngenerates knowledge-aware questions to acquire novel knowledge. We also propose\na question generation strategy based on the confidence of the knowledge-aware\nprediction of the Object Classifier. To train the Question Generator, we\nconstruct a dataset that contains knowledge-aware questions about objects in\nthe images. Our experiments show that the proposed pipeline effectively\nacquires knowledge about novel objects compared to several baselines.",
    "descriptor": "",
    "authors": [
      "Kohei Uehara",
      "Tatsuya Harada"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05879"
  },
  {
    "id": "arXiv:2210.05881",
    "title": "Deterioration Prediction using Time-Series of Three Vital Signs and  Current Clinical Features Amongst COVID-19 Patients",
    "abstract": "Unrecognized patient deterioration can lead to high morbidity and mortality.\nMost existing deterioration prediction models require a large number of\nclinical information, typically collected in hospital settings, such as medical\nimages or comprehensive laboratory tests. This is infeasible for telehealth\nsolutions and highlights a gap in deterioration prediction models that are\nbased on minimal data, which can be recorded at a large scale in any clinic,\nnursing home, or even at the patient's home. In this study, we propose and\ndevelop a prognostic model that predicts if a patient will experience\ndeterioration in the forthcoming 3-24 hours. The model sequentially processes\nroutine triadic vital signs: (a) oxygen saturation, (b) heart rate, and (c)\ntemperature. The model is also provided with basic patient information,\nincluding sex, age, vaccination status, vaccination date, and status of\nobesity, hypertension, or diabetes. We train and evaluate the model using data\ncollected from 37,006 COVID-19 patients at NYU Langone Health in New York, USA.\nThe model achieves an area under the receiver operating characteristic curve\n(AUROC) of 0.808-0.880 for 3-24 hour deterioration prediction. We also conduct\nocclusion experiments to evaluate the importance of each input feature, where\nthe results reveal the significance of continuously monitoring the variations\nof the vital signs. Our results show the prospect of accurate deterioration\nforecast using a minimum feature set that can be relatively easily obtained\nusing wearable devices and self-reported patient information.",
    "descriptor": "",
    "authors": [
      "Sarmad Mehrdad",
      "Farah E. Shamout",
      "Yao Wang",
      "S. Farokh Atashzar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.05881"
  },
  {
    "id": "arXiv:2210.05882",
    "title": "A Novel Multi-Objective Velocity-Free Boolean Particle Swarm  Optimization",
    "abstract": "This paper extends boolean particle swarm optimization to a multi-objective\nsetting, to our knowledge for the first time in the literature. Our proposed\nnew boolean algorithm, MBOnvPSO, is notably simplified by the omission of a\nvelocity update rule and has enhanced exploration ability due to the inclusion\nof a 'noise' term in the position update rule that prevents particles being\ntrapped in local optima. Our algorithm additionally makes use of an external\narchive to store non-dominated solutions and implements crowding distance to\nencourage solution diversity. In benchmark tests, MBOnvPSO produced high\nquality Pareto fronts, when compared to benchmarked alternatives, for all of\nthe multi-objective test functions considered, with competitive performance in\nsearch spaces with up to 600 discrete dimensions.",
    "descriptor": "",
    "authors": [
      "Wei Quan",
      "Denise Gorse"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.05882"
  },
  {
    "id": "arXiv:2210.05883",
    "title": "AD-DROP: Attribution-Driven Dropout for Robust Language Model  Fine-Tuning",
    "abstract": "Fine-tuning large pre-trained language models on downstream tasks is apt to\nsuffer from overfitting when limited training data is available. While dropout\nproves to be an effective antidote by randomly dropping a proportion of units,\nexisting research has not examined its effect on the self-attention mechanism.\nIn this paper, we investigate this problem through self-attention attribution\nand find that dropping attention positions with low attribution scores can\naccelerate training and increase the risk of overfitting. Motivated by this\nobservation, we propose Attribution-Driven Dropout (AD-DROP), which randomly\ndiscards some high-attribution positions to encourage the model to make\npredictions by relying more on low-attribution positions to reduce overfitting.\nWe also develop a cross-tuning strategy to alternate fine-tuning and AD-DROP to\navoid dropping high-attribution positions excessively. Extensive experiments on\nvarious benchmarks show that AD-DROP yields consistent improvements over\nbaselines. Analysis further confirms that AD-DROP serves as a strategic\nregularizer to prevent overfitting during fine-tuning.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Tao Yang",
      "Jinghao Deng",
      "Xiaojun Quan",
      "Qifan Wang",
      "Shaoliang Nie"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05883"
  },
  {
    "id": "arXiv:2210.05888",
    "title": "Calibration and Uncertainty Characterization for Ultra-Wideband  Two-Way-Ranging Measurements",
    "abstract": "Ultra-Wideband (UWB) systems are becoming increasingly popular for indoor\nlocalization, where range measurements are obtained by measuring the\ntime-of-flight of radio signals. However, the range measurements typically\nsuffer from a systematic error or bias that must be corrected for high-accuracy\nlocalization. In this paper, a ranging protocol is proposed alongside a robust\nand scalable antenna-delay calibration procedure to accurately and efficiently\ncalibrate antenna delays for many UWB tags. Additionally, the bias and\nuncertainty of the measurements are modelled as a function of the\nreceived-signal power. The full calibration procedure is presented using\nexperimental training data of 3 aerial robots fitted with 2 UWB tags each, and\nthen evaluated on 2 test experiments. A localization problem is then formulated\non the experimental test data, and the calibrated measurements and their\nmodelled uncertainty are fed into an extended Kalman filter (EKF). The proposed\ncalibration is shown to yield an average of 46% improvement in localization\naccuracy. Lastly, the paper is accompanied by an open-source UWB-calibration\nPython library, which can be found at\nhttps://github.com/decarsg/uwb_calibration.",
    "descriptor": "\nComments: 7 pages, 7 figures, submitted to 2023 International Conference on Robotics and Automation (ICRA)\n",
    "authors": [
      "Mohammed Ayman Shalaby",
      "Charles Champagne Cossette",
      "James Richard Forbes",
      "Jerome Le Ny"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05888"
  },
  {
    "id": "arXiv:2210.05889",
    "title": "Building Heterogeneous Cloud System for Machine Learning Inference",
    "abstract": "Online inference is becoming a key service product for many businesses,\ndeployed in cloud platforms to meet customer demands. Despite their\nrevenue-generation capability, these services need to operate under tight\nQuality-of-Service (QoS) and cost budget constraints. This paper introduces\nKAIROS, a novel runtime framework that maximizes the query throughput while\nmeeting QoS target and a cost budget. KAIROS designs and implements novel\ntechniques to build a pool of heterogeneous compute hardware without online\nexploration overhead, and distribute inference queries optimally at runtime.\nOur evaluation using industry-grade deep learning (DL) models shows that KAIROS\nyields up to 2X the throughput of an optimal homogeneous solution, and\noutperforms state-of-the-art schemes by up to 70\\%, despite advantageous\nimplementations of the competing schemes to ignore their exploration overhead.",
    "descriptor": "",
    "authors": [
      "Baolin Li",
      "Siddharth Samsi",
      "Vijay Gadepally",
      "Devesh Tiwari"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05889"
  },
  {
    "id": "arXiv:2210.05891",
    "title": "Point Cloud Scene Completion with Joint Color and Semantic Estimation  from Single RGB-D Image",
    "abstract": "We present a deep reinforcement learning method of progressive view\ninpainting for colored semantic point cloud scene completion under volume\nguidance, achieving high-quality scene reconstruction from only a single RGB-D\nimage with severe occlusion. Our approach is end-to-end, consisting of three\nmodules: 3D scene volume reconstruction, 2D RGB-D and segmentation image\ninpainting, and multi-view selection for completion. Given a single RGB-D\nimage, our method first predicts its semantic segmentation map and goes through\nthe 3D volume branch to obtain a volumetric scene reconstruction as a guide to\nthe next view inpainting step, which attempts to make up the missing\ninformation; the third step involves projecting the volume under the same view\nof the input, concatenating them to complete the current view RGB-D and\nsegmentation map, and integrating all RGB-D and segmentation maps into the\npoint cloud. Since the occluded areas are unavailable, we resort to a A3C\nnetwork to glance around and pick the next best view for large hole completion\nprogressively until a scene is adequately reconstructed while guaranteeing\nvalidity. All steps are learned jointly to achieve robust and consistent\nresults. We perform qualitative and quantitative evaluations with extensive\nexperiments on the 3D-FUTURE data, obtaining better results than\nstate-of-the-arts.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1903.04019\n",
    "authors": [
      "Zhaoxuan Zhang",
      "Xiaoguang Han",
      "Bo Dong",
      "Tong Li",
      "Baocai Yin",
      "Xin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05891"
  },
  {
    "id": "arXiv:2210.05892",
    "title": "Perplexity from PLM Is Unreliable for Evaluating Text Quality",
    "abstract": "Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality\nof the generated text. They suppose that if the value of PPL is smaller, the\nquality(i.e. fluency) of the text to be evaluated is better. However, we find\nthat the PPL referee is unqualified and it cannot evaluate the generated text\nfairly for the following reasons: (i) The PPL of short text is larger than long\ntext, which goes against common sense, (ii) The repeated text span could damage\nthe performance of PPL, and (iii) The punctuation marks could affect the\nperformance of PPL heavily. Experiments show that the PPL is unreliable for\nevaluating the quality of given text. Last, we discuss the key problems with\nevaluating text quality using language models.",
    "descriptor": "",
    "authors": [
      "Yequan Wang",
      "Jiawen Deng",
      "Aixin Sun",
      "Xuying Meng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05892"
  },
  {
    "id": "arXiv:2210.05894",
    "title": "Safety-Aware Human-Robot Collaborative Transportation and Manipulation  with Multiple MAVs",
    "abstract": "Human-robot interaction will play an essential role in many future industries\nand daily life tasks, enabling robots to collaborate with humans and reduce\ntheir workload effectively. Most existing approaches for human-robot physical\ncollaboration focus on collaboration between humans and grounded robots. In\nrecent years, very little progress has been made in this area when considering\naerial robots, which present increased versatility and mobility compared to\ntheir grounded counterparts. This paper proposes a novel approach for safe\nhuman-robot collaborative transportation and manipulation of a cable-suspended\npayload with multiple aerial robots. We leverage the proposed method to enable\nseamless and transparent interaction between the transported objects and a\nhuman worker while considering safety constraints during operations by\nexploiting the redundancy of the internal transportation system. The critical\nsystem components are (a) a distributed force-sensor-free payload external\nwrench estimator; (b) a 6D admittance controller for human-aerial-robot\ncollaborative transportation and manipulation; (c) a safety-aware controller\nthat exploits the internal system redundancy to guarantee the execution of\nadditional tasks devoted to preserving the human or robot safety without\naffecting the payload trajectory tracking or quality of interaction. We\nvalidate the approach through extensive real-world experiments, including the\nrobot team assisting the human in transporting and manipulating a load or the\nhuman helping the robot team navigates the environment. To our best knowledge,\nthis work is the first to create an interactive and safety-aware pipeline for\nquadrotor teams to collaborate physically with a human operator to transport\nand manipulate a payload.",
    "descriptor": "\nComments: Xinyang Liu and Guanrui Li contributed equally to this paper\n",
    "authors": [
      "Xinyang Liu",
      "Guanrui Li",
      "Giuseppe Loianno"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05894"
  },
  {
    "id": "arXiv:2210.05895",
    "title": "DG-STGCN: Dynamic Spatial-Temporal Modeling for Skeleton-based Action  Recognition",
    "abstract": "Graph convolution networks (GCN) have been widely used in skeleton-based\naction recognition. We note that existing GCN-based approaches primarily rely\non prescribed graphical structures (ie., a manually defined topology of\nskeleton joints), which limits their flexibility to capture complicated\ncorrelations between joints. To move beyond this limitation, we propose a new\nframework for skeleton-based action recognition, namely Dynamic Group\nSpatio-Temporal GCN (DG-STGCN). It consists of two modules, DG-GCN and DG-TCN,\nrespectively, for spatial and temporal modeling. In particular, DG-GCN uses\nlearned affinity matrices to capture dynamic graphical structures instead of\nrelying on a prescribed one, while DG-TCN performs group-wise temporal\nconvolutions with varying receptive fields and incorporates a dynamic\njoint-skeleton fusion module for adaptive multi-level temporal modeling. On a\nwide range of benchmarks, including NTURGB+D, Kinetics-Skeleton, BABEL, and\nToyota SmartHome, DG-STGCN consistently outperforms state-of-the-art methods,\noften by a notable margin.",
    "descriptor": "\nComments: Codes will be released in this https URL\n",
    "authors": [
      "Haodong Duan",
      "Jiaqi Wang",
      "Kai Chen",
      "Dahua Lin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05895"
  },
  {
    "id": "arXiv:2210.05896",
    "title": "Common Corruption Robustness of Point Cloud Detectors: Benchmark and  Enhancement",
    "abstract": "Object detection through LiDAR-based point cloud has recently been important\nin autonomous driving. Although achieving high accuracy on public benchmarks,\nthe state-of-the-art detectors may still go wrong and cause a heavy loss due to\nthe widespread corruptions in the real world like rain, snow, sensor noise,\netc. Nevertheless, there is a lack of a large-scale dataset covering diverse\nscenes and realistic corruption types with different severities to develop\npractical and robust point cloud detectors, which is challenging due to the\nheavy collection costs. To alleviate the challenge and start the first step for\nrobust point cloud detection, we propose the physical-aware simulation methods\nto generate degraded point clouds under different real-world common\ncorruptions. Then, for the first attempt, we construct a benchmark based on the\nphysical-aware common corruptions for point cloud detectors, which contains a\ntotal of 1,122,150 examples covering 7,481 scenes, 25 common corruption types,\nand 6 severities. With such a novel benchmark, we conduct extensive empirical\nstudies on 8 state-of-the-art detectors that contain 6 different detection\nframeworks. Thus we get several insight observations revealing the\nvulnerabilities of the detectors and indicating the enhancement directions.\nMoreover, we further study the effectiveness of existing robustness enhancement\nmethods based on data augmentation and data denoising. The benchmark can\npotentially be a new platform for evaluating point cloud detectors, opening a\ndoor for developing novel robustness enhancement methods.",
    "descriptor": "\nComments: 16 pages, 6 figures\n",
    "authors": [
      "Shuangzhi Li",
      "Zhijie Wang",
      "Felix Juefei-Xu",
      "Qing Guo",
      "Xingyu Li",
      "Lei Ma"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05896"
  },
  {
    "id": "arXiv:2210.05899",
    "title": "A Lower Bound of Hash Codes' Performance",
    "abstract": "As a crucial approach for compact representation learning, hashing has\nachieved great success in effectiveness and efficiency. Numerous heuristic\nHamming space metric learning objectives are designed to obtain high-quality\nhash codes. Nevertheless, a theoretical analysis of criteria for learning good\nhash codes remains largely unexploited. In this paper, we prove that\ninter-class distinctiveness and intra-class compactness among hash codes\ndetermine the lower bound of hash codes' performance. Promoting these two\ncharacteristics could lift the bound and improve hash learning. We then propose\na surrogate model to fully exploit the above objective by estimating the\nposterior of hash codes and controlling it, which results in a low-bias\noptimization. Extensive experiments reveal the effectiveness of the proposed\nmethod. By testing on a series of hash-models, we obtain performance\nimprovements among all of them, with an up to $26.5\\%$ increase in mean Average\nPrecision and an up to $20.5\\%$ increase in accuracy. Our code is publicly\navailable at \\url{https://github.com/VL-Group/LBHash}.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Xiaosu Zhu",
      "Jingkuan Song",
      "Yu Lei",
      "Lianli Gao",
      "Heng Tao Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05899"
  },
  {
    "id": "arXiv:2210.05901",
    "title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation  with Commonsense Reasoning",
    "abstract": "Intelligent virtual assistants are currently designed to perform tasks or\nservices explicitly mentioned by users, so multiple related domains or tasks\nneed to be performed one by one through a long conversation with many explicit\nintents. Instead, human assistants are capable of reasoning (multiple) implicit\nintents based on user utterances via commonsense knowledge, reducing complex\ninteractions and improving practicality. Therefore, this paper proposes a\nframework of multi-domain dialogue systems, which can automatically infer\nimplicit intents based on user utterances and then perform zero-shot prompting\nusing a large pre-trained language model to trigger suitable single\ntask-oriented bots. The proposed framework is demonstrated effective to realize\nimplicit intents and recommend associated bots in a zero-shot manner.",
    "descriptor": "",
    "authors": [
      "Hui-Chi Kuo",
      "Yun-Nung Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05901"
  },
  {
    "id": "arXiv:2210.05903",
    "title": "Towards Web3 Applications: Easing the Access and Transition",
    "abstract": "Web3 is leading a wave of the next generation of web services that even many\nWeb2 applications are keen to ride. However, the lack of Web3 background for\nWeb2 developers hinders easy and effective access and transition. On the other\nhand, Web3 applications desire for encouragement and advertisement from\nconventional Web2 companies and projects due to their low market shares. In\nthis paper, we propose a seamless transition framework that transits Web2 to\nWeb3, named WebttCom, after exploring the connotation of Web3 and the key\ndifferences between Web2 and Web3 applications. We also provide a full-stack\nimplementation as a use case to support the proposed framework, followed by\ninterviews with five participants that show four positive and one natural\nresponse. We confirm that the proposed framework WebttCom addresses the defined\nresearch question, and the implementation well satisfies the framework WebttCom\nin terms of strong necessity, usability, and completeness based on the\ninterview results.",
    "descriptor": "\nComments: 8 pages, 3 figures, code snippets, interviews\n",
    "authors": [
      "Guangsheng Yu",
      "Xu Wang",
      "Qin Wang",
      "Tingting Bi",
      "Yifei Dong",
      "Ren Ping Liu",
      "Nektarios Georgalas",
      "Andrew Reeves"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05903"
  },
  {
    "id": "arXiv:2210.05905",
    "title": "Discourse Analysis via Questions and Answers: Parsing Dependency  Structures of Questions Under Discussion",
    "abstract": "Automatic discourse processing, which can help understand how sentences\nconnect to each other, is bottlenecked by data: current discourse formalisms\npose highly demanding annotation tasks involving large taxonomies of discourse\nrelations, making them inaccessible to lay annotators. This work instead adopts\nthe linguistic framework of Questions Under Discussion (QUD) for discourse\nanalysis and seeks to derive QUD structures automatically. QUD views each\nsentence as an answer to a question triggered in prior context; thus, we\ncharacterize relationships between sentences as free-form questions, in\ncontrast to exhaustive fine-grained taxonomies. We develop the\nfirst-of-its-kind QUD parser that derives a dependency structure of questions\nover full documents, trained using a large question-answering dataset DCQA\nannotated in a manner consistent with the QUD framework. Importantly, data\ncollection is easily crowdsourced using DCQA's paradigm. We show that this\nleads to a parser attaining strong performance according to human evaluation.\nWe illustrate how our QUD structure is distinct from RST trees, and demonstrate\nthe utility of QUD analysis in the context of document simplification. Our\nfindings show that QUD parsing is an appealing alternative for automatic\ndiscourse processing.",
    "descriptor": "",
    "authors": [
      "Wei-Jen Ko",
      "Yating Wu",
      "Cutter Dalton",
      "Dananjay Srinivas",
      "Greg Durrett",
      "Junyi Jessy Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05905"
  },
  {
    "id": "arXiv:2210.05906",
    "title": "Travel the Same Path: A Novel TSP Solving Strategy",
    "abstract": "In this paper, we provide a novel strategy for solving Traveling Salesman\nProblem, which is a famous combinatorial optimization problem studied intensely\nin the TCS community. In particular, we consider the imitation learning\nframework, which helps a deterministic algorithm making good choices whenever\nit needs to, resulting in a speed up while maintaining the exactness of the\nsolution without suffering from the unpredictability and a potential large\ndeviation.\nFurthermore, we demonstrate a strong generalization ability of a graph neural\nnetwork trained under the imitation learning framework. Specifically, the model\nis capable of solving a large instance of TSP faster than the baseline while\nhas only seen small TSP instances when training.",
    "descriptor": "",
    "authors": [
      "Pingbang Hu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05906"
  },
  {
    "id": "arXiv:2210.05912",
    "title": "PSNet: Parallel Symmetric Network for Video Salient Object Detection",
    "abstract": "For the video salient object detection (VSOD) task, how to excavate the\ninformation from the appearance modality and the motion modality has always\nbeen a topic of great concern. The two-stream structure, including an RGB\nappearance stream and an optical flow motion stream, has been widely used as a\ntypical pipeline for VSOD tasks, but the existing methods usually only use\nmotion features to unidirectionally guide appearance features or adaptively but\nblindly fuse two modality features. However, these methods underperform in\ndiverse scenarios due to the uncomprehensive and unspecific learning schemes.\nIn this paper, following a more secure modeling philosophy, we deeply\ninvestigate the importance of appearance modality and motion modality in a more\ncomprehensive way and propose a VSOD network with up and down parallel\nsymmetry, named PSNet. Two parallel branches with different dominant modalities\nare set to achieve complete video saliency decoding with the cooperation of the\nGather Diffusion Reinforcement (GDR) module and Cross-modality Refinement and\nComplement (CRC) module. Finally, we use the Importance Perception Fusion (IPF)\nmodule to fuse the features from two parallel branches according to their\ndifferent importance in different scenarios. Experiments on four dataset\nbenchmarks demonstrate that our method achieves desirable and competitive\nperformance.",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Emerging Topics in Computational Intelligence 2022, 13 pages, 8 figures\n",
    "authors": [
      "Runmin Cong",
      "Weiyu Song",
      "Jianjun Lei",
      "Guanghui Yue",
      "Yao Zhao",
      "Sam Kwong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05912"
  },
  {
    "id": "arXiv:2210.05916",
    "title": "Hate-CLIPper: Multimodal Hateful Meme Classification based on  Cross-modal Interaction of CLIP Features",
    "abstract": "Hateful memes are a growing menace on social media. While the image and its\ncorresponding text in a meme are related, they do not necessarily convey the\nsame meaning when viewed individually. Hence, detecting hateful memes requires\ncareful consideration of both visual and textual information. Multimodal\npre-training can be beneficial for this task because it effectively captures\nthe relationship between the image and the text by representing them in a\nsimilar feature space. Furthermore, it is essential to model the interactions\nbetween the image and text features through intermediate fusion. Most existing\nmethods either employ multimodal pre-training or intermediate fusion, but not\nboth. In this work, we propose the Hate-CLIPper architecture, which explicitly\nmodels the cross-modal interactions between the image and text representations\nobtained using Contrastive Language-Image Pre-training (CLIP) encoders via a\nfeature interaction matrix (FIM). A simple classifier based on the FIM\nrepresentation is able to achieve state-of-the-art performance on the Hateful\nMemes Challenge (HMC) dataset with an AUROC of 85.8, which even surpasses the\nhuman performance of 82.65. Experiments on other meme datasets such as\nPropaganda Memes and TamilMemes also demonstrate the generalizability of the\nproposed approach. Finally, we analyze the interpretability of the FIM\nrepresentation and show that cross-modal interactions can indeed facilitate the\nlearning of meaningful concepts. The code for this work is available at\nhttps://github.com/gokulkarthik/hateclipper.",
    "descriptor": "\nComments: Accepted at EMNLP 2022 Workshop on NLP for Positive Impact\n",
    "authors": [
      "Gokul Karthik Kumar",
      "Karthik Nanadakumar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.05916"
  },
  {
    "id": "arXiv:2210.05917",
    "title": "Enemy Spotted: in-game gun sound dataset for gunshot classification and  localization",
    "abstract": "Recently, deep learning-based methods have drawn huge attention due to their\nsimple yet high performance without domain knowledge in sound classification\nand localization tasks. However, a lack of gun sounds in existing datasets has\nbeen a major obstacle to implementing a support system to spot criminals from\ntheir gunshots by leveraging deep learning models. Since the occurrence of\ngunshot is rare and unpredictable, it is impractical to collect gun sounds in\nthe real world. As an alternative, gun sounds can be obtained from an FPS game\nthat is designed to mimic real-world warfare. The recent FPS game offers a\nrealistic environment where we can safely collect gunshot data while simulating\neven dangerous situations. By exploiting the advantage of the game environment,\nwe construct a gunshot dataset, namely BGG, for the firearm classification and\ngunshot localization tasks. The BGG dataset consists of 37 different types of\nfirearms, distances, and directions between the sound source and a receiver. We\ncarefully verify that the in-game gunshot data has sufficient information to\nidentify the location and type of gunshots by training several sound\nclassification and localization baselines on the BGG dataset. Afterward, we\ndemonstrate that the accuracy of real-world firearm classification and\nlocalization tasks can be enhanced by utilizing the BGG dataset.",
    "descriptor": "\nComments: Accepted at IEEE Conference on Games (GoG) 2022\n",
    "authors": [
      "Junwoo Park",
      "Youngwoo Cho",
      "Gyuhyeon Sim",
      "Hojoon Lee",
      "Jaegul Choo"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.05917"
  },
  {
    "id": "arXiv:2210.05918",
    "title": "Finite time analysis of temporal difference learning with linear  function approximation: Tail averaging and regularisation",
    "abstract": "We study the finite-time behaviour of the popular temporal difference (TD)\nlearning algorithm when combined with tail-averaging. We derive finite time\nbounds on the parameter error of the tail-averaged TD iterate under a step-size\nchoice that does not require information about the eigenvalues of the matrix\nunderlying the projected TD fixed point. Our analysis shows that tail-averaged\nTD converges at the optimal $O\\left(1/t\\right)$ rate, both in expectation and\nwith high probability. In addition, our bounds exhibit a sharper rate of decay\nfor the initial error (bias), which is an improvement over averaging all\niterates. We also propose and analyse a variant of TD that incorporates\nregularisation. From analysis, we conclude that the regularised version of TD\nis useful for problems with ill-conditioned features.",
    "descriptor": "",
    "authors": [
      "Gandharv Patil",
      "Prashanth L.A.",
      "Dheeraj Nagaraj",
      "Doina Precup"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05918"
  },
  {
    "id": "arXiv:2210.05920",
    "title": "Boosting Graph Neural Networks via Adaptive Knowledge Distillation",
    "abstract": "Graph neural networks (GNNs) have shown remarkable performance on diverse\ngraph mining tasks. Although different GNNs can be unified as the same message\npassing framework, they learn complementary knowledge from the same graph.\nKnowledge distillation (KD) is developed to combine the diverse knowledge from\nmultiple models. It transfers knowledge from high-capacity teachers to a\nlightweight student. However, to avoid oversmoothing, GNNs are often shallow,\nwhich deviates from the setting of KD. In this context, we revisit KD by\nseparating its benefits from model compression and emphasizing its power of\ntransferring knowledge. To this end, we need to tackle two challenges: how to\ntransfer knowledge from compact teachers to a student with the same capacity;\nand, how to exploit student GNN's own strength to learn knowledge. In this\npaper, we propose a novel adaptive KD framework, called BGNN, which\nsequentially transfers knowledge from multiple GNNs into a student GNN. We also\nintroduce an adaptive temperature module and a weight boosting module. These\nmodules guide the student to the appropriate knowledge for effective learning.\nExtensive experiments have demonstrated the effectiveness of BGNN. In\nparticular, we achieve up to 3.05% improvement for node classification and\n7.67% improvement for graph classification over vanilla GNNs.",
    "descriptor": "",
    "authors": [
      "Zhichun Guo",
      "Chunhui Zhang",
      "Yujie Fan",
      "Yijun Tian",
      "Chuxu Zhang",
      "Nitesh Chawla"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05920"
  },
  {
    "id": "arXiv:2210.05921",
    "title": "Step out of KG: Knowledge Graph Completion via Knowledgeable Retrieval  and Reading Comprehension",
    "abstract": "Knowledge graphs, as the cornerstone of many AI applications, usually face\nserious incompleteness problems. In recent years, there have been many efforts\nto study automatic knowledge graph completion (KGC), most of which use existing\nknowledge to infer new knowledge. However, in our experiments, we find that not\nall relations can be obtained by inference, which constrains the performance of\nexisting models. To alleviate this problem, we propose a new model based on\ninformation retrieval and reading comprehension, namely IR4KGC. Specifically,\nwe pre-train a knowledge-based information retrieval module that can retrieve\ndocuments related to the triples to be completed. Then, the retrieved documents\nare handed over to the reading comprehension module to generate the predicted\nanswers. In experiments, we find that our model can well solve relations that\ncannot be inferred from existing knowledge, and achieve good results on KGC\ndatasets.",
    "descriptor": "",
    "authors": [
      "Xin Lv",
      "Yankai Lin",
      "Zijun Yao",
      "Kaisheng Zeng",
      "Jiajie Zhang",
      "Lei Hou",
      "Juanzi Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05921"
  },
  {
    "id": "arXiv:2210.05922",
    "title": "A Unified Framework for Alternating Offline Model Training and Policy  Learning",
    "abstract": "In offline model-based reinforcement learning (offline MBRL), we learn a\ndynamic model from historically collected data, and subsequently utilize the\nlearned model and fixed datasets for policy learning, without further\ninteracting with the environment. Offline MBRL algorithms can improve the\nefficiency and stability of policy learning over the model-free algorithms.\nHowever, in most of the existing offline MBRL algorithms, the learning\nobjectives for the dynamic models and the policies are isolated from each\nother. Such an objective mismatch may lead to inferior performance of the\nlearned agents. In this paper, we address this issue by developing an iterative\noffline MBRL framework, where we maximize a lower bound of the true expected\nreturn, by alternating between dynamic-model training and policy learning. With\nthe proposed unified model-policy learning framework, we achieve competitive\nperformance on a wide range of continuous-control offline reinforcement\nlearning datasets. Source code is publicly released.",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Shentao Yang",
      "Shujian Zhang",
      "Yihao Feng",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05922"
  },
  {
    "id": "arXiv:2210.05923",
    "title": "Solving combinational optimization problems with evolutionary  single-pixel imaging",
    "abstract": "Single-pixel imaging (SPI) is a novel optical imaging technique by replacing\nthe pixelated sensor array in a conventional camera with a single-pixel\ndetector. In previous works, SPI is usually used for capturing object images or\nperforming image processing tasks. In this work, we propose a SPI scheme for\nprocessing other types of data in addition to images. An Ising machine model is\nimplemented optically with SPI for solving combinational optimization problems\nincluding number partition and graph maximum cut. Simulated and experimental\nresults show that our proposed scheme can optimize the Hamiltonian function\nwith evolutionary illumination patterns.",
    "descriptor": "",
    "authors": [
      "Wei Huang",
      "Jiaxiang Li",
      "Shuming Jiao",
      "Zibang Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Optics (physics.optics)"
    ],
    "url": "https://arxiv.org/abs/2210.05923"
  },
  {
    "id": "arXiv:2210.05927",
    "title": "Efficient Adversarial Training without Attacking: Worst-Case-Aware  Robust Reinforcement Learning",
    "abstract": "Recent studies reveal that a well-trained deep reinforcement learning (RL)\npolicy can be particularly vulnerable to adversarial perturbations on input\nobservations. Therefore, it is crucial to train RL agents that are robust\nagainst any attacks with a bounded budget. Existing robust training methods in\ndeep RL either treat correlated steps separately, ignoring the robustness of\nlong-term rewards, or train the agents and RL-based attacker together, doubling\nthe computational burden and sample complexity of the training process. In this\nwork, we propose a strong and efficient robust training framework for RL, named\nWorst-case-aware Robust RL (WocaR-RL) that directly estimates and optimizes the\nworst-case reward of a policy under bounded l_p attacks without requiring extra\nsamples for learning an attacker. Experiments on multiple environments show\nthat WocaR-RL achieves state-of-the-art performance under various strong\nattacks, and obtains significantly higher training efficiency than prior\nstate-of-the-art robust training methods. The code of this work is available at\nhttps://github.com/umd-huang-lab/WocaR-RL.",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Yongyuan Liang",
      "Yanchao Sun",
      "Ruijie Zheng",
      "Furong Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05927"
  },
  {
    "id": "arXiv:2210.05928",
    "title": "Nonlocal Reconfigurable Intelligent Surfaces for Wireless Communication:  Modeling and Physical Layer Aspects",
    "abstract": "Delivering wireless ultrahigh-speed access at wider coverage is becoming\nconsiderably challenging due to the prohibitive investment costs per user and\nthe necessary shift to range-limited millimeter-wave (mmWave) transmissions.\nReconfigurable intelligent surfaces (RIS) are expected to extend the reach of\nmmWave and TeraHz signals more cost-effectively in situations where fiber\nbackhaul and fronthaul are not accessible or infrastructure densification is\ncostly. This paper investigates some challenges facing this technology,\nparticularly in terms of scalability and the question of what type of RIS\nconfigurations would be appropriate for mmWave networks and what design\nstrategies can be adopted to optimize the performance and minimize the\nsignaling overhead. We conclude that RIS configurations for the wireless\ninfrastructure likely need to be nonlocal (i.e., redirective,\nwavefront-selective) rather than local (i.e., reflective) to support\ncommunications and networking tasks such as integrated fronthaul and access\n(IFA) most efficiently.",
    "descriptor": "",
    "authors": [
      "Amine Mezghani",
      "Faouzi Bellili",
      "Ekram Hossain"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05928"
  },
  {
    "id": "arXiv:2210.05929",
    "title": "Few-shot Backdoor Attacks via Neural Tangent Kernels",
    "abstract": "In a backdoor attack, an attacker injects corrupted examples into the\ntraining set. The goal of the attacker is to cause the final trained model to\npredict the attacker's desired target label when a predefined trigger is added\nto test inputs. Central to these attacks is the trade-off between the success\nrate of the attack and the number of corrupted training examples injected. We\npose this attack as a novel bilevel optimization problem: construct strong\npoison examples that maximize the attack success rate of the trained model. We\nuse neural tangent kernels to approximate the training dynamics of the model\nbeing attacked and automatically learn strong poison examples. We experiment on\nsubclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt\narchitectures on periodic and patch trigger attacks and show that NTBA-designed\npoisoned examples achieve, for example, an attack success rate of 90% with ten\ntimes smaller number of poison examples injected compared to the baseline. We\nprovided an interpretation of the NTBA-designed attacks using the analysis of\nkernel linear regression. We further demonstrate a vulnerability in\noverparametrized deep neural networks, which is revealed by the shape of the\nneural tangent kernel.",
    "descriptor": "\nComments: 20 pages, 13 figures\n",
    "authors": [
      "Jonathan Hayase",
      "Sewoong Oh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05929"
  },
  {
    "id": "arXiv:2210.05931",
    "title": "Explaining Online Reinforcement Learning Decisions of Self-Adaptive  Systems",
    "abstract": "Design time uncertainty poses an important challenge when developing a\nself-adaptive system. As an example, defining how the system should adapt when\nfacing a new environment state, requires understanding the precise effect of an\nadaptation, which may not be known at design time. Online reinforcement\nlearning, i.e., employing reinforcement learning (RL) at runtime, is an\nemerging approach to realizing self-adaptive systems in the presence of design\ntime uncertainty. By using Online RL, the self-adaptive system can learn from\nactual operational data and leverage feedback only available at runtime.\nRecently, Deep RL is gaining interest. Deep RL represents learned knowledge as\na neural network whereby it can generalize over unseen inputs, as well as\nhandle continuous environment states and adaptation actions. A fundamental\nproblem of Deep RL is that learned knowledge is not explicitly represented. For\na human, it is practically impossible to relate the parametrization of the\nneural network to concrete RL decisions and thus Deep RL essentially appears as\na black box. Yet, understanding the decisions made by Deep RL is key to (1)\nincreasing trust, and (2) facilitating debugging. Such debugging is especially\nrelevant for self-adaptive systems, because the reward function, which\nquantifies the feedback to the RL algorithm, must be defined by developers. The\nreward function must be explicitly defined by developers, thus introducing a\npotential for human error. To explain Deep RL for self-adaptive systems, we\nenhance and combine two existing explainable RL techniques from the machine\nlearning literature. The combined technique, XRL-DINE, overcomes the respective\nlimitations of the individual techniques. We present a proof-of-concept\nimplementation of XRL-DINE, as well as qualitative and quantitative results of\napplying XRL-DINE to a self-adaptive system exemplar.",
    "descriptor": "\nComments: Published at 3rd Intl Conference on Autonomic Computing and Self-Organizing Systems (ACSOS 2022)\n",
    "authors": [
      "Felix Feit",
      "Andreas Metzger",
      "Klaus Pohl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05931"
  },
  {
    "id": "arXiv:2210.05935",
    "title": "Optimizing Evaluation Metrics for Multi-Task Learning via the  Alternating Direction Method of Multipliers",
    "abstract": "Multi-task learning (MTL) aims to improve the generalization performance of\nmultiple tasks by exploiting the shared factors among them. Various metrics\n(e.g., F-score, Area Under the ROC Curve) are used to evaluate the performances\nof MTL methods. Most existing MTL methods try to minimize either the\nmisclassified errors for classification or the mean squared errors for\nregression. In this paper, we propose a method to directly optimize the\nevaluation metrics for a large family of MTL problems. The formulation of MTL\nthat directly optimizes evaluation metrics is the combination of two parts: (1)\na regularizer defined on the weight matrix over all tasks, in order to capture\nthe relatedness of these tasks; (2) a sum of multiple structured hinge losses,\neach corresponding to a surrogate of some evaluation metric on one task. This\nformulation is challenging in optimization because both of its parts are\nnon-smooth. To tackle this issue, we propose a novel optimization procedure\nbased on the alternating direction scheme of multipliers, where we decompose\nthe whole optimization problem into a sub-problem corresponding to the\nregularizer and another sub-problem corresponding to the structured hinge\nlosses. For a large family of MTL problems, the first sub-problem has\nclosed-form solutions. To solve the second sub-problem, we propose an efficient\nprimal-dual algorithm via coordinate ascent. Extensive evaluation results\ndemonstrate that, in a large family of MTL problems, the proposed MTL method of\ndirectly optimization evaluation metrics has superior performance gains against\nthe corresponding baseline methods.",
    "descriptor": "",
    "authors": [
      "Ge-Yang Ke",
      "Yan Pan",
      "Jian Yin",
      "Chang-Qin Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05935"
  },
  {
    "id": "arXiv:2210.05936",
    "title": "Equal Experience in Recommender Systems",
    "abstract": "We explore the fairness issue that arises in recommender systems. Biased data\ndue to inherent stereotypes of particular groups (e.g., male students' average\nrating on mathematics is often higher than that on humanities, and vice versa\nfor females) may yield a limited scope of suggested items to a certain group of\nusers. Our main contribution lies in the introduction of a novel fairness\nnotion (that we call equal experience), which can serve to regulate such\nunfairness in the presence of biased data. The notion captures the degree of\nthe equal experience of item recommendations across distinct groups. We propose\nan optimization framework that incorporates the fairness notion as a\nregularization term, as well as introduce computationally-efficient algorithms\nthat solve the optimization. Experiments on synthetic and benchmark real\ndatasets demonstrate that the proposed framework can indeed mitigate such\nunfairness while exhibiting a minor degradation of recommendation accuracy.",
    "descriptor": "\nComments: 18 pages, 9 figures\n",
    "authors": [
      "Jaewoong Cho",
      "Moonseok Choi",
      "Changho Suh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05936"
  },
  {
    "id": "arXiv:2210.05938",
    "title": "Robust Models are less Over-Confident",
    "abstract": "Despite the success of convolutional neural networks (CNNs) in many academic\nbenchmarks for computer vision tasks, their application in the real-world is\nstill facing fundamental challenges. One of these open problems is the inherent\nlack of robustness, unveiled by the striking effectiveness of adversarial\nattacks. Current attack methods are able to manipulate the network's prediction\nby adding specific but small amounts of noise to the input. In turn,\nadversarial training (AT) aims to achieve robustness against such attacks and\nideally a better model generalization ability by including adversarial samples\nin the trainingset. However, an in-depth analysis of the resulting robust\nmodels beyond adversarial robustness is still pending. In this paper, we\nempirically analyze a variety of adversarially trained models that achieve high\nrobust accuracies when facing state-of-the-art attacks and we show that AT has\nan interesting side-effect: it leads to models that are significantly less\noverconfident with their decisions, even on clean data than non-robust models.\nFurther, our analysis of robust models shows that not only AT but also the\nmodel's building blocks (like activation functions and pooling) have a strong\ninfluence on the models' prediction confidences. Data & Project website:\nhttps://github.com/GeJulia/robustness_confidences_evaluation",
    "descriptor": "\nComments: accepted at NeuRips 2022\n",
    "authors": [
      "Julia Grabinski",
      "Paul Gavrikov",
      "Janis Keuper",
      "Margret Keuper"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05938"
  },
  {
    "id": "arXiv:2210.05941",
    "title": "Decomposed Knowledge Distillation for Class-Incremental Semantic  Segmentation",
    "abstract": "Class-incremental semantic segmentation (CISS) labels each pixel of an image\nwith a corresponding object/stuff class continually. To this end, it is crucial\nto learn novel classes incrementally without forgetting previously learned\nknowledge. Current CISS methods typically use a knowledge distillation (KD)\ntechnique for preserving classifier logits, or freeze a feature extractor, to\navoid the forgetting problem. The strong constraints, however, prevent learning\ndiscriminative features for novel classes. We introduce a CISS framework that\nalleviates the forgetting problem and facilitates learning novel classes\neffectively. We have found that a logit can be decomposed into two terms. They\nquantify how likely an input belongs to a particular class or not, providing a\nclue for a reasoning process of a model. The KD technique, in this context,\npreserves the sum of two terms (i.e., a class logit), suggesting that each\ncould be changed and thus the KD does not imitate the reasoning process. To\nimpose constraints on each term explicitly, we propose a new decomposed\nknowledge distillation (DKD) technique, improving the rigidity of a model and\naddressing the forgetting problem more effectively. We also introduce a novel\ninitialization method to train new classifiers for novel classes. In CISS, the\nnumber of negative training samples for novel classes is not sufficient to\ndiscriminate old classes. To mitigate this, we propose to transfer knowledge of\nnegatives to the classifiers successively using an auxiliary classifier,\nboosting the performance significantly. Experimental results on standard CISS\nbenchmarks demonstrate the effectiveness of our framework.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Donghyeon Baek",
      "Youngmin Oh",
      "Sanghoon Lee",
      "Junghyup Lee",
      "Bumsub Ham"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05941"
  },
  {
    "id": "arXiv:2210.05944",
    "title": "Dynamic Clustering Network for Unsupervised Semantic Segmentation",
    "abstract": "Recently, the ability of self-supervised Vision Transformer (ViT) to\nrepresent pixel-level semantic relationships promotes the development of\nunsupervised dense prediction tasks. In this work, we investigate transferring\nself-supervised ViT to unsupervised semantic segmentation task. According to\nthe analysis that the pixel-level representations of self-supervised ViT within\na single image achieve good intra-class compactness and inter-class\ndiscrimination, we propose the Dynamic Clustering Network (DCN) to dynamically\ninfer the underlying cluster centers for different images. By training with the\nproposed modularity loss, the DCN learns to project a set of prototypes to\ncluster centers for pixel representations in each image and assign pixels to\ndifferent clusters, resulting on dividing each image to class-agnostic regions.\nFor achieving unsupervised semantic segmentation task, we treat it as a region\nclassification problem. Based on the regions produced by the DCN, we explore\ndifferent ways to extract region-level representations and classify them in an\nunsupervised manner. We demonstrate the effectiveness of the proposed method\ntrough experiments on unsupervised semantic segmentation, and achieve\nstate-of-the-art performance on PASCAL VOC 2012 unsupervised semantic\nsegmentation task.",
    "descriptor": "",
    "authors": [
      "Kehan Li",
      "Zhennan Wang",
      "Zesen Cheng",
      "Runyi Yu",
      "Yian Zhao",
      "Guoli Song",
      "Li Yuan",
      "Jie Chen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05944"
  },
  {
    "id": "arXiv:2210.05947",
    "title": "Adaptive Dual Channel Convolution Hypergraph Representation Learning for  Technological Intellectual Property",
    "abstract": "In the age of big data, the demand for hidden information mining in\ntechnological intellectual property is increasing in discrete countries.\nDefinitely, a considerable number of graph learning algorithms for\ntechnological intellectual property have been proposed. The goal is to model\nthe technological intellectual property entities and their relationships\nthrough the graph structure and use the neural network algorithm to extract the\nhidden structure information in the graph. However, most of the existing graph\nlearning algorithms merely focus on the information mining of binary relations\nin technological intellectual property, ignoring the higherorder information\nhidden in non-binary relations. Therefore, a hypergraph neural network model\nbased on dual channel convolution is proposed. For the hypergraph constructed\nfrom technological intellectual property data, the hypergraph channel and the\nline expanded graph channel of the hypergraph are used to learn the hypergraph,\nand the attention mechanism is introduced to adaptively fuse the output\nrepresentations of the two channels. The proposed model outperforms the\nexisting approaches on a variety of datasets.",
    "descriptor": "",
    "authors": [
      "Yuxin Liu",
      "Yawen Li",
      "Yingxia Shao",
      "Zeli Guan"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05947"
  },
  {
    "id": "arXiv:2210.05950",
    "title": "ZITS++: Image Inpainting by Improving the Incremental Transformer on  Structural Priors",
    "abstract": "The image inpainting task fills missing areas of a corrupted image. Despite\nimpressive results have been achieved recently, it is still challenging to\nrestore corrupted images with both vivid textures and reasonable structures.\nSome previous methods only tackle regular textures while losing holistic\nstructures limited by receptive fields of Convolution Neural Networks (CNNs).\nTo this end, we study learning a Zero-initialized residual addition based\nIncremental Transformer on Structural priors (ZITS++), an improved model over\nour conference ZITS model. Specifically, given one corrupt image, we present\nthe Transformer Structure Restorer (TSR) module to restore holistic structural\npriors at low image resolution, which are further upsampled by Simple Structure\nUpsampler (SSU) module to higher image resolution. Further, to well recover\nimage texture details, we take the Fourier CNN Texture Restoration (FTR)\nmodule, which has both the Fourier and large-kernel attention convolutions.\nTypically, FTR can be independently pre-trained without image structural\npriors. Furthermore, to enhance the FTR, the upsampled structural priors from\nTSR are further processed by Structure Feature Encoder (SFE), and updating the\nFTR by a novel incremental training strategy of Zero-initialized Residual\nAddition (ZeroRA). Essentially, a new masking positional encoding is proposed\nto encode the large irregular masks. Extensive experiments on various datasets\nvalidate the efficacy of our model compared with other competitors. We also\nconduct extensive ablation to compare and verify various priors for image\ninpainting tasks.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2203.00867\n",
    "authors": [
      "Chenjie Cao",
      "Qiaole Dong",
      "Yanwei Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05950"
  },
  {
    "id": "arXiv:2210.05953",
    "title": "Classification by estimating the cumulative distribution function for  small data",
    "abstract": "In this paper, we study the classification problem by estimating the\nconditional probability function of the given data. Different from the\ntraditional expected risk estimation theory on empirical data, we calculate the\nprobability via Fredholm equation, this leads to estimate the distribution of\nthe data. Based on the Fredholm equation, a new expected risk estimation theory\nby estimating the cumulative distribution function is presented. The main\ncharacteristics of the new expected risk estimation is to measure the risk on\nthe distribution of the input space. The corresponding empirical risk\nestimation is also presented, and an $\\varepsilon$-insensitive $L_{1}$\ncumulative support vector machines ($\\varepsilon$-$L_{1}$VSVM) is proposed by\nintroducing an insensitive loss. It is worth mentioning that the classification\nmodels and the classification evaluation indicators based on the new mechanism\nare different from the traditional one. Experimental results show the\neffectiveness of the proposed $\\varepsilon$-$L_{1}$VSVM and the corresponding\ncumulative distribution function indicator on validity and interpretability of\nsmall data classification.",
    "descriptor": "\nComments: 39 pages, 34 figures, references added\n",
    "authors": [
      "Meng-Xian Zhua",
      "Yuan-Hai Shao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05953"
  },
  {
    "id": "arXiv:2210.05954",
    "title": "Projective Transformation Rectification for Camera-captured Chest X-ray  Photograph Interpretation with Synthetic Data",
    "abstract": "Automatic interpretation on smartphone-captured chest X-ray (CXR) photographs\nis challenging due to the geometric distortion (projective transformation)\ncaused by the non-ideal camera position. In this paper, we proposed an\ninnovative deep learning-based Projective Transformation Rectification Network\n(PTRN) to automatically rectify such distortions by predicting the projective\ntransformation matrix. PTRN is trained on synthetic data to avoid the expensive\ncollection of natural data. Therefore, we proposed an innovative synthetic data\nframework that accounts for the visual attributes of natural photographs\nincluding screen, background, illuminations, and visual artifacts, and generate\nsynthetic CXR photographs and projective transformation matrices as the\nground-truth labels for training PTRN. Finally, smartphone-captured CXR\nphotographs are automatically rectified by trained PTRN and interpreted by a\nclassifier trained on high-quality digital CXRs to produce final interpretation\nresults. In the CheXphoto CXR photograph interpretation competition released by\nthe Stanford University Machine Learning Group, our approach achieves a huge\nperformance improvement and won first place (ours 0.850, second-best 0.762, in\nAUC). A deeper analysis demonstrates that the use of PTRN successfully achieves\nthe performance on CXR photographs to the same level as on digital CXRs,\nindicating PTRN can eliminate all negative impacts of projective transformation\nto the interpretation performance. Additionally, there are many real-world\nscenarios where distorted photographs have to be used for image classification,\nour PTRN can be used to solve those similar problems due to its generality\ndesign.",
    "descriptor": "",
    "authors": [
      "Chak Fong Chong",
      "Yapeng Wang",
      "Benjamin Ng",
      "Xu Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05954"
  },
  {
    "id": "arXiv:2210.05956",
    "title": "Towards Theoretically Inspired Neural Initialization Optimization",
    "abstract": "Automated machine learning has been widely explored to reduce human efforts\nin designing neural architectures and looking for proper hyperparameters. In\nthe domain of neural initialization, however, similar automated techniques have\nrarely been studied. Most existing initialization methods are handcrafted and\nhighly dependent on specific architectures. In this paper, we propose a\ndifferentiable quantity, named GradCosine, with theoretical insights to\nevaluate the initial state of a neural network. Specifically, GradCosine is the\ncosine similarity of sample-wise gradients with respect to the initialized\nparameters. By analyzing the sample-wise optimization landscape, we show that\nboth the training and test performance of a network can be improved by\nmaximizing GradCosine under gradient norm constraint. Based on this\nobservation, we further propose the neural initialization optimization (NIO)\nalgorithm. Generalized from the sample-wise analysis into the real batch\nsetting, NIO is able to automatically look for a better initialization with\nnegligible cost compared with the training time. With NIO, we improve the\nclassification performance of a variety of neural architectures on CIFAR-10,\nCIFAR-100, and ImageNet. Moreover, we find that our method can even help to\ntrain large vision Transformer architecture without warmup.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Yibo Yang",
      "Hong Wang",
      "Haobo Yuan",
      "Zhouchen Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05956"
  },
  {
    "id": "arXiv:2210.05958",
    "title": "Bridging the Gap Between Vision Transformers and Convolutional Neural  Networks on Small Datasets",
    "abstract": "There still remains an extreme performance gap between Vision Transformers\n(ViTs) and Convolutional Neural Networks (CNNs) when training from scratch on\nsmall datasets, which is concluded to the lack of inductive bias. In this\npaper, we further consider this problem and point out two weaknesses of ViTs in\ninductive biases, that is, the spatial relevance and diverse channel\nrepresentation. First, on spatial aspect, objects are locally compact and\nrelevant, thus fine-grained feature needs to be extracted from a token and its\nneighbors. While the lack of data hinders ViTs to attend the spatial relevance.\nSecond, on channel aspect, representation exhibits diversity on different\nchannels. But the scarce data can not enable ViTs to learn strong enough\nrepresentation for accurate recognition. To this end, we propose Dynamic Hybrid\nVision Transformer (DHVT) as the solution to enhance the two inductive biases.\nOn spatial aspect, we adopt a hybrid structure, in which convolution is\nintegrated into patch embedding and multi-layer perceptron module, forcing the\nmodel to capture the token features as well as their neighboring features. On\nchannel aspect, we introduce a dynamic feature aggregation module in MLP and a\nbrand new \"head token\" design in multi-head self-attention module to help\nre-calibrate channel representation and make different channel group\nrepresentation interacts with each other. The fusion of weak channel\nrepresentation forms a strong enough representation for classification. With\nthis design, we successfully eliminate the performance gap between CNNs and\nViTs, and our DHVT achieves a series of state-of-the-art performance with a\nlightweight model, 85.68% on CIFAR-100 with 22.8M parameters, 82.3% on\nImageNet-1K with 24.0M parameters. Code is available at\nhttps://github.com/ArieSeirack/DHVT.",
    "descriptor": "",
    "authors": [
      "Zhiying Lu",
      "Hongtao Xie",
      "Chuanbin Liu",
      "Yongdong Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05958"
  },
  {
    "id": "arXiv:2210.05959",
    "title": "JuryGCN: Quantifying Jackknife Uncertainty on Graph Convolutional  Networks",
    "abstract": "Graph Convolutional Network (GCN) has exhibited strong empirical performance\nin many real-world applications. The vast majority of existing works on GCN\nprimarily focus on the accuracy while ignoring how confident or uncertain a GCN\nis with respect to its predictions. Despite being a cornerstone of trustworthy\ngraph mining, uncertainty quantification on GCN has not been well studied and\nthe scarce existing efforts either fail to provide deterministic quantification\nor have to change the training procedure of GCN by introducing additional\nparameters or architectures. In this paper, we propose the first\nfrequentist-based approach named JuryGCN in quantifying the uncertainty of GCN,\nwhere the key idea is to quantify the uncertainty of a node as the width of\nconfidence interval by a jackknife estimator. Moreover, we leverage the\ninfluence functions to estimate the change in GCN parameters without\nre-training to scale up the computation. The proposed JuryGCN is capable of\nquantifying uncertainty deterministically without modifying the GCN\narchitecture or introducing additional parameters. We perform extensive\nexperimental evaluation on real-world datasets in the tasks of both active\nlearning and semi-supervised node classification, which demonstrate the\nefficacy of the proposed method.",
    "descriptor": "\nComments: Accepted to KDD'22\n",
    "authors": [
      "Jian Kang",
      "Qinghai Zhou",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.05959"
  },
  {
    "id": "arXiv:2210.05965",
    "title": "Resolving the Approximability of Offline and Online Non-monotone  DR-Submodular Maximization over General Convex Sets",
    "abstract": "In recent years, maximization of DR-submodular continuous functions became an\nimportant research field, with many real-worlds applications in the domains of\nmachine learning, communication systems, operation research and economics. Most\nof the works in this field study maximization subject to down-closed convex set\nconstraints due to an inapproximability result by Vondr\\'ak (2013). However,\nDurr et al. (2021) showed that one can bypass this inapproximability by proving\napproximation ratios that are functions of $m$, the minimum\n$\\ell_{\\infty}$-norm of any feasible vector. Given this observation, it is\npossible to get results for maximizing a DR-submodular function subject to\ngeneral convex set constraints, which has led to multiple works on this\nproblem. The most recent of which is a polynomial time $\\tfrac{1}{4}(1 -\nm)$-approximation offline algorithm due to Du (2022). However, only a\nsub-exponential time $\\tfrac{1}{3\\sqrt{3}}(1 - m)$-approximation algorithm is\nknown for the corresponding online problem. In this work, we present a\npolynomial time online algorithm matching the $\\tfrac{1}{4}(1 -\nm)$-approximation of the state-of-the-art offline algorithm. We also present an\ninapproximability result showing that our online algorithm and Du's (2022)\noffline algorithm are both optimal in a strong sense. Finally, we study the\nempirical performance of our algorithm and the algorithm of Du (which was only\ntheoretically studied previously), and show that they consistently outperform\npreviously suggested algorithms on revenue maximization, location summarization\nand quadratic programming applications.",
    "descriptor": "\nComments: 29 pages, 4 figures\n",
    "authors": [
      "Loay Mualem",
      "Moran Feldman"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05965"
  },
  {
    "id": "arXiv:2210.05967",
    "title": "Effect of sociability and curiosity of senior developers in building  agile scrum team competency",
    "abstract": "This paper aims to investigate the mechanisms that contribute to propagation\nof competence in an Agile Scrum team. This study seeks to challenge the\ntraditional view of bounded rationality (BR). An Agile Scrum team (Team) is\nexpected to build problem solving competence quickly as the expected ramp up\ntime continues to shrink. But the team has a mixture of expertise, competence\nand sociability levels that affect out-of-the-box performance. The objective is\nto expand BR into the social realm and see how teams can self-organize and\nreconfigure to allow effective problem solving. Studies have shown that\nagent-based computational simulation is an appropriate technique to explore\nthis point from a theoretical perspective. (Fioretti, 2013) (Secchi, 2015). The\nfirst step is to define the problem, discuss how senior team members exhibit\nhigh curiosity and apply sociability and cognitive resources to develop overall\nteam competence. This dynamic is modeled and simulated in NetLogoR and the\nresults are analyzed. Finally, some key findings are presented and discussed.",
    "descriptor": "",
    "authors": [
      "Ravi Kalluri"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)",
      "Multiagent Systems (cs.MA)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.05967"
  },
  {
    "id": "arXiv:2210.05968",
    "title": "Boosting the Transferability of Adversarial Attacks with Reverse  Adversarial Perturbation",
    "abstract": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarial\nexamples, which can produce erroneous predictions by injecting imperceptible\nperturbations. In this work, we study the transferability of adversarial\nexamples, which is significant due to its threat to real-world applications\nwhere model architecture or parameters are usually unknown. Many existing works\nreveal that the adversarial examples are likely to overfit the surrogate model\nthat they are generated from, limiting its transfer attack performance against\ndifferent target models. To mitigate the overfitting of the surrogate model, we\npropose a novel attack method, dubbed reverse adversarial perturbation (RAP).\nSpecifically, instead of minimizing the loss of a single adversarial point, we\nadvocate seeking adversarial example located at a region with unified low loss\nvalue, by injecting the worst-case perturbation (the reverse adversarial\nperturbation) for each step of the optimization procedure. The adversarial\nattack with RAP is formulated as a min-max bi-level optimization problem. By\nintegrating RAP into the iterative process for attacks, our method can find\nmore stable adversarial examples which are less sensitive to the changes of\ndecision boundary, mitigating the overfitting of the surrogate model.\nComprehensive experimental comparisons demonstrate that RAP can significantly\nboost adversarial transferability. Furthermore, RAP can be naturally combined\nwith many existing black-box attack techniques, to further boost the\ntransferability. When attacking a real-world image recognition system, Google\nCloud Vision API, we obtain 22% performance improvement of targeted attacks\nover the compared method. Our codes are available at\nhttps://github.com/SCLBD/Transfer_attack_RAP.",
    "descriptor": "\nComments: NeurIPS 2022 conference paper\n",
    "authors": [
      "Zeyu Qin",
      "Yanbo Fan",
      "Yi Liu",
      "Li Shen",
      "Yong Zhang",
      "Jue Wang",
      "Baoyuan Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05968"
  },
  {
    "id": "arXiv:2210.05972",
    "title": "Unsupervised Learning of Equivariant Structure from Sequences",
    "abstract": "In this study, we present meta-sequential prediction (MSP), an unsupervised\nframework to learn the symmetry from the time sequence of length at least\nthree. Our method leverages the stationary property (e.g. constant velocity,\nconstant acceleration) of the time sequence to learn the underlying equivariant\nstructure of the dataset by simply training the encoder-decoder model to be\nable to predict the future observations. We will demonstrate that, with our\nframework, the hidden disentangled structure of the dataset naturally emerges\nas a by-product by applying simultaneous block-diagonalization to the\ntransition operators in the latent space, the procedure which is commonly used\nin representation theory to decompose the feature-space based on the type of\nresponse to group actions. We will showcase our method from both empirical and\ntheoretical perspectives. Our result suggests that finding a simple structured\nrelation and learning a model with extrapolation capability are two sides of\nthe same coin. The code is available at\nhttps://github.com/takerum/meta_sequential_prediction.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Takeru Miyato",
      "Masanori Koyama",
      "Kenji Fukumizu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05972"
  },
  {
    "id": "arXiv:2210.05974",
    "title": "Clustering Embedding Tables, Without First Learning Them",
    "abstract": "To work with categorical features, machine learning systems employ embedding\ntables. These tables can become exceedingly large in modern recommendation\nsystems, necessitating the development of new methods for fitting them in\nmemory, even during training.\nSome of the most successful methods for table compression are Product- and\nResidual Vector Quantization (Gray & Neuhoff, 1998). These methods replace\ntable rows with references to k-means clustered \"codewords.\" Unfortunately,\nthis means they must first know the table before compressing it, so they can\nonly save memory during inference, not training. Recent work has used\nhashing-based approaches to minimize memory usage during training, but the\ncompression obtained is inferior to that obtained by \"post-training\"\nquantization.\nWe show that the best of both worlds may be obtained by combining techniques\nbased on hashing and clustering. By first training a hashing-based \"sketch\",\nthen clustering it, and then training the clustered quantization, our method\nachieves compression ratios close to those of post-training quantization with\nthe training time memory reductions of hashing-based methods.\nWe show experimentally that our method provides better compression and/or\naccuracy that previous methods, and we prove that our method always converges\nto the optimal embedding table for least-squares training.",
    "descriptor": "",
    "authors": [
      "Henry Ling-Hei Tsang",
      "Thomas Dybdahl Ahle"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.05974"
  },
  {
    "id": "arXiv:2210.05976",
    "title": "Human Joint Kinematics Diffusion-Refinement for Stochastic Motion  Prediction",
    "abstract": "Stochastic human motion prediction aims to forecast multiple plausible future\nmotions given a single pose sequence from the past. Most previous works focus\non designing elaborate losses to improve the accuracy, while the diversity is\ntypically characterized by randomly sampling a set of latent variables from the\nlatent prior, which is then decoded into possible motions. This joint training\nof sampling and decoding, however, suffers from posterior collapse as the\nlearned latent variables tend to be ignored by a strong decoder, leading to\nlimited diversity. Alternatively, inspired by the diffusion process in\nnonequilibrium thermodynamics, we propose MotionDiff, a diffusion probabilistic\nmodel to treat the kinematics of human joints as heated particles, which will\ndiffuse from original states to a noise distribution. This process offers a\nnatural way to obtain the \"whitened\" latents without any trainable parameters,\nand human motion prediction can be regarded as the reverse diffusion process\nthat converts the noise distribution into realistic future motions conditioned\non the observed sequence. Specifically, MotionDiff consists of two parts: a\nspatial-temporal transformer-based diffusion network to generate diverse yet\nplausible motions, and a graph convolutional network to further refine the\noutputs. Experimental results on two datasets demonstrate that our model yields\nthe competitive performance in terms of both accuracy and diversity.",
    "descriptor": "",
    "authors": [
      "Dong Wei",
      "Huaijiang Sun",
      "Bin Li",
      "Jianfeng Lu",
      "Weiqing Li",
      "Xiaoning Sun",
      "Shengxiang Hu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05976"
  },
  {
    "id": "arXiv:2210.05977",
    "title": "BORA: Bayesian Optimization for Resource Allocation",
    "abstract": "Optimal resource allocation is gaining a renewed interest due its relevance\nas a core problem in managing, over time, cloud and high-performance computing\nfacilities. Semi-Bandit Feedback (SBF) is the reference method for efficiently\nsolving this problem. In this paper we propose (i) an extension of the optimal\nresource allocation to a more general class of problems, specifically with\nresources availability changing over time, and (ii) Bayesian Optimization as a\nmore efficient alternative to SBF. Three algorithms for Bayesian Optimization\nfor Resource Allocation, namely BORA, are presented, working on allocation\ndecisions represented as numerical vectors or distributions. The second option\nrequired to consider the Wasserstein distance as a more suitable metric to use\ninto one of the BORA algorithms. Results on (i) the original SBF case study\nproposed in the literature, and (ii) a real-life application (i.e., the\noptimization of multi-channel marketing) empirically prove that BORA is a more\nefficient and effective learning-and-optimization framework than SBF.",
    "descriptor": "\nComments: 31 pages, 12 figures\n",
    "authors": [
      "Antonio Candelieri",
      "Andrea Ponti",
      "Francesco Archetti"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05977"
  },
  {
    "id": "arXiv:2210.05980",
    "title": "Efficient Offline Policy Optimization with a Learned Model",
    "abstract": "MuZero Unplugged presents a promising approach for offline policy learning\nfrom logged data. It conducts Monte-Carlo Tree Search (MCTS) with a learned\nmodel and leverages Reanalyze algorithm to learn purely from offline data. For\ngood performance, MCTS requires accurate learned models and a large number of\nsimulations, thus costing huge computing time. This paper investigates a few\nhypotheses where MuZero Unplugged may not work well under the offline RL\nsettings, including 1) learning with limited data coverage; 2) learning from\noffline data of stochastic environments; 3) improperly parameterized models\ngiven the offline data; 4) with a low compute budget. We propose to use a\nregularized one-step look-ahead approach to tackle the above issues. Instead of\nplanning with the expensive MCTS, we use the learned model to construct an\nadvantage estimation based on a one-step rollout. Policy improvements are\ntowards the direction that maximizes the estimated advantage with\nregularization of the dataset. We conduct extensive empirical studies with\nBSuite environments to verify the hypotheses and then run our algorithm on the\nRL Unplugged Atari benchmark. Experimental results show that our proposed\napproach achieves stable performance even with an inaccurate learned model. On\nthe large-scale Atari benchmark, the proposed method outperforms MuZero\nUnplugged by 43%. Most significantly, it uses only 5.6% wall-clock time (i.e.,\n1 hour) compared to MuZero Unplugged (i.e., 17.8 hours) to achieve a 150% IQM\nnormalized score with the same hardware and software stacks.",
    "descriptor": "",
    "authors": [
      "Zichen Liu",
      "Siyi Li",
      "Wee Sun Lee",
      "Shuicheng Yan",
      "Zhongwen Xu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05980"
  },
  {
    "id": "arXiv:2210.05982",
    "title": "A nearly optimal randomized algorithm for explorable heap selection",
    "abstract": "Explorable heap selection is the problem of selecting the $n$th smallest\nvalue in a binary heap. The key values can only be accessed by traversing\nthrough the underlying infinite binary tree, and the complexity of the\nalgorithm is measured by the total distance traveled in the tree (each edge has\nunit cost). This problem was originally proposed as a model to study search\nstrategies for the branch-and-bound algorithm with storage restrictions by\nKarp, Saks and Widgerson (FOCS '86), who gave deterministic and randomized\n$n\\cdot \\exp(O(\\sqrt{\\log{n}}))$ time algorithms using $O(\\log(n)^{2.5})$ and\n$O(\\sqrt{\\log n})$ space respectively. We present a new randomized algorithm\nwith running time $O(n\\log(n)^3)$ using $O(\\log n)$ space, substantially\nimproving the previous best randomized running time at the expense of slightly\nincreased space usage. We also show an $\\Omega(\\log(n)n/\\log(\\log(n)))$ for any\nalgorithm that solves the problem in the same amount of space, indicating that\nour algorithm is nearly optimal.",
    "descriptor": "",
    "authors": [
      "Sander Borst",
      "Daniel Dadush",
      "Sophie Huiberts",
      "Danish Kashaev"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.05982"
  },
  {
    "id": "arXiv:2210.05984",
    "title": "RING++: Roto-translation Invariant Gram for Global Localization on a  Sparse Scan Map",
    "abstract": "Global localization plays a critical role in many robot applications.\nLiDAR-based global localization draws the community's focus with its robustness\nagainst illumination and seasonal changes. To further improve the localization\nunder large viewpoint differences, we propose RING++ which has roto-translation\ninvariant representation for place recognition, and global convergence for both\nrotation and translation estimation. With the theoretical guarantee, RING++ is\nable to address the large viewpoint difference using a lightweight map with\nsparse scans. In addition, we derive sufficient conditions of feature\nextractors for the representation preserving the roto-translation invariance,\nmaking RING++ a framework applicable to generic multi-channel features. To the\nbest of our knowledge, this is the first learning-free framework to address all\nsubtasks of global localization in the sparse scan map. Validations on\nreal-world datasets show that our approach demonstrates better performance than\nstate-of-the-art learning-free methods, and competitive performance with\nlearning-based methods. Finally, we integrate RING++ into a multi-robot/session\nSLAM system, performing its effectiveness in collaborative applications.",
    "descriptor": "",
    "authors": [
      "Xuecheng Xu",
      "Sha Lu",
      "Jun Wu",
      "Haojian Lu",
      "Qiuguo Zhu",
      "Yiyi Liao",
      "Rong Xiong",
      "Yue Wang"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05984"
  },
  {
    "id": "arXiv:2210.05989",
    "title": "Probabilities Are Not Enough: Formal Controller Synthesis for Stochastic  Dynamical Models with Epistemic Uncertainty",
    "abstract": "Capturing uncertainty in models of complex dynamical systems is crucial to\ndesigning safe controllers. Stochastic noise causes aleatoric uncertainty,\nwhereas imprecise knowledge of model parameters and the presence of external\ndisturbances lead to epistemic uncertainty. Several approaches use formal\nabstractions to synthesize policies that satisfy temporal specifications\nrelated to safety and reachability. However, the underlying models exclusively\ncapture aleatoric but not epistemic uncertainty, and thus require that model\nparameters and disturbances are known precisely. Our contribution to overcoming\nthis restriction is a novel abstraction-based controller synthesis method for\ncontinuous-state models with stochastic noise, uncertain parameters, and\nexternal disturbances. By sampling techniques and robust analysis, we capture\nboth aleatoric and epistemic uncertainty, with a user-specified confidence\nlevel, in the transition probability intervals of a so-called interval Markov\ndecision process (iMDP). We then synthesize an optimal policy on this abstract\niMDP, which translates (with the specified confidence level) to a feedback\ncontroller for the continuous model, with the same performance guarantees. Our\nexperimental benchmarks confirm that accounting for epistemic uncertainty leads\nto controllers that are more robust against variations in parameter values.",
    "descriptor": "",
    "authors": [
      "Thom Badings",
      "Licio Romao",
      "Alessandro Abate",
      "Nils Jansen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.05989"
  },
  {
    "id": "arXiv:2210.05990",
    "title": "GGViT:Multistream Vision Transformer Network in Face2Face Facial  Reenactment Detection",
    "abstract": "Detecting manipulated facial images and videos on social networks has been an\nurgent problem to be solved. The compression of videos on social media has\ndestroyed some pixel details that could be used to detect forgeries. Hence, it\nis crucial to detect manipulated faces in videos of different quality. We\npropose a new multi-stream network architecture named GGViT, which utilizes\nglobal information to improve the generalization of the model. The embedding of\nthe whole face extracted by ViT will guide each stream network. Through a large\nnumber of experiments, we have proved that our proposed model achieves\nstate-of-the-art classification accuracy on FF++ dataset, and has been greatly\nimproved on scenarios of different compression rates. The accuracy of Raw/C23,\nRaw/C40 and C23/C40 was increased by 24.34%, 15.08% and 10.14% respectively.",
    "descriptor": "\nComments: 6 pages,4 figures,to be published in ICPR2022\n",
    "authors": [
      "Haotian Wu",
      "Peipei Wang",
      "Xin Wang",
      "Ji Xiang",
      "Rui Gong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05990"
  },
  {
    "id": "arXiv:2210.05991",
    "title": "Distilling Knowledge from Language Models for Video-based Action  Anticipation",
    "abstract": "Anticipating future actions in a video is useful for many autonomous and\nassistive technologies. Prior action anticipation work mostly treats this as a\nvision modality problem, where the models learn the task information primarily\nfrom the video features in the target action anticipation datasets. In this\nwork, we propose a method to make use of the text-modality that is available\nduring the training, to bring in complementary information that is not present\nin the target action anticipation datasets. In particular, we leverage\npre-trained language models to build a text-modality teacher that is able to\npredict future actions based on text labels of the past actions extracted from\nthe input video. To further adapt the teacher to the target domain (cooking),\nwe also pretrain the teacher on textual instructions from a recipes dataset\n(Recipe1M). Then, we distill the knowledge gained by the text-modality teacher\ninto a vision-modality student to further improve it's performance. We\nempirically evaluate this simple cross-modal distillation strategy on two video\ndatasets EGTEA-GAZE+ and EPIC-KITCHEN 55. Distilling this text-modality\nknowledge into a strong vision model (Anticipative Vision Transformer) yields\nconsistent gains across both datasets, 3.5% relative improvement on top1 class\nmean recall for EGTEA-GAZE+, 7.2% on top5 many-shot class mean recall for\nEPIC-KITCHEN 55 and achieves new state-of-the-results.",
    "descriptor": "",
    "authors": [
      "Sayontan Ghosh",
      "Tanvi Aggarwal",
      "Minh Hoai",
      "Niranjan Balasubramanian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05991"
  },
  {
    "id": "arXiv:2210.05992",
    "title": "Fast Convergence to Unanimity in Dense Erd\u0151s-R\u00e9nyi Graphs",
    "abstract": "Majority dynamics on the binomial Erd\\H{o}s-R\\'enyi graph $\\mathsf{G}(n,p)$\nwith $p=\\lambda/\\sqrt{n}$ is studied. In this process, each vertex has a state\nin $\\{0,1\\}$ and at each round, every vertex adopts the state of the majority\nof its neighbors, retaining its state in the case of a tie. It was conjectured\nby Benjamini et al. and proved by Fountoulakis et al. that this process reaches\nunanimity with high probability in at most four rounds. By adding some extra\nrandomness and allowing the underlying graph to be drawn anew in each\ncommunication round, we improve on their result and prove that this process\nreaches consensus in only three communication rounds with probability\napproaching $1$ as $n$ grows to infinity. We also provide a converse result,\nshowing that three rounds are not only sufficient, but also necessary.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:2104.04996\n",
    "authors": [
      "Ran Tamir"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Discrete Mathematics (cs.DM)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.05992"
  },
  {
    "id": "arXiv:2210.05993",
    "title": "Feasible and Desirable Counterfactual Generation by Preserving Human  Defined Constraints",
    "abstract": "We present a human-in-the-loop approach to generate counterfactual (CF)\nexplanations that preserve global and local feasibility constraints. Global\nfeasibility constraints refer to the causal constraints that are necessary for\ngenerating actionable CF explanation. Assuming a domain expert with knowledge\non unary and binary causal constraints, our approach efficiently employs this\nknowledge to generate CF explanation by rejecting gradient steps that violate\nthese constraints. Local feasibility constraints encode end-user's constraints\nfor generating desirable CF explanation. We extract these constraints from the\nend-user of the model and exploit them during CF generation via user-defined\ndistance metric. Through user studies, we demonstrate that incorporating causal\nconstraints during CF generation results in significantly better explanations\nin terms of feasibility and desirability for participants. Adopting local and\nglobal feasibility constraints simultaneously, although improves user\nsatisfaction, does not significantly improve desirability of the participants\ncompared to only incorporating global constraints.",
    "descriptor": "",
    "authors": [
      "Homayun Afrabandpey",
      "Michael Spranger"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.05993"
  },
  {
    "id": "arXiv:2210.05996",
    "title": "Line Search-Based Feature Transformation for Fast, Stable, and Tunable  Content-Style Control in Photorealistic Style Transfer",
    "abstract": "Photorealistic style transfer is the task of synthesizing a realistic-looking\nimage when adapting the content from one image to appear in the style of\nanother image. Modern models commonly embed a transformation that fuses\nfeatures describing the content image and style image and then decodes the\nresulting feature into a stylized image. We introduce a general-purpose\ntransformation that enables controlling the balance between how much content is\npreserved and the strength of the infused style. We offer the first experiments\nthat demonstrate the performance of existing transformations across different\nstyle transfer models and demonstrate how our transformation performs better in\nits ability to simultaneously run fast, produce consistently reasonable\nresults, and control the balance between content and style in different models.\nTo support reproducing our method and models, we share the code at\nhttps://github.com/chiutaiyin/LS-FT.",
    "descriptor": "\nComments: WACV2023\n",
    "authors": [
      "Tai-Yin Chiu",
      "Danna Gurari"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05996"
  },
  {
    "id": "arXiv:2210.05999",
    "title": "Improving Graph-Based Text Representations with Character and Word Level  N-grams",
    "abstract": "Graph-based text representation focuses on how text documents are represented\nas graphs for exploiting dependency information between tokens and documents\nwithin a corpus. Despite the increasing interest in graph representation\nlearning, there is limited research in exploring new ways for graph-based text\nrepresentation, which is important in downstream natural language processing\ntasks. In this paper, we first propose a new heterogeneous word-character text\ngraph that combines word and character n-gram nodes together with document\nnodes, allowing us to better learn dependencies among these entities.\nAdditionally, we propose two new graph-based neural models, WCTextGCN and\nWCTextGAT, for modeling our proposed text graph. Extensive experiments in text\nclassification and automatic text summarization benchmarks demonstrate that our\nproposed models consistently outperform competitive baselines and\nstate-of-the-art graph-based models.",
    "descriptor": "",
    "authors": [
      "Wenzhe Li",
      "Nikolaos Aletras"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05999"
  },
  {
    "id": "arXiv:2210.06001",
    "title": "Estimating the Pose of a Euro Pallet with an RGB Camera based on  Synthetic Training Data",
    "abstract": "Estimating the pose of a pallet and other logistics objects is crucial for\nvarious use cases, such as automatized material handling or tracking.\nInnovations in computer vision, computing power, and machine learning open up\nnew opportunities for device-free localization based on cameras and neural\nnetworks. Large image datasets with annotated poses are required for training\nthe network. Manual annotation, especially of 6D poses, is an extremely\nlabor-intensive process. Hence, newer approaches often leverage synthetic\ntraining data to automatize the process of generating annotated image datasets.\nIn this work, the generation of synthetic training data for 6D pose estimation\nof pallets is presented. The data is then used to train the Deep Object Pose\nEstimation (DOPE) algorithm. The experimental validation of the algorithm\nproves that the 6D pose estimation of a standardized Euro pallet with a\nRed-Green-Blue (RGB) camera is feasible. The comparison of the results from\nthree varying datasets under different lighting conditions shows the relevance\nof an appropriate dataset design to achieve an accurate and robust\nlocalization. The quantitative evaluation shows an average position error of\nless than 20 cm for the preferred dataset. The validated training dataset and a\nphotorealistic model of a Euro pallet are publicly provided.",
    "descriptor": "",
    "authors": [
      "Markus Knitt",
      "Jakob Schyga",
      "Asan Adamanov",
      "Johannes Hinckeldeyn",
      "Jochen Kreutzfeldt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06001"
  },
  {
    "id": "arXiv:2210.06002",
    "title": "Face Super-Resolution with Progressive Embedding of Multi-scale Face  Priors",
    "abstract": "The face super-resolution (FSR) task is to reconstruct high-resolution face\nimages from low-resolution inputs. Recent works have achieved success on this\ntask by utilizing facial priors such as facial landmarks. Most existing methods\npay more attention to global shape and structure information, but less to local\ntexture information, which makes them cannot recover local details well. In\nthis paper, we propose a novel recurrent convolutional network based framework\nfor face super-resolution, which progressively introduces both global shape and\nlocal texture information. We take full advantage of the intermediate outputs\nof the recurrent network, and landmarks information and facial action units\n(AUs) information are extracted in the output of the first and second steps\nrespectively, rather than low-resolution input. Moreover, we introduced AU\nclassification results as a novel quantitative metric for facial details\nrestoration. Extensive experiments show that our proposed method significantly\noutperforms state-of-the-art FSR methods in terms of image quality and facial\ndetails restoration.",
    "descriptor": "\nComments: Accepted by IJCB 2022\n",
    "authors": [
      "Chenggong Zhang",
      "Zhilei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06002"
  },
  {
    "id": "arXiv:2210.06003",
    "title": "A Complementary Framework for Human-Robot Collaboration with a Mixed  AR-Haptic Interface",
    "abstract": "There is invariably a trade-off between safety and efficiency for\ncollaborative robots (cobots) in human-robot collaborations. Robots that\ninteract minimally with humans can work with high speed and accuracy but cannot\nadapt to new tasks or respond to unforeseen changes, whereas robots that work\nclosely with humans can but only by becoming passive to humans, meaning that\ntheir main tasks suspended and efficiency compromised. Accordingly, this paper\nproposes a new complementary framework for human-robot collaboration that\nbalances the safety of humans and the efficiency of robots. In this framework,\nthe robot carries out given tasks using a vision-based adaptive controller, and\nthe human expert collaborates with the robot in the null space. Such a\ndecoupling drives the robot to deal with existing issues in task space (e.g.,\nuncalibrated camera, limited field of view) and in null space (e.g., joint\nlimits) by itself while allowing the expert to adjust the configuration of the\nrobot body to respond to unforeseen changes (e.g., sudden invasion, change of\nenvironment) without affecting the robot's main task. Additionally, the robot\ncan simultaneously learn the expert's demonstration in task space and null\nspace beforehand with dynamic movement primitives (DMP). Therefore, an expert's\nknowledge and a robot's capability are both explored and complementary. Human\ndemonstration and involvement are enabled via a mixed interaction interface,\ni.e., augmented reality (AR) and haptic devices. The stability of the\nclosed-loop system is rigorously proved with Lyapunov methods. Experimental\nresults in various scenarios are presented to illustrate the performance of the\nproposed method.",
    "descriptor": "",
    "authors": [
      "Xiangjie Yan",
      "Yongpeng Jiang",
      "Chen Chen",
      "Leiliang Gong",
      "Ming Ge",
      "Tao Zhang",
      "Xiang Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06003"
  },
  {
    "id": "arXiv:2210.06005",
    "title": "Generative Adversarial Nets: Can we generate a new dataset based on only  one training set?",
    "abstract": "A generative adversarial network (GAN) is a class of machine learning\nframeworks designed by Goodfellow et al. in 2014. In the GAN framework, the\ngenerative model is pitted against an adversary: a discriminative model that\nlearns to determine whether a sample is from the model distribution or the data\ndistribution. GAN generates new samples from the same distribution as the\ntraining set. In this work, we aim to generate a new dataset that has a\ndifferent distribution from the training set. In addition, the Jensen-Shannon\ndivergence between the distributions of the generative and training datasets\ncan be controlled by some target $\\delta \\in [0, 1]$. Our work is motivated by\napplications in generating new kinds of rice that have similar characteristics\nas good rice.",
    "descriptor": "\nComments: Under review for possible publication\n",
    "authors": [
      "Lan V. Truong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06005"
  },
  {
    "id": "arXiv:2210.06006",
    "title": "BEV Lane Det: Fast Lane Detection on BEV Ground",
    "abstract": "Recently, 3D lane detection has been an actively developing area in\nautonomous driving which is the key to routing the vehicle. This work proposes\na deployment-oriented monocular 3D lane detector with only naive CNN and FC\nlayers. This detector achieved state-of-the-art results on the Apollo 3D Lane\nSynthetic dataset and OpenLane real-world dataset with 96 FPS runtime speed. We\nconduct three techniques in our detector: (1) Virtual Camera eliminates the\ndifference in poses of cameras mounted on different vehicles. (2) Spatial\nFeature Pyramid Transform as a light-weighed image-view to bird-eye view\ntransformer can utilize scales of image-view featmaps. (3) Yolo Style Lane\nRepresentation makes a good balance between bird-eye view resolution and\nruntime speed. Meanwhile, it can reduce the inefficiency caused by the class\nimbalance due to the sparsity of the lane detection task during training.\nCombining these three techniques, we obtained a 58.4% F1-score on the OpenLane\ndataset, which is a 10.6% improvement over the baseline. On the Apollo dataset,\nwe achieved an F1-score of 96.9%, which is 4% points of supremacy over the best\non the leaderboard. The source code will release soon.",
    "descriptor": "\nComments: 10 pages, 3 figures, 5 tables\n",
    "authors": [
      "Ruihao Wang",
      "Jian Qin",
      "Kaiying Li",
      "Dong Cao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06006"
  },
  {
    "id": "arXiv:2210.06007",
    "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment  Generation via Transformer VQ-VA",
    "abstract": "This paper proposes a model that generates a drum track in the audio domain\nto play along to a user-provided drum-free recording. Specifically, using\npaired data of drumless tracks and the corresponding human-made drum tracks, we\ntrain a Transformer model to improvise the drum part of an unseen drumless\nrecording. We combine two approaches to encode the input audio. First, we train\na vector-quantized variational autoencoder (VQ-VAE) to represent the input\naudio with discrete codes, which can then be readily used in a Transformer.\nSecond, using an audio-domain beat tracking model, we compute beat-related\nfeatures of the input audio and use them as embeddings in the Transformer.\nInstead of generating the drum track directly as waveforms, we use a separate\nVQ-VAE to encode the mel-spectrogram of a drum track into another set of\ndiscrete codes, and train the Transformer to predict the sequence of\ndrum-related discrete codes. The output codes are then converted to a\nmel-spectrogram with a decoder, and then to the waveform with a vocoder. We\nreport both objective and subjective evaluations of variants of the proposed\nmodel, demonstrating that the model with beat information generates drum\naccompaniment that is rhythmically and stylistically consistent with the input\naudio.",
    "descriptor": "\nComments: Accepted at ISMIR 2022\n",
    "authors": [
      "Yueh-Kao Wu",
      "Ching-Yu Chiu",
      "Yi-Hsuan Yang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06007"
  },
  {
    "id": "arXiv:2210.06008",
    "title": "BoxMask: Revisiting Bounding Box Supervision for Video Object Detection",
    "abstract": "We present a new, simple yet effective approach to uplift video object\ndetection. We observe that prior works operate on instance-level feature\naggregation that imminently neglects the refined pixel-level representation,\nresulting in confusion among objects sharing similar appearance or motion\ncharacteristics. To address this limitation, we propose BoxMask, which\neffectively learns discriminative representations by incorporating class-aware\npixel-level information. We simply consider bounding box-level annotations as a\ncoarse mask for each object to supervise our method. The proposed module can be\neffortlessly integrated into any region-based detector to boost detection.\nExtensive experiments on ImageNet VID and EPIC KITCHENS datasets demonstrate\nconsistent and significant improvement when we plug our BoxMask module into\nnumerous recent state-of-the-art methods.",
    "descriptor": "\nComments: WACV 2023\n",
    "authors": [
      "Khurram Azeem Hashmi",
      "Alain Pagani",
      "Didier Stricker",
      "Muhammamd Zeshan Afzal"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06008"
  },
  {
    "id": "arXiv:2210.06010",
    "title": "Simulating Spreading of Multiple Interacting Processes in Complex  Networks",
    "abstract": "Investigating the interaction between spreading processes in complex networks\nis one of the most important challenges in network science. However, whether we\nwould like to know how the information campaign will affect virus spreading or\nhow the advertising campaign of the new iPhone will affect the sales of Samsung\nphones, we need an environment that will allow us to evaluate under what\nconditions our spreading campaign will be effective. Network Diffusion is a\nPython package that should help do that. In this paper, we introduce its\noperating principle and main functionalities, including simple examples of\nsimulations that can be performed using it.",
    "descriptor": "",
    "authors": [
      "Micha\u0142 Czuba",
      "Piotr Br\u00f3dka"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2210.06010"
  },
  {
    "id": "arXiv:2210.06012",
    "title": "Phantom -- An RL-driven framework for agent-based modeling of complex  economic systems and markets",
    "abstract": "Agent based modeling (ABM) is a computational approach to modeling complex\nsystems by specifying the behavior of autonomous decision-making components or\nagents in the system and allowing the system dynamics to emerge from their\ninteractions. Recent advances in the field of Multi-agent reinforcement\nlearning (MARL) have made it feasible to learn the equilibrium of complex\nenvironments where multiple agents learn at the same time - opening up the\npossibility of building ABMs where agent behaviors are learned and system\ndynamics can be analyzed. However, most ABM frameworks are not RL-native, in\nthat they do not offer concepts and interfaces that are compatible with the use\nof MARL to learn agent behaviors. In this paper, we introduce a new framework,\nPhantom, to bridge the gap between ABM and MARL. Phantom is an RL-driven\nframework for agent-based modeling of complex multi-agent systems such as\neconomic systems and markets. To enable this, the framework provides tools to\nspecify the ABM in MARL-compatible terms - including features to encode dynamic\npartial observability, agent utility / reward functions, heterogeneity in agent\npreferences or types, and constraints on the order in which agents can act\n(e.g. Stackelberg games, or complex turn-taking environments). In this paper,\nwe present these features, their design rationale and show how they were used\nto model and simulate Over-The-Counter (OTC) markets.",
    "descriptor": "\nComments: 2022 ACM International Conference on Artificial Intelligence in Finance - Benchmarks for AI in Finance Workshop\n",
    "authors": [
      "Leo Ardon",
      "Jared Vann",
      "Deepeka Garg",
      "Tom Spooner",
      "Sumitra Ganesh"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2210.06012"
  },
  {
    "id": "arXiv:2210.06014",
    "title": "cuFasterTucker: A Stochastic Optimization Strategy for Parallel Sparse  FastTucker Decomposition on GPU Platform",
    "abstract": "Currently, the size of scientific data is growing at an unprecedented rate.\nData in the form of tensors exhibit high-order, high-dimensional, and highly\nsparse features. Although tensor-based analysis methods are very effective, the\nlarge increase in data size makes the original tensor impossible to process.\nTensor decomposition decomposes a tensor into multiple low-rank matrices or\ntensors that can be exploited by tensor-based analysis methods. Tucker\ndecomposition is such an algorithm, which decomposes a $n$-order tensor into\n$n$ low-rank factor matrices and a low-rank core tensor. However, most Tucker\ndecomposition methods are accompanied by huge intermediate variables and huge\ncomputational load, making them unable to process high-order and\nhigh-dimensional tensors.\nIn this paper, we propose FasterTucker decomposition based on FastTucker\ndecomposition, which is a variant of Tucker decomposition. And an efficient\nparallel FasterTucker decomposition algorithm cuFasterTucker on GPU platform is\nproposed. It has very low storage and computational requirements, and\neffectively solves the problem of high-order and high-dimensional sparse tensor\ndecomposition. Compared with the state-of-the-art algorithm, it achieves a\nspeedup of around $15X$ and $7X$ in updating the factor matrices and updating\nthe core matrices, respectively.",
    "descriptor": "",
    "authors": [
      "Zixuan Li"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06014"
  },
  {
    "id": "arXiv:2210.06015",
    "title": "Energy Consumption-Aware Tabular Benchmarks for Neural Architecture  Search",
    "abstract": "The demand for large-scale computational resources for Neural Architecture\nSearch (NAS) has been lessened by tabular benchmarks for NAS. Evaluating NAS\nstrategies is now possible on extensive search spaces and at a moderate\ncomputational cost. But so far, NAS has mainly focused on maximising\nperformance on some hold-out validation/test set. However, energy consumption\nis a partially conflicting objective that should not be neglected. We\nhypothesise that constraining NAS to include the energy consumption of training\nthe models could reveal a sub-space of undiscovered architectures that are more\ncomputationally efficient with a smaller carbon footprint. To support the\nhypothesis, an existing tabular benchmark for NAS is augmented with the energy\nconsumption of each architecture. We then perform multi-objective optimisation\nthat includes energy consumption as an additional objective. We demonstrate the\nusefulness of multi-objective NAS for uncovering the trade-off between\nperformance and energy consumption as well as for finding more energy-efficient\narchitectures. The updated tabular benchmark, EC-NAS-Bench, is open-sourced to\nencourage the further exploration of energy consumption-aware NAS.",
    "descriptor": "\nComments: Source code at this https URL\n",
    "authors": [
      "Pedram Bakhtiarifard",
      "Christian Igel",
      "Raghavendra Selvan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06015"
  },
  {
    "id": "arXiv:2210.06019",
    "title": "Orthogonal Approximate Message-Passing for Spatially Coupled Systems",
    "abstract": "Orthogonal approximate message-passing (OAMP) is proposed for signal recovery\nfrom right-orthogonally invariant linear measurements with spatial coupling.\nConventional state evolution is generalized to a unified framework of state\nevolution for the spatial coupling and long-memory case. The unified framework\nis used to formulate the so-called Onsager correction in OAMP for spatially\ncoupled systems. The state evolution recursion of Bayes-optimal OAMP is proved\nto converge for spatially coupled systems via Bayes-optimal long-memory OAMP\nand its state evolution. This paper proves the information-theoretic optimality\nof Bayes-optimal OAMP for noiseless spatially coupled systems with\nright-orthogonally invariant sensing matrices.",
    "descriptor": "\nComments: submitted to IEEE Trans. Inf. Theory\n",
    "authors": [
      "Keigo Takeuchi"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06019"
  },
  {
    "id": "arXiv:2210.06020",
    "title": "Non-Autoregressive Machine Translation with Translation Memories",
    "abstract": "Non-autoregressive machine translation (NAT) has recently made great\nprogress. However, most works to date have focused on standard translation\ntasks, even though some edit-based NAT models, such as the Levenshtein\nTransformer (LevT), seem well suited to translate with a Translation Memory\n(TM). This is the scenario considered here. We first analyze the vanilla LevT\nmodel and explain why it does not do well in this setting. We then propose a\nnew variant, TM-LevT, and show how to effectively train this model. By\nmodifying the data presentation and introducing an extra deletion operation, we\nobtain performance that are on par with an autoregressive approach, while\nreducing the decoding load. We also show that incorporating TMs during training\ndispenses to use knowledge distillation, a well-known trick used to mitigate\nthe multimodality issue.",
    "descriptor": "",
    "authors": [
      "Jitao Xu",
      "Josep Crego",
      "Fran\u00e7ois Yvon"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06020"
  },
  {
    "id": "arXiv:2210.06023",
    "title": "Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval  on Predefined Topics",
    "abstract": "In this paper, we consider the task of retrieving documents with predefined\ntopics from an unlabeled document dataset using an unsupervised approach. The\nproposed unsupervised approach requires only a small number of keywords\ndescribing the respective topics and no labeled document. Existing approaches\neither heavily relied on a large amount of additionally encoded world knowledge\nor on term-document frequencies. Contrariwise, we introduce a method that\nlearns jointly embedded document and word vectors solely from the unlabeled\ndocument dataset in order to find documents that are semantically similar to\nthe topics described by the keywords. The proposed method requires almost no\ntext preprocessing but is simultaneously effective at retrieving relevant\ndocuments with high probability. When successively retrieving documents on\ndifferent predefined topics from publicly available and commonly used datasets,\nwe achieved an average area under the receiver operating characteristic curve\nvalue of 0.95 on one dataset and 0.92 on another. Further, our method can be\nused for multiclass document classification, without the need to assign labels\nto the dataset in advance. Compared with an unsupervised classification\nbaseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the\nrespective datasets. For easy replication of our approach, we make the\ndeveloped Lbl2Vec code publicly available as a ready-to-use tool under the\n3-Clause BSD license.",
    "descriptor": "",
    "authors": [
      "Tim Schopf",
      "Daniel Braun",
      "Florian Matthes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06023"
  },
  {
    "id": "arXiv:2210.06028",
    "title": "VL4Pose: Active Learning Through Out-Of-Distribution Detection For Pose  Estimation",
    "abstract": "Advances in computing have enabled widespread access to pose estimation,\ncreating new sources of data streams. Unlike mock set-ups for data collection,\ntapping into these data streams through on-device active learning allows us to\ndirectly sample from the real world to improve the spread of the training\ndistribution. However, on-device computing power is limited, implying that any\ncandidate active learning algorithm should have a low compute footprint while\nalso being reliable. Although multiple algorithms cater to pose estimation,\nthey either use extensive compute to power state-of-the-art results or are not\ncompetitive in low-resource settings. We address this limitation with VL4Pose\n(Visual Likelihood For Pose Estimation), a first principles approach for active\nlearning through out-of-distribution detection. We begin with a simple premise:\npose estimators often predict incoherent poses for out-of-distribution samples.\nHence, can we identify a distribution of poses the model has been trained on,\nto identify incoherent poses the model is unsure of? Our solution involves\nmodelling the pose through a simple parametric Bayesian network trained via\nmaximum likelihood estimation. Therefore, poses incurring a low likelihood\nwithin our framework are out-of-distribution samples making them suitable\ncandidates for annotation. We also observe two useful side-outcomes: VL4Pose\nin-principle yields better uncertainty estimates by unifying joint and pose\nlevel ambiguity, as well as the unintentional but welcome ability of VL4Pose to\nperform pose refinement in limited scenarios. We perform qualitative and\nquantitative experiments on three datasets: MPII, LSP and ICVL, spanning human\nand hand pose estimation. Finally, we note that VL4Pose is simple,\ncomputationally inexpensive and competitive, making it suitable for challenging\ntasks such as on-device active learning.",
    "descriptor": "\nComments: Accepted: BMVC 2022\n",
    "authors": [
      "Megh Shukla",
      "Roshan Roy",
      "Pankaj Singh",
      "Shuaib Ahmed",
      "Alexandre Alahi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06028"
  },
  {
    "id": "arXiv:2210.06031",
    "title": "Long-Form Video-Language Pre-Training with Multimodal Temporal  Contrastive Learning",
    "abstract": "Large-scale video-language pre-training has shown significant improvement in\nvideo-language understanding tasks. Previous studies of video-language\npretraining mainly focus on short-form videos (i.e., within 30 seconds) and\nsentences, leaving long-form video-language pre-training rarely explored.\nDirectly learning representation from long-form videos and language may benefit\nmany long-form video-language understanding tasks. However, it is challenging\ndue to the difficulty of modeling long-range relationships and the heavy\ncomputational burden caused by more frames. In this paper, we introduce a\nLong-Form VIdeo-LAnguage pre-training model (LF-VILA) and train it on a\nlarge-scale long-form video and paragraph dataset constructed from an existing\npublic dataset. To effectively capture the rich temporal dynamics and to better\nalign video and language in an efficient end-to-end manner, we introduce two\nnovel designs in our LF-VILA model. We first propose a Multimodal Temporal\nContrastive (MTC) loss to learn the temporal relation across different\nmodalities by encouraging fine-grained alignment between long-form videos and\nparagraphs. Second, we propose a Hierarchical Temporal Window Attention (HTWA)\nmechanism to effectively capture long-range dependency while reducing\ncomputational cost in Transformer. We fine-tune the pre-trained LF-VILA model\non seven downstream long-form video-language understanding tasks of\nparagraph-to-video retrieval and long-form video question-answering, and\nachieve new state-of-the-art performances. Specifically, our model achieves\n16.1% relative improvement on ActivityNet paragraph-to-video retrieval task and\n2.4% on How2QA task, respectively. We release our code, dataset, and\npre-trained models at https://github.com/microsoft/XPretrain.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Yuchong Sun",
      "Hongwei Xue",
      "Ruihua Song",
      "Bei Liu",
      "Huan Yang",
      "Jianlong Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06031"
  },
  {
    "id": "arXiv:2210.06032",
    "title": "Modular Flows: Differential Molecular Generation",
    "abstract": "Generating new molecules is fundamental to advancing critical applications\nsuch as drug discovery and material synthesis. Flows can generate molecules\neffectively by inverting the encoding process, however, existing flow models\neither require artifactual dequantization or specific node/edge orderings, lack\ndesiderata such as permutation invariance or induce discrepancy between the\nencoding and the decoding steps that necessitates {\\em post hoc} validity\ncorrection. We circumvent these issues with novel continuous normalizing\nE(3)-equivariant flows, based on a system of node ODEs coupled as a graph PDE,\nthat repeatedly reconcile locally toward globally aligned densities. Our models\ncan be cast as message-passing temporal networks, and result in superlative\nperformance on the tasks of density estimation and molecular generation. In\nparticular, our generated samples achieve state-of-the-art on both the standard\nQM9 and ZINC250K benchmarks.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Yogesh Verma",
      "Samuel Kaski",
      "Markus Heinonen",
      "Vikas Garg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Emerging Technologies (cs.ET)",
      "Biomolecules (q-bio.BM)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06032"
  },
  {
    "id": "arXiv:2210.06033",
    "title": "Local Planner Bench: Benchmarking for Local Motion Planning",
    "abstract": "Local motion planning is a heavily researched topic in the field of robotics\nwith many promising algorithms being published every year. However, it is\ndifficult and time-consuming to compare different methods in the field. In this\npaper, we present localPlannerBench, a new benchmarking suite that allows quick\nand seamless comparison between local motion planning algorithms. The key focus\nof the project lies in the extensibility of the environment and the simulation\ncases. Out-of-the-box, localPlannerBench already supports many simulation cases\nranging from a simple 2D point mass to full-fledged 3D 7DoF manipulators, and\nit is straightforward to add your own custom robot using a URDF file. A\npost-processor is built-in that can be extended with custom metrics and plots.\nTo integrate your own motion planner, simply create a wrapper that derives from\nthe provided base class. Ultimately we aim to improve the reproducibility of\nlocal motion planning algorithms and encourage standardized open-source\ncomparison.",
    "descriptor": "\nComments: Workshop @IROS2022: Evaluating Motion Planning Performance, 4 pages\n",
    "authors": [
      "Max Spahn",
      "Chadi Salmi",
      "Javier Alonso-Mora"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06033"
  },
  {
    "id": "arXiv:2210.06036",
    "title": "Guaranteed Conservation of Momentum for Learning Particle-based Fluid  Dynamics",
    "abstract": "We present a novel method for guaranteeing linear momentum in learned physics\nsimulations. Unlike existing methods, we enforce conservation of momentum with\na hard constraint, which we realize via antisymmetrical continuous\nconvolutional layers. We combine these strict constraints with a hierarchical\nnetwork architecture, a carefully constructed resampling scheme, and a training\napproach for temporal coherence. In combination, the proposed method allows us\nto increase the physical accuracy of the learned simulator substantially. In\naddition, the induced physical bias leads to significantly better\ngeneralization performance and makes our method more reliable in unseen test\ncases. We evaluate our method on a range of different, challenging fluid\nscenarios. Among others, we demonstrate that our approach generalizes to new\nscenarios with up to one million particles. Our results show that the proposed\nalgorithm can learn complex dynamics while outperforming existing approaches in\ngeneralization and training performance. An implementation of our approach is\navailable at https://github.com/tum-pbs/DMCF.",
    "descriptor": "",
    "authors": [
      "Lukas Prantl",
      "Benjamin Ummenhofer",
      "Vladlen Koltun",
      "Nils Thuerey"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06036"
  },
  {
    "id": "arXiv:2210.06038",
    "title": "Approximation-free Prescribed Performance Control with Prescribed Input  Constraints",
    "abstract": "This paper considers the tracking control problem for an unknown nonlinear\nsystem with time-varying bounded disturbance subjected to a prescribed\nperformance and input constraints. When performance and input constraints are\nspecified simultaneously for such a problem, a trade-off is inevitable.\nConsequently, a feasibility condition for prescribing performance and input\nconstraints is devised to address such difficulties of arbitrary prescription.\nIn addition, an approximation-free controller with low complexity is proposed,\nwhich ensures that the constraints are never violated, provided that the\nfeasibility condition holds. Finally, simulation results corroborate the\neffectiveness of the proposed controller.",
    "descriptor": "",
    "authors": [
      "Pankaj K Mishra",
      "Pushpak Jagtap"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06038"
  },
  {
    "id": "arXiv:2210.06040",
    "title": "Question Answering Over Biological Knowledge Graph via Amazon Alexa",
    "abstract": "Structured and unstructured data and facts about drugs, genes, protein,\nviruses, and their mechanism are spread across a huge number of scientific\narticles. These articles are a large-scale knowledge source and can have a huge\nimpact on disseminating knowledge about the mechanisms of certain biological\nprocesses. A knowledge graph (KG) can be constructed by integrating such facts\nand data and be used for data integration, exploration, and federated queries.\nHowever, exploration and querying large-scale KGs is tedious for certain groups\nof users due to a lack of knowledge about underlying data assets or semantic\ntechnologies. A question-answering (QA) system allows the answer of natural\nlanguage questions over KGs automatically using triples contained in a KG.\nRecently, the use and adaption of digital assistants are getting wider owing to\ntheir capability at enabling users to voice commands to control smart systems\nor devices. This paper is about using Amazon Alexa's voice-enabled interface\nfor QA over KGs. As a proof-of-concept, we use the well-known DisgeNET KG,\nwhich contains knowledge covering 1.13 million gene-disease associations\nbetween 21,671 genes and 30,170 diseases, disorders, and clinical or abnormal\nhuman phenotypes. Our study shows how Alex could be of help to find facts about\ncertain biological entities from large-scale knowledge bases.",
    "descriptor": "\nComments: This paper is based on the Knowledge Graph Lab course (this https URL) offered at Computer Science 5 - Information Systems and Databases, RWTH Aachen University, Germany, and a joint collaboration with Osthus GmbH (this https URL), Aachen, Germany\n",
    "authors": [
      "Md. Rezaul Karim",
      "Hussain Ali",
      "Prinon Das",
      "Mohamed Abdelwaheb",
      "Stefan Decker"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Databases (cs.DB)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.06040"
  },
  {
    "id": "arXiv:2210.06041",
    "title": "Reinforcement Learning with Automated Auxiliary Loss Search",
    "abstract": "A good state representation is crucial to solving complicated reinforcement\nlearning (RL) challenges. Many recent works focus on designing auxiliary losses\nfor learning informative representations. Unfortunately, these handcrafted\nobjectives rely heavily on expert knowledge and may be sub-optimal. In this\npaper, we propose a principled and universal method for learning better\nrepresentations with auxiliary loss functions, named Automated Auxiliary Loss\nSearch (A2LS), which automatically searches for top-performing auxiliary loss\nfunctions for RL. Specifically, based on the collected trajectory data, we\ndefine a general auxiliary loss space of size $7.5 \\times 10^{20}$ and explore\nthe space with an efficient evolutionary search strategy. Empirical results\nshow that the discovered auxiliary loss (namely, A2-winner) significantly\nimproves the performance on both high-dimensional (image) and low-dimensional\n(vector) unseen tasks with much higher efficiency, showing promising\ngeneralization ability to different settings and even different benchmark\ndomains. We conduct a statistical analysis to reveal the relations between\npatterns of auxiliary losses and RL performance.",
    "descriptor": "\nComments: NeurIPS 2022 accepted paper\n",
    "authors": [
      "Tairan He",
      "Yuge Zhang",
      "Kan Ren",
      "Minghuan Liu",
      "Che Wang",
      "Weinan Zhang",
      "Yuqing Yang",
      "Dongsheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06041"
  },
  {
    "id": "arXiv:2210.06042",
    "title": "Efficient Hamiltonian Reduction for Quantum Annealing on SatCom Beam  Placement Problem",
    "abstract": "Beam Placement (BP) is a well-known problem in Low-Earth Orbit (LEO)\nsatellite communication (SatCom) systems, which can be modelled as an NP-hard\nclique cover problem. Recently, quantum computing has emerged as a novel\ntechnology which revolutionizes how to solve challenging optimization problems\nby formulating Quadratic Unconstrained Binary Optimization (QUBO), then\npreparing Hamiltonians as inputs for quantum computers. In this paper, we study\nhow to use quantum computing to solve BP problems. However, due to limited\nhardware resources, existing quantum computers are unable to tackle large\noptimization spaces. Therefore, we propose an efficient Hamiltonian Reduction\nmethod that allows quantum processors to solve large BP instances encountered\nin LEO systems. We conduct our simulations on real quantum computers (D-Wave\nAdvantage) using a real dataset of vessel locations in the US. Numerical\nresults show that our algorithm outperforms commercialized solutions of D-Wave\nby allowing existing quantum annealers to solve 17.5 times larger BP instances\nwhile maintaining high solution quality. Although quantum computing cannot\ntheoretically overcome the hardness of BP problems, this work contributes early\nefforts to applying quantum computing in satellite optimization problems,\nespecially applications formulated as clique cover/graph coloring problems.",
    "descriptor": "",
    "authors": [
      "Thinh Q. Dinh",
      "Son Hoang Dau",
      "Eva Lagunas",
      "Symeon Chatzinotas"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06042"
  },
  {
    "id": "arXiv:2210.06044",
    "title": "Multi-Granularity Cross-modal Alignment for Generalized Medical Visual  Representation Learning",
    "abstract": "Learning medical visual representations directly from paired radiology\nreports has become an emerging topic in representation learning. However,\nexisting medical image-text joint learning methods are limited by instance or\nlocal supervision analysis, ignoring disease-level semantic correspondences. In\nthis paper, we present a novel Multi-Granularity Cross-modal Alignment (MGCA)\nframework for generalized medical visual representation learning by harnessing\nthe naturally exhibited semantic correspondences between medical image and\nradiology reports at three different levels, i.e., pathological region-level,\ninstance-level, and disease-level. Specifically, we first incorporate the\ninstance-wise alignment module by maximizing the agreement between image-report\npairs. Further, for token-wise alignment, we introduce a bidirectional\ncross-attention strategy to explicitly learn the matching between fine-grained\nvisual tokens and text tokens, followed by contrastive learning to align them.\nMore important, to leverage the high-level inter-subject relationship semantic\n(e.g., disease) correspondences, we design a novel cross-modal disease-level\nalignment paradigm to enforce the cross-modal cluster assignment consistency.\nExtensive experimental results on seven downstream medical image datasets\ncovering image classification, object detection, and semantic segmentation\ntasks demonstrate the stable and superior performance of our framework.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Fuying Wang",
      "Yuyin Zhou",
      "Shujun Wang",
      "Varut Vardhanabhuti",
      "Lequan Yu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06044"
  },
  {
    "id": "arXiv:2210.06048",
    "title": "AIMY: An Open-source Table Tennis Ball Launcher for Versatile and  High-fidelity Trajectory Generation",
    "abstract": "To approach the level of advanced human players in table tennis with robots,\ngenerating varied ball trajectories in a reproducible and controlled manner is\nessential. Current ball launchers used in robot table tennis either do not\nprovide an interface for automatic control or are limited in their capabilities\nto adapt speed, direction, and spin of the ball. For these reasons, we present\nAIMY, a three-wheeled open-hardware and open-source table tennis ball launcher,\nwhich can generate ball speeds and spins of up to 15.44 m/s and 192/s,\nrespectively, which are comparable to advanced human players. The wheel speeds,\nlaunch orientation and time can be fully controlled via an open Ethernet or\nWi-Fi interface. We provide a detailed overview of the core design features, as\nwell as open source the software to encourage distribution and duplication\nwithin and beyond the robot table tennis research community. We also\nextensively evaluate the ball launcher's accuracy for different system settings\nand learn to launch a ball to desired locations. With this ball launcher, we\nenable long-duration training of robot table tennis approaches where the\ncomplexity of the ball trajectory can be automatically adjusted, enabling\nlarge-scale real-world online reinforcement learning for table tennis robots.",
    "descriptor": "",
    "authors": [
      "Alexander Dittrich",
      "Jan Schneider",
      "Simon Guist",
      "Bernhard Sch\u00f6lkopf",
      "Dieter B\u00fcchler"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Hardware Architecture (cs.AR)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06048"
  },
  {
    "id": "arXiv:2210.06052",
    "title": "System theoretic approach of information processing in nested cellular  automata",
    "abstract": "The subject of this paper is the evolution of the concept of information\nprocessing in regular structures based on multi-level processing in nested\ncellular automata. The essence of the proposed model is a discrete space-time\ncontaining nested orthogonal space-times at its points. The factorization of\nthe function describing the global behavior of a system is the key element of\nthe mathematical description. Factorization describes the relations of physical\nconnections, signal propagation times and signal processing to global behavior.\nIn the model appear expressions similar to expressions used in the Special\nRelativity Theory.",
    "descriptor": "\nComments: 10 pages, 10 figures\n",
    "authors": [
      "Jerzy Szynka"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06052"
  },
  {
    "id": "arXiv:2210.06063",
    "title": "ControlVAE: Model-Based Learning of Generative Controllers for  Physics-Based Characters",
    "abstract": "In this paper, we introduce ControlVAE, a novel model-based framework for\nlearning generative motion control policies based on variational autoencoders\n(VAE). Our framework can learn a rich and flexible latent representation of\nskills and a skill-conditioned generative control policy from a diverse set of\nunorganized motion sequences, which enables the generation of realistic human\nbehaviors by sampling in the latent space and allows high-level control\npolicies to reuse the learned skills to accomplish a variety of downstream\ntasks. In the training of ControlVAE, we employ a learnable world model to\nrealize direct supervision of the latent space and the control policy. This\nworld model effectively captures the unknown dynamics of the simulation system,\nenabling efficient model-based learning of high-level downstream tasks. We also\nlearn a state-conditional prior distribution in the VAE-based generative\ncontrol policy, which generates a skill embedding that outperforms the\nnon-conditional priors in downstream tasks. We demonstrate the effectiveness of\nControlVAE using a diverse set of tasks, which allows realistic and interactive\ncontrol of the simulated characters.",
    "descriptor": "\nComments: SIGGRAPH Asia 2022 (Journal Track);\n",
    "authors": [
      "Heyuan Yao",
      "Zhenhua Song",
      "Baoquan Chen",
      "Libin Liu"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06063"
  },
  {
    "id": "arXiv:2210.06065",
    "title": "Matern Cluster Process with Holes at the Cluster Centers",
    "abstract": "Inspired by recent applications of point processes to biological\nnanonetworks, this paper presents a novel variant of a Mat\\'ern cluster process\n(MCP) in which the points located within a certain distance from the cluster\ncenters are removed. We term this new process the MCP with holes at the cluster\ncenter (MCP-H, in short). Focusing on the three-dimensional (3D) space, we\nfirst characterize the conditional distribution of the distance between an\narbitrary point of a given cluster to the origin, conditioned on the location\nof that cluster, for both MCP and MCP-H. These distributions are shown to admit\nremarkably simple closed forms in the 3D space, which is not even possible in\nthe simpler two-dimensional (2D) case. Using these distributions, the contact\ndistance distribution and the probability generating functional (PGFL) are\ncharacterized for both MCP and MCP-H.",
    "descriptor": "\nComments: Stochastic geometry, Mat\\'ern cluster process, Poisson hole process, biological nanonetworks, wireless networks\n",
    "authors": [
      "Seyed Mohammad Azimi-Abarghouyi",
      "Harpreet S. Dhillon"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2210.06065"
  },
  {
    "id": "arXiv:2210.06066",
    "title": "On the Optimality of Coded Caching With Heterogeneous User Profiles",
    "abstract": "In this paper, we consider a coded caching scenario where users have\nheterogeneous interests. Taking into consideration the system model originally\nproposed by Wang and Peleato, for which the end-receiving users are divided\ninto groups according to their file preferences, we develop a novel\ninformation-theoretic converse on the optimal worst-case communication load\nunder uncoded cache placement. Interestingly, the developed converse bound,\njointly with one of the coded schemes proposed by Wang and Peleato, allows us\nto characterize the optimal worst-case communication load under uncoded\nprefetching within a constant multiplicative gap of $2$. Although we restrict\nthe caching policy to be uncoded, our work improves the previously known order\noptimality results for the considered caching problem.",
    "descriptor": "\nComments: 6 pages. Accepted to 2022 IEEE Information Theory Workshop (ITW)\n",
    "authors": [
      "Federico Brunero",
      "Petros Elia"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06066"
  },
  {
    "id": "arXiv:2210.06068",
    "title": "Using Massive Multilingual Pre-Trained Language Models Towards Real  Zero-Shot Neural Machine Translation in Clinical Domain",
    "abstract": "Massively multilingual pre-trained language models (MMPLMs) are developed in\nrecent years demonstrating superpowers and the pre-knowledge they acquire for\ndownstream tasks. In this work, we investigate whether MMPLMs can be applied to\nzero-shot machine translation (MT) toward entirely new language pairs and new\ndomains. We carry out an experimental investigation using Meta-AI's MMPLMs\n\"wmt21-dense-24-wide-en-X and X-en (WMT21fb)\" which were pre-trained on 7\nlanguage pairs and 14 translation directions including English to Czech,\nGerman, Hausa, Icelandic, Japanese, Russian, and Chinese, and opposite\ndirection. We fine-tune these MMPLMs towards English-Spanish language pair\nwhich did not exist at all in their original pre-trained corpora both\nimplicitly and explicitly. We prepare carefully aligned clinical domain data\nfor this fine-tuning, which is different from their original mixed domain\nknowledge as well. Our experimental result shows that the fine-tuning is very\nsuccessful using just 250k well-aligned in-domain EN-ES pairs/sentences for\nthree sub-task translation tests: clinical cases, clinical terms, and ontology\nconcepts. It achieves very close evaluation scores to another MMPLM NLLB from\nMeta-AI, which included Spanish as a high-resource setting in the pre-training.\nTo the best of our knowledge, this is the first work on using MMPLMs towards\nreal zero-shot NMT successfully for totally unseen languages during\npre-training, and also the first in clinical domain for such a study.",
    "descriptor": "\nComments: 8 pages, 2 figures\n",
    "authors": [
      "Lifeng Han",
      "Gleb Erofeev",
      "Irina Sorokina",
      "Serge Gladkoff",
      "Goran Nenadic"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06068"
  },
  {
    "id": "arXiv:2210.06071",
    "title": "Self-Validated Physics-Embedding Network: A General Framework for  Inverse Modelling",
    "abstract": "Physics-based inverse modeling techniques are typically restricted to\nparticular research fields, whereas popular machine-learning-based ones are too\ndata-dependent to guarantee the physical compatibility of the solution. In this\npaper, Self-Validated Physics-Embedding Network (SVPEN), a general neural\nnetwork framework for inverse modeling is proposed. As its name suggests, the\nembedded physical forward model ensures that any solution that successfully\npasses its validation is physically reasonable. SVPEN operates in two modes:\n(a) the inverse function mode offers rapid state estimation as conventional\nsupervised learning, and (b) the optimization mode offers a way to iteratively\ncorrect estimations that fail the validation process. Furthermore, the\noptimization mode provides SVPEN with reconfigurability i.e., replacing\ncomponents like neural networks, physical models, and error calculations at\nwill to solve a series of distinct inverse problems without pretraining. More\nthan ten case studies in two highly nonlinear and entirely distinct\napplications: molecular absorption spectroscopy and Turbofan cycle analysis,\ndemonstrate the generality, physical reliability, and reconfigurability of\nSVPEN. More importantly, SVPEN offers a solid foundation to use existing\nphysical models within the context of AI, so as to striking a balance between\ndata-driven and physics-driven models.",
    "descriptor": "\nComments: 32 pages, 25 figures, four tables\n",
    "authors": [
      "Ruiyuan Kang",
      "Dimitrios C. Kyritsis",
      "Panos Liatsis"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06071"
  },
  {
    "id": "arXiv:2210.06077",
    "title": "Double Bubble, Toil and Trouble: Enhancing Certified Robustness through  Transitivity",
    "abstract": "In response to subtle adversarial examples flipping classifications of neural\nnetwork models, recent research has promoted certified robustness as a\nsolution. There, invariance of predictions to all norm-bounded attacks is\nachieved through randomised smoothing of network inputs. Today's\nstate-of-the-art certifications make optimal use of the class output scores at\nthe input instance under test: no better radius of certification (under the\n$L_2$ norm) is possible given only these score. However, it is an open question\nas to whether such lower bounds can be improved using local information around\nthe instance under test. In this work, we demonstrate how today's \"optimal\"\ncertificates can be improved by exploiting both the transitivity of\ncertifications, and the geometry of the input space, giving rise to what we\nterm Geometrically-Informed Certified Robustness. By considering the smallest\ndistance to points on the boundary of a set of certifications this approach\nimproves certifications for more than $80\\%$ of Tiny-Imagenet instances,\nyielding an on average $5 \\%$ increase in the associated certification. When\nincorporating training time processes that enhance the certified radius, our\ntechnique shows even more promising results, with a uniform $4$ percentage\npoint increase in the achieved certified radius.",
    "descriptor": "\nComments: Accepted for Neurips`22, 19 pages, 14 figures, for associated code see this https URL\n",
    "authors": [
      "Andrew C. Cullen",
      "Paul Montague",
      "Shijie Liu",
      "Sarah M. Erfani",
      "Benjamin I.P. Rubinstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06077"
  },
  {
    "id": "arXiv:2210.06080",
    "title": "Computing Power Network: A Survey",
    "abstract": "With the rapid development of cloud computing, edge computing, and smart\ndevices, computing power resources indicate a trend of ubiquitous deployment.\nThe traditional network architecture cannot efficiently leverage these\ndistributed computing power resources due to computing power island effect. To\novercome these problems and improve network efficiency, a new network computing\nparadigm is proposed, i.e., Computing Power Network (CPN). Computing power\nnetwork can connect ubiquitous and heterogenous computing power resources\nthrough networking to realize computing power scheduling flexibly. In this\nsurvey, we make an exhaustive review on the state-of-the-art research efforts\non computing power network. We first give an overview of computing power\nnetwork, including definition, architecture, and advantages. Next, a\ncomprehensive elaboration of issues on computing power modeling, information\nawareness and announcement, resource allocation, network forwarding, computing\npower transaction platform and resource orchestration platform is presented.\nThe computing power network testbed is built and evaluated. The applications\nand use cases in computing power network are discussed. Then, the key enabling\ntechnologies for computing power network are introduced. Finally, open\nchallenges and future research directions are presented as well.",
    "descriptor": "",
    "authors": [
      "Yukun Sun",
      "Junlin Liu",
      "Haonan Huang",
      "Xing Zhang",
      "Bo Lei",
      "Jing Peng",
      "Wenbo Wang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06080"
  },
  {
    "id": "arXiv:2210.06088",
    "title": "Annihilation of Spurious Minima in Two-Layer ReLU Networks",
    "abstract": "We study the optimization problem associated with fitting two-layer ReLU\nneural networks with respect to the squared loss, where labels are generated by\na target network. Use is made of the rich symmetry structure to develop a novel\nset of tools for studying the mechanism by which over-parameterization\nannihilates spurious minima. Sharp analytic estimates are obtained for the loss\nand the Hessian spectrum at different minima, and it is proved that adding\nneurons can turn symmetric spurious minima into saddles; minima of lesser\nsymmetry require more neurons. Using Cauchy's interlacing theorem, we prove the\nexistence of descent directions in certain subspaces arising from the symmetry\nstructure of the loss function. This analytic approach uses techniques, new to\nthe field, from algebraic geometry, representation theory and symmetry\nbreaking, and confirms rigorously the effectiveness of over-parameterization in\nmaking the associated loss landscape accessible to gradient-based methods. For\na fixed number of neurons and inputs, the spectral results remain true under\nsymmetry breaking perturbation of the target.",
    "descriptor": "",
    "authors": [
      "Yossi Arjevani",
      "Michael Field"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.06088"
  },
  {
    "id": "arXiv:2210.06089",
    "title": "When are Local Queries Useful for Robust Learning?",
    "abstract": "Distributional assumptions have been shown to be necessary for the robust\nlearnability of concept classes when considering the exact-in-the-ball robust\nrisk and access to random examples by Gourdeau et al. (2019). In this paper, we\nstudy learning models where the learner is given more power through the use of\nlocal queries, and give the first distribution-free algorithms that perform\nrobust empirical risk minimization (ERM) for this notion of robustness. The\nfirst learning model we consider uses local membership queries (LMQ), where the\nlearner can query the label of points near the training sample. We show that,\nunder the uniform distribution, LMQs do not increase the robustness threshold\nof conjunctions and any superclass, e.g., decision lists and halfspaces. Faced\nwith this negative result, we introduce the local equivalence query (LEQ)\noracle, which returns whether the hypothesis and target concept agree in the\nperturbation region around a point in the training sample, as well as a\ncounterexample if it exists. We show a separation result: on one hand, if the\nquery radius $\\lambda$ is strictly smaller than the adversary's perturbation\nbudget $\\rho$, then distribution-free robust learning is impossible for a wide\nvariety of concept classes; on the other hand, the setting $\\lambda=\\rho$\nallows us to develop robust ERM algorithms. We then bound the query complexity\nof these algorithms based on online learning guarantees and further improve\nthese bounds for the special case of conjunctions. We finish by giving robust\nlearning algorithms for halfspaces with margins on both $\\{0,1\\}^n$ and\n$\\mathbb{R}^n$.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Pascale Gourdeau",
      "Varun Kanade",
      "Marta Kwiatkowska",
      "James Worrell"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06089"
  },
  {
    "id": "arXiv:2210.06091",
    "title": "Summary on the ISCSLP 2022 Chinese-English Code-Switching ASR Challenge",
    "abstract": "Code-switching automatic speech recognition becomes one of the most\nchallenging and the most valuable scenarios of automatic speech recognition,\ndue to the code-switching phenomenon between multilingual language and the\nfrequent occurrence of code-switching phenomenon in daily life. The ISCSLP 2022\nChinese-English Code-Switching Automatic Speech Recognition (CSASR) Challenge\naims to promote the development of code-switching automatic speech recognition.\nThe ISCSLP 2022 CSASR challenge provided two training sets, TAL_CSASR corpus\nand MagicData-RAMC corpus, a development and a test set for participants, which\nare used for CSASR model training and evaluation. Along with the challenge, we\nalso provide the baseline system performance for reference. As a result, more\nthan 40 teams participated in this challenge, and the winner team achieved\n16.70% Mixture Error Rate (MER) performance on the test set and has achieved\n9.8% MER absolute improvement compared with the baseline system. In this paper,\nwe will describe the datasets, the associated baselines system and the\nrequirements, and summarize the CSASR challenge results and major techniques\nand tricks used in the submitted systems.",
    "descriptor": "\nComments: accepted by ISCSLP 2022\n",
    "authors": [
      "Shuhao Deng",
      "Chengfei Li",
      "infeng Bai",
      "Qingqing Zhang",
      "Wei-Qiang Zhang",
      "Runyan Yang",
      "Gaofeng Cheng",
      "Pengyuan Zhang",
      "Yonghong Yan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06091"
  },
  {
    "id": "arXiv:2210.06092",
    "title": "Ergodic numerical approximations for stochastic Maxwell equations",
    "abstract": "In this paper, we propose a novel kind of numerical approximations to inherit\nthe ergodicity of stochastic Maxwell equations. The key to proving the\nergodicity lies in the uniform regularity estimates of the numerical solutions\nwith respect to time, which are established by analyzing some important\nphysical quantities. By introducing an auxiliary process, we show that the\nmean-square convergence order of the ergodic discontinuous Galerkin full\ndiscretization is $\\frac{1}{2}$ in the temporal direction and $\\frac{1}{2}$ in\nthe spatial direction, which provides the convergence order of the numerical\ninvariant measure to the exact one in $L^2$-Wasserstein distance.",
    "descriptor": "",
    "authors": [
      "Chuchu Chen",
      "Jialin Hong",
      "Lihai Ji",
      "Ge Liang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06092"
  },
  {
    "id": "arXiv:2210.06094",
    "title": "Teeth3DS: a benchmark for teeth segmentation and labeling from  intra-oral 3D scans",
    "abstract": "Teeth segmentation and labeling are critical components of Computer-Aided\nDentistry (CAD) systems. Indeed, before any orthodontic or prosthetic treatment\nplanning, a CAD system needs to first accurately segment and label each\ninstance of teeth visible in the 3D dental scan, this is to avoid\ntime-consuming manual adjustments by the dentist. Nevertheless, developing such\nan automated and accurate dental segmentation and labeling tool is very\nchallenging, especially given the lack of publicly available datasets or\nbenchmarks. This article introduces the first public benchmark, named Teeth3DS,\nwhich has been created in the frame of the 3DTeethSeg 2022 MICCAI challenge to\nboost the research field and inspire the 3D vision research community to work\non intra-oral 3D scans analysis such as teeth identification, segmentation,\nlabeling, 3D modeling and 3D reconstruction. Teeth3DS is made of 1800\nintra-oral scans (23999 annotated teeth) collected from 900 patients covering\nthe upper and lower jaws separately, acquired and validated by\northodontists/dental surgeons with more than 5 years of professional\nexperience.",
    "descriptor": "\nComments: 8 pages, 5 figures, 1 tables\n",
    "authors": [
      "Achraf Ben-Hamadou",
      "Oussama Smaoui",
      "Houda Chaabouni-Chouayakh",
      "Ahmed Rekik",
      "Sergi Pujades",
      "Edmond Boyer",
      "Julien Strippoli",
      "Aur\u00e9lien Thollot",
      "Hugo Setbon",
      "Cyril Trosset",
      "Edouard Ladroit"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06094"
  },
  {
    "id": "arXiv:2210.06095",
    "title": "A language for evaluating derivatives of functionals using automatic  differentiation",
    "abstract": "We present a simple functional programming language, called Dual PCF, that\nimplements forward mode automatic differentiation using dual numbers. The main\nnew feature of this language is the ability to evaluate - in a simple and\ndirect way - the directional derivative of functionals. We provide a wide range\nof examples of Lipschitz functions and functionals that can be defined in Dual\nPCF. We use domain theory both to give a denotational semantics to the language\nand to prove the correctness of the new derivative operator using logical\nrelations. To be able to differentiate functionals-including on function spaces\nequipped with their Scott topology that do not admit a norm-we develop a\ndomain-theoretic directional derivative that is Scott continuous and extends\nClarke's subgradient of real-valued locally Lipschitz maps on Banach spaces to\nreal-valued continuous maps on topological vector spaces. Finally, we show that\nwe can express arbitrary computable linear functionals in Dual PCF.",
    "descriptor": "\nComments: 44 pages, no figures\n",
    "authors": [
      "Pietro Di Gianantonio",
      "Abbas Edalat",
      "Ran Gutin"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.06095"
  },
  {
    "id": "arXiv:2210.06096",
    "title": "M$^3$Video: Masked Motion Modeling for Self-Supervised Video  Representation Learning",
    "abstract": "We study self-supervised video representation learning that seeks to learn\nvideo features from unlabeled videos, which is widely used for video analysis\nas labeling videos is labor-intensive. Current methods often mask some video\nregions and then train a model to reconstruct spatial information in these\nregions (e.g., original pixels). However, the model is easy to reconstruct this\ninformation by considering content in a single frame. As a result, it may\nneglect to learn the interactions between frames, which are critical for video\nanalysis. In this paper, we present a new self-supervised learning task, called\nMasked Motion Modeling (M$^3$Video), for learning representation by enforcing\nthe model to predict the motion of moving objects in the masked regions. To\ngenerate motion targets for this task, we track the objects using optical flow.\nThe motion targets consist of position transitions and shape changes of the\ntracked objects, thus the model has to consider multiple frames\ncomprehensively. Besides, to help the model capture fine-grained motion\ndetails, we enforce the model to predict trajectory motion targets in high\ntemporal resolution based on a video in low temporal resolution. After\npre-training using our M$^3$Video task, the model is able to anticipate\nfine-grained motion details even taking a sparsely sampled video as input. We\nconduct extensive experiments on four benchmark datasets. Remarkably, when\ndoing pre-training with 400 epochs, we improve the accuracy from 67.6\\% to\n69.2\\% and from 78.8\\% to 79.7\\% on Something-Something V2 and Kinetics-400\ndatasets, respectively.",
    "descriptor": "",
    "authors": [
      "Xinyu Sun",
      "Peihao Chen",
      "Liangwei Chen",
      "Thomas H. Li",
      "Mingkui Tan",
      "Chuang Gan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06096"
  },
  {
    "id": "arXiv:2210.06101",
    "title": "Federated Continual Learning for Text Classification via Selective  Inter-client Transfer",
    "abstract": "In this work, we combine the two paradigms: Federated Learning (FL) and\nContinual Learning (CL) for text classification task in cloud-edge continuum.\nThe objective of Federated Continual Learning (FCL) is to improve deep learning\nmodels over life time at each client by (relevant and efficient) knowledge\ntransfer without sharing data. Here, we address challenges in minimizing\ninter-client interference while knowledge sharing due to heterogeneous tasks\nacross clients in FCL setup. In doing so, we propose a novel framework,\nFederated Selective Inter-client Transfer (FedSeIT) which selectively combines\nmodel parameters of foreign clients. To further maximize knowledge transfer, we\nassess domain overlap and select informative tasks from the sequence of\nhistorical tasks at each foreign client while preserving privacy. Evaluating\nagainst the baselines, we show improved performance, a gain of (average) 12.4\\%\nin text classification over a sequence of tasks using five datasets from\ndiverse domains. To the best of our knowledge, this is the first work that\napplies FCL to NLP.",
    "descriptor": "\nComments: EMNLP2022 (Findings): 11 pages, 5 figures, 4 tables\n",
    "authors": [
      "Yatin Chaudhary",
      "Pranav Rai",
      "Matthias Schubert",
      "Hinrich Sch\u00fctze",
      "Pankaj Gupta"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06101"
  },
  {
    "id": "arXiv:2210.06104",
    "title": "EduQG: A Multi-format Multiple Choice Dataset for the Educational Domain",
    "abstract": "We introduce a high-quality dataset that contains 3,397 samples comprising\n(i) multiple choice questions, (ii) answers (including distractors), and (iii)\ntheir source documents, from the educational domain. Each question is phrased\nin two forms, normal and close. Correct answers are linked to source documents\nwith sentence-level annotations. Thus, our versatile dataset can be used for\nboth question and distractor generation, as well as to explore new challenges\nsuch as question format conversion. Furthermore, 903 questions are accompanied\nby their cognitive complexity level as per Bloom's taxonomy. All questions have\nbeen generated by educational experts rather than crowd workers to ensure they\nare maintaining educational and learning standards. Our analysis and\nexperiments suggest distinguishable differences between our dataset and\ncommonly used ones for question generation for educational purposes. We believe\nthis new dataset can serve as a valuable resource for research and evaluation\nin the educational domain. The dataset and baselines will be released to\nsupport further research in question generation.",
    "descriptor": "",
    "authors": [
      "Amir Hadifar",
      "Semere Kiros Bitew",
      "Johannes Deleu",
      "Chris Develder",
      "Thomas Demeester"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06104"
  },
  {
    "id": "arXiv:2210.06105",
    "title": "SpecRNet: Towards Faster and More Accessible Audio DeepFake Detection",
    "abstract": "Audio DeepFakes are utterances generated with the use of deep neural\nnetworks. They are highly misleading and pose a threat due to use in fake news,\nimpersonation, or extortion. In this work, we focus on increasing accessibility\nto the audio DeepFake detection methods by providing SpecRNet, a neural network\narchitecture characterized by a quick inference time and low computational\nrequirements. Our benchmark shows that SpecRNet, requiring up to about 40% less\ntime to process an audio sample, provides performance comparable to LCNN\narchitecture - one of the best audio DeepFake detection models. Such a method\ncan not only be used by online multimedia services to verify a large bulk of\ncontent uploaded daily but also, thanks to its low requirements, by average\ncitizens to evaluate materials on their devices. In addition, we provide\nbenchmarks in three unique settings that confirm the correctness of our model.\nThey reflect scenarios of low-resource datasets, detection on short utterances\nand limited attacks benchmark in which we take a closer look at the influence\nof particular attacks on given architectures.",
    "descriptor": "\nComments: Accepted by TrustCom 2022: The 21st IEEE International Conference on Trust, Security and Privacy in Computing and Communications\n",
    "authors": [
      "Piotr Kawa",
      "Marcin Plata",
      "Piotr Syga"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06105"
  },
  {
    "id": "arXiv:2210.06106",
    "title": "DiPA: Diverse and Probabilistically Accurate Interactive Prediction",
    "abstract": "Accurate prediction is important for operating an autonomous vehicle in\ninteractive scenarios. Previous interactive predictors have used closest-mode\nevaluations, which test if one of a set of predictions covers the ground-truth,\nbut not if additional unlikely predictions are made. The presence of unlikely\npredictions can interfere with planning, by indicating conflict with the ego\nplan when it is not likely to occur. Closest-mode evaluations are not\nsufficient for showing a predictor is useful, an effective predictor also needs\nto accurately estimate mode probabilities, and to be evaluated using\nprobabilistic measures. These two evaluation approaches, eg. predicted-mode RMS\nand minADE/FDE, are analogous to precision and recall in binary classification,\nand there is a challenging trade-off between prediction strategies for each. We\npresent DiPA, a method for producing diverse predictions while also capturing\naccurate probabilistic estimates. DiPA uses a flexible representation that\ncaptures interactions in widely varying road topologies, and uses a novel\ntraining regime for a Gaussian Mixture Model that supports diversity of\npredicted modes, along with accurate spatial distribution and mode probability\nestimates. DiPA achieves state-of-the-art performance on INTERACTION and NGSIM,\nand improves over a baseline (MFP) when both closest-mode and probabilistic\nevaluations are used at the same time.",
    "descriptor": "",
    "authors": [
      "Anthony Knittel",
      "Majd Hawasly",
      "Stefano V. Albrecht",
      "John Redford",
      "Subramanian Ramamoorthy"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06106"
  },
  {
    "id": "arXiv:2210.06107",
    "title": "Auto-bidding Equilibrium in ROI-Constrained Online Advertising Markets",
    "abstract": "Most of the work in auction design literature assumes that bidders behave\nrationally based on the information available. However, in today's online\nadvertising markets, one of the most important real-life applications of\nauction design, the data and computational power required to bid optimally are\nonly available to the auction designer, and an advertiser can only participate\nby setting performance objectives (clicks, conversions, etc.) for the campaign.\nIn this paper, we focus on value-maximizing campaigns with\nreturn-on-investment (ROI) constraints, which is widely adopted in many\nglobal-scale auto-bidding platforms. Through theoretical analysis and empirical\nexperiments on both synthetic and realistic data, we find that second price\nauction exhibits counter-intuitive behaviors in the resulted equilibrium and\nloses its dominant theoretical advantages in single-item scenarios. At the\nmarket scale, the equilibrium structure becomes complicated and opens up space\nfor bidders and even auctioneers to exploit. We also explore the broader\nimpacts of the auto-bidding mechanism beyond efficiency and strategyproofness.\nIn particular, multiplicity of equilibria and input sensitivity make the\nutility unstable. In addition, the interference among both bidders and goods\nintroduces bias into A/B testing, which hinders the development of even\nnon-bidding components. The aforementioned phenomena have been widely observed\nin practice, and our results indicate that one of the reasons might be\nintrinsic to the underlying auto-bidding mechanism. To deal with these\nchallenges, we provide suggestions and potential solutions for practitioners.",
    "descriptor": "",
    "authors": [
      "Juncheng Li",
      "Pingzhong Tang"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2210.06107"
  },
  {
    "id": "arXiv:2210.06108",
    "title": "Reconstructing Personalized Semantic Facial NeRF Models From Monocular  Video",
    "abstract": "We present a novel semantic model for human head defined with neural radiance\nfield. The 3D-consistent head model consist of a set of disentangled and\ninterpretable bases, and can be driven by low-dimensional expression\ncoefficients. Thanks to the powerful representation ability of neural radiance\nfield, the constructed model can represent complex facial attributes including\nhair, wearings, which can not be represented by traditional mesh blendshape. To\nconstruct the personalized semantic facial model, we propose to define the\nbases as several multi-level voxel fields. With a short monocular RGB video as\ninput, our method can construct the subject's semantic facial NeRF model with\nonly ten to twenty minutes, and can render a photo-realistic human head image\nin tens of miliseconds with a given expression coefficient and view direction.\nWith this novel representation, we apply it to many tasks like facial\nretargeting and expression editing. Experimental results demonstrate its strong\nrepresentation ability and training/inference speed. Demo videos and released\ncode are provided in our project page:\nhttps://ustc3dv.github.io/NeRFBlendShape/",
    "descriptor": "\nComments: Accepted by SIGGRAPH Asia 2022 (Journal Track). Project page: this https URL\n",
    "authors": [
      "Xuan Gao",
      "Chenglai Zhong",
      "Jun Xiang",
      "Yang Hong",
      "Yudong Guo",
      "Juyong Zhang"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06108"
  },
  {
    "id": "arXiv:2210.06110",
    "title": "Uplift and Upsample: Efficient 3D Human Pose Estimation with Uplifting  Transformers",
    "abstract": "The state-of-the-art for monocular 3D human pose estimation in videos is\ndominated by the paradigm of 2D-to-3D pose uplifting. While the uplifting\nmethods themselves are rather efficient, the true computational complexity\ndepends on the per-frame 2D pose estimation. In this paper, we present a\nTransformer-based pose uplifting scheme that can operate on temporally sparse\n2D pose sequences but still produce temporally dense 3D pose estimates. We show\nhow masked token modeling can be utilized for temporal upsampling within\nTransformer blocks. This allows to decouple the sampling rate of input 2D poses\nand the target frame rate of the video and drastically decreases the total\ncomputational complexity. Additionally, we explore the option of pre-training\non large motion capture archives, which has been largely neglected so far. We\nevaluate our method on two popular benchmark datasets: Human3.6M and\nMPI-INF-3DHP. With an MPJPE of 45.0 mm and 46.9 mm, respectively, our proposed\nmethod can compete with the state-of-the-art while reducing inference time by a\nfactor of 12. This enables real-time throughput with variable consumer hardware\nin stationary and mobile applications. We release our code and models at\nhttps://github.com/goldbricklemon/uplift-upsample-3dhpe",
    "descriptor": "\nComments: Accepted at IEEE/CVF WACV 2023\n",
    "authors": [
      "Moritz Einfalt",
      "Katja Ludwig",
      "Rainer Lienhart"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06110"
  },
  {
    "id": "arXiv:2210.06111",
    "title": "THUEE system description for NIST 2020 SRE CTS challenge",
    "abstract": "This paper presents the system description of the THUEE team for the NIST\n2020 Speaker Recognition Evaluation (SRE) conversational telephone speech (CTS)\nchallenge. The subsystems including ResNet74, ResNet152, and RepVGG-B2 are\ndeveloped as speaker embedding extractors in this evaluation. We used combined\nAM-Softmax and AAM-Softmax based loss functions, namely CM-Softmax. We adopted\na two-staged training strategy to further improve system performance. We fused\nall individual systems as our final submission. Our approach leads to excellent\nperformance and ranks 1st in the challenge.",
    "descriptor": "\nComments: 3 pages, 1 table; System desciption of NIST 2020 SRE CTS challenge\n",
    "authors": [
      "Yu Zheng",
      "Jinghan Peng",
      "Miao Zhao",
      "Yufeng Ma",
      "Min Liu",
      "Xinyue Ma",
      "Tianyu Liang",
      "Tianlong Kong",
      "Liang He",
      "Minqiang Xu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06111"
  },
  {
    "id": "arXiv:2210.06112",
    "title": "Fast Bayesian Updates for Deep Learning with a Use Case in Active  Learning",
    "abstract": "Retraining deep neural networks when new data arrives is typically\ncomputationally expensive. Moreover, certain applications do not allow such\ncostly retraining due to time or computational constraints. Fast Bayesian\nupdates are a possible solution to this issue. Therefore, we propose a Bayesian\nupdate based on Monte-Carlo samples and a last-layer Laplace approximation for\ndifferent Bayesian neural network types, i.e., Dropout, Ensemble, and Spectral\nNormalized Neural Gaussian Process (SNGP). In a large-scale evaluation study,\nwe show that our updates combined with SNGP represent a fast and competitive\nalternative to costly retraining. As a use case, we combine the Bayesian\nupdates for SNGP with different sequential query strategies to exemplarily\ndemonstrate their improved selection performance in active learning.",
    "descriptor": "\nComments: 25 pages, 10 figures, submitted to ICLR\n",
    "authors": [
      "Marek Herde",
      "Zhixin Huang",
      "Denis Huseljic",
      "Daniel Kottke",
      "Stephan Vogt",
      "Bernhard Sick"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06112"
  },
  {
    "id": "arXiv:2210.06113",
    "title": "Timestamp tokens: a better coordination primitive for data-processing  systems",
    "abstract": "Distributed data processing systems have advanced through models that expose\nmore and more opportunities for concurrency within a computation. The\nscheduling of these increasingly sophisticated models has become the bottleneck\nfor improved throughput and reduced latency.\nWe present a new coordination primitive for dataflow systems, the timestamp\ntoken, which minimizes the volume of information shared between the computation\nand host system, without surrendering precision about concurrency. Several\nprojects have now used timestamp tokens, and were able to explore computational\nidioms that could not be expressed easily, if at all, in other platforms.\nImportantly, these projects did not need to design and implement whole systems\nto support their research.",
    "descriptor": "",
    "authors": [
      "Andrea Lattuada",
      "Frank McSherry"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06113"
  },
  {
    "id": "arXiv:2210.06116",
    "title": "Self-stabilization and byzantine tolerance for maximal independt setb  ELF-STABILIZATION",
    "abstract": "We analyze the impact of transient and Byzantine faults on the construction\nof a maximal independent set in a general network. We adapt the\nself-stabilizing algorithm presented by Turau `for computing such a vertex set.\nOur algorithm is self-stabilizing and also works under the more difficult\ncontext of arbitrary Byzantine faults.\nByzantine nodes can prevent nodes close to them from taking part in the\nindependent set for an arbitrarily long time. We give boundaries to their\nimpact by focusing on the set of all nodes excluding nodes at distance 1 or\nless of Byzantine nodes, and excluding some of the nodes at distance 2. As far\nas we know, we present the first algorithm tolerating both transient and\nByzantine faults under the fair distributed daemon.\nWe prove that this algorithm converges in $ \\mathcal O(\\Delta n)$ rounds\nw.h.p., where $n$ and $\\Delta$ are the size and the maximum degree of the\nnetwork, resp. Additionally, we present a modified version of this algorithm\nfor anonymous systems under the adversarial distributed daemon that converges\nin\n$ \\mathcal O(n^{2})$ expected number of steps.",
    "descriptor": "\nComments: it is an extented version of Self-stabilization and Byzantine Tolerance for Maximal Independent Set, Cohen, Johanne and Pilard, Laurence and S{\\'e}nizergues, Jonas, in International Symposium on Stabilizing, Safety, and Security of Distributed Systems, 2021. arXiv admin note: substantial text overlap with arXiv:2111.08348\n",
    "authors": [
      "Johanne Cohen",
      "Laurence Pilard",
      "Fran\u00e7ois Pirot",
      "Jonas S\u00e9nizergues"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06116"
  },
  {
    "id": "arXiv:2210.06118",
    "title": "Towards Mining Creative Thinking Patterns from Educational Data",
    "abstract": "Creativity, i.e., the process of generating and developing fresh and original\nideas or products that are useful or effective, is a valuable skill in a\nvariety of domains. Creativity is called an essential 21st-century skill that\nshould be taught in schools. The use of educational technology to promote\ncreativity is an active study field, as evidenced by several studies linking\ncreativity in the classroom to beneficial learning outcomes. Despite the\nburgeoning body of research on adaptive technology for education, mining\ncreative thinking patterns from educational data remains a challenging task. In\nthis paper, to address this challenge, we put the first step towards\nformalizing educational knowledge by constructing a domain-specific Knowledge\nBase to identify essential concepts, facts, and assumptions in identifying\ncreative patterns. We then introduce a pipeline to contextualize the raw\neducational data, such as assessments and class activities. Finally, we present\na rule-based approach to learning from the Knowledge Base, and facilitate\nmining creative thinking patterns from contextualized data and knowledge. We\nevaluate our approach with real-world datasets and highlight how the proposed\npipeline can help instructors understand creative thinking patterns from\nstudents' activities and assessment tasks.",
    "descriptor": "",
    "authors": [
      "Nasrin Shabani"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06118"
  },
  {
    "id": "arXiv:2210.06120",
    "title": "Efficient Gaussian Process Model on Class-Imbalanced Datasets for  Generalized Zero-Shot Learning",
    "abstract": "Zero-Shot Learning (ZSL) models aim to classify object classes that are not\nseen during the training process. However, the problem of class imbalance is\nrarely discussed, despite its presence in several ZSL datasets. In this paper,\nwe propose a Neural Network model that learns a latent feature embedding and a\nGaussian Process (GP) regression model that predicts latent feature prototypes\nof unseen classes. A calibrated classifier is then constructed for ZSL and\nGeneralized ZSL tasks. Our Neural Network model is trained efficiently with a\nsimple training strategy that mitigates the impact of class-imbalanced training\ndata. The model has an average training time of 5 minutes and can achieve\nstate-of-the-art (SOTA) performance on imbalanced ZSL benchmark datasets like\nAWA2, AWA1 and APY, while having relatively good performance on the SUN and CUB\ndatasets.",
    "descriptor": "\nComments: Paper Accepted in ICPR 2022\n",
    "authors": [
      "Changkun Ye",
      "Nick Barnes",
      "Lars Petersson",
      "Russell Tsuchida"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06120"
  },
  {
    "id": "arXiv:2210.06121",
    "title": "Specializing Scope Graph Resolution Queries: Extended Edition",
    "abstract": "To warrant programmer productivity, type checker results should be correct\nand available quickly. Correctness can be provided when a type checker\nimplementation corresponds to a declarative type system specification. Statix\nis a type system specification language which achieves this by automatically\nderiving type checker implementations from declarative typing rules. A key\nfeature of Statix is that it uses scope graphs for declarative specification of\nname resolution. However, compared to hand-written type checkers, type checkers\nderived from Statix specifications have sub-optimal run time performance.\nIn this paper, we identify and resolve a performance bottleneck in the Statix\nsolver, namely part of the name resolution algorithm, using partial evaluation.\nTo this end, we introduce a tailored procedural intermediate query resolution\nlanguage, and provide a specializer that translates declarative queries to this\nlanguage. Evaluating this specializer by comparing type checking run time\nperformance on three benchmarks (Apache Commons CSV, IO, and Lang3), shows that\nour specializer improves query resolution time up to 7.7x, which reduces the\ntotal type checking run time by 38 - 48%.",
    "descriptor": "",
    "authors": [
      "Aron Zwaan"
    ],
    "subjectives": [
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.06121"
  },
  {
    "id": "arXiv:2210.06126",
    "title": "Regularized Graph Structure Learning with Semantic Knowledge for  Multi-variates Time-Series Forecasting",
    "abstract": "Multivariate time-series forecasting is a critical task for many\napplications, and graph time-series network is widely studied due to its\ncapability to capture the spatial-temporal correlation simultaneously. However,\nmost existing works focus more on learning with the explicit prior graph\nstructure, while ignoring potential information from the implicit graph\nstructure, yielding incomplete structure modeling. Some recent works attempt to\nlearn the intrinsic or implicit graph structure directly while lacking a way to\ncombine explicit prior structure with implicit structure together. In this\npaper, we propose Regularized Graph Structure Learning (RGSL) model to\nincorporate both explicit prior structure and implicit structure together, and\nlearn the forecasting deep networks along with the graph structure. RGSL\nconsists of two innovative modules. First, we derive an implicit dense\nsimilarity matrix through node embedding, and learn the sparse graph structure\nusing the Regularized Graph Generation (RGG) based on the Gumbel Softmax trick.\nSecond, we propose a Laplacian Matrix Mixed-up Module (LM3) to fuse the\nexplicit graph and implicit graph together. We conduct experiments on three\nreal-word datasets. Results show that the proposed RGSL model outperforms\nexisting graph forecasting algorithms with a notable margin, while learning\nmeaningful graph structure simultaneously. Our code and models are made\npublicly available at https://github.com/alipay/RGSL.git.",
    "descriptor": "\nComments: to be published in IJCAI2022\n",
    "authors": [
      "Hongyuan Yu",
      "Ting Li",
      "Weichen Yu",
      "Jianguo Li",
      "Yan Huang",
      "Liang Wang",
      "Alex Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06126"
  },
  {
    "id": "arXiv:2210.06132",
    "title": "Integrating Accessibility in a Mobile App Development Course",
    "abstract": "The growing interest in accessible software reflects in computing educators'\nand education researchers' efforts to include accessibility in core computing\neducation. We integrated accessibility in a junior/senior-level Android app\ndevelopment course at a large private university in India. The course\nintroduced three accessibility-related topics using various interventions:\nAccessibility Awareness (a guest lecture by a legal expert), Technical\nKnowledge (lectures on Android accessibility guidelines and testing practices\nand graded components for implementing accessibility in programming\nassignments), and Empathy (an activity that required students to blindfold\nthemselves and interact with their phones using a screen-reader). We evaluated\ntheir impact on student learning using three instruments: (A) A pre/post-course\nquestionnaire, (B) Reflective questions on each of the four programming\nassignments, and (C) Midterm and Final exam questions. Our findings demonstrate\nthat: (A) significantly more ($p<.05$) students considered disabilities when\ndesigning an app after taking this course, (B) many students developed empathy\ntowards the challenges persons with disabilities face while using inaccessible\napps, and (C) all students could correctly identify at least one accessibility\nissue in the user interface of a real-world app given its screenshot, and 90%\nof them could provide a correct solution to fix it.",
    "descriptor": "\nComments: 7 pages, 1 figure, submitted to ACM SIGCSE 2023\n",
    "authors": [
      "Jaskaran Singh Bhatia",
      "Parthasarathy P D",
      "Snigdha Tiwari",
      "Dhruv Nagpal",
      "Swaroop Joshi"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.06132"
  },
  {
    "id": "arXiv:2210.06134",
    "title": "Identity, Crimes, and Law Enforcement in the Metaverse",
    "abstract": "With the boom in metaverse-related projects in major areas of the public's\nlife, the safety of users becomes a pressing concern. We believe that an\ninternational legal framework should be established to promote collaboration\namong nations, facilitate crime investigation, and support democratic\ngovernance. In this paper, we discuss the legal concerns of identity, crimes\nthat could occur based on incidents in existing virtual worlds, and challenges\nto unified law enforcement in the metaverse.",
    "descriptor": "",
    "authors": [
      "Hua Xuan Qin",
      "Yuyang Wang",
      "Pan Hui"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2210.06134"
  },
  {
    "id": "arXiv:2210.06138",
    "title": "Improved Data Augmentation for Translation Suggestion",
    "abstract": "Translation suggestion (TS) models are used to automatically provide\nalternative suggestions for incorrect spans in sentences generated by machine\ntranslation. This paper introduces the system used in our submission to the\nWMT'22 Translation Suggestion shared task. Our system is based on the ensemble\nof different translation architectures, including Transformer, SA-Transformer,\nand DynamicConv. We use three strategies to construct synthetic data from\nparallel corpora to compensate for the lack of supervised data. In addition, we\nintroduce a multi-phase pre-training strategy, adding an additional\npre-training phase with in-domain data. We rank second and third on the\nEnglish-German and English-Chinese bidirectional tasks, respectively.",
    "descriptor": "",
    "authors": [
      "Hongxiao Zhang",
      "Siyu Lai",
      "Songming Zhang",
      "Hui Huang",
      "Yufeng Chen",
      "Jinan Xu",
      "Jian Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06138"
  },
  {
    "id": "arXiv:2210.06139",
    "title": "Zero-Knowledge Optimal Monetary Policy under Stochastic Dominance",
    "abstract": "Optimal simple rules for the monetary policy of the first stochastically\ndominant crypto-currency are derived in a Dynamic Stochastic General\nEquilibrium (DSGE) model, in order to provide optimal responses to changes in\ninflation, output, and other sources of uncertainty. The optimal monetary\npolicy stochastically dominates all the previous crypto-currencies, thus the\nefficient portfolio is to go long on the stochastically dominant\ncrypto-currency: a strategy-proof arbitrage featuring a higher Omega ratio with\nhigher expected returns, inducing an investment-efficient Nash equilibrium over\nthe crypto-market. Zero-knowledge proofs of the monetary policy are committed\non the blockchain: an implementation is provided.",
    "descriptor": "\nComments: Implementation available at: this https URL\n",
    "authors": [
      "David Cerezo S\u00e1nchez"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "General Economics (econ.GN)"
    ],
    "url": "https://arxiv.org/abs/2210.06139"
  },
  {
    "id": "arXiv:2210.06143",
    "title": "On the Importance of Gradient Norm in PAC-Bayesian Bounds",
    "abstract": "Generalization bounds which assess the difference between the true risk and\nthe empirical risk, have been studied extensively. However, to obtain bounds,\ncurrent techniques use strict assumptions such as a uniformly bounded or a\nLipschitz loss function. To avoid these assumptions, in this paper, we follow\nan alternative approach: we relax uniform bounds assumptions by using\non-average bounded loss and on-average bounded gradient norm assumptions.\nFollowing this relaxation, we propose a new generalization bound that exploits\nthe contractivity of the log-Sobolev inequalities. These inequalities add an\nadditional loss-gradient norm term to the generalization bound, which is\nintuitively a surrogate of the model complexity. We apply the proposed bound on\nBayesian deep nets and empirically analyze the effect of this new loss-gradient\nnorm term on different neural architectures.",
    "descriptor": "\nComments: NeurIPS 22. arXiv admin note: text overlap with arXiv:2002.09866\n",
    "authors": [
      "Itai Gat",
      "Yossi Adi",
      "Alexander Schwing",
      "Tamir Hazan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06143"
  },
  {
    "id": "arXiv:2210.06145",
    "title": "Can Artificial Intelligence Reconstruct Ancient Mosaics?",
    "abstract": "A large number of ancient mosaics have not reached us because they have been\ndestroyed by erosion, earthquakes, looting or even used as materials in newer\nconstruction. To make things worse, among the small fraction of mosaics that we\nhave been able to recover, many are damaged or incomplete. Therefore,\nrestoration and reconstruction of mosaics play a fundamental role to preserve\ncultural heritage and to understand the role of mosaics in ancient cultures.\nThis reconstruction has traditionally been done manually and more recently\nusing computer graphics programs but always by humans. In the last years,\nArtificial Intelligence (AI) has made impressive progress in the generation of\nimages from text descriptions and reference images. State of the art AI tools\nsuch as DALL-E2 can generate high quality images from text prompts and can take\na reference image to guide the process. In august 2022, DALL-E2 launched a new\nfeature called outpainting that takes as input an incomplete image and a text\nprompt and then generates a complete image filling the missing parts. In this\npaper, we explore whether this innovative technology can be used to reconstruct\nmosaics with missing parts. Hence a set of ancient mosaics have been used and\nreconstructed using DALL-E2; results are promising showing that AI is able to\ninterpret the key features of the mosaics and is able to produce\nreconstructions that capture the essence of the scene. However, in some cases\nAI fails to reproduce some details, geometric forms or introduces elements that\nare not consistent with the rest of the mosaic. This suggests that as AI image\ngeneration technology matures in the next few years, it could be a valuable\ntool for mosaic reconstruction going forward.",
    "descriptor": "",
    "authors": [
      "Fernando Moral-Andr\u00e9s",
      "Elena Merino-G\u00f3mez",
      "Pedro Reviriego",
      "Fabrizio Lombardi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06145"
  },
  {
    "id": "arXiv:2210.06149",
    "title": "On the Internal Stability of Diffusively Coupled Multi-Agent Systems and  the Dangers of Cancel Culture",
    "abstract": "We study internal stability in the context of diffusively-coupled control\narchitectures, common in multi-agent systems (i.e. the celebrated consensus\nprotocol), for linear time-invariant agents. We derive a condition under which\nthe system can not be stabilized by any controller from that class. In the\nfinite-dimensional case the condition states that diffusive controllers cannot\nstabilize agents that share common unstable dynamics, directions included. This\nclass always contains the group of homogeneous unstable agents, like\nintegrators. We argue that the underlying reason is intrinsic cancellations of\nunstable agent dynamics by such controllers, even static ones, where\ndirectional properties play a key role. The intrinsic lack of internal\nstability explains the notorious behavior of some distributed control protocols\nwhen affected by measurement noise or exogenous disturbances.",
    "descriptor": "\nComments: 9 pages, 3 figure, submitted to Automatica\n",
    "authors": [
      "Gal Barkai",
      "Leonid Mirkin",
      "Daniel Zelazo"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.06149"
  },
  {
    "id": "arXiv:2210.06150",
    "title": "Annotating Norwegian Language Varieties on Twitter for Part-of-Speech",
    "abstract": "Norwegian Twitter data poses an interesting challenge for Natural Language\nProcessing (NLP) tasks. These texts are difficult for models trained on\nstandardized text in one of the two Norwegian written forms (Bokm{\\aa}l and\nNynorsk), as they contain both the typical variation of social media text, as\nwell as a large amount of dialectal variety. In this paper we present a novel\nNorwegian Twitter dataset annotated with POS-tags. We show that models trained\non Universal Dependency (UD) data perform worse when evaluated against this\ndataset, and that models trained on Bokm{\\aa}l generally perform better than\nthose trained on Nynorsk. We also see that performance on dialectal tweets is\ncomparable to the written standards for some models. Finally we perform a\ndetailed analysis of the errors that models commonly make on this data.",
    "descriptor": "\nComments: Accepted at the Ninth Workshop on NLP for Similar Languages, Varieties and Dialects (Vardial2022). Collocated with COLING2022\n",
    "authors": [
      "Petter M\u00e6hlum",
      "Andre K\u00e5sen",
      "Samia Touileb",
      "Jeremy Barnes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06150"
  },
  {
    "id": "arXiv:2210.06154",
    "title": "Aergia: Leveraging Heterogeneity in Federated Learning Systems",
    "abstract": "Federated Learning (FL) is a popular approach for distributed deep learning\nthat prevents the pooling of large amounts of data in a central server. FL\nrelies on clients to update a global model using their local datasets.\nClassical FL algorithms use a central federator that, for each training round,\nwaits for all clients to send their model updates before aggregating them. In\npractical deployments, clients might have different computing powers and\nnetwork capabilities, which might lead slow clients to become performance\nbottlenecks. Previous works have suggested to use a deadline for each learning\nround so that the federator ignores the late updates of slow clients, or so\nthat clients send partially trained models before the deadline. To speed up the\ntraining process, we instead propose Aergia, a novel approach where slow\nclients (i) freeze the part of their model that is the most computationally\nintensive to train; (ii) train the unfrozen part of their model; and (iii)\noffload the training of the frozen part of their model to a faster client that\ntrains it using its own dataset. The offloading decisions are orchestrated by\nthe federator based on the training speed that clients report and on the\nsimilarities between their datasets, which are privately evaluated thanks to a\ntrusted execution environment. We show through extensive experiments that\nAergia maintains high accuracy and significantly reduces the training time\nunder heterogeneous settings by up to 27% and 53% compared to FedAvg and TiFL,\nrespectively.",
    "descriptor": "\nComments: This paper is accepted at the 23rd ACM/IFIP International Middleware Conference (Middleware '22)\n",
    "authors": [
      "Bart Cox",
      "Lydia Y. Chen",
      "J\u00e9r\u00e9mie Decouchant"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06154"
  },
  {
    "id": "arXiv:2210.06155",
    "title": "ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich  Document Understanding",
    "abstract": "Recent years have witnessed the rise and success of pre-training techniques\nin visually-rich document understanding. However, most existing methods lack\nthe systematic mining and utilization of layout-centered knowledge, leading to\nsub-optimal performances. In this paper, we propose ERNIE-Layout, a novel\ndocument pre-training solution with layout knowledge enhancement in the whole\nworkflow, to learn better representations that combine the features from text,\nlayout, and image. Specifically, we first rearrange input sequences in the\nserialization stage, and then present a correlative pre-training task, reading\norder prediction, to learn the proper reading order of documents. To improve\nthe layout awareness of the model, we integrate a spatial-aware disentangled\nattention into the multi-modal transformer and a replaced regions prediction\ntask into the pre-training phase. Experimental results show that ERNIE-Layout\nachieves superior performance on various downstream tasks, setting new\nstate-of-the-art on key information extraction, document image classification,\nand document question answering datasets. The code and models are publicly\navailable at\nthis http URL",
    "descriptor": "\nComments: Accepted to EMNLP 2022 (Findings)\n",
    "authors": [
      "Qiming Peng",
      "Yinxu Pan",
      "Wenjin Wang",
      "Bin Luo",
      "Zhenyu Zhang",
      "Zhengjie Huang",
      "Teng Hu",
      "Weichong Yin",
      "Yongfeng Chen",
      "Yin Zhang",
      "Shikun Feng",
      "Yu Sun",
      "Hao Tian",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06155"
  },
  {
    "id": "arXiv:2210.06158",
    "title": "A Hybrid System for Real-time Rendering of Depth of Field Effect in  Games",
    "abstract": "Real-time depth of field in game cinematics tends to approximate the\nsemi-transparent silhouettes of out-of-focus objects through post-processing\ntechniques. We leverage ray tracing hardware acceleration and spatio-temporal\nreconstruction to improve the realism of such semi-transparent regions through\nhybrid rendering, while maintaining interactive frame rates for immersive\ngaming. This paper extends our previous work with a complete presentation of\nour technique and details on its design, implementation, and future work.",
    "descriptor": "\nComments: Best Student Paper Award (GRAPP 2022)\n",
    "authors": [
      "Yu Wei Tan",
      "Nicholas Chua",
      "Nathan Biette",
      "Anand Bhojan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06158"
  },
  {
    "id": "arXiv:2210.06159",
    "title": "Hybrid MBlur: A Systematic Approach to Augment Rasterization with Ray  Tracing for Rendering Motion Blur in Games",
    "abstract": "Motion blur is commonly used in game cinematics to achieve photorealism by\nmodelling the behaviour of the camera shutter and simulating its effect\nassociated with the relative motion of scene objects. A common real-time\npost-process approach is spatial sampling, where the directional blur of a\nmoving object is rendered by integrating its colour based on velocity\ninformation within a single frame. However, such screen space approaches\ntypically cannot produce accurate partial occlusion semi-transparencies. Our\nreal-time hybrid rendering technique leverages hardware-accelerated ray tracing\nto correct post-process partial occlusion artifacts by advancing rays\nrecursively into the scene to retrieve background information for\nmotion-blurred regions, with reasonable additional performance cost for\nrendering game contents. We extend our previous work with details on the\ndesign, implementation, and future work of the technique as well as performance\ncomparisons with post-processing.",
    "descriptor": "",
    "authors": [
      "Yu Wei Tan",
      "Xiaohan Cui",
      "Anand Bhojan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06159"
  },
  {
    "id": "arXiv:2210.06160",
    "title": "RTSDF: Real-time Signed Distance Fields for Soft Shadow Approximation in  Games",
    "abstract": "Signed distance fields (SDFs) are a form of surface representation widely\nused in computer graphics, having applications in rendering, collision\ndetection and modelling. In interactive media such as games, high-resolution\nSDFs are commonly produced offline and subsequently loaded into the\napplication, representing rigid meshes only. This work develops a novel\ntechnique that combines jump flooding and ray tracing to generate approximate\nSDFs in real-time. Our approach can produce relatively accurate scene\nrepresentation for rendering soft shadows while maintaining interactive frame\nrates. We extend our previous work with details on the design and\nimplementation as well as visual quality and performance evaluation of the\ntechnique.",
    "descriptor": "",
    "authors": [
      "Yu Wei Tan",
      "Nicholas Chua",
      "Clarence Koh",
      "Anand Bhojan"
    ],
    "subjectives": [
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06160"
  },
  {
    "id": "arXiv:2210.06163",
    "title": "Privacy of federated QR decomposition using additive secure multiparty  computation",
    "abstract": "Federated learning (FL) is a privacy-aware data mining strategy keeping the\nprivate data on the owners' machine and thereby confidential. The clients\ncompute local models and send them to an aggregator which computes a global\nmodel. In hybrid FL, the local parameters are additionally masked using secure\naggregation, such that only the global aggregated statistics become available\nin clear text, not the client specific updates. Federated QR decomposition has\nnot been studied extensively in the context of cross-silo federated learning.\nIn this article, we investigate the suitability of three QR decomposition\nalgorithms for cross-silo FL and suggest a privacy-aware QR decomposition\nscheme based on the Gram-Schmidt algorithm which does not blatantly leak raw\ndata. We apply the algorithm to compute linear regression in a federated\nmanner.",
    "descriptor": "\nComments: 10 pages, 2 figures, 2 tables\n",
    "authors": [
      "Anne Hartebrodt",
      "Richard R\u00f6ttger"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06163"
  },
  {
    "id": "arXiv:2210.06164",
    "title": "Focusing on Context is NICE: Improving Overshadowed Entity  Disambiguation",
    "abstract": "Entity disambiguation (ED) is the task of mapping an ambiguous entity mention\nto the corresponding entry in a structured knowledge base. Previous research\nshowed that entity overshadowing is a significant challenge for existing ED\nmodels: when presented with an ambiguous entity mention, the models are much\nmore likely to rank a more frequent yet less contextually relevant entity at\nthe top. Here, we present NICE, an iterative approach that uses entity type\ninformation to leverage context and avoid over-relying on the frequency-based\nprior. Our experiments show that NICE achieves the best performance results on\nthe overshadowed entities while still performing competitively on the frequent\nentities.",
    "descriptor": "",
    "authors": [
      "Vera Provatorova",
      "Simone Tedeschi",
      "Svitlana Vakulenko",
      "Roberto Navigli",
      "Evangelos Kanoulas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2210.06164"
  },
  {
    "id": "arXiv:2210.06168",
    "title": "The Role of Exploration for Task Transfer in Reinforcement Learning",
    "abstract": "The exploration--exploitation trade-off in reinforcement learning (RL) is a\nwell-known and much-studied problem that balances greedy action selection with\nnovel experience, and the study of exploration methods is usually only\nconsidered in the context of learning the optimal policy for a single learning\ntask. However, in the context of online task transfer, where there is a change\nto the task during online operation, we hypothesize that exploration strategies\nthat anticipate the need to adapt to future tasks can have a pronounced impact\non the efficiency of transfer. As such, we re-examine the\nexploration--exploitation trade-off in the context of transfer learning. In\nthis work, we review reinforcement learning exploration methods, define a\ntaxonomy with which to organize them, analyze these methods' differences in the\ncontext of task transfer, and suggest avenues for future investigation.",
    "descriptor": "\nComments: 4 pages\n",
    "authors": [
      "Jonathan C Balloch",
      "Julia Kim",
      "and Jessica L Inman",
      "Mark O Riedl"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06168"
  },
  {
    "id": "arXiv:2210.06169",
    "title": "Model order reduction of solidification problems",
    "abstract": "Advection driven problems are known to be difficult to model with a reduced\nbasis because of a slow decay of the Kolmogorov $N$-width. This paper\ninvestigates how this challenge transfers to the context of solidification\nproblems and tries to answer when and to what extend reduced order models\n(ROMs) work for solidification problems. In solidification problems, the\nchallenge is not the advection per se, but rather a moving solidification\nfront. This paper studies reduced spaces for 1D step functions that move in\ntime, which can either be seen as advection of a quantity or as a moving\nsolidification front. Furthermore, the reduced space of a 2D solidification\ntest case is compared with the reduced space of an alloy solidification\nfeaturing a mushy zone. The results show that not only the PDE itself, but the\nsmoothness of the solution is crucial for the decay of the singular values and\nthus the quality of a reduced space representation.",
    "descriptor": "",
    "authors": [
      "Florian Arbes",
      "\u00d8yvind Jensen",
      "Kent-Andre Mardal",
      "J\u00f8rgen S. Dokken"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06169"
  },
  {
    "id": "arXiv:2210.06171",
    "title": "Learning to Optimize Quasi-Newton Methods",
    "abstract": "We introduce a novel machine learning optimizer called LODO, which online\nmeta-learns an implicit inverse Hessian of the loss as a subroutine of\nquasi-Newton optimization. Our optimizer merges Learning to Optimize (L2O)\ntechniques with quasi-Newton methods to learn neural representations of\nsymmetric matrix vector products, which are more flexible than those in other\nquasi-Newton methods. Unlike other L2O methods, ours does not require any\nmeta-training on a training task distribution, and instead learns to optimize\non the fly while optimizing on the test task, adapting to the local\ncharacteristics of the loss landscape while traversing it. Theoretically, we\nshow that our optimizer approximates the inverse Hessian in noisy loss\nlandscapes and is capable of representing a wide range of inverse Hessians. We\nexperimentally verify our algorithm's performance in the presence of noise, and\nshow that simpler alternatives for representing the inverse Hessians worsen\nperformance. Lastly, we use our optimizer to train a semi-realistic deep neural\nnetwork with 95k parameters, and obtain competitive results against standard\nneural network optimizers.",
    "descriptor": "\nComments: Paper under double-blind review at ICLR 2023\n",
    "authors": [
      "Isaac Liao",
      "Rumen R. Dangovski",
      "Jakob N. Foerster",
      "Marin Solja\u010di\u0107"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06171"
  },
  {
    "id": "arXiv:2210.06177",
    "title": "VCSE: Time-Domain Visual-Contextual Speaker Extraction Network",
    "abstract": "Speaker extraction seeks to extract the target speech in a multi-talker\nscenario given an auxiliary reference. Such reference can be auditory, i.e., a\npre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic\nsequence. References in different modalities provide distinct and complementary\ninformation that could be fused to form top-down attention on the target\nspeaker. Previous studies have introduced visual and contextual modalities in a\nsingle model. In this paper, we propose a two-stage time-domain\nvisual-contextual speaker extraction network named VCSE, which incorporates\nvisual and self-enrolled contextual cues stage by stage to take full advantage\nof every modality. In the first stage, we pre-extract a target speech with\nvisual cues and estimate the underlying phonetic sequence. In the second stage,\nwe refine the pre-extracted target speech with the self-enrolled contextual\ncues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3)\ndatabase demonstrate that our proposed VCSE network consistently outperforms\nother state-of-the-art baselines.",
    "descriptor": "",
    "authors": [
      "Junjie Li",
      "Meng Ge",
      "Zexu Pan",
      "Longbiao Wang",
      "Jianwu Dang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06177"
  },
  {
    "id": "arXiv:2210.06183",
    "title": "Transfer Learning on Heterogeneous Feature Spaces for Treatment Effects  Estimation",
    "abstract": "Consider the problem of improving the estimation of conditional average\ntreatment effects (CATE) for a target domain of interest by leveraging related\ninformation from a source domain with a different feature space. This\nheterogeneous transfer learning problem for CATE estimation is ubiquitous in\nareas such as healthcare where we may wish to evaluate the effectiveness of a\ntreatment for a new patient population for which different clinical covariates\nand limited data are available. In this paper, we address this problem by\nintroducing several building blocks that use representation learning to handle\nthe heterogeneous feature spaces and a flexible multi-task architecture with\nshared and private layers to transfer information between potential outcome\nfunctions across domains. Then, we show how these building blocks can be used\nto recover transfer learning equivalents of the standard CATE learners. On a\nnew semi-synthetic data simulation benchmark for heterogeneous transfer\nlearning we not only demonstrate performance improvements of our heterogeneous\ntransfer causal effect learners across datasets, but also provide insights into\nthe differences between these learners from a transfer perspective.",
    "descriptor": "",
    "authors": [
      "Ioana Bica",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.06183"
  },
  {
    "id": "arXiv:2210.06184",
    "title": "Images as Weight Matrices: Sequential Image Generation Through Synaptic  Learning Rules",
    "abstract": "Work on fast weight programmers has demonstrated the effectiveness of\nkey/value outer product-based learning rules for sequentially generating a\nweight matrix (WM) of a neural net (NN) by another NN or itself. However, the\nweight generation steps are typically not visually interpretable by humans,\nbecause the contents stored in the WM of an NN are not. Here we apply the same\nprinciple to generate natural images. The resulting fast weight painters (FPAs)\nlearn to execute sequences of delta learning rules to sequentially generate\nimages as sums of outer products of self-invented keys and values, one rank at\na time, as if each image was a WM of an NN. We train our FPAs in the generative\nadversarial networks framework, and evaluate on various image datasets. We show\nhow these generic learning rules can generate images with respectable visual\nquality without any explicit inductive bias for images. While the performance\nlargely lags behind the one of specialised state-of-the-art image generators,\nour approach allows for visualising how synaptic learning rules iteratively\nproduce complex connection patterns, yielding human-interpretable meaningful\nimages. Finally, we also show that an additional convolutional U-Net (now\npopular in diffusion models) at the output of an FPA can learn one-step\n\"denoising\" of FPA-generated images to enhance their quality. Our code is\npublic.",
    "descriptor": "",
    "authors": [
      "Kazuki Irie",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06184"
  },
  {
    "id": "arXiv:2210.06185",
    "title": "Polynomial tractability for integration in an unweighted function space  with absolutely convergent Fourier series",
    "abstract": "In this note, we prove that the following function space with absolutely\nconvergent Fourier series \\[ F_d:=\\left\\{ f\\in L^2([0,1)^d)\\:\\middle| \\:\n\\|f\\|:=\\sum_{\\boldsymbol{k}\\in\n\\mathbb{Z}^d}|\\hat{f}(\\boldsymbol{k})|\\max(1,\\log|\\boldsymbol{k}|_{\\infty})<\\infty\n\\right\\}\\] with $\\hat{f}(\\boldsymbol{k})$ being the $\\boldsymbol{k}$-th Fourier\ncoefficient of $f$ and $|\\boldsymbol{k}|_{\\infty}:=\\max_{j}|k_j|$ is\npolynomially tractable for multivariate integration in the worst-case setting.\nHere polynomial tractability means that the minimum number of function\nevaluations required to make the worst-case error less than or equal to a\ntolerance $\\varepsilon$ grows only polynomially with respect to\n$\\varepsilon^{-1}$ and $d$. It is important to remark that the function space\n$F_d$ is unweighted, that is, all variables contribute equally to the norm of\nfunctions. Our tractability result is in contrast to those for most of the\nunweighted integration problems studied in the literature, in which polynomial\ntractability does not hold and a weaker notion of tractability is necessary.\nOur proof is constructive in the sense that we provide an explicit quasi-Monte\nCarlo rule that attains a desired worst-case error bound.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Takashi Goda"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06185"
  },
  {
    "id": "arXiv:2210.06186",
    "title": "Gotcha: A Challenge-Response System for Real-Time Deepfake Detection",
    "abstract": "The integrity of online video interactions is threatened by the widespread\nrise of AI-enabled high-quality deepfakes that are now deployable in real-time.\nThis paper presents Gotcha, a real-time deepfake detection system for live\nvideo interactions. The core principle underlying Gotcha is the presentation of\na specially chosen cascade of both active and passive challenges to video\nconference participants. Active challenges include inducing changes in face\nocclusion, face expression, view angle, and ambiance; passive challenges\ninclude digital manipulation of the webcam feed. The challenges are designed to\ntarget vulnerabilities in the structure of modern deepfake generators and\ncreate perceptible artifacts for the human eye while inducing robust signals\nfor ML-based automatic deepfake detectors. We present a comprehensive taxonomy\nof a large set of challenge tasks, which reveals a natural hierarchy among\ndifferent challenges. Our system leverages this hierarchy by cascading\nprogressively more demanding challenges to a suspected deepfake. We evaluate\nour system on a novel dataset of live users emulating deepfakes and show that\nour system provides consistent, measurable degradation of deepfake quality,\nshowcasing its promise for robust real-time deepfake detection when deployed in\nthe wild.",
    "descriptor": "",
    "authors": [
      "Govind Mittal",
      "Jiraphon Yenphraphai",
      "Chinmay Hegde",
      "Nasir Memon"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06186"
  },
  {
    "id": "arXiv:2210.06188",
    "title": "Anomaly Detection using Generative Models and Sum-Product Networks in  Mammography Scans",
    "abstract": "Unsupervised anomaly detection models which are trained solely by healthy\ndata, have gained importance in the recent years, as the annotation of medical\ndata is a tedious task. Autoencoders and generative adversarial networks are\nthe standard anomaly detection methods that are utilized to learn the data\ndistribution. However, they fall short when it comes to inference and\nevaluation of the likelihood of test samples. We propose a novel combination of\ngenerative models and a probabilistic graphical model. After encoding image\nsamples by autoencoders, the distribution of data is modeled by Random and\nTensorized Sum-Product Networks ensuring exact and efficient inference at test\ntime. We evaluate different autoencoder architectures in combination with\nRandom and Tensorized Sum-Product Networks on mammography images using\npatch-wise processing and observe superior performance over utilizing the\nmodels standalone and state-of-the-art in anomaly detection for medical data.",
    "descriptor": "\nComments: Submitted to DGM4MICCAI 2022 Workshop. This preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in LNCS 13609, and is available online at this https URL\n",
    "authors": [
      "Marc Dietrichstein",
      "David Major",
      "Maria Wimmer",
      "Dimitrios Lenis",
      "Philip Winter",
      "Astrid Berg",
      "Theresa Neubauer",
      "Katja B\u00fchler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06188"
  },
  {
    "id": "arXiv:2210.06189",
    "title": "Overview on uncertainty quantification in traffic models via intrusive  method",
    "abstract": "We consider traffic flow models at different scales of observation. Starting\nfrom the well known hierarchy between microscopic, kinetic and macroscopic\nscales, we will investigate the propagation of uncertainties through the models\nusing the stochastic Galerkin approach. Connections between the scales will be\npresented in the stochastic scenario and numerical simulations will be\nperformed.",
    "descriptor": "",
    "authors": [
      "Elisa Iacomini"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06189"
  },
  {
    "id": "arXiv:2210.06190",
    "title": "\"Seeing the Faces Is So Important\" -- Experiences From Online Team  Meetings on Commercial Virtual Reality Platforms",
    "abstract": "During the Covid-19 pandemic, online meetings became common for daily\nteamwork in the home office. To understand the opportunities and challenges of\nmeeting in virtual reality (VR) compared to video conferences, we conducted the\nweekly team meetings of our human-computer interaction research lab on five\noff-the-shelf online meeting platforms over four months. After each of the 12\nmeetings, we asked the participants (N = 32) to share their experiences,\nresulting in 200 completed online questionnaires. We evaluated the ratings of\nthe overall meeting experience and conducted an exploratory factor analysis of\nthe quantitative data to compare VR meetings and video calls in terms of\nmeeting involvement and co-presence. In addition, a thematic analysis of the\nqualitative data revealed genuine insights covering five themes: spatial\naspects, meeting atmosphere, expression of emotions, meeting productivity, and\nuser needs. We reflect on our findings gained under authentic working\nconditions, derive lessons learned for running successful team meetings in VR\nsupporting different kinds of meeting formats, and discuss the team's long-term\nplatform choice.",
    "descriptor": "\nComments: This article is currently under review at Frontiers in Virtual Reality, Research Topic \"Everyday Virtual and Augmented Reality: Methods and Applications, Volume II\": this https URL\n",
    "authors": [
      "Michael Bonfert",
      "Anke V. Reinschluessel",
      "Susanne Putze",
      "Yenchin Lai",
      "Dmitry Alexandrovsky",
      "Rainer Malaka",
      "Tanja D\u00f6ring"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06190"
  },
  {
    "id": "arXiv:2210.06192",
    "title": "Pose-Guided Graph Convolutional Networks for Skeleton-Based Action  Recognition",
    "abstract": "Graph convolutional networks (GCNs), which can model the human body skeletons\nas spatial and temporal graphs, have shown remarkable potential in\nskeleton-based action recognition. However, in the existing GCN-based methods,\ngraph-structured representation of the human skeleton makes it difficult to be\nfused with other modalities, especially in the early stages. This may limit\ntheir scalability and performance in action recognition tasks. In addition, the\npose information, which naturally contains informative and discriminative clues\nfor action recognition, is rarely explored together with skeleton data in\nexisting methods. In this work, we propose pose-guided GCN (PG-GCN), a\nmulti-modal framework for high-performance human action recognition. In\nparticular, a multi-stream network is constructed to simultaneously explore the\nrobust features from both the pose and skeleton data, while a dynamic attention\nmodule is designed for early-stage feature fusion. The core idea of this module\nis to utilize a trainable graph to aggregate features from the skeleton stream\nwith that of the pose stream, which leads to a network with more robust feature\nrepresentation ability. Extensive experiments show that the proposed PG-GCN can\nachieve state-of-the-art performance on the NTU RGB+D 60 and NTU RGB+D 120\ndatasets.",
    "descriptor": "",
    "authors": [
      "Han Chen",
      "Yifan Jiang",
      "Hanseok Ko"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06192"
  },
  {
    "id": "arXiv:2210.06196",
    "title": "On the Preservation of Properties when Changing Communication Models",
    "abstract": "In a system of processes that communicate asynchronously by means of FIFO\nchannels, there are many options in which these channels can be laid out. In\nthis paper, we compare channel layouts in how they affect the behaviour of the\nsystem using an ordering based on splitting and merging channels. This order\ninduces a simulation relation, from which the preservation of safety properties\nfollows. Also, we identify conditions under which the properties reachability,\ndeadlock freedom and confluence are preserved when changing the channel layout.",
    "descriptor": "",
    "authors": [
      "Olav Bunte",
      "Louis C.M. van Gool",
      "Tim A.C. Willemse"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.06196"
  },
  {
    "id": "arXiv:2210.06201",
    "title": "Diffusion Models for Causal Discovery via Topological Ordering",
    "abstract": "Discovering causal relations from observational data becomes possible with\nadditional assumptions such as considering the functional relations to be\nconstrained as nonlinear with additive noise. In this case, the Hessian of the\ndata log-likelihood can be used for finding leaf nodes in a causal graph.\nTopological ordering approaches for causal discovery exploit this by performing\ngraph discovery in two steps, first sequentially identifying nodes in reverse\norder of depth (topological ordering), and secondly pruning the potential\nrelations. This is more efficient since the search is performed over a\npermutation rather than a graph space. However, existing computational methods\nfor obtaining the Hessian still do not scale as the number of variables and the\nnumber of samples are increased. Therefore, inspired by recent innovations in\ndiffusion probabilistic models (DPMs), we propose DiffAN, a topological\nordering algorithm that leverages DPMs. Further, we introduce theory for\nupdating the learned Hessian without re-training the neural network, and we\nshow that computing with a subset of samples gives an accurate approximation of\nthe ordering, which allows scaling to datasets with more samples and variables.\nWe show empirically that our method scales exceptionally well to datasets with\nup to $500$ nodes and up to $10^5$ samples while still performing on par over\nsmall datasets with state-of-the-art causal discovery methods. Implementation\nis available at https://github.com/vios-s/DiffAN .",
    "descriptor": "\nComments: Implementation is available at this https URL\n",
    "authors": [
      "Pedro Sanchez",
      "Xiao Liu",
      "Alison Q O'Neil",
      "Sotirios A. Tsaftaris"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06201"
  },
  {
    "id": "arXiv:2210.06202",
    "title": "A general theoretical scheme for shape-programming of incompressible  hyperelastic shells through differential growth",
    "abstract": "In this paper, we study the problem of shape-programming of incompressible\nhyperelastic shells through differential growth. The aim of the current work is\nto determine the growth tensor (or growth functions) that can produce the\ndeformation of a shell to the desired shape. First, a consistent finite-strain\nshell theory is introduced. The shell equation system is established from the\n3D governing system through a series expansion and truncation approach. Based\non the shell theory, the problem of shape-programming is studied under the\nstress-free assumption. For a special case in which the parametric coordinate\ncurves generate a net of curvature lines on the target surface, the sufficient\ncondition to ensure the vanishing of the stress components is analyzed, from\nwhich the explicit expression of the growth tensor can be derived. In the\ngeneral case, we conduct the variable changes and derive the total growth\ntensor by considering a two-step deformation of the shell. With these obtained\nresults, a general theoretical scheme for shape-programming of thin\nhyperelastic shells through differential growth is proposed. To demonstrate the\nfeasibility and efficiency of the proposed scheme, several nature-inspired\nexamples are studied. The derived growth tensors in these examples have also\nbeen implemented in the numerical simulations to verify their correctness and\naccuracy. The simulation results show that the target shapes of the shell\nsamples can be recovered completely. The scheme for shape-programming proposed\nin the current work is helpful in designing and manufacturing intelligent soft\ndevices.",
    "descriptor": "",
    "authors": [
      "Zhanfeng Li",
      "Jiong Wang",
      "Mokarram Hossain",
      "Chennakesava Kadapa"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2210.06202"
  },
  {
    "id": "arXiv:2210.06205",
    "title": "On Divergence Measures for Bayesian Pseudocoresets",
    "abstract": "A Bayesian pseudocoreset is a small synthetic dataset for which the posterior\nover parameters approximates that of the original dataset. While promising, the\nscalability of Bayesian pseudocoresets is not yet validated in realistic\nproblems such as image classification with deep neural networks. On the other\nhand, dataset distillation methods similarly construct a small dataset such\nthat the optimization using the synthetic dataset converges to a solution with\nperformance competitive with optimization using full data. Although dataset\ndistillation has been empirically verified in large-scale settings, the\nframework is restricted to point estimates, and their adaptation to Bayesian\ninference has not been explored. This paper casts two representative dataset\ndistillation algorithms as approximations to methods for constructing\npseudocoresets by minimizing specific divergence measures: reverse KL\ndivergence and Wasserstein distance. Furthermore, we provide a unifying view of\nsuch divergence measures in Bayesian pseudocoreset construction. Finally, we\npropose a novel Bayesian pseudocoreset algorithm based on minimizing forward KL\ndivergence. Our empirical results demonstrate that the pseudocoresets\nconstructed from these methods reflect the true posterior even in\nhigh-dimensional Bayesian inference problems.",
    "descriptor": "",
    "authors": [
      "Balhae Kim",
      "Jungwon Choi",
      "Seanie Lee",
      "Yoonho Lee",
      "Jung-Woo Ha",
      "Juho Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06205"
  },
  {
    "id": "arXiv:2210.06206",
    "title": "Parallel efficiency of monolithic and fixed-strain solution strategies  for poroelasticity problems",
    "abstract": "Poroelasticity is an example of coupled processes which are crucial for many\napplications including safety assessment of radioactive waste repositories.\nNumerical solution of poroelasticity problems leads to systems of algebraic\nequations, which may be solved simultaneously or iteratively. In this work,\nparallel scalability of the monolithic strategy and of the fixed-strain\nsplitting strategy is examined, which depends mostly on linear solver\nperformance. It was expected that splitting strategy would show better\nscalability due to better performance of a black-box linear solver on systems\nwith simpler structure. However, this is not always the case.",
    "descriptor": "",
    "authors": [
      "Denis Anuprienko"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06206"
  },
  {
    "id": "arXiv:2210.06207",
    "title": "SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word  Alignment",
    "abstract": "Word alignments are essential for a variety of NLP tasks. Therefore, choosing\nthe best approaches for their creation is crucial. However, the scarce\navailability of gold evaluation data makes the choice difficult. We propose\nSilverAlign, a new method to automatically create silver data for the\nevaluation of word aligners by exploiting machine translation and minimal\npairs. We show that performance on our silver data correlates well with gold\nbenchmarks for 9 language pairs, making our approach a valid resource for\nevaluation of different domains and languages when gold data are not available.\nThis addresses the important scenario of missing gold data alignments for\nlow-resource languages.",
    "descriptor": "",
    "authors": [
      "Abdullatif K\u00f6ksal",
      "Silvia Severini",
      "Hinrich Sch\u00fctze"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06207"
  },
  {
    "id": "arXiv:2210.06210",
    "title": "Pruning Pre-trained Language Models Without Fine-Tuning",
    "abstract": "To overcome the overparameterized problem in Pre-trained Language Models\n(PLMs), pruning is widely used as a simple and straightforward compression\nmethod by directly removing unimportant weights. Previous first-order methods\nsuccessfully compress PLMs to extremely high sparsity with little performance\ndrop. These methods, such as movement pruning, use first-order information to\nprune PLMs while fine-tuning the remaining weights. In this work, we argue\nfine-tuning is redundant for first-order pruning, since first-order pruning is\nsufficient to converge PLMs to downstream tasks without fine-tuning. Under this\nmotivation, we propose Static Model Pruning (SMP), which only uses first-order\npruning to adapt PLMs to downstream tasks while achieving the target sparsity\nlevel. In addition, we also design a new masking function and training\nobjective to further improve SMP. Extensive experiments at various sparsity\nlevels show SMP has significant improvements over first-order and zero-order\nmethods. Unlike previous first-order methods, SMP is also applicable to low\nsparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter\nefficient than other methods due to it does not require fine-tuning.",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Ting Jiang",
      "Deqing Wang",
      "Fuzhen Zhuang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06210"
  },
  {
    "id": "arXiv:2210.06213",
    "title": "Probabilistic Inverse Modeling: An Application in Hydrology",
    "abstract": "The astounding success of these methods has made it imperative to obtain more\nexplainable and trustworthy estimates from these models. In hydrology, basin\ncharacteristics can be noisy or missing, impacting streamflow prediction. For\nsolving inverse problems in such applications, ensuring explainability is\npivotal for tackling issues relating to data bias and large search space. We\npropose a probabilistic inverse model framework that can reconstruct robust\nhydrology basin characteristics from dynamic input weather driver and\nstreamflow response data. We address two aspects of building more explainable\ninverse models, uncertainty estimation and robustness. This can help improve\nthe trust of water managers, handling of noisy data and reduce costs. We\npropose uncertainty based learning method that offers 6\\% improvement in $R^2$\nfor streamflow prediction (forward modeling) from inverse model inferred basin\ncharacteristic estimates, 17\\% reduction in uncertainty (40\\% in presence of\nnoise) and 4\\% higher coverage rate for basin characteristics.",
    "descriptor": "",
    "authors": [
      "Somya Sharma",
      "Rahul Ghosh",
      "Arvind Renganathan",
      "Xiang Li",
      "Snigdhansu Chatterjee",
      "John Nieber",
      "Christopher Duffy",
      "Vipin Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06213"
  },
  {
    "id": "arXiv:2210.06216",
    "title": "Hierarchical Instance Mixing across Domains in Aerial Segmentation",
    "abstract": "We investigate the task of unsupervised domain adaptation in aerial semantic\nsegmentation and discover that the current state-of-the-art algorithms designed\nfor autonomous driving based on domain mixing do not translate well to the\naerial setting. This is due to two factors: (i) a large disparity in the\nextension of the semantic categories, which causes a domain imbalance in the\nmixed image, and (ii) a weaker structural consistency in aerial scenes than in\ndriving scenes since the same scene might be viewed from different perspectives\nand there is no well-defined and repeatable structure of the semantic elements\nin the images. Our solution to these problems is composed of: (i) a new mixing\nstrategy for aerial segmentation across domains called Hierarchical Instance\nMixing (HIMix), which extracts a set of connected components from each semantic\nmask and mixes them according to a semantic hierarchy and, (ii) a twin-head\narchitecture in which two separate segmentation heads are fed with variations\nof the same images in a contrastive fashion to produce finer segmentation maps.\nWe conduct extensive experiments on the LoveDA benchmark, where our solution\noutperforms the current state-of-the-art.",
    "descriptor": "",
    "authors": [
      "Edoardo Arnaudo",
      "Antonio Tavera",
      "Fabrizio Dominici",
      "Carlo Masone",
      "Barbara Caputo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06216"
  },
  {
    "id": "arXiv:2210.06223",
    "title": "Latency-aware Spatial-wise Dynamic Networks",
    "abstract": "Spatial-wise dynamic convolution has become a promising approach to improving\nthe inference efficiency of deep networks. By allocating more computation to\nthe most informative pixels, such an adaptive inference paradigm reduces the\nspatial redundancy in image features and saves a considerable amount of\nunnecessary computation. However, the theoretical efficiency achieved by\nprevious methods can hardly translate into a realistic speedup, especially on\nthe multi-core processors (e.g. GPUs). The key challenge is that the existing\nliterature has only focused on designing algorithms with minimal computation,\nignoring the fact that the practical latency can also be influenced by\nscheduling strategies and hardware properties. To bridge the gap between\ntheoretical computation and practical efficiency, we propose a latency-aware\nspatial-wise dynamic network (LASNet), which performs coarse-grained spatially\nadaptive inference under the guidance of a novel latency prediction model. The\nlatency prediction model can efficiently estimate the inference latency of\ndynamic networks by simultaneously considering algorithms, scheduling\nstrategies, and hardware properties. We use the latency predictor to guide both\nthe algorithm design and the scheduling optimization on various hardware\nplatforms. Experiments on image classification, object detection and instance\nsegmentation demonstrate that the proposed framework significantly improves the\npractical inference efficiency of deep networks. For example, the average\nlatency of a ResNet-101 on the ImageNet validation set could be reduced by 36%\nand 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson\nTX2 GPU) respectively without sacrificing the accuracy. Code is available at\nhttps://github.com/LeapLabTHU/LASNet.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Yizeng Han",
      "Zhihang Yuan",
      "Yifan Pu",
      "Chenhao Xue",
      "Shiji Song",
      "Guangyu Sun",
      "Gao Huang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06223"
  },
  {
    "id": "arXiv:2210.06225",
    "title": "On the Generalizability of ECG-based Stress Detection Models",
    "abstract": "Stress is prevalent in many aspects of everyday life including work,\nhealthcare, and social interactions. Many works have studied handcrafted\nfeatures from various bio-signals that are indicators of stress. Recently, deep\nlearning models have also been proposed to detect stress. Typically, stress\nmodels are trained and validated on the same dataset, often involving one\nstressful scenario. However, it is not practical to collect stress data for\nevery scenario. So, it is crucial to study the generalizability of these models\nand determine to what extent they can be used in other scenarios. In this\npaper, we explore the generalization capabilities of Electrocardiogram\n(ECG)-based deep learning models and models based on handcrafted ECG features,\ni.e., Heart Rate Variability (HRV) features. To this end, we train three HRV\nmodels and two deep learning models that use ECG signals as input. We use ECG\nsignals from two popular stress datasets - WESAD and SWELL-KW - differing in\nterms of stressors and recording devices. First, we evaluate the models using\nleave-one-subject-out (LOSO) cross-validation using training and validation\nsamples from the same dataset. Next, we perform a cross-dataset validation of\nthe models, that is, LOSO models trained on the WESAD dataset are validated\nusing SWELL-KW samples and vice versa. While deep learning models achieve the\nbest results on the same dataset, models based on HRV features considerably\noutperform them on data from a different dataset. This trend is observed for\nall the models on both datasets. Therefore, HRV models are a better choice for\nstress recognition in applications that are different from the dataset\nscenario. To the best of our knowledge, this is the first work to compare the\ncross-dataset generalizability between ECG-based deep learning models and HRV\nmodels.",
    "descriptor": "\nComments: Accepted to IEEE ICMLA 2022\n",
    "authors": [
      "Pooja Prajod",
      "Elisabeth Andr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06225"
  },
  {
    "id": "arXiv:2210.06229",
    "title": "Towards visually prompted keyword localisation for zero-resource spoken  languages",
    "abstract": "Imagine being able to show a system a visual depiction of a keyword and\nfinding spoken utterances that contain this keyword from a zero-resource speech\ncorpus. We formalise this task and call it visually prompted keyword\nlocalisation (VPKL): given an image of a keyword, detect and predict where in\nan utterance the keyword occurs. To do VPKL, we propose a speech-vision model\nwith a novel localising attention mechanism which we train with a new keyword\nsampling scheme. We show that these innovations give improvements in VPKL over\nan existing speech-vision model. We also compare to a visual bag-of-words (BoW)\nmodel where images are automatically tagged with visual labels and paired with\nunlabelled speech. Although this visual BoW can be queried directly with a\nwritten keyword (while our's takes image queries), our new model still\noutperforms the visual BoW in both detection and localisation, giving a 16%\nrelative improvement in localisation F1.",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Leanne Nortje",
      "Herman Kamper"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06229"
  },
  {
    "id": "arXiv:2210.06230",
    "title": "Quasi-symbolic explanatory NLI via disentanglement: A geometrical  examination",
    "abstract": "Disentangling the encodings of neural models is a fundamental aspect for\nimproving interpretability, semantic control, and understanding downstream task\nperformance in Natural Language Processing. The connection points between\ndisentanglement and downstream tasks, however, remains underexplored from a\nexplanatory standpoint. This work presents a methodology for assessment of\ngeometrical properties of the resulting latent space w.r.t. vector operations\nand semantic disentanglement in quantitative and qualitative terms, based on a\nVAE-based supervised framework. Empirical results indicate that the\nrole-contents of explanations, such as \\textit{ARG0-animal}, are disentangled\nin the latent space, which provides us a chance for controlling the explanation\ngeneration by manipulating the traversal of vector over latent space.",
    "descriptor": "\nComments: 16 pages. arXiv admin note: text overlap with arXiv:2210.02898\n",
    "authors": [
      "Yingji Zhang",
      "Danilo S. Carvalho",
      "Ian Pratt-Hartmann",
      "Andr\u00e9 Freitas"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06230"
  },
  {
    "id": "arXiv:2210.06232",
    "title": "Time exponential integrator Fourier pseudospectral methods with high  accuracy and multiple conservation laws for three-dimensional Maxwell's  equations",
    "abstract": "Maxwell equations describe the propagation of electromagnetic waves and are\ntherefore fundamental to understanding many problems encountered in the study\nof antennas and electromagnetics. The aim of this paper is to propose and\nanalyse an efficient fully discrete scheme for solving three-dimensional\nMaxwell's equations. This is accomplished by combining time exponential\nintegrator and Fourier pseudospectral methods. Fast computation is implemented\nin the scheme by using the Fast Fourier Transform algorithm which is well known\nin scientific computations. An optimal error estimate which is not encumbered\nby the CFL condition is established and the resulting scheme is proved to be of\nspectral accuracy in space and infinite-order accuracy in time. Furthermore,\nthe scheme is shown to have multiple conservation laws including discrete\nenergy, helicity, momentum, symplecticity, and divergence-free field\nconservations. All the theoretical results of the accuracy and conservations\nare numerically illustrated by two numerical tests.",
    "descriptor": "\nComments: 22 pages\n",
    "authors": [
      "Bin Wang",
      "Yaolin Jiang"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06232"
  },
  {
    "id": "arXiv:2210.06235",
    "title": "Listening to Users' Voice: Automatic Summarization of Helpful App  Reviews",
    "abstract": "App reviews are crowdsourcing knowledge of user experience with the apps,\nproviding valuable information for app release planning, such as major bugs to\nfix and important features to add. There exist prior explorations on app review\nmining for release planning, however, most of the studies strongly rely on\npre-defined classes or manually-annotated reviews. Also, the new review\ncharacteristic, i.e., the number of users who rated the review as helpful,\nwhich can help capture important reviews, has not been considered previously.\nIn the paper, we propose a novel framework, named SOLAR, aiming at accurately\nsummarizing helpful user reviews to developers. The framework mainly contains\nthree modules: The review helpfulness prediction module, topic-sentiment\nmodeling module, and multi-factor ranking module. The review helpfulness\nprediction module assesses the helpfulness of reviews, i.e., whether the review\nis useful for developers. The topic-sentiment modeling module groups the topics\nof the helpful reviews and also predicts the associated sentiment, and the\nmulti-factor ranking module aims at prioritizing semantically representative\nreviews for each topic as the review summary. Experiments on five popular apps\nindicate that SOLAR is effective for review summarization and promising for\nfacilitating app release planning.",
    "descriptor": "",
    "authors": [
      "Cuiyun Gao",
      "Yaoxian Li",
      "Shuhan Qi",
      "Yang Liu",
      "Xuan Wang",
      "Zibin Zheng",
      "Qing Liao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.06235"
  },
  {
    "id": "arXiv:2210.06236",
    "title": "IPv6 over Bluetooth Advertisements: An alternative approach to IP over  BLE",
    "abstract": "The IPv6 over Bluetooth Low Energy (BLE) standard defines the transfer of IP\ndata via BLE connections. This connection-oriented approach provides high\nreliability but increases packet delays and requires substantial overhead to\nmanage BLE connections. To overcome these drawbacks we present the design and\nimplementation of IPv6 over BLE advertisements, a standard-compliant\nconnection-less approach. We deploy our proposal on low-power IoT hardware and\ncomparatively measure key network performance metrics in a public testbed. Our\nresults show that IP over BLE advertisements offers network performance\ncharacteristics complementary to IP over connection-based BLE, trading lower\nreliability for shorter~latency.",
    "descriptor": "",
    "authors": [
      "Hauke Petersen",
      "J\u00e1nos Brodbeck",
      "Thomas C. Schmidt",
      "Matthias W\u00e4hlisch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06236"
  },
  {
    "id": "arXiv:2210.06239",
    "title": "FCT-GAN: Enhancing Table Synthesis via Fourier Transform",
    "abstract": "Synthetic tabular data emerges as an alternative for sharing knowledge while\nadhering to restrictive data access regulations, e.g., European General Data\nProtection Regulation (GDPR). Mainstream state-of-the-art tabular data\nsynthesizers draw methodologies from Generative Adversarial Networks (GANs),\nwhich are composed of a generator and a discriminator. While convolution neural\nnetworks are shown to be a better architecture than fully connected networks\nfor tabular data synthesizing, two key properties of tabular data are\noverlooked: (i) the global correlation across columns, and (ii) invariant\nsynthesizing to column permutations of input data. To address the above\nproblems, we propose a Fourier conditional tabular generative adversarial\nnetwork (FCT-GAN). We introduce feature tokenization and Fourier networks to\nconstruct a transformer-style generator and discriminator, and capture both\nlocal and global dependencies across columns. The tokenizer captures local\nspatial features and transforms original data into tokens. Fourier networks\ntransform tokens to frequency domains and element-wisely multiply a learnable\nfilter. Extensive evaluation on benchmarks and real-world data shows that\nFCT-GAN can synthesize tabular data with high machine learning utility (up to\n27.8% better than state-of-the-art baselines) and high statistical similarity\nto the original data (up to 26.5% better), while maintaining the global\ncorrelation across columns, especially on high dimensional dataset.",
    "descriptor": "",
    "authors": [
      "Zilong Zhao",
      "Robert Birke",
      "Lydia Y. Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06239"
  },
  {
    "id": "arXiv:2210.06240",
    "title": "Explore Contextual Information for 3D Scene Graph Generation",
    "abstract": "3D scene graph generation (SGG) has been of high interest in computer vision.\nAlthough the accuracy of 3D SGG on coarse classification and single relation\nlabel has been gradually improved, the performance of existing works is still\nfar from being perfect for fine-grained and multi-label situations. In this\npaper, we propose a framework fully exploring contextual information for the 3D\nSGG task, which attempts to satisfy the requirements of fine-grained entity\nclass, multiple relation labels, and high accuracy simultaneously. Our proposed\napproach is composed of a Graph Feature Extraction module and a Graph\nContextual Reasoning module, achieving appropriate information-redundancy\nfeature extraction, structured organization, and hierarchical inferring. Our\napproach achieves superior or competitive performance over previous methods on\nthe 3DSSG dataset, especially on the relationship prediction sub-task.",
    "descriptor": "",
    "authors": [
      "Yuanyuan Liu",
      "Chengjiang Long",
      "Zhaoxuan Zhang",
      "Bokai Liu",
      "Qiang Zhang",
      "Baocai Yin",
      "Xin Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06240"
  },
  {
    "id": "arXiv:2210.06241",
    "title": "Two conjectures on the largest minimum distances of binary  self-orthogonal codes with dimension 5",
    "abstract": "The purpose of this paper is to solve the two conjectures on the largest\nminimum distance $d_{so}(n,5)$ of a binary self-orthogonal $[n,5]$ code\nproposed by Kim and Choi (IEEE Trans. Inf. Theory, 2022). The determination of\n$d_{so}(n,k)$ has been a fundamental and difficult problem in coding theory\nbecause there are too many binary self-orthogonal codes as the dimension $k$\nincreases. Recently, Kim et al. (2021) considered the shortest self-orthogonal\nembedding of a binary linear code, and many binary optimal self-orthogonal\n$[n,k]$ codes were constructed for $k=4,5$. Kim and Choi (2022) improved some\nresults of Kim et al. (2021) and made two conjectures on $d_{so}(n,5)$. In this\npaper, we develop a general method to determine the exact value of\n$d_{so}(n,k)$ for $k=5,6$ and show that the two conjectures made by Kim and\nChoi (2022) are true.",
    "descriptor": "",
    "authors": [
      "Minjia Shi",
      "Shitao Li",
      "Jon-Lark Kim"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06241"
  },
  {
    "id": "arXiv:2210.06242",
    "title": "Entity Aware Negative Sampling with Auxiliary Loss of False Negative  Prediction for Knowledge Graph Embedding",
    "abstract": "Knowledge graph (KG) embedding is widely used in many downstream applications\nusing KGs. Generally, since KGs contain only ground truth triples, it is\nnecessary to construct arbitrary negative samples for representation learning\nof KGs. Recently, various methods for sampling high-quality negatives have been\nstudied because the quality of negative triples has great effect on KG\nembedding. In this paper, we propose a novel method called Entity Aware\nNegative Sampling (EANS), which is able to sample negative entities resemble to\npositive one by adopting Gaussian distribution to the aligned entity index\nspace. Additionally, we introduce auxiliary loss for false negative prediction\nthat can alleviate the impact of the sampled false negative triples. The\nproposed method can generate high-quality negative samples regardless of\nnegative sample size and effectively mitigate the influence of false negative\nsamples. The experimental results on standard benchmarks show that our EANS\noutperforms existing the state-of-the-art methods of negative sampling on\nseveral knowledge graph embedding models. Moreover, the proposed method\nachieves competitive performance even when the number of negative samples is\nlimited to only one.",
    "descriptor": "",
    "authors": [
      "Sang-Hyun Je"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06242"
  },
  {
    "id": "arXiv:2210.06244",
    "title": "A context-aware knowledge transferring strategy for CTC-based ASR",
    "abstract": "Non-autoregressive automatic speech recognition (ASR) modeling has received\nincreasing attention recently because of its fast decoding speed and superior\nperformance. Among representatives, methods based on the connectionist temporal\nclassification (CTC) are still a dominating stream. However, the theoretically\ninherent flaw, the assumption of independence between tokens, creates a\nperformance barrier for the school of works. To mitigate the challenge, we\npropose a context-aware knowledge transferring strategy, consisting of a\nknowledge transferring module and a context-aware training strategy, for\nCTC-based ASR. The former is designed to distill linguistic information from a\npre-trained language model, and the latter is framed to modulate the\nlimitations caused by the conditional independence assumption. As a result, a\nknowledge-injected context-aware CTC-based ASR built upon the wav2vec2.0 is\npresented in this paper. A series of experiments on the AISHELL-1 and AISHELL-2\ndatasets demonstrate the effectiveness of the proposed method.",
    "descriptor": "\nComments: Accepted by SLT 2022\n",
    "authors": [
      "Ke-Han Lu",
      "Kuan-Yu Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06244"
  },
  {
    "id": "arXiv:2210.06245",
    "title": "Back to the Future: On Potential Histories in NLP",
    "abstract": "Machine learning and NLP require the construction of datasets to train and\nfine-tune models. In this context, previous work has demonstrated the\nsensitivity of these data sets. For instance, potential societal biases in this\ndata are likely to be encoded and to be amplified in the models we deploy. In\nthis work, we draw from developments in the field of history and take a novel\nperspective on these problems: considering datasets and models through the lens\nof historical fiction surfaces their political nature, and affords\nre-configuring how we view the past, such that marginalized discourses are\nsurfaced. Building on such insights, we argue that contemporary methods for\nmachine learning are prejudiced towards dominant and hegemonic histories.\nEmploying the example of neopronouns, we show that by surfacing marginalized\nhistories within contemporary conditions, we can create models that better\nrepresent the lived realities of traditionally marginalized and excluded\ncommunities.",
    "descriptor": "",
    "authors": [
      "Zeerak Talat",
      "Anne Lauscher"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06245"
  },
  {
    "id": "arXiv:2210.06246",
    "title": "CIKQA: Learning Commonsense Inference with a Unified  Knowledge-in-the-loop QA Paradigm",
    "abstract": "Recently, the community has achieved substantial progress on many commonsense\nreasoning benchmarks. However, it is still unclear what is learned from the\ntraining process: the knowledge, inference capability, or both? We argue that\ndue to the large scale of commonsense knowledge, it is infeasible to annotate a\nlarge enough training set for each task to cover all commonsense for learning.\nThus we should separate the commonsense knowledge acquisition and inference\nover commonsense knowledge as two separate tasks. In this work, we focus on\ninvestigating models' commonsense inference capabilities from two perspectives:\n(1) Whether models can know if the knowledge they have is enough to solve the\ntask; (2) Whether models can develop commonsense inference capabilities that\ngeneralize across commonsense tasks. We first align commonsense tasks with\nrelevant knowledge from commonsense knowledge bases and ask humans to annotate\nwhether the knowledge is enough or not. Then, we convert different commonsense\ntasks into a unified question answering format to evaluate models'\ngeneralization capabilities. We name the benchmark as Commonsense Inference\nwith Knowledge-in-the-loop Question Answering (CIKQA).",
    "descriptor": "",
    "authors": [
      "Hongming Zhang",
      "Yintong Huo",
      "Yanai Elazar",
      "Yangqiu Song",
      "Yoav Goldberg",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06246"
  },
  {
    "id": "arXiv:2210.06249",
    "title": "Managing Service Dependency for Cloud Reliability: The Industrial  Practice",
    "abstract": "Interactions between cloud services result in service dependencies.\nEvaluating and managing the cascading impacts caused by service dependencies is\ncritical to the reliability of cloud systems. This paper summarizes the\ndependency types in cloud systems and demonstrates the design of the Dependency\nManagement System (DMS), a platform for managing the service dependencies in\nthe production cloud system. DMS features full-lifecycle support for service\nreliability (i.e., initial service deployment, service upgrade, proactive\narchitectural optimization, and reactive failure mitigation) and refined\ncharacterization of the intensity of dependencies.",
    "descriptor": "\nComments: In Proceedings of the 33rd IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW'22)\n",
    "authors": [
      "Tianyi Yang",
      "Baitong Li",
      "Jiacheng Shen",
      "Yuxin Su",
      "Yongqiang Yang",
      "Michael R. Lyu"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06249"
  },
  {
    "id": "arXiv:2210.06254",
    "title": "Zero-Shot On-the-Fly Event Schema Induction",
    "abstract": "What are the events involved in a pandemic outbreak? What steps should be\ntaken when planning a wedding? The answers to these questions can be found by\ncollecting many documents on the complex event of interest, extracting relevant\ninformation, and analyzing it. We present a new approach in which large\nlanguage models are utilized to generate source documents that allow\npredicting, given a high-level event definition, the specific events,\narguments, and relations between them to construct a schema that describes the\ncomplex event in its entirety. Using our model, complete schemas on any topic\ncan be generated on-the-fly without any manual data collection, i.e., in a\nzero-shot manner. Moreover, we develop efficient methods to extract pertinent\ninformation from texts and demonstrate in a series of experiments that these\nschemas are considered to be more complete than human-curated ones in the\nmajority of examined scenarios. Finally, we show that this framework is\ncomparable in performance with previous supervised schema induction methods\nthat rely on collecting real texts while being more general and flexible\nwithout the need for a predefined ontology.",
    "descriptor": "",
    "authors": [
      "Rotem Dror",
      "Haoyu Wang",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06254"
  },
  {
    "id": "arXiv:2210.06257",
    "title": "What can we learn about a generated image corrupting its latent  representation?",
    "abstract": "Generative adversarial networks (GANs) offer an effective solution to the\nimage-to-image translation problem, thereby allowing for new possibilities in\nmedical imaging. They can translate images from one imaging modality to another\nat a low cost. For unpaired datasets, they rely mostly on cycle loss. Despite\nits effectiveness in learning the underlying data distribution, it can lead to\na discrepancy between input and output data. The purpose of this work is to\ninvestigate the hypothesis that we can predict image quality based on its\nlatent representation in the GANs bottleneck. We achieve this by corrupting the\nlatent representation with noise and generating multiple outputs. The degree of\ndifferences between them is interpreted as the strength of the representation:\nthe more robust the latent representation, the fewer changes in the output\nimage the corruption causes. Our results demonstrate that our proposed method\nhas the ability to i) predict uncertain parts of synthesized images, and ii)\nidentify samples that may not be reliable for downstream tasks, e.g., liver\nsegmentation task.",
    "descriptor": "",
    "authors": [
      "Agnieszka Tomczak",
      "Aarushi Gupta",
      "Slobodan Ilic",
      "Nassir Navab",
      "Shadi Albarqouni"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.06257"
  },
  {
    "id": "arXiv:2210.06258",
    "title": "Exploring Children's Use of Self-Made Tangibles in Programming",
    "abstract": "Defining abstract algorithmic structures like functions and variables using\nself-made tangibles can enhance the usability and affordability of the tangible\nprogramming experience by maintaining the input modality and physical\ninteraction throughout the activity and reducing the dependence on electronic\ndevices. However, existing tangible programming environments use digital\ninterfaces to save abstract definitions such as functions and variables, as\ndesigning new tangibles is challenging for children due to their limited\nexperience using abstract definitions. We conducted a series of design\nworkshops with children to understand their self-made tangible creation\nabilities and develop design considerations for tangible computing such as\npaper programming.\nThis paper reports:\n1) Our insights on how students conceptualize and design tangible programming\nblocks,\n2) Design considerations of self-made tangibles to yield higher\nunderstandability and memorability,\n3) Tests of our design considerations in creating self-made tangibles in\nreal-life coding activities.",
    "descriptor": "",
    "authors": [
      "Alpay Sabuncuoglu",
      "T. Metin Sezgin"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06258"
  },
  {
    "id": "arXiv:2210.06261",
    "title": "Predicting housing prices and analyzing real estate market in the  Chicago suburbs using Machine Learning",
    "abstract": "The pricing of housing properties is determined by a variety of factors.\nHowever, post-pandemic markets have experienced volatility in the Chicago\nsuburb area, which have affected house prices greatly. In this study, analysis\nwas done on the Naperville/Bolingbrook real estate market to predict property\nprices based on these housing attributes through machine learning models, and\nto evaluate the effectiveness of such models in a volatile market space.\nGathering data from Redfin, a real estate website, sales data from 2018 up\nuntil the summer season of 2022 were collected for research. By analyzing these\nsales in this range of time, we can also look at the state of the housing\nmarket and identify trends in price. For modeling the data, the models used\nwere linear regression, support vector regression, decision tree regression,\nrandom forest regression, and XGBoost regression. To analyze results,\ncomparison was made on the MAE, RMSE, and R-squared values for each model. It\nwas found that the XGBoost model performs the best in predicting house prices\ndespite the additional volatility sponsored by post-pandemic conditions. After\nmodeling, Shapley Values (SHAP) were used to evaluate the weights of the\nvariables in constructing models.",
    "descriptor": "",
    "authors": [
      "Kevin Xu",
      "Hieu Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06261"
  },
  {
    "id": "arXiv:2210.06267",
    "title": "Optimizing Tensor Programs on Flexible Storage",
    "abstract": "Tensor programs often need to process large tensors (vectors, matrices, or\nhigher order tensors) that require a specialized storage format for their\nmemory layout. Several such layouts have been proposed in the literature, such\nas the Coordinate Format, the Compressed Sparse Row format, and many others,\nthat were especially designed to optimally store tensors with specific sparsity\nproperties. However, existing tensor processing systems require specialized\nextensions in order to take advantage of every new storage format. In this\npaper we describe a system that allows users to define flexible storage formats\nin a declarative tensor query language, similar to the language used by the\ntensor program. The programmer only needs to write storage mappings, which\ndescribe, in a declarative way, how the tensors are laid out in main memory.\nThen, we describe a cost-based optimizer that optimizes the tensor program for\nthe specific memory layout. We demonstrate empirically significant performance\nimprovements compared to state-of-the-art tensor processing systems.",
    "descriptor": "",
    "authors": [
      "Maximilian Schleich",
      "Amir Shaikhha",
      "Dan Suciu"
    ],
    "subjectives": [
      "Databases (cs.DB)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.06267"
  },
  {
    "id": "arXiv:2210.06270",
    "title": "Event-based Non-Rigid Reconstruction from Contours",
    "abstract": "Visual reconstruction of fast non-rigid object deformations over time is a\nchallenge for conventional frame-based cameras. In this paper, we propose a\nnovel approach for reconstructing such deformations using measurements from\nevent-based cameras. Under the assumption of a static background, where all\nevents are generated by the motion, our approach estimates the deformation of\nobjects from events generated at the object contour in a probabilistic\noptimization framework. It associates events to mesh faces on the contour and\nmaximizes the alignment of the line of sight through the event pixel with the\nassociated face. In experiments on synthetic and real data, we demonstrate the\nadvantages of our method over state-of-the-art optimization and learning-based\napproaches for reconstructing the motion of human hands. A video of the\nexperiments is available at https://youtu.be/gzfw7i5OKjg",
    "descriptor": "\nComments: Accepted to BMVC2022\n",
    "authors": [
      "Yuxuan Xue",
      "Haolong Li",
      "Stefan Leutenegger",
      "J\u00f6rg St\u00fcckler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06270"
  },
  {
    "id": "arXiv:2210.06272",
    "title": "Deep Koopman Representation of Nonlinear Time Varying Systems",
    "abstract": "A data-driven method is developed to approximate an nonlinear time-varying\nsystem (NTVS) by a linear time-varying system (LTVS), based on Koopman Operator\nand deep neural networks. Analysis on the approximation error in system states\nof the proposed method is investigated. It is further shown by simulation on a\nsimple NTVS that the resulted LTVS approximate the NTVS very well with small\napproximation errors in states. Furthermore, simulations on a cartpole further\nshow that optimal controller developed based on the achieved LTVS works very\nwell to control the original NTVS.",
    "descriptor": "",
    "authors": [
      "Wenjian Hao",
      "Bowen Huang",
      "Wei Pan",
      "Di Wu",
      "Shaoshuai Mou"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06272"
  },
  {
    "id": "arXiv:2210.06274",
    "title": "Centralized Training with Hybrid Execution in Multi-Agent Reinforcement  Learning",
    "abstract": "We introduce hybrid execution in multi-agent reinforcement learning (MARL), a\nnew paradigm in which agents aim to successfully perform cooperative tasks with\nany communication level at execution time by taking advantage of\ninformation-sharing among the agents. Under hybrid execution, the communication\nlevel can range from a setting in which no communication is allowed between\nagents (fully decentralized), to a setting featuring full communication (fully\ncentralized). To formalize our setting, we define a new class of multi-agent\npartially observable Markov decision processes (POMDPs) that we name\nhybrid-POMDPs, which explicitly models a communication process between the\nagents. We contribute MARO, an approach that combines an autoregressive\npredictive model to estimate missing agents' observations, and a dropout-based\nRL training scheme that simulates different communication levels during the\ncentralized training phase. We evaluate MARO on standard scenarios and\nextensions of previous benchmarks tailored to emphasize the negative impact of\npartial observability in MARL. Experimental results show that our method\nconsistently outperforms baselines, allowing agents to act with faulty\ncommunication while successfully exploiting shared information.",
    "descriptor": "",
    "authors": [
      "Pedro P. Santos",
      "Diogo S. Carvalho",
      "Miguel Vasco",
      "Alberto Sardinha",
      "Pedro A. Santos",
      "Ana Paiva",
      "Francisco S. Melo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06274"
  },
  {
    "id": "arXiv:2210.06277",
    "title": "Task Compass: Scaling Multi-task Pre-training with Task Prefix",
    "abstract": "Leveraging task-aware annotated data as supervised signals to assist with\nself-supervised learning on large-scale unlabeled data has become a new trend\nin pre-training language models. Existing studies show that multi-task learning\nwith large-scale supervised tasks suffers from negative effects across tasks.\nTo tackle the challenge, we propose a task prefix guided multi-task\npre-training framework to explore the relationships among tasks. We conduct\nextensive experiments on 40 datasets, which show that our model can not only\nserve as the strong foundation backbone for a wide range of tasks but also be\nfeasible as a probing tool for analyzing task relationships. The task\nrelationships reflected by the prefixes align transfer learning performance\nbetween tasks. They also suggest directions for data augmentation with\ncomplementary tasks, which help our model achieve human-parity results on\ncommonsense reasoning leaderboards. Code is available at\nhttps://github.com/cooelf/CompassMTL",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Zhuosheng Zhang",
      "Shuohang Wang",
      "Yichong Xu",
      "Yuwei Fang",
      "Wenhao Yu",
      "Yang Liu",
      "Hai Zhao",
      "Chenguang Zhu",
      "Michael Zeng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06277"
  },
  {
    "id": "arXiv:2210.06278",
    "title": "On the Nonlinear Shaping Gain with Probabilistic Shaping and Carrier  Phase Recovery",
    "abstract": "The performance of different probabilistic amplitude shaping (PAS) techniques\nin the nonlinear regime is investigated, highlighting its dependence on the PAS\nblock length and the interaction with carrier phase recovery. Different PAS\nimplementations are considered, based on different distribution matching (DM)\ntechniques. When carrier phase recovery is not included, PAS with optimal block\nlength provides a nonlinear shaping gain with respect to a linearly optimized\nPAS; among the considered DM techniques, the largest gain is obtained with\nsphere shaping. On the other hand, the nonlinear shaping gain becomes smaller,\nor completely vanishes, when carrier phase recovery is included, meaning that\nin this case all the considered implementations achieve a similar performance\nfor a sufficiently long block length. Similar results are obtained in different\nlink configurations, and also including laser phase noise, except when inline\ndispersion compensation is used. Furthermore, we define a new metric, the\nnonlinear phase noise (NPN) metric, which is based on the frequency resolved\nlogarithmic perturbation model and explains the interaction of carrier phase\nrecovery and PAS. We show that the NPN metric is highly correlated with the\nperformance of the system. Our results suggest that, in general, the\noptimization of PAS in the nonlinear regime should always account for the\npresence of a carrier phase recovery algorithm. In this case, the reduction of\nthe rate loss turns out to be more important than the mitigation of the\nnonlinear phase noise, the latter being already granted by the carrier phase\nrecovery algorithm.",
    "descriptor": "",
    "authors": [
      "Stella Civelli",
      "Emanuele Parente",
      "Enrico Forestieri",
      "Marco Secondini"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06278"
  },
  {
    "id": "arXiv:2210.06280",
    "title": "Language Models are Realistic Tabular Data Generators",
    "abstract": "Tabular data is among the oldest and most ubiquitous forms of data. However,\nthe generation of synthetic samples with the original data's characteristics\nstill remains a significant challenge for tabular data. While many generative\nmodels from the computer vision domain, such as autoencoders or generative\nadversarial networks, have been adapted for tabular data generation, less\nresearch has been directed towards recent transformer-based large language\nmodels (LLMs), which are also generative in nature. To this end, we propose\nGReaT (Generation of Realistic Tabular data), which exploits an auto-regressive\ngenerative LLM to sample synthetic and yet highly realistic tabular data.\nFurthermore, GReaT can model tabular data distributions by conditioning on any\nsubset of features; the remaining features are sampled without additional\noverhead. We demonstrate the effectiveness of the proposed approach in a series\nof experiments that quantify the validity and quality of the produced data\nsamples from multiple angles. We find that GReaT maintains state-of-the-art\nperformance across many real-world data sets with heterogeneous feature types.",
    "descriptor": "",
    "authors": [
      "Vadim Borisov",
      "Kathrin Se\u00dfler",
      "Tobias Leemann",
      "Martin Pawelczyk",
      "Gjergji Kasneci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06280"
  },
  {
    "id": "arXiv:2210.06281",
    "title": "TwiRGCN: Temporally Weighted Graph Convolution for Question Answering  over Temporal Knowledge Graphs",
    "abstract": "Recent years have witnessed much interest in temporal reasoning over\nknowledge graphs (KG) for complex question answering (QA), but there remains a\nsubstantial gap in human capabilities. We explore how to generalize relational\ngraph convolutional networks (RGCN) for temporal KGQA. Specifically, we propose\na novel, intuitive and interpretable scheme to modulate the messages passed\nthrough a KG edge during convolution, based on the relevance of its associated\ntime period to the question. We also introduce a gating device to predict if\nthe answer to a complex temporal question is likely to be a KG entity or time\nand use this prediction to guide our scoring mechanism. We evaluate the\nresulting system, which we call TwiRGCN, on TimeQuestions, a recently released,\nchallenging dataset for multi-hop complex temporal QA. We show that TwiRGCN\nsignificantly outperforms state-of-the-art systems on this dataset across\ndiverse question types. Notably, TwiRGCN improves accuracy by 9--10 percentage\npoints for the most difficult ordinal and implicit question types.",
    "descriptor": "\nComments: 9 pages + references + appendix\n",
    "authors": [
      "Aditya Sharma",
      "Apoorv Saxena",
      "Chitrank Gupta",
      "Seyed Mehran Kazemi",
      "Partha Talukdar",
      "Soumen Chakrabarti"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06281"
  },
  {
    "id": "arXiv:2210.06282",
    "title": "Towards Generalized and Explainable Long-Range Context Representation  for Dialogue Systems",
    "abstract": "Context representation is crucial to both dialogue understanding and\ngeneration. Recently, the most popular method for dialog context representation\nis to concatenate the last-$k$ previous utterances as context and use a large\ntransformer-based model to generate the next response. However, this method may\nnot be ideal for conversations containing long-range dependencies. In this\nwork, we propose DialoGX, a novel encoder-decoder based framework for\nconversational response generation with a generalized and explainable context\nrepresentation that can look beyond the last-$k$ utterances. Hence the method\nis adaptive to conversations with long-range dependencies. Our proposed\nsolution is based on two key ideas: a) computing a dynamic representation of\nthe entire context, and b) finding the previous utterances that are relevant\nfor generating the next response. Instead of last-$k$ utterances, DialoGX uses\nthe concatenation of the dynamic context vector and encoding of the most\nrelevant utterances as input which enables it to represent conversations of any\nlength in a compact and generalized fashion. We conduct our experiments on\nDailyDialog, a popular open-domain chit-chat dataset. DialoGX achieves\ncomparable performance with the state-of-the-art models on the automated\nmetrics. We also justify our context representation through the lens of\npsycholinguistics and show that the relevance score of previous utterances\nagrees well with human cognition which makes DialoGX explainable as well.",
    "descriptor": "",
    "authors": [
      "Suvodip Dey",
      "Maunendra Sankar Desarkar",
      "P. K. Srijith"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06282"
  },
  {
    "id": "arXiv:2210.06284",
    "title": "Visual Prompting for Adversarial Robustness",
    "abstract": "In this work, we leverage visual prompting (VP) to improve adversarial\nrobustness of a fixed, pre-trained model at testing time. Compared to\nconventional adversarial defenses, VP allows us to design universal (i.e.,\ndata-agnostic) input prompting templates, which have plug-and-play capabilities\nat testing time to achieve desired model performance without introducing much\ncomputation overhead. Although VP has been successfully applied to improving\nmodel generalization, it remains elusive whether and how it can be used to\ndefend against adversarial attacks. We investigate this problem and show that\nthe vanilla VP approach is not effective in adversarial defense since a\nuniversal input prompt lacks the capacity for robust learning against\nsample-specific adversarial perturbations. To circumvent it, we propose a new\nVP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate\nclass-wise visual prompts so as to not only leverage the strengths of ensemble\nprompts but also optimize their interrelations to improve model robustness. Our\nexperiments show that C-AVP outperforms the conventional VP method, with 2.1X\nstandard accuracy gain and 2X robust accuracy gain. Compared to classical\ntest-time defenses, C-AVP also yields a 42X inference time speedup.",
    "descriptor": "\nComments: 6 pages, 4 figures, 3 tables\n",
    "authors": [
      "Aochuan Chen",
      "Peter Lorenz",
      "Yuguang Yao",
      "Pin-Yu Chen",
      "Sijia Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06284"
  },
  {
    "id": "arXiv:2210.06289",
    "title": "An Efficient and Robust Object-Level Cooperative Perception Framework  for Connected and Automated Driving",
    "abstract": "Cooperative perception is challenging for connected and automated driving\nbecause of the real-time requirements and bandwidth limitation, especially when\nthe vehicle location and pose information are inaccurate. We propose an\nefficient object-level cooperative perception framework, in which data of the\n3D bounding boxes, location, and pose are broadcast and received between the\nconnected vehicles, then fused at the object level. Two Iterative Closest Point\n(ICP) and Optimal Transport theory-based matching algorithms are developed to\nmaximize the total correlations between the 3D bounding boxes jointly detected\nby the vehicles. Experiment results show that it only takes 5ms to associate\nobjects from different vehicles for each frame, and robust performance is\nachieved for different levels of location and heading errors. Meanwhile, the\nproposed framework outperforms the state-of-the-art benchmark methods when\nlocation or pose errors occur.",
    "descriptor": "",
    "authors": [
      "Zhiying Song",
      "Fuxi Wen",
      "Hailiang Zhang",
      "Jun Li"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06289"
  },
  {
    "id": "arXiv:2210.06299",
    "title": "SeKron: A Decomposition Method Supporting Many Factorization Structures",
    "abstract": "While convolutional neural networks (CNNs) have become the de facto standard\nfor most image processing and computer vision applications, their deployment on\nedge devices remains challenging. Tensor decomposition methods provide a means\nof compressing CNNs to meet the wide range of device constraints by imposing\ncertain factorization structures on their convolution tensors. However, being\nlimited to the small set of factorization structures presented by\nstate-of-the-art decomposition approaches can lead to sub-optimal performance.\nWe propose SeKron, a novel tensor decomposition method that offers a wide\nvariety of factorization structures, using sequences of Kronecker products. By\nrecursively finding approximating Kronecker factors, we arrive at optimal\ndecompositions for each of the factorization structures. We show that SeKron is\na flexible decomposition that generalizes widely used methods, such as\nTensor-Train (TT), Tensor-Ring (TR), Canonical Polyadic (CP) and Tucker\ndecompositions. Crucially, we derive an efficient convolution projection\nalgorithm shared by all SeKron structures, leading to seamless compression of\nCNN models. We validate SeKron for model compression on both high-level and\nlow-level computer vision tasks and find that it outperforms state-of-the-art\ndecomposition methods.",
    "descriptor": "",
    "authors": [
      "Marawan Gamal Abdel Hameed",
      "Ali Mosleh",
      "Marzieh S. Tahaei",
      "Vahid Partovi Nia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06299"
  },
  {
    "id": "arXiv:2210.06301",
    "title": "FontTransformer: Few-shot High-resolution Chinese Glyph Image Synthesis  via Stacked Transformers",
    "abstract": "Automatic generation of high-quality Chinese fonts from a few online training\nsamples is a challenging task, especially when the amount of samples is very\nsmall. Existing few-shot font generation methods can only synthesize\nlow-resolution glyph images that often possess incorrect topological structures\nor/and incomplete strokes. To address the problem, this paper proposes\nFontTransformer, a novel few-shot learning model, for high-resolution Chinese\nglyph image synthesis by using stacked Transformers. The key idea is to apply\nthe parallel Transformer to avoid the accumulation of prediction errors and\nutilize the serial Transformer to enhance the quality of synthesized strokes.\nMeanwhile, we also design a novel encoding scheme to feed more glyph\ninformation and prior knowledge to our model, which further enables the\ngeneration of high-resolution and visually-pleasing glyph images. Both\nqualitative and quantitative experimental results demonstrate the superiority\nof our method compared to other existing approaches in the few-shot Chinese\nfont synthesis task.",
    "descriptor": "\nComments: 23 pages, 14 Figures\n",
    "authors": [
      "Yitian Liu",
      "Zhouhui Lian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06301"
  },
  {
    "id": "arXiv:2210.06302",
    "title": "Maximum entropy exploration in contextual bandits with neural networks  and energy based models",
    "abstract": "Contextual bandits can solve a huge range of real-world problems. However,\ncurrent popular algorithms to solve them either rely on linear models, or\nunreliable uncertainty estimation in non-linear models, which are required to\ndeal with the exploration-exploitation trade-off. Inspired by theories of human\ncognition, we introduce novel techniques that use maximum entropy exploration,\nrelying on neural networks to find optimal policies in settings with both\ncontinuous and discrete action spaces. We present two classes of models, one\nwith neural networks as reward estimators, and the other with energy based\nmodels, which model the probability of obtaining an optimal reward given an\naction. We evaluate the performance of these models in static and dynamic\ncontextual bandit simulation environments. We show that both techniques\noutperform well-known standard algorithms, where energy based models have the\nbest overall performance. This provides practitioners with new techniques that\nperform well in static and dynamic settings, and are particularly well suited\nto non-linear scenarios with continuous action spaces.",
    "descriptor": "\nComments: 12 pages, 2 figures\n",
    "authors": [
      "Adam Elwood",
      "Marco Leonardi",
      "Ashraf Mohamed",
      "Alessandro Rozza"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06302"
  },
  {
    "id": "arXiv:2210.06307",
    "title": "DinoDroid: Testing Android Apps Using Deep Q-Networks",
    "abstract": "The large demand of mobile devices creates significant concerns about the\nquality of mobile applications (apps). Developers need to guarantee the quality\nof mobile apps before it is released to the market. There have been many\napproaches using different strategies to test the GUI of mobile apps. However,\nthey still need improvement due to their limited effectiveness. In this paper,\nwe propose DinoDroid, an approach based on deep Q-networks to automate testing\nof Android apps. DinoDroid learns a behavior model from a set of existing apps\nand the learned model can be used to explore and generate tests for new apps.\nDinoDroid is able to capture the fine-grained details of GUI events (e.g., the\ncontent of GUI widgets) and use them as features that are fed into deep neural\nnetwork, which acts as the agent to guide app exploration. DinoDroid\nautomatically adapts the learned model during the exploration without the need\nof any modeling strategies or pre-defined rules. We conduct experiments on 64\nopen-source Android apps. The results showed that DinoDroid outperforms\nexisting Android testing tools in terms of code coverage and bug detection.",
    "descriptor": "",
    "authors": [
      "Yu Zhao",
      "Brent Harrison",
      "Tingting Yu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2210.06307"
  },
  {
    "id": "arXiv:2210.06311",
    "title": "Semantic Cross Attention for Few-shot Learning",
    "abstract": "Few-shot learning (FSL) has attracted considerable attention recently. Among\nexisting approaches, the metric-based method aims to train an embedding network\nthat can make similar samples close while dissimilar samples as far as possible\nand achieves promising results. FSL is characterized by using only a few images\nto train a model that can generalize to novel classes in image classification\nproblems, but this setting makes it difficult to learn the visual features that\ncan identify the images' appearance variations. The model training is likely to\nmove in the wrong direction, as the images in an identical semantic class may\nhave dissimilar appearances, whereas the images in different semantic classes\nmay share a similar appearance. We argue that FSL can benefit from additional\nsemantic features to learn discriminative feature representations. Thus, this\nstudy proposes a multi-task learning approach to view semantic features of\nlabel text as an auxiliary task to help boost the performance of the FSL task.\nOur proposed model uses word-embedding representations as semantic features to\nhelp train the embedding network and a semantic cross-attention module to\nbridge the semantic features into the typical visual modal. The proposed\napproach is simple, but produces excellent results. We apply our proposed\napproach to two previous metric-based FSL methods, all of which can\nsubstantially improve performance. The source code for our model is accessible\nfrom github.",
    "descriptor": "\nComments: ACML2022\n",
    "authors": [
      "Bin Xiao",
      "Chien-Liang Liu",
      "Wen-Hoar Hsaio"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06311"
  },
  {
    "id": "arXiv:2210.06312",
    "title": "Changing the Representation: Examining Language Representation for  Neural Sign Language Production",
    "abstract": "Neural Sign Language Production (SLP) aims to automatically translate from\nspoken language sentences to sign language videos. Historically the SLP task\nhas been broken into two steps; Firstly, translating from a spoken language\nsentence to a gloss sequence and secondly, producing a sign language video\ngiven a sequence of glosses. In this paper we apply Natural Language Processing\ntechniques to the first step of the SLP pipeline. We use language models such\nas BERT and Word2Vec to create better sentence level embeddings, and apply\nseveral tokenization techniques, demonstrating how these improve performance on\nthe low resource translation task of Text to Gloss. We introduce Text to\nHamNoSys (T2H) translation, and show the advantages of using a phonetic\nrepresentation for sign language translation rather than a sign level gloss\nrepresentation. Furthermore, we use HamNoSys to extract the hand shape of a\nsign and use this as additional supervision during training, further increasing\nthe performance on T2H. Assembling best practise, we achieve a BLEU-4 score of\n26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art\nbaselines.",
    "descriptor": "\nComments: 8 pages, 4 figures, 5 tables, SLTAT 2022\n",
    "authors": [
      "Harry Walsh",
      "Ben Saunders",
      "Richard Bowden"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06312"
  },
  {
    "id": "arXiv:2210.06313",
    "title": "Large Models are Parsimonious Learners: Activation Sparsity in Trained  Transformers",
    "abstract": "This paper studies the curious phenomenon for machine learning models with\nTransformer architectures that their activation maps are sparse. By activation\nmap we refer to the intermediate output of the multi-layer perceptrons (MLPs)\nafter a ReLU activation function, and by \"sparse\" we mean that on average very\nfew entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each\ninput to MLP. Moreover, larger Transformers with more layers and wider MLP\nhidden dimensions are sparser as measured by the percentage of nonzero entries.\nThrough extensive experiments we demonstrate that the emergence of sparsity is\na prevalent phenomenon that occurs for both natural language processing and\nvision tasks, on both training and evaluation data, for Transformers of various\nconfigurations, at layers of all depth levels, as well as for other\narchitectures including MLP-mixers and 2-layer MLPs. We show that sparsity also\nemerges using training datasets with random labels, or with random inputs, or\nwith infinite amount of data, demonstrating that sparsity is not a result of a\nspecific family of datasets. We discuss how sparsity immediately implies a way\nto significantly reduce the FLOP count and improve efficiency for Transformers.\nMoreover, we demonstrate perhaps surprisingly that enforcing an even sparser\nactivation via Top-k thresholding with a small value of k brings a collection\nof desired but missing properties for Transformers, namely less sensitivity to\nnoisy training data, more robustness to input corruptions, and better\ncalibration for their prediction confidence.",
    "descriptor": "",
    "authors": [
      "Zonglin Li",
      "Chong You",
      "Srinadh Bhojanapalli",
      "Daliang Li",
      "Ankit Singh Rawat",
      "Sashank J. Reddi",
      "Ke Ye",
      "Felix Chern",
      "Felix Yu",
      "Ruiqi Guo",
      "Sanjiv Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06313"
  },
  {
    "id": "arXiv:2210.06316",
    "title": "Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic  Reasoning",
    "abstract": "This paper presents Non-Axiomatic Term Logic (NATL) as a theoretical\ncomputational framework of humanlike symbolic reasoning in artificial\nintelligence. NATL unites a discrete syntactic system inspired from Aristotle's\nterm logic and a continuous semantic system based on the modern idea of\ndistributed representations, or embeddings. This paper positions the proposed\napproach in the phylogeny and the literature of logic, and explains the\nframework. As it is yet no more than a theory and it requires much further\nelaboration to implement it, no quantitative evaluation is presented. Instead,\nqualitative analyses of arguments using NATL, some applications to possible\ncognitive science/robotics-related research, and remaining issues towards a\nmachinery implementation are discussed.",
    "descriptor": "",
    "authors": [
      "Kotaro Funakoshi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.06316"
  },
  {
    "id": "arXiv:2210.06323",
    "title": "AISFormer: Amodal Instance Segmentation with Transformer",
    "abstract": "Amodal Instance Segmentation (AIS) aims to segment the region of both visible\nand possible occluded parts of an object instance. While Mask R-CNN-based AIS\napproaches have shown promising results, they are unable to model high-level\nfeatures coherence due to the limited receptive field. The most recent\ntransformer-based models show impressive performance on vision tasks, even\nbetter than Convolution Neural Networks (CNN). In this work, we present\nAISFormer, an AIS framework, with a Transformer-based mask head. AISFormer\nexplicitly models the complex coherence between occluder, visible, amodal, and\ninvisible masks within an object's regions of interest by treating them as\nlearnable queries. Specifically, AISFormer contains four modules: (i) feature\nencoding: extract ROI and learn both short-range and long-range visual\nfeatures. (ii) mask transformer decoding: generate the occluder, visible, and\namodal mask query embeddings by a transformer decoder (iii) invisible mask\nembedding: model the coherence between the amodal and visible masks, and (iv)\nmask predicting: estimate output masks including occluder, visible, amodal and\ninvisible. We conduct extensive experiments and ablation studies on three\nchallenging benchmarks i.e. KINS, D2SA, and COCOA-cls to evaluate the\neffectiveness of AISFormer. The code is available at:\nhttps://github.com/UARK-AICV/AISFormer",
    "descriptor": "\nComments: Accepted to BMVC2022\n",
    "authors": [
      "Minh Tran",
      "Khoa Vo",
      "Kashu Yamazaki",
      "Arthur Fernandes",
      "Michael Kidd",
      "Ngan Le"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06323"
  },
  {
    "id": "arXiv:2210.06324",
    "title": "SQuId: Measuring Speech Naturalness in Many Languages",
    "abstract": "Much of text-to-speech research relies on human evaluation, which incurs\nheavy costs and slows down the development process. The problem is particularly\nacute in heavily multilingual applications, where recruiting and polling judges\ncan take weeks. We introduce SQuId (Speech Quality Identification), a\nmultilingual naturalness prediction model trained on over a million ratings and\ntested in 65 locales-the largest effort of this type to date. The main insight\nis that training one model on many locales consistently outperforms mono-locale\nbaselines. We present our task, the model, and show that it outperforms a\ncompetitive baseline based on w2v-BERT and VoiceMOS by 50.0%. We then\ndemonstrate the effectiveness of cross-locale transfer during fine-tuning and\nhighlight its effect on zero-shot locales, i.e., locales for which there is no\nfine-tuning data. Through a series of analyses, we highlight the role of\nnon-linguistic effects such as sound artifacts in cross-locale transfer.\nFinally, we present the effect of our design decision, e.g., model size,\npre-training diversity, and language rebalancing with several ablation\nexperiments.",
    "descriptor": "",
    "authors": [
      "Thibault Sellam",
      "Ankur Bapna",
      "Joshua Camp",
      "Diana Mackinnon",
      "Ankur P. Parikh",
      "Jason Riesa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06324"
  },
  {
    "id": "arXiv:2210.06327",
    "title": "Betting the system: Using lineups to predict football scores",
    "abstract": "This paper aims to reduce randomness in football by analysing the role of\nlineups in final scores using machine learning prediction models we have\ndeveloped. Football clubs invest millions of dollars on lineups and knowing how\nindividual statistics translate to better outcomes can optimise investments.\nMoreover, sports betting is growing exponentially and being able to predict the\nfuture is profitable and desirable. We use machine learning models and\nhistorical player data from English Premier League (2020-2022) to predict\nscores and to understand how individual performance can improve the outcome of\na match. We compared different prediction techniques to maximise the\npossibility of finding useful models. We created heuristic and machine learning\nmodels predicting football scores to compare different techniques. We used\ndifferent sets of features and shown goalkeepers stats are more important than\nattackers stats to predict goals scored. We applied a broad evaluation process\nto assess the efficacy of the models in real world applications. We managed to\npredict correctly all relegated teams after forecast 100 consecutive matches.\nWe show that Support Vector Regression outperformed other techniques predicting\nfinal scores and that lineups do improve predictions. Finally, our model was\nprofitable (42% return) when emulating a betting system using real world odds\ndata.",
    "descriptor": "\nComments: 8 Page paper submitted for review for SDM23\n",
    "authors": [
      "George Peters",
      "Diogo Pacheco"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06327"
  },
  {
    "id": "arXiv:2210.06328",
    "title": "Momentum Aggregation for Private Non-convex ERM",
    "abstract": "We introduce new algorithms and convergence guarantees for privacy-preserving\nnon-convex Empirical Risk Minimization (ERM) on smooth $d$-dimensional\nobjectives. We develop an improved sensitivity analysis of stochastic gradient\ndescent on smooth objectives that exploits the recurrence of examples in\ndifferent epochs. By combining this new approach with recent analysis of\nmomentum with private aggregation techniques, we provide an\n$(\\epsilon,\\delta)$-differential private algorithm that finds a gradient of\nnorm $\\tilde O\\left(\\frac{d^{1/3}}{(\\epsilon N)^{2/3}}\\right)$ in\n$O\\left(\\frac{N^{7/3}\\epsilon^{4/3}}{d^{2/3}}\\right)$ gradient evaluations,\nimproving the previous best gradient bound of $\\tilde\nO\\left(\\frac{d^{1/4}}{\\sqrt{\\epsilon N}}\\right)$.",
    "descriptor": "",
    "authors": [
      "Hoang Tran",
      "Ashok Cutkosky"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06328"
  },
  {
    "id": "arXiv:2210.06331",
    "title": "RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims  on Social Media",
    "abstract": "We present Reddit Health Online Talk (RedHOT), a corpus of 22,000 richly\nannotated social media posts from Reddit spanning 24 health conditions.\nAnnotations include demarcations of spans corresponding to medical claims,\npersonal experiences, and questions. We collect additional granular annotations\non identified claims. Specifically, we mark snippets that describe patient\nPopulations, Interventions, and Outcomes (PIO elements) within these. Using\nthis corpus, we introduce the task of retrieving trustworthy evidence relevant\nto a given claim made on social media. We propose a new method to automatically\nderive (noisy) supervision for this task which we use to train a dense\nretrieval model; this outperforms baseline models. Manual evaluation of\nretrieval results performed by medical doctors indicate that while our system\nperformance is promising, there is considerable room for improvement. Collected\nannotations (and scripts to assemble the dataset), are available at\nhttps://github.com/sominw/redhot.",
    "descriptor": "",
    "authors": [
      "Somin Wadhwa",
      "Vivek Khetan",
      "Silvio Amir",
      "Byron Wallace"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06331"
  },
  {
    "id": "arXiv:2210.06332",
    "title": "ViewBirdiformer: Learning to recover ground-plane crowd trajectories and  ego-motion from a single ego-centric view",
    "abstract": "We introduce a novel learning-based method for view birdification, the task\nof recovering ground-plane trajectories of pedestrians of a crowd and their\nobserver in the same crowd just from the observed ego-centric video. View\nbirdification becomes essential for mobile robot navigation and localization in\ndense crowds where the static background is hard to see and reliably track. It\nis challenging mainly for two reasons; i) absolute trajectories of pedestrians\nare entangled with the movement of the observer which needs to be decoupled\nfrom their observed relative movements in the ego-centric video, and ii) a\ncrowd motion model describing the pedestrian movement interactions is specific\nto the scene yet unknown a priori. For this, we introduce a Transformer-based\nnetwork referred to as ViewBirdiformer which implicitly models the crowd motion\nthrough self-attention and decomposes relative 2D movement observations onto\nthe ground-plane trajectories of the crowd and the camera through\ncross-attention between views. Most important, ViewBirdiformer achieves view\nbirdification in a single forward pass which opens the door to accurate\nreal-time, always-on situational awareness. Extensive experimental results\ndemonstrate that ViewBirdiformer achieves accuracy similar to or better than\nstate-of-the-art with three orders of magnitude reduction in execution time.",
    "descriptor": "",
    "authors": [
      "Mai Nishimura",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06332"
  },
  {
    "id": "arXiv:2210.06333",
    "title": "Pattern Characterization Using Topological Data Analysis: Application to  Piezo Vibration Striking Treatment",
    "abstract": "Quantifying patterns in visual or tactile textures provides important\ninformation about the process or phenomena that generated these patterns. In\nmanufacturing, these patterns can be intentionally introduced as a design\nfeature, or they can be a byproduct of a specific process. Since surface\ntexture has significant impact on the mechanical properties and the longevity\nof the workpiece, it is important to develop tools for quantifying surface\npatterns and, when applicable, comparing them to their nominal counterparts.\nWhile existing tools may be able to indicate the existence of a pattern, they\ntypically do not provide more information about the pattern structure, or how\nmuch it deviates from a nominal pattern. Further, prior works do not provide\nautomatic or algorithmic approaches for quantifying other pattern\ncharacteristics such as depths' consistency, and variations in the pattern\nmotifs at different level sets. This paper leverages persistent homology from\nTopological Data Analysis (TDA) to derive noise-robust scores for quantifying\nmotifs' depth and roundness in a pattern. Specifically, sublevel persistence is\nused to derive scores that quantify the consistency of indentation depths at\nany level set in Piezo Vibration Striking Treatment (PVST) surfaces. Moreover,\nwe combine sublevel persistence with the distance transform to quantify the\nconsistency of the indentation radii, and to compare them with the nominal\nones. Although the tool in our PVST experiments had a semi-spherical profile,\nwe present a generalization of our approach to tools/motifs of arbitrary shapes\nthus making our method applicable to other pattern-generating manufacturing\nprocesses.",
    "descriptor": "",
    "authors": [
      "Max M. Chumley",
      "Melih C. Yesilli",
      "Jisheng Chen",
      "Firas A. Khasawneh",
      "Yang Guo"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)",
      "Algebraic Topology (math.AT)"
    ],
    "url": "https://arxiv.org/abs/2210.06333"
  },
  {
    "id": "arXiv:2210.06336",
    "title": "Synthetic Text Detection: Systemic Literature Review",
    "abstract": "Within the text analysis and processing fields, generated text attacks have\nbeen made easier to create than ever before. To combat these attacks open\nsourcing models and datasets have become a major trend to create automated\ndetection algorithms in defense of authenticity. For this purpose, synthetic\ntext detection has become an increasingly viable topic of research. This review\nis written for the purpose of creating a snapshot of the state of current\nliterature and easing the barrier to entry for future authors. Towards that\ngoal, we identified few research trends and challenges in this field.",
    "descriptor": "",
    "authors": [
      "Jesus Guerrero",
      "Izzat Alsmadi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06336"
  },
  {
    "id": "arXiv:2210.06339",
    "title": "Self-Attention Message Passing for Contrastive Few-Shot Learning",
    "abstract": "Humans have a unique ability to learn new representations from just a handful\nof examples with little to no supervision. Deep learning models, however,\nrequire an abundance of data and supervision to perform at a satisfactory\nlevel. Unsupervised few-shot learning (U-FSL) is the pursuit of bridging this\ngap between machines and humans. Inspired by the capacity of graph neural\nnetworks (GNNs) in discovering complex inter-sample relationships, we propose a\nnovel self-attention based message passing contrastive learning approach\n(coined as SAMP-CLR) for U-FSL pre-training. We also propose an optimal\ntransport (OT) based fine-tuning strategy (we call OpT-Tune) to efficiently\ninduce task awareness into our novel end-to-end unsupervised few-shot\nclassification framework (SAMPTransfer). Our extensive experimental results\ncorroborate the efficacy of SAMPTransfer in a variety of downstream few-shot\nclassification scenarios, setting a new state-of-the-art for U-FSL on both\nminiImagenet and tieredImagenet benchmarks, offering up to 7%+ and 5%+\nimprovements, respectively. Our further investigations also confirm that\nSAMPTransfer remains on-par with some supervised baselines on miniImagenet and\noutperforms all existing U-FSL baselines in a challenging cross-domain\nscenario. Our code can be found in our GitHub repository at\nhttps://github.com/ojss/SAMPTransfer/.",
    "descriptor": "",
    "authors": [
      "Ojas Kishorkumar Shirekar",
      "Anuj Singh",
      "Hadi Jamali-Rad"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06339"
  },
  {
    "id": "arXiv:2210.06340",
    "title": "Improving Radiology Report Generation Systems by Removing Hallucinated  References to Non-existent Priors",
    "abstract": "Current deep learning models trained to generate radiology reports from chest\nradiographs are capable of producing clinically accurate, clear, and actionable\ntext that can advance patient care. However, such systems all succumb to the\nsame problem: making hallucinated references to non-existent prior reports.\nSuch hallucinations occur because these models are trained on datasets of\nreal-world patient reports that inherently refer to priors. To this end, we\npropose two methods to remove references to priors in radiology reports: (1) a\nGPT-3-based few-shot approach to rewrite medical reports without references to\npriors; and (2) a BioBERT-based token classification approach to directly\nremove words referring to priors. We use the aforementioned approaches to\nmodify MIMIC-CXR, a publicly available dataset of chest X-rays and their\nassociated free-text radiology reports; we then retrain CXR-RePaiR, a radiology\nreport generation system, on the adapted MIMIC-CXR dataset. We find that our\nre-trained model--which we call CXR-ReDonE--outperforms previous report\ngeneration methods on clinical metrics, achieving an average BERTScore of\n0.2351 (2.57% absolute improvement). We expect our approach to be broadly\nvaluable in enabling current radiology report generation systems to be more\ndirectly integrated into clinical pipelines.",
    "descriptor": "\nComments: 13 pages, 1 figure, 11 tables; Under review at ML4H 2022 Symposium (Collocated with NeurIPS 2022)\n",
    "authors": [
      "Vignav Ramesh",
      "Nathan Andrew Chi",
      "Pranav Rajpurkar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06340"
  },
  {
    "id": "arXiv:2210.06341",
    "title": "TaskMix: Data Augmentation for Meta-Learning of Spoken Intent  Understanding",
    "abstract": "Meta-Learning has emerged as a research direction to better transfer\nknowledge from related tasks to unseen but related tasks. However,\nMeta-Learning requires many training tasks to learn representations that\ntransfer well to unseen tasks; otherwise, it leads to overfitting, and the\nperformance degenerates to worse than Multi-task Learning. We show that a\nstate-of-the-art data augmentation method worsens this problem of overfitting\nwhen the task diversity is low. We propose a simple method, TaskMix, which\nsynthesizes new tasks by linearly interpolating existing tasks. We compare\nTaskMix against many baselines on an in-house multilingual intent\nclassification dataset of N-Best ASR hypotheses derived from real-life\nhuman-machine telephony utterances and two datasets derived from MTOP. We show\nthat TaskMix outperforms baselines, alleviates overfitting when task diversity\nis low, and does not degrade performance even when it is high.",
    "descriptor": "\nComments: Accepted at Findings of AACL-IJCNLP 2022\n",
    "authors": [
      "Surya Kant Sahu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06341"
  },
  {
    "id": "arXiv:2210.06345",
    "title": "Variational Open-Domain Question Answering",
    "abstract": "We introduce the Variational Open-Domain (VOD) framework for end-to-end\ntraining and evaluation of retrieval-augmented models (open-domain question\nanswering and language modelling). We show that the R\\'enyi variational bound,\na lower bound to the task marginal likelihood, can be exploited to aid\noptimization and use importance sampling to estimate the task log-likelihood\nlower bound and its gradients using samples drawn from an auxiliary retriever\n(approximate posterior). The framework can be used to train modern\nretrieval-augmented systems end-to-end using tractable and consistent estimates\nof the R\\'enyi variational bound and its gradients. We demonstrate the\nframework's versatility by training reader-retriever BERT-based models on\nmultiple-choice medical exam questions (MedMCQA and USMLE). We registered a new\nstate-of-the-art for both datasets (MedMCQA: $62.9$\\%, USMLE: $55.0$\\%). Last,\nwe show that the retriever part of the learned reader-retriever model trained\non the medical board exam questions can be used in search engines for a medical\nknowledge base.",
    "descriptor": "\nComments: 27 pages, 5 figures\n",
    "authors": [
      "Valentin Li\u00e9vin",
      "Andreas Geert Motzfeldt",
      "Ida Riis Jensen",
      "Ole Winther"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06345"
  },
  {
    "id": "arXiv:2210.06346",
    "title": "Predicting the clinical citation count of biomedical papers using  multilayer perceptron neural network",
    "abstract": "The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.",
    "descriptor": "\nComments: 25 pages, 8 figures\n",
    "authors": [
      "Xin Li",
      "Xuli Tang",
      "Qikai Cheng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06346"
  },
  {
    "id": "arXiv:2210.06349",
    "title": "Context Generation Improves Open Domain Question Answering",
    "abstract": "Closed-book question answering (QA) requires a model to directly answer an\nopen-domain question without access to any external knowledge. Prior work on\nclosed-book QA either directly finetunes or prompts a pretrained language model\n(LM) to leverage the stored knowledge. However, they do not fully exploit the\nparameterized knowledge. To address this issue, we propose a two-stage,\nclosed-book QA framework which employs a coarse-to-fine approach to extract\nrelevant knowledge and answer a question. Our approach first generates a\nrelated context for a given question by prompting a pretrained LM. We then\nprompt the same LM for answer prediction using the generated context and the\nquestion. Additionally, to eliminate failure caused by context uncertainty, we\nmarginalize over generated contexts. Experimental results on three QA\nbenchmarks show that our method significantly outperforms previous closed-book\nQA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book\nmethods that exploit external knowledge sources (e.g. 68.6% vs. 68.0%). Our\nmethod is able to better exploit the stored knowledge in pretrained LMs without\nadding extra learnable parameters or needing finetuning, and paves the way for\nhybrid models that integrate pretrained LMs with external knowledge.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Dan Su",
      "Mostofa Patwary",
      "Shrimai Prabhumoye",
      "Peng Xu",
      "Ryan Prenger",
      "Mohammad Shoeybi",
      "Pascale Fung",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06349"
  },
  {
    "id": "arXiv:2210.06350",
    "title": "CTL++: Evaluating Generalization on Never-Seen Compositional Patterns of  Known Functions, and Compatibility of Neural Representations",
    "abstract": "Well-designed diagnostic tasks have played a key role in studying the failure\nof neural nets (NNs) to generalize systematically. Famous examples include SCAN\nand Compositional Table Lookup (CTL). Here we introduce CTL++, a new diagnostic\ndataset based on compositions of unary symbolic functions. While the original\nCTL is used to test length generalization or productivity, CTL++ is designed to\ntest systematicity of NNs, that is, their capability to generalize to unseen\ncompositions of known functions. CTL++ splits functions into groups and tests\nperformance on group elements composed in a way not seen during training. We\nshow that recent CTL-solving Transformer variants fail on CTL++. The simplicity\nof the task design allows for fine-grained control of task difficulty, as well\nas many insightful analyses. For example, we measure how much overlap between\ngroups is needed by tested NNs for learning to compose. We also visualize how\nlearned symbol representations in outputs of functions from different groups\nare compatible in case of success but not in case of failure. These results\nprovide insights into failure cases reported on more complex compositions in\nthe natural language domain. Our code is public.",
    "descriptor": "\nComments: Accepted to EMNLP 2022\n",
    "authors": [
      "R\u00f3bert Csord\u00e1s",
      "Kazuki Irie",
      "J\u00fcrgen Schmidhuber"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06350"
  },
  {
    "id": "arXiv:2210.06351",
    "title": "A Keyword Based Approach to Understanding the Overpenalization of  Marginalized Groups by English Marginal Abuse Models on Twitter",
    "abstract": "Harmful content detection models tend to have higher false positive rates for\ncontent from marginalized groups. In the context of marginal abuse modeling on\nTwitter, such disproportionate penalization poses the risk of reduced\nvisibility, where marginalized communities lose the opportunity to voice their\nopinion on the platform. Current approaches to algorithmic harm mitigation, and\nbias detection for NLP models are often very ad hoc and subject to human bias.\nWe make two main contributions in this paper. First, we design a novel\nmethodology, which provides a principled approach to detecting and measuring\nthe severity of potential harms associated with a text-based model. Second, we\napply our methodology to audit Twitter's English marginal abuse model, which is\nused for removing amplification eligibility of marginally abusive content.\nWithout utilizing demographic labels or dialect classifiers, we are still able\nto detect and measure the severity of issues related to the over-penalization\nof the speech of marginalized communities, such as the use of reclaimed speech,\ncounterspeech, and identity related terms. In order to mitigate the associated\nharms, we experiment with adding additional true negative examples and find\nthat doing so provides improvements to our fairness metrics without large\ndegradations in model performance.",
    "descriptor": "",
    "authors": [
      "Kyra Yee",
      "Alice Schoenauer Sebag",
      "Olivia Redfield",
      "Emily Sheng",
      "Matthias Eck",
      "Luca Belli"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06351"
  },
  {
    "id": "arXiv:2210.06352",
    "title": "Automatic Discovery of Composite SPMD Partitioning Strategies in PartIR",
    "abstract": "Large neural network models are commonly trained through a combination of\nadvanced parallelism strategies in a single program, multiple data (SPMD)\nparadigm. For example, training large transformer models requires combining\ndata, model, and pipeline partitioning; and optimizer sharding techniques.\nHowever, identifying efficient combinations for many model architectures and\naccelerator systems requires significant manual analysis. In this work, we\npresent an automatic partitioner that identifies these combinations through a\ngoal-oriented search. Our key findings are that a Monte Carlo Tree Search-based\npartitioner leveraging partition-specific compiler analysis directly into the\nsearch and guided goals matches expert-level strategies for various models.",
    "descriptor": "",
    "authors": [
      "Sami Alabed",
      "Dominik Grewe",
      "Juliana Franco",
      "Bart Chrzaszcz",
      "Tom Natan",
      "Tamara Norman",
      "Norman A. Rink",
      "Dimitrios Vytiniotis",
      "Michael Schaarschmidt"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06352"
  },
  {
    "id": "arXiv:2210.06353",
    "title": "Russian Web Tables: A Public Corpus of Web Tables for Russian Language  Based on Wikipedia",
    "abstract": "Corpora that contain tabular data such as WebTables are a vital resource for\nthe academic community. Essentially, they are the backbone of any modern\nresearch in information management. They are used for various tasks of data\nextraction, knowledge base construction, question answering, column semantic\ntype detection and many other. Such corpora are useful not only as a source of\ndata, but also as a base for building test datasets. So far, there were no such\ncorpora for the Russian language and this seriously hindered research in the\naforementioned areas.\nIn this paper, we present the first corpus of Web tables created specifically\nout of Russian language material. It was built via a special toolkit we have\ndeveloped to crawl the Russian Wikipedia. Both the corpus and the toolkit are\nopen-source and publicly available. Finally, we present a short study that\ndescribes Russian Wikipedia tables and their statistics.",
    "descriptor": "",
    "authors": [
      "Platon Fedorov",
      "Alexey Mironov",
      "George Chernishev"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Digital Libraries (cs.DL)",
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06353"
  },
  {
    "id": "arXiv:2210.06354",
    "title": "Text-to-Audio Grounding Based Novel Metric for Evaluating Audio Caption  Similarity",
    "abstract": "Automatic Audio Captioning (AAC) refers to the task of translating an audio\nsample into a natural language (NL) text that describes the audio events,\nsource of the events and their relationships. Unlike NL text generation tasks,\nwhich rely on metrics like BLEU, ROUGE, METEOR based on lexical semantics for\nevaluation, the AAC evaluation metric requires an ability to map NL text\n(phrases) that correspond to similar sounds in addition lexical semantics.\nCurrent metrics used for evaluation of AAC tasks lack an understanding of the\nperceived properties of sound represented by text. In this paper, wepropose a\nnovel metric based on Text-to-Audio Grounding (TAG), which is, useful for\nevaluating cross modal tasks like AAC. Experiments on publicly available AAC\ndata-set shows our evaluation metric to perform better compared to existing\nmetrics used in NL text and image captioning literature.",
    "descriptor": "\nComments: 9 pages, 8 figures,\n",
    "authors": [
      "Swapnil Bhosale",
      "Rupayan Chakraborty",
      "Sunil Kumar Kopparapu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06354"
  },
  {
    "id": "arXiv:2210.06356",
    "title": "Extractive Question Answering on Queries in Hindi and Tamil",
    "abstract": "Indic languages like Hindi and Tamil are underrepresented in the natural\nlanguage processing (NLP) field compared to languages like English. Due to this\nunderrepresentation, performance on NLP tasks (such as search algorithms) in\nIndic languages are inferior to their English counterparts. This difference\ndisproportionately affects those who come from lower socioeconomic statuses\nbecause they consume the most Internet content in local languages. The goal of\nthis project is to build an NLP model that performs better than pre-existing\nmodels for the task of extractive question-answering (QA) on a public dataset\nin Hindi and Tamil. Extractive QA is an NLP task where answers to questions are\nextracted from a corresponding body of text. To build the best solution, we\nused three different models. The first model is an unmodified cross-lingual\nversion of the NLP model RoBERTa, known as XLM-RoBERTa, that is pretrained on\n100 languages. The second model is based on the pretrained RoBERTa model with\nan extra classification head for the question answering, but we used a custom\nIndic tokenizer, then optimized hyperparameters and fine tuned on the Indic\ndataset. The third model is based on XLM-RoBERTa, but with extra finetuning and\ntraining on the Indic dataset. We hypothesize the third model will perform best\nbecause of the variety of languages the XLM-RoBERTa model has been pretrained\non and the additional finetuning on the Indic dataset. This hypothesis was\nproven wrong because the paired RoBERTa models performed the best as the\ntraining data used was most specific to the task performed as opposed to the\nXLM-RoBERTa models which had much data that was not in either Hindi or Tamil.",
    "descriptor": "\nComments: 8 pages, 1 figure, 1 table, submitted to the Pittsburgh Regional Science and Engineering Fair in the Computer Science and Math Senior Division\n",
    "authors": [
      "Adhitya Thirumala",
      "Elisa Ferracane"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06356"
  },
  {
    "id": "arXiv:2210.06361",
    "title": "MFFN: Multi-view Feature Fusion Network for Camouflaged Object Detection",
    "abstract": "Recent research about camouflaged object detection (COD) aims to segment\nhighly concealed objects hidden in complex surroundings. The tiny, fuzzy\ncamouflaged objects result in visually indistinguishable properties. However,\ncurrent single-view COD detectors are sensitive to background distractors.\nTherefore, blurred boundaries and variable shapes of the camouflaged objects\nare challenging to be fully captured with a single-view detector. To overcome\nthese obstacles, we propose a behavior-inspired framework, called Multi-view\nFeature Fusion Network (MFFN), which mimics the human behaviors of finding\nindistinct objects in images, i.e., observing from multiple angles, distances,\nperspectives. Specifically, the key idea behind it is to generate multiple ways\nof observation (multi-view) by data augmentation and apply them as inputs. MFFN\ncaptures critical edge and semantic information by comparing and fusing\nextracted multi-view features. In addition, our MFFN exploits the dependence\nand interaction between views by the designed hierarchical view and channel\nintegration modules. Furthermore, our methods leverage the complementary\ninformation between different views through a two-stage attention module called\nCo-attention of Multi-view (CAMV). And we designed a local-overall module\ncalled Channel Fusion Unit (CFU) to explore the channel-wise contextual clues\nof diverse feature maps in an iterative manner. The experiment results show\nthat our method performs favorably against existing state-of-the-art methods\nvia training with the same data. The code will be available at https:\n//github.com/dwardzheng/MFFN_COD.",
    "descriptor": "\nComments: In Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)\n",
    "authors": [
      "Dehua Zheng",
      "Xiaochen Zheng",
      "Laurence T. Yang",
      "Yuan Gao",
      "Chenlu Zhu",
      "Yiheng Ruan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06361"
  },
  {
    "id": "arXiv:2210.06363",
    "title": "On Extremal Rates of Storage over Graphs",
    "abstract": "A storage code over a graph maps $K$ independent source symbols, each of\n$L_w$ bits, to $N$ coded symbols, each of $L_v$ bits, such that each coded\nsymbol is stored in a node of the graph and each edge of the graph is\nassociated with one source symbol. From a pair of nodes connected by an edge,\nthe source symbol that is associated with the edge can be decoded. The ratio\n$L_w/L_v$ is called the symbol rate of a storage code and the highest symbol\nrate is called the capacity. We show that the three highest capacity values of\nstorage codes over graphs are $2, 3/2, 4/3$. We characterize all graphs over\nwhich the storage code capacity is $2$ and $3/2$, and for capacity value of\n$4/3$, necessary condition and sufficient condition (that do not match) on the\ngraphs are given.",
    "descriptor": "\nComments: 28 pages, 11 figures\n",
    "authors": [
      "Zhou Li",
      "Hua Sun"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.06363"
  },
  {
    "id": "arXiv:2210.06364",
    "title": "AdaNorm: Adaptive Gradient Norm Correction based Optimizer for CNNs",
    "abstract": "The stochastic gradient descent (SGD) optimizers are generally used to train\nthe convolutional neural networks (CNNs). In recent years, several adaptive\nmomentum based SGD optimizers have been introduced, such as Adam, diffGrad,\nRadam and AdaBelief. However, the existing SGD optimizers do not exploit the\ngradient norm of past iterations and lead to poor convergence and performance.\nIn this paper, we propose a novel AdaNorm based SGD optimizers by correcting\nthe norm of gradient in each iteration based on the adaptive training history\nof gradient norm. By doing so, the proposed optimizers are able to maintain\nhigh and representive gradient throughout the training and solves the low and\natypical gradient problems. The proposed concept is generic and can be used\nwith any existing SGD optimizer. We show the efficacy of the proposed AdaNorm\nwith four state-of-the-art optimizers, including Adam, diffGrad, Radam and\nAdaBelief. We depict the performance improvement due to the proposed optimizers\nusing three CNN models, including VGG16, ResNet18 and ResNet50, on three\nbenchmark object recognition datasets, including CIFAR10, CIFAR100 and\nTinyImageNet. Code: \\url{https://github.com/shivram1987/AdaNorm}.",
    "descriptor": "\nComments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023\n",
    "authors": [
      "Shiv Ram Dubey",
      "Satish Kumar Singh",
      "Bidyut Baran Chaudhuri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06364"
  },
  {
    "id": "arXiv:2210.06366",
    "title": "A Generalist Framework for Panoptic Segmentation of Images and Videos",
    "abstract": "Panoptic segmentation assigns semantic and instance ID labels to every pixel\nof an image. As permutations of instance IDs are also valid solutions, the task\nrequires learning of high-dimensional one-to-many mapping. As a result,\nstate-of-the-art approaches use customized architectures and task-specific loss\nfunctions. We formulate panoptic segmentation as a discrete data generation\nproblem, without relying on inductive bias of the task. A diffusion model based\non analog bits is used to model panoptic masks, with a simple, generic\narchitecture and loss function. By simply adding past predictions as a\nconditioning signal, our method is capable of modeling video (in a streaming\nsetting) and thereby learns to track object instances automatically. With\nextensive experiments, we demonstrate that our generalist approach can perform\ncompetitively to state-of-the-art specialist methods in similar settings.",
    "descriptor": "",
    "authors": [
      "Ting Chen",
      "Lala Li",
      "Saurabh Saxena",
      "Geoffrey Hinton",
      "David J. Fleet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2210.06366"
  },
  {
    "id": "arXiv:2210.06368",
    "title": "Individualized Conditioning and Negative Distances for Speaker  Separation",
    "abstract": "Speaker separation aims to extract multiple voices from a mixed signal. In\nthis paper, we propose two speaker-aware designs to improve the existing\nspeaker separation solutions. The first model is a speaker conditioning network\nthat integrates speech samples to generate individualized speaker conditions,\nwhich then provide informed guidance for a separation module to produce\nwell-separated outputs.\nThe second design aims to reduce non-target voices in the separated speech.\nTo this end, we propose negative distances to penalize the appearance of any\nnon-target voice in the channel outputs, and positive distances to drive the\nseparated voices closer to the clean targets. We explore two different setups,\nweighted-sum and triplet-like, to integrate these two distances to form a\ncombined auxiliary loss for the separation networks. Experiments conducted on\nLibriMix demonstrate the effectiveness of our proposed models.",
    "descriptor": "\nComments: Accepted to ICMLA 2022\n",
    "authors": [
      "Tao Sun",
      "Nidal Abuhajar",
      "Shuyu Gong",
      "Zhewei Wang",
      "Charles D. Smith",
      "Xianhui Wang",
      "Li Xu",
      "Jundong Liu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.06368"
  },
  {
    "id": "arXiv:2210.06373",
    "title": "Learning Multi-resolution Functional Maps with Spectral Attention for  Robust Shape Matching",
    "abstract": "In this work, we present a novel non-rigid shape matching framework based on\nmulti-resolution functional maps with spectral attention. Existing functional\nmap learning methods all rely on the critical choice of the spectral resolution\nhyperparameter, which can severely affect the overall accuracy or lead to\noverfitting, if not chosen carefully. In this paper, we show that spectral\nresolution tuning can be alleviated by introducing spectral attention. Our\nframework is applicable in both supervised and unsupervised settings, and we\nshow that it is possible to train the network so that it can adapt the spectral\nresolution, depending on the given shape input. More specifically, we propose\nto compute multi-resolution functional maps that characterize correspondence\nacross a range of spectral resolutions, and introduce a spectral attention\nnetwork that helps to combine this representation into a single coherent final\ncorrespondence. Our approach is not only accurate with near-isometric input,\nfor which a high spectral resolution is typically preferred, but also robust\nand able to produce reasonable matching even in the presence of significant\nnon-isometric distortion, which poses great challenges to existing methods. We\ndemonstrate the superior performance of our approach through experiments on a\nsuite of challenging near-isometric and non-isometric shape matching\nbenchmarks.",
    "descriptor": "\nComments: NeurIPS 2022. Code and data: this https URL\n",
    "authors": [
      "Lei Li",
      "Nicolas Donati",
      "Maks Ovsjanikov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06373"
  },
  {
    "id": "arXiv:2210.06375",
    "title": "Superpolynomial Lower Bounds for Decision Tree Learning and Testing",
    "abstract": "We establish new hardness results for decision tree optimization problems,\nadding to a line of work that dates back to Hyafil and Rivest in 1976. We\nprove, under randomized ETH, superpolynomial lower bounds for two basic\nproblems: given an explicit representation of a function $f$ and a generator\nfor a distribution $\\mathcal{D}$, construct a small decision tree approximator\nfor $f$ under $\\mathcal{D}$, and decide if there is a small decision tree\napproximator for $f$ under $\\mathcal{D}$.\nOur results imply new lower bounds for distribution-free PAC learning and\ntesting of decision trees, settings in which the algorithm only has restricted\naccess to $f$ and $\\mathcal{D}$. Specifically, we show: $n$-variable size-$s$\ndecision trees cannot be properly PAC learned in time $n^{\\tilde{O}(\\log\\log\ns)}$, and depth-$d$ decision trees cannot be tested in time $\\exp(d^{\\,O(1)})$.\nFor learning, the previous best lower bound only ruled out\n$\\text{poly}(n)$-time algorithms (Alekhnovich, Braverman, Feldman, Klivans, and\nPitassi, 2009). For testing, recent work gives similar though incomparable\nbounds in the setting where $f$ is random and $\\mathcal{D}$ is nonexplicit\n(Blais, Ferreira Pinto Jr., and Harms, 2021). Assuming a plausible conjecture\non the hardness of Set-Cover, we show our lower bound for learning decision\ntrees can be improved to $n^{\\Omega(\\log s)}$, matching the best known upper\nbound of $n^{O(\\log s)}$ due to Ehrenfeucht and Haussler (1989).\nWe obtain our results within a unified framework that leverages recent\nprogress in two lines of work: the inapproximability of Set-Cover and XOR\nlemmas for query complexity. Our framework is versatile and yields results for\nrelated concept classes such as juntas and DNF formulas.",
    "descriptor": "\nComments: 44 pages, 5 figures. SODA 2023\n",
    "authors": [
      "Caleb Koch",
      "Carmen Strassle",
      "Li-Yang Tan"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06375"
  },
  {
    "id": "arXiv:2210.06376",
    "title": "Probing Commonsense Knowledge in Pre-trained Language Models with  Sense-level Precision and Expanded Vocabulary",
    "abstract": "Progress on commonsense reasoning is usually measured from performance\nimprovements on Question Answering tasks designed to require commonsense\nknowledge. However, fine-tuning large Language Models (LMs) on these specific\ntasks does not directly evaluate commonsense learned during pre-training. The\nmost direct assessments of commonsense knowledge in pre-trained LMs are\narguably cloze-style tasks targeting commonsense assertions (e.g., A pen is\nused for [MASK].). However, this approach is restricted by the LM's vocabulary\navailable for masked predictions, and its precision is subject to the context\nprovided by the assertion. In this work, we present a method for enriching LMs\nwith a grounded sense inventory (i.e., WordNet) available at the vocabulary\nlevel, without further training. This modification augments the prediction\nspace of cloze-style prompts to the size of a large ontology while enabling\nfiner-grained (sense-level) queries and predictions. In order to evaluate LMs\nwith higher precision, we propose SenseLAMA, a cloze-style task featuring\nverbalized relations from disambiguated triples sourced from WordNet, WikiData,\nand ConceptNet. Applying our method to BERT, producing a WordNet-enriched\nversion named SynBERT, we find that LMs can learn non-trivial commonsense\nknowledge from self-supervision, covering numerous relations, and more\neffectively than comparable similarity-based approaches.",
    "descriptor": "",
    "authors": [
      "Daniel Loureiro",
      "Al\u00edpio M\u00e1rio Jorge"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06376"
  },
  {
    "id": "arXiv:2210.06377",
    "title": "Smooth Trajectory Collision Avoidance through Deep Reinforcement  Learning",
    "abstract": "Collision avoidance is a crucial task in vision-guided autonomous navigation.\nSolutions based on deep reinforcement learning (DRL) has become increasingly\npopular. In this work, we proposed several novel agent state and reward\nfunction designs to tackle two critical issues in DRL-based navigation\nsolutions: 1) smoothness of the trained flight trajectories; and 2) model\ngeneralization to handle unseen environments.\nFormulated under a DRL framework, our model relies on margin reward and\nsmoothness constraints to ensure UAVs fly smoothly while greatly reducing the\nchance of collision. The proposed smoothness reward minimizes a combination of\nfirst-order and second-order derivatives of flight trajectories, which can also\ndrive the points to be evenly distributed, leading to stable flight speed. To\nenhance the agent's capability of handling new unseen environments, two\npractical setups are proposed to improve the invariance of both the state and\nreward function when deploying in different scenes. Experiments demonstrate the\neffectiveness of our overall design and individual components.",
    "descriptor": "\nComments: Accepted to ICMLA 2022\n",
    "authors": [
      "Sirui Song",
      "Kirk Saunders",
      "Ye Yue",
      "Jundong Liu"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06377"
  },
  {
    "id": "arXiv:2210.06379",
    "title": "One does not fit all! On the Complementarity of Vision Encoders for  Vision and Language Tasks",
    "abstract": "Current multimodal models, aimed at solving Vision and Language (V+L) tasks,\npredominantly repurpose Vision Encoders (VE) as feature extractors. While many\nVEs -- of different architectures, trained on different data and objectives --\nare publicly available, they are not designed for the downstream V+L tasks.\nNonetheless, most current work assumes that a \\textit{single} pre-trained VE\ncan serve as a general-purpose encoder. In this work, we evaluate whether the\ninformation stored within different VEs is complementary, i.e. if providing the\nmodel with features from multiple VEs can improve the performance on a target\ntask. We exhaustively experiment with three popular VEs on six downstream V+L\ntasks and analyze the attention and VE-dropout patterns. Our results and\nanalyses suggest that diverse VEs complement each other, resulting in improved\ndownstream V+L task performance, where the improvements are not due to simple\nensemble effects (i.e. the performance does not always improve when increasing\nthe number of encoders). We demonstrate that future VEs, which are not\n\\textit{repurposed}, but explicitly \\textit{designed} for V+L tasks, have the\npotential of improving performance on the target V+L tasks.",
    "descriptor": "",
    "authors": [
      "Gregor Geigle",
      "Chen Liu",
      "Jonas Pfeiffer",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06379"
  },
  {
    "id": "arXiv:2210.06380",
    "title": "Near-Optimal Multi-Agent Learning for Safe Coverage Control",
    "abstract": "In multi-agent coverage control problems, agents navigate their environment\nto reach locations that maximize the coverage of some density. In practice, the\ndensity is rarely known $\\textit{a priori}$, further complicating the original\nNP-hard problem. Moreover, in many applications, agents cannot visit arbitrary\nlocations due to $\\textit{a priori}$ unknown safety constraints. In this paper,\nwe aim to efficiently learn the density to approximately solve the coverage\nproblem while preserving the agents' safety. We first propose a conditionally\nlinear submodular coverage function that facilitates theoretical analysis.\nUtilizing this structure, we develop MacOpt, a novel algorithm that efficiently\ntrades off the exploration-exploitation dilemma due to partial observability,\nand show that it achieves sublinear regret. Next, we extend results on\nsingle-agent safe exploration to our multi-agent setting and propose SafeMac\nfor safe coverage and exploration. We analyze SafeMac and give first of its\nkind results: near optimal coverage in finite time while provably guaranteeing\nsafety. We extensively evaluate our algorithms on synthetic and real problems,\nincluding a bio-diversity monitoring task under safety constraints, where\nSafeMac outperforms competing methods.",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Manish Prajapat",
      "Matteo Turchetta",
      "Melanie N. Zeilinger",
      "Andreas Krause"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)",
      "Robotics (cs.RO)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2210.06380"
  },
  {
    "id": "arXiv:2210.06381",
    "title": "Good Intentions, Bad Inventions: How Employees Judge Pervasive  Technologies in the Workplace",
    "abstract": "Pervasive technologies combined with powerful AI have been recently\nintroduced to enhance work productivity. Yet, some of these technologies are\njudged to be invasive. To identify which ones, we should understand how\nemployees tend to judge these technologies. We considered 16 technologies that\ntrack productivity, and conducted a study in which 131 crowd-workers judged\nthese scenarios. We found that a technology was judged harshly depending on the\nfollowing three aspects of increasing importance. That is, whether the\ntechnology: 1) was currently supported by existing tools; 2) did not interfere\nwith work or was fit for purpose; and 3) did not cause any harm or did not\ninfringe on any individual rights. Ubicomp research currently focuses on how to\ndesign better technologies by making them more accurate, or by increasingly\nblending them into the background. It might be time to design better ubiquitous\ntechnologies by unpacking AI ethics as well.",
    "descriptor": "\nComments: 9 pages, 2 figures, 3 tables\n",
    "authors": [
      "Marios Constantinides",
      "Daniele Quercia"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2210.06381"
  },
  {
    "id": "arXiv:2210.06384",
    "title": "GMP*: Well-Tuned Global Magnitude Pruning Can Outperform Most  BERT-Pruning Methods",
    "abstract": "We revisit the performance of the classic gradual magnitude pruning (GMP)\nbaseline for large language models, focusing on the classic BERT benchmark on\nvarious popular tasks. Despite existing evidence in the literature that GMP\nperforms poorly, we show that a simple and general variant, which we call GMP*,\ncan match and sometimes outperform more complex state-of-the-art methods. Our\nresults provide a simple yet strong baseline for future work, highlight the\nimportance of parameter tuning for baselines, and even improve the performance\nof the state-of-the-art second-order pruning method in this setting.",
    "descriptor": "",
    "authors": [
      "Eldar Kurtic",
      "Dan Alistarh"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06384"
  },
  {
    "id": "arXiv:2210.06386",
    "title": "Multi-Level Firing with Spiking DS-ResNet: Enabling Better and Deeper  Directly-Trained Spiking Neural Networks",
    "abstract": "Spiking neural networks (SNNs) are bio-inspired neural networks with\nasynchronous discrete and sparse characteristics, which have increasingly\nmanifested their superiority in low energy consumption. Recent research is\ndevoted to utilizing spatio-temporal information to directly train SNNs by\nbackpropagation. However, the binary and non-differentiable properties of spike\nactivities force directly trained SNNs to suffer from serious gradient\nvanishing and network degradation, which greatly limits the performance of\ndirectly trained SNNs and prevents them from going deeper. In this paper, we\npropose a multi-level firing (MLF) method based on the existing spatio-temporal\nback propagation (STBP) method, and spiking dormant-suppressed residual network\n(spiking DS-ResNet). MLF enables more efficient gradient propagation and the\nincremental expression ability of the neurons. Spiking DS-ResNet can\nefficiently perform identity mapping of discrete spikes, as well as provide a\nmore suitable connection for gradient propagation in deep SNNs. With the\nproposed method, our model achieves superior performances on a non-neuromorphic\ndataset and two neuromorphic datasets with much fewer trainable parameters and\ndemonstrates the great ability to combat the gradient vanishing and degradation\nproblem in deep SNNs.",
    "descriptor": "",
    "authors": [
      "Lang Feng",
      "Qianhui Liu",
      "Huajin Tang",
      "De Ma",
      "Gang Pan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06386"
  },
  {
    "id": "arXiv:2210.06391",
    "title": "What Makes Graph Neural Networks Miscalibrated?",
    "abstract": "Given the importance of getting calibrated predictions and reliable\nuncertainty estimations, various post-hoc calibration methods have been\ndeveloped for neural networks on standard multi-class classification tasks.\nHowever, these methods are not well suited for calibrating graph neural\nnetworks (GNNs), which presents unique challenges such as accounting for the\ngraph structure and the graph-induced correlations between the nodes. In this\nwork, we conduct a systematic study on the calibration qualities of GNN node\npredictions. In particular, we identify five factors which influence the\ncalibration of GNNs: general under-confident tendency, diversity of nodewise\npredictive distributions, distance to training nodes, relative confidence\nlevel, and neighborhood similarity. Furthermore, based on the insights from\nthis study, we design a novel calibration method named Graph Attention\nTemperature Scaling (GATS), which is tailored for calibrating graph neural\nnetworks. GATS incorporates designs that address all the identified influential\nfactors and produces nodewise temperature scaling using an attention-based\narchitecture. GATS is accuracy-preserving, data-efficient, and expressive at\nthe same time. Our experiments empirically verify the effectiveness of GATS,\ndemonstrating that it can consistently achieve state-of-the-art calibration\nresults on various graph datasets for different GNN backbones.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Hans Hao-Hsun Hsu",
      "Yuesong Shen",
      "Christian Tomani",
      "Daniel Cremers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06391"
  },
  {
    "id": "arXiv:2210.06393",
    "title": "Application Scheduling with Multiplexed Sensing of Monitoring Points in  Multi-purpose IoT Wireless Sensor Networks",
    "abstract": "Wireless sensor networks (WSNs) have many applications and are an essential\npart of IoT systems. The primary functionality of a WSN is gathering data from\nspecific points that are covered with sensor nodes and transmitting the\ncollected data to remote units for further processing. In IoT use cases, a WSN\ninfrastructure may need to be shared by many applications, which requires\nscheduling those applications to time-share the node and network resources. In\nthis paper, we investigate the problem of application scheduling in WSN\ninfrastructures. We focus on the scenarios where applications request a set of\nmonitoring points to be sensed in the region a WSN spans and propose a\nshared-data approach utilizing multiplexed sensing of monitoring points\nrequested by multiple applications, which reduces sensing and communication\nload on the network. We also propose a genetic algorithm called GABAS, and\nthree greedy algorithms for scheduling applications onto a WSN infrastructure\nconsidering different criteria. We performed extensive simulation experiments\nto evaluate our algorithms and compare them to some standard scheduling\nmethods. The results show that our proposed methods perform much better than\nthe standard scheduling methods in terms of makespan, turnaround time, waiting\ntime, and successful execution rate metrics. We also observed that our genetic\nalgorithm is very effective in scheduling applications with respect to these\nmetrics.",
    "descriptor": "",
    "authors": [
      "Mustafa Can \u00c7avdar",
      "Ibrahim Korpeoglu",
      "\u00d6zg\u00fcr Ulusoy"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06393"
  },
  {
    "id": "arXiv:2210.06394",
    "title": "On Text Style Transfer via Style Masked Language Models",
    "abstract": "Text Style Transfer (TST) is performable through approaches such as latent\nspace disentanglement, cycle-consistency losses, prototype editing etc. The\nprototype editing approach, which is known to be quite successful in TST,\ninvolves two key phases a) Masking of source style-associated tokens and b)\nReconstruction of this source-style masked sentence conditioned with the target\nstyle. We follow a similar transduction method, in which we transpose the more\ndifficult direct source to target TST task to a simpler Style-Masked Language\nModel (SMLM) Task, wherein, similar to BERT \\cite{bert}, the goal of our model\nis now to reconstruct the source sentence from its style-masked version. We\narrive at the SMLM mechanism naturally by formulating prototype editing/\ntransduction methods in a probabilistic framework, where TST resolves into\nestimating a hypothetical parallel dataset from a partially observed parallel\ndataset, wherein each domain is assumed to have a common latent style-masked\nprior. To generate this style-masked prior, we use \"Explainable Attention\" as\nour choice of attribution for a more precise style-masking step and also\nintroduce a cost-effective and accurate \"Attribution-Surplus\" method of\ndetermining the position of masks from any arbitrary attribution model in O(1)\ntime. We empirically show that this non-generational approach well suites the\n\"content preserving\" criteria for a task like TST, even for a complex style\nlike Discourse Manipulation. Our model, the Style MLM, outperforms strong TST\nbaselines and is on par with state-of-the-art TST models, which use complex\narchitectures and orders of more parameters.",
    "descriptor": "",
    "authors": [
      "Sharan Narasimhan",
      "Pooja Shekar",
      "Suvodip Dey",
      "Maunendra Sankar Desarkar"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06394"
  },
  {
    "id": "arXiv:2210.06397",
    "title": "Star Anagram Detection and Classification",
    "abstract": "A star anagram is a rearrangement of the letters of one word to produce\nanother word where no letter retains its original neighbors. These maximally\nshuffled anagrams are rare, comprising only about 5.7% of anagrams in English.\nThey can also be depicted as unicursal polygons with varying forms, including\nthe eponymous stars. We develop automated methods for detecting stars among\nother anagrams and for classifying them based on their polygon's degree of both\nrotational and reflective symmetry. Next, we explore several properties of star\nanagrams including proofs for two results about the edge lengths of perfect,\ni.e., maximally symmetric, stars leveraging perhaps surprising connections to\nmodular arithmetic and the celebrated Chinese Remainder Theorem. Finally, we\nconduct an exhaustive search of English for star anagrams and provide numerical\nresults about their clustering into common shapes along with examples of\ngeometrically noteworthy stars.",
    "descriptor": "\nComments: 14 pages, 14 figures in main article. Appendix contains several thousand figures over 250+ pages. In preparation for submission to Computational Geometry\n",
    "authors": [
      "Jason Parker",
      "Dan Barker"
    ],
    "subjectives": [
      "Other Computer Science (cs.OH)"
    ],
    "url": "https://arxiv.org/abs/2210.06397"
  },
  {
    "id": "arXiv:2210.06399",
    "title": "DQLAP: Deep Q-Learning Recommender Algorithm with Update Policy for a  Real Steam Turbine System",
    "abstract": "In modern industrial systems, diagnosing faults in time and using the best\nmethods becomes more and more crucial. It is possible to fail a system or to\nwaste resources if faults are not detected or are detected late. Machine\nlearning and deep learning have proposed various methods for data-based fault\ndiagnosis, and we are looking for the most reliable and practical ones. This\npaper aims to develop a framework based on deep learning and reinforcement\nlearning for fault detection. We can increase accuracy, overcome data\nimbalance, and better predict future defects by updating the reinforcement\nlearning policy when new data is received. By implementing this method, we will\nsee an increase of $3\\%$ in all evaluation metrics, an improvement in\nprediction speed, and $3\\%$ - $4\\%$ in all evaluation metrics compared to\ntypical backpropagation multi-layer neural network prediction with similar\nparameters.",
    "descriptor": "",
    "authors": [
      "M.H. Modirrousta",
      "M. Aliyari Shoorehdeli",
      "M. Yari",
      "A. Ghahremani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06399"
  },
  {
    "id": "arXiv:2210.06401",
    "title": "Improving information retention in large scale online continual learning",
    "abstract": "Given a stream of data sampled from non-stationary distributions, online\ncontinual learning (OCL) aims to adapt efficiently to new data while retaining\nexisting knowledge. The typical approach to address information retention (the\nability to retain previous knowledge) is keeping a replay buffer of a fixed\nsize and computing gradients using a mixture of new data and the replay buffer.\nSurprisingly, the recent work (Cai et al., 2021) suggests that information\nretention remains a problem in large scale OCL even when the replay buffer is\nunlimited, i.e., the gradients are computed using all past data. This paper\nfocuses on this peculiarity to understand and address information retention. To\npinpoint the source of this problem, we theoretically show that, given limited\ncomputation budgets at each time step, even without strict storage limit,\nnaively applying SGD with constant or constantly decreasing learning rates\nfails to optimize information retention in the long term. We propose using a\nmoving average family of methods to improve optimization for non-stationary\nobjectives. Specifically, we design an adaptive moving average (AMA) optimizer\nand a moving-average-based learning rate schedule (MALR). We demonstrate the\neffectiveness of AMA+MALR on large-scale benchmarks, including Continual\nLocalization (CLOC), Google Landmarks, and ImageNet. Code will be released upon\npublication.",
    "descriptor": "",
    "authors": [
      "Zhipeng Cai",
      "Vladlen Koltun",
      "Ozan Sener"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06401"
  },
  {
    "id": "arXiv:2210.06402",
    "title": "Relaxed Kacanov scheme for the p-Laplacian with large p",
    "abstract": "We introduce a globally convergent relaxed Kacanov scheme for the computation\nof the discrete minimizer to the $p$-Laplace problem with $2 \\leq p < \\infty$.\nThe iterative scheme is easy to implement since each iterate results only from\nthe solve of a weighted, linear Poisson problem. It neither requires an\nadditional line search nor involves unknown constants for the step length. The\nrate of convergence is independent of the underlying mesh.",
    "descriptor": "",
    "authors": [
      "Anna Kh. Balci",
      "Lars Diening",
      "Johannes Storn"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06402"
  },
  {
    "id": "arXiv:2210.06404",
    "title": "Graph Neural Network Surrogate for seismic reliability analysis of  highway bridge system",
    "abstract": "Rapid reliability assessment of transportation networks can enhance\npreparedness, risk mitigation and response management procedures related to\nthese systems. Network reliability approaches commonly consider network-level\nresponses, and due to computational cost do not consider the more detailed\nnode-level responses. In this paper, we propose a rapid seismic reliability\nassessment approach for bridge networks based on graph neural networks, where\nnode-level connectivities, between points of interest and other nodes, are\nquantified under probabilistic bridge conditions and earthquake events. Via\nnumerical experiments on transportation systems in California, we demonstrate\nthe accuracy, computational efficiency and robustness of the proposed approach\ncompared to the Monte Carlo approach.",
    "descriptor": "",
    "authors": [
      "Tong Liu",
      "Hadi Meidani"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06404"
  },
  {
    "id": "arXiv:2210.06405",
    "title": "Transformer-based Text Classification on Unified Bangla Multi-class  Emotion Corpus",
    "abstract": "Because of its importance in studying people's thoughts on various Web 2.0\nservices, emotion classification (EC) is an important undertaking. Existing\nresearch, on the other hand, is mostly focused on the English language, with\nlittle work on low-resource languages. Though sentiment analysis, particularly\nthe EC in English, has received a lot of attention in recent years, little\nstudy has been done in the context of Bangla, one of the world's most widely\nspoken languages. We propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts in this research. We provide a Bangla\nemotion classifier for six classes (anger, disgust, fear, joy, sadness, and\nsurprise) from Bangla words, using transformer-based models which exhibit\nphenomenal results in recent days, especially for high resource languages. The\n\"Unified Bangla Multi-class Emotion Corpus (UBMEC)\" is used to assess the\nperformance of our models. UBMEC was created by combining two previously\nreleased manually labeled datasets of Bangla comments on 6-emotion classes with\nfresh manually tagged Bangla comments created by us. The corpus dataset and\ncode we used in this work is publicly available.",
    "descriptor": "\nComments: this https URL\n",
    "authors": [
      "Md Sakib Ullah Sourav",
      "Huidong Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06405"
  },
  {
    "id": "arXiv:2210.06407",
    "title": "Interactive Language: Talking to Robots in Real Time",
    "abstract": "We present a framework for building interactive, real-time, natural\nlanguage-instructable robots in the real world, and we open source related\nassets (dataset, environment, benchmark, and policies). Trained with behavioral\ncloning on a dataset of hundreds of thousands of language-annotated\ntrajectories, a produced policy can proficiently execute an order of magnitude\nmore commands than previous works: specifically we estimate a 93.5% success\nrate on a set of 87,000 unique natural language strings specifying raw\nend-to-end visuo-linguo-motor skills in the real world. We find that the same\npolicy is capable of being guided by a human via real-time language to address\na wide range of precise long-horizon rearrangement goals, e.g. \"make a smiley\nface out of blocks\". The dataset we release comprises nearly 600,000\nlanguage-labeled trajectories, an order of magnitude larger than prior\navailable datasets. We hope the demonstrated results and associated assets\nenable further advancement of helpful, capable, natural-language-interactable\nrobots. See videos at https://interactive-language.github.io.",
    "descriptor": "",
    "authors": [
      "Corey Lynch",
      "Ayzaan Wahid",
      "Jonathan Tompson",
      "Tianli Ding",
      "James Betker",
      "Robert Baruch",
      "Travis Armstrong",
      "Pete Florence"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06407"
  },
  {
    "id": "arXiv:2210.06408",
    "title": "PriMeSRL-Eval: A Practical Quality Metric for Semantic Role Labeling  Systems Evaluation",
    "abstract": "Semantic role labeling (SRL) identifies the predicate-argument structure in a\nsentence. This task is usually accomplished in four steps: predicate\nidentification, predicate sense disambiguation, argument identification, and\nargument classification. Errors introduced at one step propagate to later\nsteps. Unfortunately, the existing SRL evaluation scripts do not consider the\nfull effect of this error propagation aspect. They either evaluate arguments\nindependent of predicate sense (CoNLL09) or do not evaluate predicate sense at\nall (CoNLL05), yielding an inaccurate SRL model performance on the argument\nclassification task. In this paper, we address key practical issues with\nexisting evaluation scripts and propose a more strict SRL evaluation metric\nPriMeSRL. We observe that by employing PriMeSRL, the quality evaluation of all\nSoTA SRL models drops significantly, and their relative rankings also change.\nWe also show that PriMeSRLsuccessfully penalizes actual failures in SoTA SRL\nmodels.",
    "descriptor": "",
    "authors": [
      "Ishan Jindal",
      "Alexandre Rademaker",
      "Khoi-Nguyen Tran",
      "Huaiyu Zhu",
      "Hiroshi Kanayama",
      "Marina Danilevsky",
      "Yunyao Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06408"
  },
  {
    "id": "arXiv:2210.06409",
    "title": "A Unified Framework with Meta-dropout for Few-shot Learning",
    "abstract": "Conventional training of deep neural networks usually requires a substantial\namount of data with expensive human annotations. In this paper, we utilize the\nidea of meta-learning to explain two very different streams of few-shot\nlearning, i.e., the episodic meta-learning-based and pre-train finetune-based\nfew-shot learning, and form a unified meta-learning framework. In order to\nimprove the generalization power of our framework, we propose a simple yet\neffective strategy named meta-dropout, which is applied to the transferable\nknowledge generalized from base categories to novel categories. The proposed\nstrategy can effectively prevent neural units from co-adapting excessively in\nthe meta-training stage. Extensive experiments on the few-shot object detection\nand few-shot image classification datasets, i.e., Pascal VOC, MS COCO, CUB, and\nmini-ImageNet, validate the effectiveness of our method.",
    "descriptor": "",
    "authors": [
      "Shaobo Lin",
      "Xingyu Zeng",
      "Rui Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06409"
  },
  {
    "id": "arXiv:2210.06410",
    "title": "Pinning control of networks: dimensionality reduction through  simultaneous block-diagonalization of matrices",
    "abstract": "In this paper, we study the network pinning control problem in the presence\nof two different types of coupling: (i) node-to-node coupling among the network\nnodes and (ii) input-to-node coupling from the source node to the `pinned\nnodes'. Previous work has mainly focused on the case that (i) and (ii) are of\nthe same type. We decouple the stability analysis of the target synchronous\nsolution into subproblems of the lowest dimension by using the techniques of\nsimultaneous block diagonalization (SBD) of matrices. Interestingly, we obtain\ntwo different types of blocks, driven and undriven. The overall dimension of\nthe driven blocks is equal to the dimension of an appropriately defined\ncontrollable subspace, while all the remaining undriven blocks are scalar. Our\nmain result is a decomposition of the stability problem into four independent\nsets of equations, which we call quotient controllable, quotient\nuncontrollable, redundant controllable, and redundant uncontrollable. Our\nanalysis shows that the number and location of the pinned nodes affect the\nnumber and the dimension of each set of equations. We also observe that in a\nlarge variety of complex networks, stability of the target synchronous solution\nis de facto only determined by a single quotient controllable block.",
    "descriptor": "\nComments: Accepted for Publication in Chaos: An Interdisciplinary Journal of Nonlinear Science\n",
    "authors": [
      "Shirin Panahi",
      "Matteo Lodi",
      "Marco Storace",
      "Francesco Sorrentino"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06410"
  },
  {
    "id": "arXiv:2210.06413",
    "title": "EleutherAI: Going Beyond \"Open Science\" to \"Science in the Open\"",
    "abstract": "Over the past two years, EleutherAI has established itself as a radically\nnovel initiative aimed at both promoting open-source research and conducting\nresearch in a transparent, openly accessible and collaborative manner.\nEleutherAI's approach to research goes beyond transparency: by doing research\nentirely in public, anyone in the world can observe and contribute at every\nstage. Our work has been received positively and has resulted in several\nhigh-impact projects in Natural Language Processing and other fields. In this\npaper, we describe our experience doing public-facing machine learning\nresearch, the benefits we believe this approach brings, and the pitfalls we\nhave encountered.",
    "descriptor": "",
    "authors": [
      "Jason Phang",
      "Herbie Bradley",
      "Leo Gao",
      "Louis Castricato",
      "Stella Biderman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06413"
  },
  {
    "id": "arXiv:2210.06417",
    "title": "BiaScope: Visual Unfairness Diagnosis for Graph Embeddings",
    "abstract": "The issue of bias (i.e., systematic unfairness) in machine learning models\nhas recently attracted the attention of both researchers and practitioners. For\nthe graph mining community in particular, an important goal toward algorithmic\nfairness is to detect and mitigate bias incorporated into graph embeddings\nsince they are commonly used in human-centered applications, e.g., social-media\nrecommendations. However, simple analytical methods for detecting bias\ntypically involve aggregate statistics which do not reveal the sources of\nunfairness. Instead, visual methods can provide a holistic fairness\ncharacterization of graph embeddings and help uncover the causes of observed\nbias. In this work, we present BiaScope, an interactive visualization tool that\nsupports end-to-end visual unfairness diagnosis for graph embeddings. The tool\nis the product of a design study in collaboration with domain experts. It\nallows the user to (i) visually compare two embeddings with respect to\nfairness, (ii) locate nodes or graph communities that are unfairly embedded,\nand (iii) understand the source of bias by interactively linking the relevant\nembedding subspace with the corresponding graph topology. Experts' feedback\nconfirms that our tool is effective at detecting and diagnosing unfairness.\nThus, we envision our tool both as a companion for researchers in designing\ntheir algorithms as well as a guide for practitioners who use off-the-shelf\ngraph embeddings.",
    "descriptor": "\nComments: Accepted to VDS at IEEE VIS 2022\n",
    "authors": [
      "Agapi Rissaki",
      "Bruno Scarone",
      "David Liu",
      "Aditeya Pandey",
      "Brennan Klein",
      "Tina Eliassi-Rad",
      "Michelle A. Borkin"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2210.06417"
  },
  {
    "id": "arXiv:2210.06418",
    "title": "Relational Graph Convolutional Neural Networks for Multihop Reasoning: A  Comparative Study",
    "abstract": "Multihop Question Answering is a complex Natural Language Processing task\nthat requires multiple steps of reasoning to find the correct answer to a given\nquestion. Previous research has explored the use of models based on Graph\nNeural Networks for tackling this task. Various architectures have been\nproposed, including Relational Graph Convolutional Networks (RGCN). For these\nmany node types and relations between them have been introduced, such as simple\nentity co-occurrences, modelling coreferences, or \"reasoning paths\" from\nquestions to answers via intermediary entities. Nevertheless, a thoughtful\nanalysis on which relations, node types, embeddings and architecture are the\nmost beneficial for this task is still missing. In this paper we explore a\nnumber of RGCN-based Multihop QA models, graph relations, and node embeddings,\nand empirically explore the influence of each on Multihop QA performance on the\nWikiHop dataset.",
    "descriptor": "\nComments: 8 pages + 2 pages references, 3 figures, 3 tables\n",
    "authors": [
      "Ieva Stali\u016bnait\u0117",
      "Philip John Gorinski",
      "Ignacio Iacobacci"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06418"
  },
  {
    "id": "arXiv:2210.06422",
    "title": "A New Family of Generalization Bounds Using Samplewise Evaluated CMI",
    "abstract": "We present a new family of information-theoretic generalization bounds, in\nwhich the training loss and the population loss are compared through a jointly\nconvex function. This function is upper-bounded in terms of the disintegrated,\nsamplewise, evaluated conditional mutual information (CMI), an information\nmeasure that depends on the losses incurred by the selected hypothesis, rather\nthan on the hypothesis itself, as is common in probably approximately correct\n(PAC)-Bayesian results. We demonstrate the generality of this framework by\nrecovering and extending previously known information-theoretic bounds.\nFurthermore, using the evaluated CMI, we derive a samplewise, average version\nof Seeger's PAC-Bayesian bound, where the convex function is the binary KL\ndivergence. In some scenarios, this novel bound results in a tighter\ncharacterization of the population loss of deep neural networks than previous\nbounds. Finally, we derive high-probability versions of some of these average\nbounds. We demonstrate the unifying nature of the evaluated CMI bounds by using\nthem to recover average and high-probability generalization bounds for\nmulticlass classification with finite Natarajan dimension.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Fredrik Hellstr\u00f6m",
      "Giuseppe Durisi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06422"
  },
  {
    "id": "arXiv:2210.06423",
    "title": "Foundation Transformers",
    "abstract": "A big convergence of model architectures across language, vision, speech, and\nmultimodal is emerging. However, under the same name \"Transformers\", the above\nareas use different implementations for better performance, e.g.,\nPost-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We\ncall for the development of Foundation Transformer for true general-purpose\nmodeling, which serves as a go-to architecture for various tasks and modalities\nwith guaranteed training stability. In this work, we introduce a Transformer\nvariant, named Magneto, to fulfill the goal. Specifically, we propose\nSub-LayerNorm for good expressivity, and the initialization strategy\ntheoretically derived from DeepNet for stable scaling up. Extensive experiments\ndemonstrate its superior performance and better stability than the de facto\nTransformer variants designed for various applications, including language\nmodeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e.,\nBEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).",
    "descriptor": "\nComments: Work in progress\n",
    "authors": [
      "Hongyu Wang",
      "Shuming Ma",
      "Shaohan Huang",
      "Li Dong",
      "Wenhui Wang",
      "Zhiliang Peng",
      "Yu Wu",
      "Payal Bajaj",
      "Saksham Singhal",
      "Alon Benhaim",
      "Barun Patra",
      "Zhun Liu",
      "Vishrav Chaudhary",
      "Xia Song",
      "Furu Wei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06423"
  },
  {
    "id": "arXiv:2210.06425",
    "title": "MiniALBERT: Model Distillation via Parameter-Efficient Recursive  Transformers",
    "abstract": "Pre-trained Language Models (LMs) have become an integral part of Natural\nLanguage Processing (NLP) in recent years, due to their superior performance in\ndownstream applications. In spite of this resounding success, the usability of\nLMs is constrained by computational and time complexity, along with their\nincreasing size; an issue that has been referred to as `overparameterisation'.\nDifferent strategies have been proposed in the literature to alleviate these\nproblems, with the aim to create effective compact models that nearly match the\nperformance of their bloated counterparts with negligible performance losses.\nOne of the most popular techniques in this area of research is model\ndistillation. Another potent but underutilised technique is cross-layer\nparameter sharing. In this work, we combine these two strategies and present\nMiniALBERT, a technique for converting the knowledge of fully parameterised LMs\n(such as BERT) into a compact recursive student. In addition, we investigate\nthe application of bottleneck adapters for layer-wise adaptation of our\nrecursive student, and also explore the efficacy of adapter tuning for\nfine-tuning of compact models. We test our proposed models on a number of\ngeneral and biomedical NLP tasks to demonstrate their viability and compare\nthem with the state-of-the-art and other existing compact models. All the codes\nused in the experiments are available at\nhttps://github.com/nlpie-research/MiniALBERT. Our pre-trained compact models\ncan be accessed from https://huggingface.co/nlpie.",
    "descriptor": "",
    "authors": [
      "Mohammadmahdi Nouriborji",
      "Omid Rohanian",
      "Samaneh Kouchaki",
      "David A. Clifton"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06425"
  },
  {
    "id": "arXiv:2210.06427",
    "title": "Peer-to-Peer Energy Trading meets IOTA: Toward a Scalable, Low-Cost, and  Efficient Trading System",
    "abstract": "Peer-to-Peer (P2P) energy trading provides various benefits over conventional\nwholesale energy markets and makes renewable energy more accessible. This paper\nproposes a novel multi-layer P2P energy trading system for microgrids based on\nIOTA 2.0, which is a distributed ledger technology (DLT) primarily designed for\nInternet-of-Things (IoT) applications. The proposed energy trading system,\nwhich is a manifestation of a cyber-physical system (CPS), exploits the\nbenefits brought by IOTA's unique ledger structure as well as the recently\nintroduced IOTA smart contract protocol (ISCP). Further, it implements a\nuniform double-auction market mechanism and a hierarchical routing structure\nfor interconnected microgrids. Performance evaluation demonstrates key benefits\nover wholesale markets as well as speed, energy efficiency and cost benefits\nover conventional blockchain-based P2P energy trading systems.",
    "descriptor": "\nComments: Accepted for publication in BlockCPS 2022 at 15th IEEE/ACM International Conference on Utility and Cloud Computing\n",
    "authors": [
      "Conor Mullaney",
      "Adnan Aijaz",
      "Nathan Sealey",
      "Ben Holden"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2210.06427"
  },
  {
    "id": "arXiv:2210.06428",
    "title": "Trap and Replace: Defending Backdoor Attacks by Trapping Them into an  Easy-to-Replace Subnetwork",
    "abstract": "Deep neural networks (DNNs) are vulnerable to backdoor attacks. Previous\nworks have shown it extremely challenging to unlearn the undesired backdoor\nbehavior from the network, since the entire network can be affected by the\nbackdoor samples. In this paper, we propose a brand-new backdoor defense\nstrategy, which makes it much easier to remove the harmful influence of\nbackdoor samples from the model. Our defense strategy, \\emph{Trap and Replace},\nconsists of two stages. In the first stage, we bait and trap the backdoors in a\nsmall and easy-to-replace subnetwork. Specifically, we add an auxiliary image\nreconstruction head on top of the stem network shared with a light-weighted\nclassification head. The intuition is that the auxiliary image reconstruction\ntask encourages the stem network to keep sufficient low-level visual features\nthat are hard to learn but semantically correct, instead of overfitting to the\neasy-to-learn but semantically incorrect backdoor correlations. As a result,\nwhen trained on backdoored datasets, the backdoors are easily baited towards\nthe unprotected classification head, since it is much more vulnerable than the\nshared stem, leaving the stem network hardly poisoned. In the second stage, we\nreplace the poisoned light-weighted classification head with an untainted one,\nby re-training it from scratch only on a small holdout dataset with clean\nsamples, while fixing the stem network. As a result, both the stem and the\nclassification head in the final network are hardly affected by backdoor\ntraining samples. We evaluate our method against ten different backdoor\nattacks. Our method outperforms previous state-of-the-art methods by up to\n$20.57\\%$, $9.80\\%$, and $13.72\\%$ attack success rate and on-average $3.14\\%$,\n$1.80\\%$, and $1.21\\%$ clean classification accuracy on CIFAR10, GTSRB, and\nImageNet-12, respectively. Code is available online.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. Code is available at this https URL\n",
    "authors": [
      "Haotao Wang",
      "Junyuan Hong",
      "Aston Zhang",
      "Jiayu Zhou",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06428"
  },
  {
    "id": "arXiv:2210.06431",
    "title": "BLAB Reporter: Automated journalism covering the Blue Amazon",
    "abstract": "This demo paper introduces the BLAB Reporter, a robot-journalist covering the\nBrazilian Blue Amazon. The Reporter is based on a pipeline architecture for\nNatural Language Generation; it offers daily reports, news summaries and\ncurious facts in Brazilian Portuguese. By collecting, storing and analysing\nstructured data from publicly available sources, the robot-journalist uses\ndomain knowledge to generate and publish texts in Twitter. Code and corpus are\npublicly available",
    "descriptor": "\nComments: Accepted at the 15th International Natural Language Generation Conference (INLG 2022)\n",
    "authors": [
      "Yan V. Sym",
      "Jo\u00e3o Gabriel M. Campos",
      "Fabio G. Cozman"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06431"
  },
  {
    "id": "arXiv:2210.06432",
    "title": "InfoCSE: Information-aggregated Contrastive Learning of Sentence  Embeddings",
    "abstract": "Contrastive learning has been extensively studied in sentence embedding\nlearning, which assumes that the embeddings of different views of the same\nsentence are closer. The constraint brought by this assumption is weak, and a\ngood sentence representation should also be able to reconstruct the original\nsentence fragments. Therefore, this paper proposes an information-aggregated\ncontrastive learning framework for learning unsupervised sentence embeddings,\ntermed InfoCSE. InfoCSE forces the representation of [CLS] positions to\naggregate denser sentence information by introducing an additional Masked\nlanguage model task and a well-designed network. We evaluate the proposed\nInfoCSE on several benchmark datasets w.r.t the semantic text similarity (STS)\ntask. Experimental results show that InfoCSE outperforms SimCSE by an average\nSpearman correlation of 2.60% on BERT-base, and 1.77% on BERT-large, achieving\nstate-of-the-art results among unsupervised sentence representation learning\nmethods. Our code are available at https://github.com/caskcsg/sentemb/info",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Xing Wu",
      "Chaochen Gao",
      "Zijia Lin",
      "Jizhong Han",
      "Zhongyuan Wang",
      "Songlin Hu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06432"
  },
  {
    "id": "arXiv:2210.06433",
    "title": "Self-supervised video pretraining yields strong image representations",
    "abstract": "Videos contain far more information than still images and hold the potential\nfor learning rich representations of the visual world. Yet, pretraining on\nimage datasets has remained the dominant paradigm for learning representations\nthat capture spatial information, and previous attempts at video pretraining\nhave fallen short on image understanding tasks. In this work we revisit\nself-supervised learning of image representations from the dynamic evolution of\nvideo frames. To that end, we propose a dataset curation procedure that\naddresses the domain mismatch between video and image datasets, and develop a\ncontrastive learning framework which handles the complex transformations\npresent in natural videos. This simple paradigm for distilling knowledge from\nvideos to image representations, called VITO, performs surprisingly well on a\nvariety of image-based transfer learning tasks. For the first time, our\nvideo-pretrained model closes the gap with ImageNet pretraining on semantic\nsegmentation on PASCAL and ADE20K and object detection on COCO and LVIS,\nsuggesting that video-pretraining could become the new default for learning\nimage representations.",
    "descriptor": "\nComments: Technical report\n",
    "authors": [
      "Nikhil Parthasarathy",
      "S. M. Ali Eslami",
      "Jo\u00e3o Carreira",
      "Olivier J. H\u00e9naff"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06433"
  },
  {
    "id": "arXiv:2210.06434",
    "title": "FedProp: Cross-client Label Propagation for Federated Semi-supervised  Learning",
    "abstract": "Federated learning (FL) allows multiple clients to jointly train a machine\nlearning model in such a way that no client has to share their data with any\nother participating party. In the supervised setting, where all client data is\nfully labeled, FL has been widely adopted for learning tasks that require data\nprivacy. However, it is an ongoing research question how to best perform\nfederated learning in a semi-supervised setting, where the clients possess data\nthat is only partially labeled or even completely unlabeled. In this work, we\npropose a new method, FedProp, that follows a manifold-based approach to\nsemi-supervised learning (SSL). It estimates the data manifold jointly from the\ndata of multiple clients and computes pseudo-labels using cross-client label\npropagation. To avoid that clients have to share their data with anyone,\nFedProp employs two cryptographically secure yet highly efficient protocols:\nsecure Hamming distance computation and secure summation. Experiments on three\nstandard benchmarks show that FedProp achieves higher classification accuracy\nthan previous federated SSL methods. Furthermore, as a pseudolabel-based\ntechnique, FedProp is complementary to other federated SSL approaches, in\nparticular consistency-based ones. We demonstrate experimentally that further\naccuracy gains are possible by combining both.",
    "descriptor": "",
    "authors": [
      "Jonathan Scott",
      "Michelle Yeo",
      "Christoph H. Lampert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06434"
  },
  {
    "id": "arXiv:2210.06436",
    "title": "Deep Combinatorial Aggregation",
    "abstract": "Neural networks are known to produce poor uncertainty estimations, and a\nvariety of approaches have been proposed to remedy this issue. This includes\ndeep ensemble, a simple and effective method that achieves state-of-the-art\nresults for uncertainty-aware learning tasks. In this work, we explore a\ncombinatorial generalization of deep ensemble called deep combinatorial\naggregation (DCA). DCA creates multiple instances of network components and\naggregates their combinations to produce diversified model proposals and\npredictions. DCA components can be defined at different levels of granularity.\nAnd we discovered that coarse-grain DCAs can outperform deep ensemble for\nuncertainty-aware learning both in terms of predictive performance and\nuncertainty estimation. For fine-grain DCAs, we discover that an average\nparameterization approach named deep combinatorial weight averaging (DCWA) can\nimprove the baseline training. It is on par with stochastic weight averaging\n(SWA) but does not require any custom training schedule or adaptation of\nBatchNorm layers. Furthermore, we propose a consistency enforcing loss that\nhelps the training of DCWA and modelwise DCA. We experiment on in-domain,\ndistributional shift, and out-of-distribution image classification tasks, and\nempirically confirm the effectiveness of DCWA and DCA approaches.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Yuesong Shen",
      "Daniel Cremers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06436"
  },
  {
    "id": "arXiv:2210.06437",
    "title": "Distributed, combined CPU and GPU profiling within HPX using APEX",
    "abstract": "Benchmarking and comparing performance of a scientific simulation across\nhardware platforms is a complex task. When the simulation in question is\nconstructed with an asynchronous, many-task (AMT) runtime offloading work to\nGPUs, the task becomes even more complex. In this paper, we discuss the use of\na uniquely suited performance measurement library, APEX, to capture the\nperformance behavior of a simulation built on HPX, a highly scalable,\ndistributed AMT runtime. We examine the performance of the astrophysics\nsimulation carried-out by Octo-Tiger on two different supercomputing\narchitectures. We analyze the results of scaling and measurement overheads. In\naddition, we look in-depth at two similarly configured executions on the two\nsystems to study how architectural differences affect performance and identify\nopportunities for optimization. As one such opportunity, we optimize the\ncommunication for the hydro solver and investigated its performance impact.",
    "descriptor": "",
    "authors": [
      "Patrick Diehl",
      "Gregor Daiss",
      "Kevin Huck",
      "Dominic Marcello",
      "Sagiv Shiber",
      "Hartmut Kaiser",
      "Juhan Frank",
      "Geoffrey C. Clayton",
      "Dirk Pflueger"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06437"
  },
  {
    "id": "arXiv:2210.06438",
    "title": "From Task-Based GPU Work Aggregation to Stellar Mergers: Turning  Fine-Grained CPU Tasks into Portable GPU Kernels",
    "abstract": "Meeting both scalability and performance portability requirements is a\nchallenge for any HPC application, especially for adaptively refined ones. In\nOcto-Tiger, an astrophysics application for the simulation of stellar mergers,\nwe approach this with existing solutions: We employ HPX to obtain fine-grained\ntasks to easily distribute work and finely overlap communication and\ncomputation. For the computations themselves, we use Kokkos to turn these tasks\ninto compute kernels capable of running on hardware ranging from a few CPU\ncores to powerful accelerators. There is a missing link, however: while the\nfine-grained parallelism exposed by HPX is useful for scalability, it can\nhinder GPU performance when the tasks become too small to saturate the device,\ncausing low resource utilization. To bridge this gap, we investigate multiple\ndifferent GPU work aggregation strategies within Octo-Tiger, adding one new\nstrategy, and evaluate the node-level performance impact on recent AMD and\nNVIDIA GPUs, achieving noticeable speedups.",
    "descriptor": "",
    "authors": [
      "Gregor Dai\u00df",
      "Patrick Diehl",
      "Dominic Marcello",
      "Alireza Kheirkhahan",
      "Hartmut Kaiser",
      "Dirk Pfl\u00fcger"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06438"
  },
  {
    "id": "arXiv:2210.06439",
    "title": "From Merging Frameworks to Merging Stars: Experiences using HPX, Kokkos  and SIMD Types",
    "abstract": "Octo-Tiger, a large-scale 3D AMR code for the merger of stars, uses a\ncombination of HPX, Kokkos and explicit SIMD types, aiming to achieve\nperformance-portability for a broad range of heterogeneous hardware. However,\non A64FX CPUs, we encountered several missing pieces, hindering performance by\ncausing problems with the SIMD vectorization. Therefore, we add\nstd::experimental::simd as an option to use in Octo-Tiger's Kokkos kernels\nalongside Kokkos SIMD, and further add a new SVE (Scalable Vector Extensions)\nSIMD backend. Additionally, we amend missing SIMD implementations in the Kokkos\nkernels within Octo-Tiger's hydro solver. We test our changes by running\nOcto-Tiger on three different CPUs: An A64FX, an Intel Icelake and an AMD EPYC\nCPU, evaluating SIMD speedup and node-level performance. We get a good SIMD\nspeedup on the A64FX CPU, as well as noticeable speedups on the other two CPU\nplatforms. However, we also experience a scaling issue on the EPYC CPU.",
    "descriptor": "",
    "authors": [
      "Gregor Dai\u00df",
      "Srinivas Yadav Singanaboina",
      "Patrick Diehl",
      "Hartmut Kaiser",
      "Dirk Pfl\u00fcger"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2210.06439"
  },
  {
    "id": "arXiv:2210.06440",
    "title": "The Devil is in the Details: On Models and Training Regimes for Few-Shot  Intent Classification",
    "abstract": "Few-shot Intent Classification (FSIC) is one of the key challenges in modular\ntask-oriented dialog systems. While advanced FSIC methods are similar in using\npretrained language models to encode texts and nearest neighbour-based\ninference for classification, these methods differ in details. They start from\ndifferent pretrained text encoders, use different encoding architectures with\nvarying similarity functions, and adopt different training regimes. Coupling\nthese mostly independent design decisions and the lack of accompanying ablation\nstudies are big obstacle to identify the factors that drive the reported FSIC\nperformance. We study these details across three key dimensions: (1) Encoding\narchitectures: Cross-Encoder vs Bi-Encoders; (2) Similarity function:\nParameterized (i.e., trainable) functions vs non-parameterized function; (3)\nTraining regimes: Episodic meta-learning vs the straightforward (i.e.,\nnon-episodic) training. Our experimental results on seven FSIC benchmarks\nreveal three important findings. First, the unexplored combination of the\ncross-encoder architecture (with parameterized similarity scoring function) and\nepisodic meta-learning consistently yields the best FSIC performance. Second,\nEpisodic training yields a more robust FSIC classifier than non-episodic one.\nThird, in meta-learning methods, splitting an episode to support and query sets\nis not a must. Our findings paves the way for conducting state-of-the-art\nresearch in FSIC and more importantly raise the community's attention to\ndetails of FSIC methods. We release our code and data publicly.",
    "descriptor": "",
    "authors": [
      "Mohsen Mesgar",
      "Thy Thy Tran",
      "Goran Glavas",
      "Iryna Gurevych"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06440"
  },
  {
    "id": "arXiv:2210.06441",
    "title": "How Much Data Are Augmentations Worth? An Investigation into Scaling  Laws, Invariance, and Implicit Regularization",
    "abstract": "Despite the clear performance benefits of data augmentations, little is known\nabout why they are so effective. In this paper, we disentangle several key\nmechanisms through which data augmentations operate. Establishing an exchange\nrate between augmented and additional real data, we find that in\nout-of-distribution testing scenarios, augmentations which yield samples that\nare diverse, but inconsistent with the data distribution can be even more\nvaluable than additional training data. Moreover, we find that data\naugmentations which encourage invariances can be more valuable than invariance\nalone, especially on small and medium sized training sets. Following this\nobservation, we show that augmentations induce additional stochasticity during\ntraining, effectively flattening the loss landscape.",
    "descriptor": "\nComments: 27 pages, 20 figures. Code at this https URL\n",
    "authors": [
      "Jonas Geiping",
      "Micah Goldblum",
      "Gowthami Somepalli",
      "Ravid Shwartz-Ziv",
      "Tom Goldstein",
      "Andrew Gordon Wilson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06441"
  },
  {
    "id": "arXiv:2210.06442",
    "title": "Can Pretrained Language Models (Yet) Reason Deductively?",
    "abstract": "Acquiring factual knowledge with Pretrained Language Models (PLMs) has\nattracted increasing attention, showing promising performance in many\nknowledge-intensive tasks. Their good performance has led the community to\nbelieve that the models do possess a modicum of reasoning competence rather\nthan merely memorising the knowledge. In this paper, we conduct a comprehensive\nevaluation of the learnable deductive (also known as explicit) reasoning\ncapability of PLMs. Through a series of controlled experiments, we posit two\nmain findings. (i) PLMs inadequately generalise learned logic rules and perform\ninconsistently against simple adversarial surface form edits. (ii) While the\ndeductive reasoning fine-tuning of PLMs does improve their performance on\nreasoning over unseen knowledge facts, it results in catastrophically\nforgetting the previously learnt knowledge. Our main results suggest that PLMs\ncannot yet perform reliable deductive reasoning, demonstrating the importance\nof controlled examinations and probing of PLMs' reasoning abilities; we reach\nbeyond (misleading) task performance, revealing that PLMs are still far from\nhuman-level reasoning capabilities, even for simple deductive tasks.",
    "descriptor": "",
    "authors": [
      "Zhangdie Yuan",
      "Songbo Hu",
      "Ivan Vuli\u0107",
      "Anna Korhonen",
      "Zaiqiao Meng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06442"
  },
  {
    "id": "arXiv:2210.06443",
    "title": "On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning",
    "abstract": "Rehearsal approaches enjoy immense popularity with Continual Learning (CL)\npractitioners. These methods collect samples from previously encountered data\ndistributions in a small memory buffer; subsequently, they repeatedly optimize\non the latter to prevent catastrophic forgetting. This work draws attention to\na hidden pitfall of this widespread practice: repeated optimization on a small\npool of data inevitably leads to tight and unstable decision boundaries, which\nare a major hindrance to generalization. To address this issue, we propose\nLipschitz-DrivEn Rehearsal (LiDER), a surrogate objective that induces\nsmoothness in the backbone network by constraining its layer-wise Lipschitz\nconstants w.r.t.\\ replay examples. By means of extensive experiments, we show\nthat applying LiDER delivers a stable performance gain to several\nstate-of-the-art rehearsal CL methods across multiple datasets, both in the\npresence and absence of pre-training. Through additional ablative experiments,\nwe highlight peculiar aspects of buffer overfitting in CL and better\ncharacterize the effect produced by LiDER. Code is available at\nhttps://github.com/aimagelab/LiDER",
    "descriptor": "\nComments: 22 pages, 8 figures. Accepted at the thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), New Orleans, US\n",
    "authors": [
      "Lorenzo Bonicelli",
      "Matteo Boschini",
      "Angelo Porrello",
      "Concetto Spampinato",
      "Simone Calderara"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06443"
  },
  {
    "id": "arXiv:2210.06444",
    "title": "Frustratingly Simple Entity Tracking with Effective Use of Multi-Task  Learning Models",
    "abstract": "We present SET, a frustratingly Simple-yet-effective approach for Entity\nTracking in procedural text. Compared with state-of-the-art entity tracking\nmodels that require domain-specific pre-training, SET simply fine-tunes\noff-the-shelf T5 with customized formats and gets comparable or even better\nperformance on multiple datasets. Concretely, SET tackles the state and\nlocation prediction in entity tracking independently and formulates them as\nmulti-choice and extractive QA problems, respectively. Through a series of\ncareful analyses, we show that T5's supervised multi-task learning plays an\nimportant role in the success of SET. In addition, we reveal that SET has a\nstrong capability of understanding implicit entity transformations, suggesting\nthat multi-task transfer learning should be further explored in future entity\ntracking research.",
    "descriptor": "\nComments: 7 pages, 1 figure\n",
    "authors": [
      "Janvijay Singh",
      "Fan Bai",
      "Zhen Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.06444"
  },
  {
    "id": "arXiv:2210.06447",
    "title": "Sampling in Constrained Domains with Orthogonal-Space Variational  Gradient Descent",
    "abstract": "Sampling methods, as important inference and learning techniques, are\ntypically designed for unconstrained domains. However, constraints are\nubiquitous in machine learning problems, such as those on safety, fairness,\nrobustness, and many other properties that must be satisfied to apply sampling\nresults in real-life applications. Enforcing these constraints often leads to\nimplicitly-defined manifolds, making efficient sampling with constraints very\nchallenging. In this paper, we propose a new variational framework with a\ndesigned orthogonal-space gradient flow (O-Gradient) for sampling on a manifold\n$\\mathcal{G}_0$ defined by general equality constraints. O-Gradient decomposes\nthe gradient into two parts: one decreases the distance to $\\mathcal{G}_0$ and\nthe other decreases the KL divergence in the orthogonal space. While most\nexisting manifold sampling methods require initialization on $\\mathcal{G}_0$,\nO-Gradient does not require such prior knowledge. We prove that O-Gradient\nconverges to the target constrained distribution with rate\n$\\widetilde{O}(1/\\text{the number of iterations})$ under mild conditions. Our\nproof relies on a new Stein characterization of conditional measure which could\nbe of independent interest. We implement O-Gradient through both Langevin\ndynamics and Stein variational gradient descent and demonstrate its\neffectiveness in various experiments, including Bayesian deep neural networks.",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Ruqi Zhang",
      "Qiang Liu",
      "Xin T. Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06447"
  },
  {
    "id": "arXiv:2210.06455",
    "title": "Token-Label Alignment for Vision Transformers",
    "abstract": "Data mixing strategies (e.g., CutMix) have shown the ability to greatly\nimprove the performance of convolutional neural networks (CNNs). They mix two\nimages as inputs for training and assign them with a mixed label with the same\nratio. While they are shown effective for vision transformers (ViTs), we\nidentify a token fluctuation phenomenon that has suppressed the potential of\ndata mixing strategies. We empirically observe that the contributions of input\ntokens fluctuate as forward propagating, which might induce a different mixing\nratio in the output tokens. The training target computed by the original data\nmixing strategy can thus be inaccurate, resulting in less effective training.\nTo address this, we propose a token-label alignment (TL-Align) method to trace\nthe correspondence between transformed tokens and the original tokens to\nmaintain a label for each token. We reuse the computed attention at each layer\nfor efficient token-label alignment, introducing only negligible additional\ntraining costs. Extensive experiments demonstrate that our method improves the\nperformance of ViTs on image classification, semantic segmentation, objective\ndetection, and transfer learning tasks. Code is available at:\nhttps://github.com/Euphoria16/TL-Align.",
    "descriptor": "\nComments: Source code available at this https URL\n",
    "authors": [
      "Han Xiao",
      "Wenzhao Zheng",
      "Zheng Zhu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06455"
  },
  {
    "id": "arXiv:2210.06456",
    "title": "Are Sample-Efficient NLP Models More Robust?",
    "abstract": "Recent work has observed that pre-trained models have higher\nout-of-distribution (OOD) robustness when they are exposed to less\nin-distribution (ID) training data (Radford et al., 2021). In particular,\nzero-shot models (e.g., GPT-3 and CLIP) have higher robustness than\nconventionally fine-tuned models, but these robustness gains fade as zero-shot\nmodels are fine-tuned on more ID data. We study this relationship between\nsample efficiency and robustness -- if two models have the same ID performance,\ndoes the model trained on fewer examples (higher sample efficiency) perform\nbetter OOD (higher robustness)?\nSurprisingly, experiments across three tasks, 23 total ID-OOD settings, and\n14 models do not reveal a consistent relationship between sample efficiency and\nrobustness -- while models with higher sample efficiency are sometimes more\nrobust, most often there is no change in robustness, with some cases even\nshowing decreased robustness. Since results vary on a case-by-case basis, we\nconduct detailed case studies of two particular ID-OOD pairs (SST-2 -> IMDb\nsentiment and SNLI -> HANS) to better understand why better sample efficiency\nmay or may not yield higher robustness; attaining such an understanding\nrequires case-by-case analysis of why models are not robust on a particular\nID-OOD setting and how modeling techniques affect model capabilities.",
    "descriptor": "\nComments: 25 pages, 21 figures\n",
    "authors": [
      "Nelson F. Liu",
      "Ananya Kumar",
      "Percy Liang",
      "Robin Jia"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06456"
  },
  {
    "id": "arXiv:2210.06458",
    "title": "Efficient Knowledge Distillation from Model Checkpoints",
    "abstract": "Knowledge distillation is an effective approach to learn compact models\n(students) with the supervision of large and strong models (teachers). As\nempirically there exists a strong correlation between the performance of\nteacher and student models, it is commonly believed that a high performing\nteacher is preferred. Consequently, practitioners tend to use a well trained\nnetwork or an ensemble of them as the teacher. In this paper, we make an\nintriguing observation that an intermediate model, i.e., a checkpoint in the\nmiddle of the training procedure, often serves as a better teacher compared to\nthe fully converged model, although the former has much lower accuracy. More\nsurprisingly, a weak snapshot ensemble of several intermediate models from a\nsame training trajectory can outperform a strong ensemble of independently\ntrained and fully converged models, when they are used as teachers. We show\nthat this phenomenon can be partially explained by the information bottleneck\nprinciple: the feature representations of intermediate models can have higher\nmutual information regarding the input, and thus contain more \"dark knowledge\"\nfor effective distillation. We further propose an optimal intermediate teacher\nselection algorithm based on maximizing the total task-related mutual\ninformation. Experiments verify its effectiveness and applicability.",
    "descriptor": "\nComments: Accepted by NeurIPS2022\n",
    "authors": [
      "Chaofei Wang",
      "Qisen Yang",
      "Rui Huang",
      "Shiji Song",
      "Gao Huang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06458"
  },
  {
    "id": "arXiv:2210.06461",
    "title": "Better Smatch = Better Parser? AMR evaluation is not so simple anymore",
    "abstract": "Recently, astonishing advances have been observed in AMR parsing, as measured\nby the structural Smatch metric. In fact, today's systems achieve performance\nlevels that seem to surpass estimates of human inter annotator agreement (IAA).\nTherefore, it is unclear how well Smatch (still) relates to human estimates of\nparse quality, as in this situation potentially fine-grained errors of similar\nweight may impact the AMR's meaning to different degrees.\nWe conduct an analysis of two popular and strong AMR parsers that --\naccording to Smatch -- reach quality levels on par with human IAA, and assess\nhow human quality ratings relate to Smatch and other AMR metrics. Our main\nfindings are: i) While high Smatch scores indicate otherwise, we find that AMR\nparsing is far from being solved: we frequently find structurally small, but\nsemantically unacceptable errors that substantially distort sentence meaning.\nii) Considering high-performance parsers, better Smatch scores may not\nnecessarily indicate consistently better parsing quality. To obtain a\nmeaningful and comprehensive assessment of quality differences of parse(r)s, we\nrecommend augmenting evaluations with macro statistics, use of additional\nmetrics, and more human analysis.",
    "descriptor": "\nComments: accepted at \"Evaluation and Comparison of NLP Systems\" Workshop (Eval4NLP 2022)\n",
    "authors": [
      "Juri Opitz",
      "Anette Frank"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06461"
  },
  {
    "id": "arXiv:2210.06462",
    "title": "Self-Guided Diffusion Models",
    "abstract": "Diffusion models have demonstrated remarkable progress in image generation\nquality, especially when guidance is used to control the generative process.\nHowever, guidance requires a large amount of image-annotation pairs for\ntraining and is thus dependent on their availability, correctness and\nunbiasedness. In this paper, we eliminate the need for such annotation by\ninstead leveraging the flexibility of self-supervision signals to design a\nframework for self-guided diffusion models. By leveraging a feature extraction\nfunction and a self-annotation function, our method provides guidance signals\nat various image granularities: from the level of holistic images to object\nboxes and even segmentation masks. Our experiments on single-label and\nmulti-label image datasets demonstrate that self-labeled guidance always\noutperforms diffusion models without guidance and may even surpass guidance\nbased on ground-truth labels, especially on unbalanced data. When equipped with\nself-supervised box or mask proposals, our method further generates visually\ndiverse yet semantically consistent images, without the need for any class,\nbox, or segment label annotation. Self-guided diffusion is simple, flexible and\nexpected to profit from deployment at scale.",
    "descriptor": "",
    "authors": [
      "Vincent Tao Hu",
      "David W Zhang",
      "Yuki M. Asano",
      "Gertjan J. Burghouts",
      "Cees G. M. Snoek"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06462"
  },
  {
    "id": "arXiv:2210.06463",
    "title": "Holo-Dex: Teaching Dexterity with Immersive Mixed Reality",
    "abstract": "A fundamental challenge in teaching robots is to provide an effective\ninterface for human teachers to demonstrate useful skills to a robot. This\nchallenge is exacerbated in dexterous manipulation, where teaching\nhigh-dimensional, contact-rich behaviors often require esoteric teleoperation\ntools. In this work, we present Holo-Dex, a framework for dexterous\nmanipulation that places a teacher in an immersive mixed reality through\ncommodity VR headsets. The high-fidelity hand pose estimator onboard the\nheadset is used to teleoperate the robot and collect demonstrations for a\nvariety of general-purpose dexterous tasks. Given these demonstrations, we use\npowerful feature learning combined with non-parametric imitation to train\ndexterous skills. Our experiments on six common dexterous tasks, including\nin-hand rotation, spinning, and bottle opening, indicate that Holo-Dex can both\ncollect high-quality demonstration data and train skills in a matter of hours.\nFinally, we find that our trained skills can exhibit generalization on objects\nnot seen in training. Videos of Holo-Dex are available at\nhttps://holo-dex.github.io.",
    "descriptor": "\nComments: Data, code and videos are available at this https URL\n",
    "authors": [
      "Sridhar Pandian Arunachalam",
      "Irmak G\u00fczey",
      "Soumith Chintala",
      "Lerrel Pinto"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06463"
  },
  {
    "id": "arXiv:2210.06464",
    "title": "Predictive Querying for Autoregressive Neural Sequence Models",
    "abstract": "In reasoning about sequential events it is natural to pose probabilistic\nqueries such as \"when will event A occur next\" or \"what is the probability of A\noccurring before B\", with applications in areas such as user modeling,\nmedicine, and finance. However, with machine learning shifting towards neural\nautoregressive models such as RNNs and transformers, probabilistic querying has\nbeen largely restricted to simple cases such as next-event prediction. This is\nin part due to the fact that future querying involves marginalization over\nlarge path spaces, which is not straightforward to do efficiently in such\nmodels. In this paper we introduce a general typology for predictive queries in\nneural autoregressive sequence models and show that such queries can be\nsystematically represented by sets of elementary building blocks. We leverage\nthis typology to develop new query estimation methods based on beam search,\nimportance sampling, and hybrids. Across four large-scale sequence datasets\nfrom different application domains, as well as for the GPT-2 language model, we\ndemonstrate the ability to make query answering tractable for arbitrary queries\nin exponentially-large predictive path-spaces, and find clear differences in\ncost-accuracy tradeoffs between search and sampling methods.",
    "descriptor": "\nComments: Presented at the Conference on Neural Information Processing Systems (NeurIPs 2022)\n",
    "authors": [
      "Alex Boyd",
      "Sam Showalter",
      "Stephan Mandt",
      "Padhraic Smyth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.06464"
  },
  {
    "id": "arXiv:2210.06465",
    "title": "AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars",
    "abstract": "Although 2D generative models have made great progress in face image\ngeneration and animation, they often suffer from undesirable artifacts such as\n3D inconsistency when rendering images from different camera viewpoints. This\nprevents them from synthesizing video animations indistinguishable from real\nones. Recently, 3D-aware GANs extend 2D GANs for explicit disentanglement of\ncamera pose by leveraging 3D scene representations. These methods can well\npreserve the 3D consistency of the generated images across different views, yet\nthey cannot achieve fine-grained control over other attributes, among which\nfacial expression control is arguably the most useful and desirable for face\nanimation. In this paper, we propose an animatable 3D-aware GAN for multiview\nconsistent face animation generation. The key idea is to decompose the 3D\nrepresentation of the 3D-aware GAN into a template field and a deformation\nfield, where the former represents different identities with a canonical\nexpression, and the latter characterizes expression variations of each\nidentity. To achieve meaningful control over facial expressions via\ndeformation, we propose a 3D-level imitative learning scheme between the\ngenerator and a parametric 3D face model during adversarial training of the\n3D-aware GAN. This helps our method achieve high-quality animatable face image\ngeneration with strong visual 3D consistency, even though trained with only\nunstructured 2D images. Extensive experiments demonstrate our superior\nperformance over prior works. Project page:\nhttps://yuewuhkust.github.io/AniFaceGAN",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. Project Page: this https URL\n",
    "authors": [
      "Yue Wu",
      "Yu Deng",
      "Jiaolong Yang",
      "Fangyun Wei",
      "Qifeng Chen",
      "Xin Tong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06465"
  },
  {
    "id": "arXiv:2210.06466",
    "title": "Prompt Generation Networks for Efficient Adaptation of Frozen Vision  Transformers",
    "abstract": "Large-scale pretrained models, especially those trained from vision-language\ndata have demonstrated the tremendous value that can be gained from both larger\ntraining datasets and models. Thus, in order to benefit from these\ndevelopments, there is renewed interest in transfer learning and adapting\nmodels from large-scale general pretraining to particular downstream tasks.\nHowever, the continuously increasing size of the models means that even the\nclassic approach of finetuning is becoming infeasible for all but big\ninstitutions. Prompt leaning has emerged as a flexible way to adapt models by\nsolely learning additional inputs to a model that is kept frozen, but so far\nperformances remained inferior to finetuning. To address this, we propose the\nPrompt Generation Network (PGN) that generates input-dependent prompts by\nsampling from a learned library of tokens. We show the PGN is effective in\nadapting pretrained models to various new datasets. It surpasses previous\nprompt-learning methods by a large margin and even fullfinetuning on 5 out of\n12 datasets while requiring 100x less parameters. PGN can even be used for\ntraining and inferring on multiple datasets simultaneously and learns to\nallocate tokens between domains. Given these findings, we conclude that PGN is\na viable and scalable approach for downstream adaptation of frozen models. Code\nis available at https://github.com/jochemloedeman/PGN.",
    "descriptor": "\nComments: Tech report\n",
    "authors": [
      "Jochem Loedeman",
      "Maarten C. Stol",
      "Tengda Han",
      "Yuki M. Asano"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06466"
  },
  {
    "id": "arXiv:2210.05673",
    "title": "Performance Deterioration of Deep Learning Models after Clinical  Deployment: A Case Study with Auto-segmentation for Definitive Prostate  Cancer Radiotherapy",
    "abstract": "In the past decade, deep learning (DL)-based artificial intelligence (AI) has\nwitnessed unprecedented success and has led to much excitement in medicine.\nHowever, many successful models have not been implemented in the clinic\npredominantly due to concerns regarding the lack of interpretability and\ngeneralizability in both spatial and temporal domains. In this work, we used a\nDL-based auto segmentation model for intact prostate patients to observe any\ntemporal performance changes and then correlate them to possible explanatory\nvariables. We retrospectively simulated the clinical implementation of our DL\nmodel to investigate temporal performance trends. Our cohort included 912\npatients with prostate cancer treated with definitive radiotherapy from January\n2006 to August 2021 at the University of Texas Southwestern Medical Center\n(UTSW). We trained a U-Net-based DL auto segmentation model on the data\ncollected before 2012 and tested it on data collected from 2012 to 2021 to\nsimulate the clinical deployment of the trained model starting in 2012. We\nvisualize the trends using a simple moving average curve and used ANOVA and\nt-test to investigate the impact of various clinical factors. The prostate and\nrectum contour quality decreased rapidly after 2016-2017. Stereotactic body\nradiotherapy (SBRT) and hydrogel spacer use were significantly associated with\nprostate contour quality (p=5.6e-12 and 0.002, respectively). SBRT and\nphysicians' styles are significantly associated with the rectum contour quality\n(p=0.0005 and 0.02, respectively). Only the presence of contrast within the\nbladder significantly affected the bladder contour quality (p=1.6e-7). We\nshowed that DL model performance decreased over time in concordance with\nchanges in clinical practice patterns and changes in clinical personnel.",
    "descriptor": "",
    "authors": [
      "Biling Wang",
      "Michael Dohopolski",
      "Ti Bai",
      "Junjie Wu",
      "Raquibul Hannan",
      "Neil Desai",
      "Aurelie Garant",
      "Dan Nguyen",
      "Xinlei Wang",
      "Mu-Han Lin",
      "Robert Timmerman",
      "Steve Jiang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.05673"
  },
  {
    "id": "arXiv:2210.05677",
    "title": "Application of Deep Learning on Single-Cell RNA-sequencing Data  Analysis: A Review",
    "abstract": "Single-cell RNA-sequencing (scRNA-seq) has become a routinely used technique\nto quantify the gene expression profile of thousands of single cells\nsimultaneously. Analysis of scRNA-seq data plays an important role in the study\nof cell states and phenotypes, and has helped elucidate biological processes,\nsuch as those occurring during development of complex organisms and improved\nour understanding of disease states, such as cancer, diabetes, and COVID, among\nothers. Deep learning, a recent advance of artificial intelligence that has\nbeen used to address many problems involving large datasets, has also emerged\nas a promising tool for scRNA-seq data analysis, as it has a capacity to\nextract informative, compact features from noisy, heterogeneous, and\nhigh-dimensional scRNA-seq data to improve downstream analysis. The present\nreview aims at surveying recently developed deep learning techniques in\nscRNA-seq data analysis, identifying key steps within the scRNA-seq data\nanalysis pipeline that have been advanced by deep learning, and explaining the\nbenefits of deep learning over more conventional analysis tools. Finally, we\nsummarize the challenges in current deep learning approaches faced within\nscRNA-seq data and discuss potential directions for improvements in deep\nalgorithms for scRNA-seq data analysis.",
    "descriptor": "",
    "authors": [
      "Matthew Brendel",
      "Chang Su",
      "Zilong Bai",
      "Hao Zhang",
      "Olivier Elemento",
      "Fei Wang"
    ],
    "subjectives": [
      "Genomics (q-bio.GN)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05677"
  },
  {
    "id": "arXiv:2210.05686",
    "title": "Neural Importance Sampling for Rapid and Reliable Gravitational-Wave  Inference",
    "abstract": "We combine amortized neural posterior estimation with importance sampling for\nfast and accurate gravitational-wave inference. We first generate a rapid\nproposal for the Bayesian posterior using neural networks, and then attach\nimportance weights based on the underlying likelihood and prior. This provides\n(1) a corrected posterior free from network inaccuracies, (2) a performance\ndiagnostic (the sample efficiency) for assessing the proposal and identifying\nfailure cases, and (3) an unbiased estimate of the Bayesian evidence. By\nestablishing this independent verification and correction mechanism we address\nsome of the most frequent criticisms against deep learning for scientific\ninference. We carry out a large study analyzing 42 binary black hole mergers\nobserved by LIGO and Virgo with the SEOBNRv4PHM and IMRPhenomXPHM waveform\nmodels. This shows a median sample efficiency of $\\approx 10\\%$ (two\norders-of-magnitude better than standard samplers) as well as a ten-fold\nreduction in the statistical uncertainty in the log evidence. Given these\nadvantages, we expect a significant impact on gravitational-wave inference, and\nfor this approach to serve as a paradigm for harnessing deep learning methods\nin scientific applications.",
    "descriptor": "\nComments: 7+7 pages, 1+5 figures\n",
    "authors": [
      "Maximilian Dax",
      "Stephen R. Green",
      "Jonathan Gair",
      "Michael P\u00fcrrer",
      "Jonas Wildberger",
      "Jakob H. Macke",
      "Alessandra Buonanno",
      "Bernhard Sch\u00f6lkopf"
    ],
    "subjectives": [
      "General Relativity and Quantum Cosmology (gr-qc)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05686"
  },
  {
    "id": "arXiv:2210.05710",
    "title": "Predictive Event Segmentation and Representation with Neural Networks: A  Self-Supervised Model Assessed by Psychological Experiments",
    "abstract": "People segment complex, ever-changing and continuous experience into basic,\nstable and discrete spatio-temporal experience units, called events. Event\nsegmentation literature investigates the mechanisms that allow people to\nextract events. Event segmentation theory points out that people predict\nongoing activities and observe prediction error signals to find event\nboundaries that keep events apart. In this study, we investigated the mechanism\ngiving rise to this ability by a computational model and accompanying\npsychological experiments. Inspired from event segmentation theory and\npredictive processing, we introduced a self-supervised model of event\nsegmentation. This model consists of neural networks that predict the sensory\nsignal in the next time-step to represent different events, and a cognitive\nmodel that regulates these networks on the basis of their prediction errors. In\norder to verify the ability of our model in segmenting events, learning them\nduring passive observation, and representing them in its internal\nrepresentational space, we prepared a video that depicts human behaviors\nrepresented by point-light displays. We compared event segmentation behaviors\nof participants and our model with this video in two hierarchical event\nsegmentation levels. By using point-biserial correlation technique, we\ndemonstrated that event segmentation decisions of our model correlated with the\nresponses of participants. Moreover, by approximating representation space of\nparticipants by a similarity-based technique, we showed that our model formed a\nsimilar representation space with those of participants. The result suggests\nthat our model that tracks the prediction error signals can produce human-like\nevent boundaries and event representations. Finally, we discussed our\ncontribution to the literature of event cognition and our understanding of how\nevent segmentation is implemented in the brain.",
    "descriptor": "\nComments: 27 pages, 9 figures, supplementary document can be found in the latex file\n",
    "authors": [
      "Hamit Basgol",
      "Inci Ayhan",
      "Emre Ugur"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05710"
  },
  {
    "id": "arXiv:2210.05713",
    "title": "Explainable fMRI-based Brain Decoding via Spatial Temporal-pyramid Graph  Convolutional Network",
    "abstract": "Brain decoding, aiming to identify the brain states using neural activity, is\nimportant for cognitive neuroscience and neural engineering. However, existing\nmachine learning methods for fMRI-based brain decoding either suffer from low\nclassification performance or poor explainability. Here, we address this issue\nby proposing a biologically inspired architecture, Spatial Temporal-pyramid\nGraph Convolutional Network (STpGCN), to capture the spatial-temporal graph\nrepresentation of functional brain activities. By designing multi-scale\nspatial-temporal pathways and bottom-up pathways that mimic the information\nprocess and temporal integration in the brain, STpGCN is capable of explicitly\nutilizing the multi-scale temporal dependency of brain activities via graph,\nthereby achieving high brain decoding performance. Additionally, we propose a\nsensitivity analysis method called BrainNetX to better explain the decoding\nresults by automatically annotating task-related brain regions from the\nbrain-network standpoint. We conduct extensive experiments on fMRI data under\n23 cognitive tasks from Human Connectome Project (HCP) S1200. The results show\nthat STpGCN significantly improves brain decoding performance compared to\ncompeting baseline models; BrainNetX successfully annotates task-relevant brain\nregions. Post hoc analysis based on these regions further validates that the\nhierarchical structure in STpGCN significantly contributes to the\nexplainability, robustness and generalization of the model. Our methods not\nonly provide insights into information representation in the brain under\nmultiple cognitive tasks but also indicate a bright future for fMRI-based brain\ndecoding.",
    "descriptor": "",
    "authors": [
      "Ziyuan Ye",
      "Youzhi Qu",
      "Zhichao Liang",
      "Mo Wang",
      "Quanying Liu"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.05713"
  },
  {
    "id": "arXiv:2210.05737",
    "title": "Context-aware Bayesian choice models",
    "abstract": "The mixed multinomial logit (MMNL) model assumes constant preference\nparameters of a decision-maker throughout different choice situations, which\nmay be considered too strong for certain choice modelling applications. This\npaper proposes an effective approach to model context-dependent\nintra-respondent heterogeneity and introduces the idea of Context-aware\nBayesian Mixed Multinomial Logit (C-MMNL) Model, where a neural network maps\ncontextual information to shifts in the preference parameters of each\nindividual in each choice occasion. The proposed model offers several key\nadvantages. First, it supports for both continuous and discrete variables, as\nwell as complex non-linear interactions between both types of variables.\nSecondly, each specification of the context is considered jointly as a whole by\nthe neural network rather than each variable being considered independently.\nFinally, since the parameters of the neural network are shared across all\ndecision-makers, it can leverage information from other decision-makers and use\nit to infer the effect of a particular context. Even though the C-MMNL model\nallows for flexible interactions between attributes, there is hardly an\nincrease in the complexity of the model and the computation time, compared to\nthe MMNL model. We present two real-world case studies from travel behaviour\ndomain - a travel mode choice model and a bicycle route choice model. The\nbicycle route choice model is based on a large-scale, crowdsourced dataset of\nGPS trajectories including 110,083 trips made by 8,555 cyclists.",
    "descriptor": "",
    "authors": [
      "Miros\u0142awa \u0141ukawska",
      "Anders Fjendbo Jensen",
      "Filipe Rodrigues"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05737"
  },
  {
    "id": "arXiv:2210.05746",
    "title": "On RKHS Choices for Assessing Graph Generators via Kernel Stein  Statistics",
    "abstract": "Score-based kernelised Stein discrepancy (KSD) tests have emerged as a\npowerful tool for the goodness of fit tests, especially in high dimensions;\nhowever, the test performance may depend on the choice of kernels in an\nunderlying reproducing kernel Hilbert space (RKHS). Here we assess the effect\nof RKHS choice for KSD tests of random networks models, developed for\nexponential random graph models (ERGMs) in Xu and Reinert (2021)and for\nsynthetic graph generators in Xu and Reinert (2022). We investigate the power\nperformance and the computational runtime of the test in different scenarios,\nincluding both dense and sparse graph regimes. Experimental results on kernel\nperformance for model assessment tasks are shown and discussed on synthetic and\nreal-world network applications.",
    "descriptor": "",
    "authors": [
      "Moritz Weckbecker",
      "Wenkai Xu",
      "Gesine Reinert"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05746"
  },
  {
    "id": "arXiv:2210.05748",
    "title": "Critical Points at Infinity for Hyperplanes of Directions",
    "abstract": "Analytic combinatorics in several variables (ACSV) analyzes the asymptotic\ngrowth of the coefficients of a meromorphic generating function $F = G/H$ in a\ndirection $\\mathbf{r}$. It uses Morse theory on the pole variety $V := \\{ H = 0\n\\} \\subseteq (\\mathbb{C}^*)^d$ of $F$ to deform the torus $T$ in the\nmultivariate Cauchy Integral Formula via the downward gradient flow for the\n\\textit{height} function $h = h_{\\mathbf{r}} = -\\sum_{j=1}^d r_j \\log |z_j|$,\ngiving a homology decomposition of $T$ into cycles around \\textit{critical\npoints} of $h$ on $V$. The deformation can flow to infinity at finite height\nwhen the height function is not a proper map. This happens only in the presence\nof a critical point at infinity (CPAI): a sequence of points on $V$ approaching\na point at infinity, and such that log-normals to $V$ converge projectively to\n$\\mathbf{r}$. The CPAI is called \\textit{heighted} if the height function also\nconverges to a finite value. This paper studies whether all CPAI are heighted,\nand in which directions CPAI can occur. We study these questions by examining\nsequences converging to faces of a toric compactification defined by a multiple\nof the Newton polytope $\\mathcal{P}$ of the polynomial $H$. Under generically\nsatisfied conditions, any projective limit of log-normals of a sequence\nconverging to a face $F$ must be parallel to $F$; this implies that CPAI must\nalways be heighted and can only occur in directions parallel to some face of\n$\\mathcal{P}$. When this generic condition fails, we show under a smoothness\ncondition, that a point in a codimension-1 face $F$ can still only be a CPAI\nfor directions parallel to $F$, and that the directions for a codimension-2\nface can be a larger set, which can be computed explicitly and still has\npositive codimension.",
    "descriptor": "",
    "authors": [
      "Stephen Gillen"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Symbolic Computation (cs.SC)"
    ],
    "url": "https://arxiv.org/abs/2210.05748"
  },
  {
    "id": "arXiv:2210.05762",
    "title": "Joint localization and classification of breast tumors on ultrasound  images using a novel auxiliary attention-based framework",
    "abstract": "Automatic breast lesion detection and classification is an important task in\ncomputer-aided diagnosis, in which breast ultrasound (BUS) imaging is a common\nand frequently used screening tool. Recently, a number of deep learning-based\nmethods have been proposed for joint localization and classification of breast\nlesions using BUS images. In these methods, features extracted by a shared\nnetwork trunk are appended by two independent network branches to achieve\nclassification and localization. Improper information sharing might cause\nconflicts in feature optimization in the two branches and leads to performance\ndegradation. Also, these methods generally require large amounts of pixel-level\nannotated data for model training. To overcome these limitations, we proposed a\nnovel joint localization and classification model based on the attention\nmechanism and disentangled semi-supervised learning strategy. The model used in\nthis study is composed of a classification network and an auxiliary\nlesion-aware network. By use of the attention mechanism, the auxiliary\nlesion-aware network can optimize multi-scale intermediate feature maps and\nextract rich semantic information to improve classification and localization\nperformance. The disentangled semi-supervised learning strategy only requires\nincomplete training datasets for model training. The proposed modularized\nframework allows flexible network replacement to be generalized for various\napplications. Experimental results on two different breast ultrasound image\ndatasets demonstrate the effectiveness of the proposed method. The impacts of\nvarious network factors on model performance are also investigated to gain deep\ninsights into the designed framework.",
    "descriptor": "",
    "authors": [
      "Zong Fan",
      "Ping Gong",
      "Shanshan Tang",
      "Christine U. Lee",
      "Xiaohui Zhang",
      "Pengfei Song",
      "Shigao Chen",
      "Hua Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05762"
  },
  {
    "id": "arXiv:2210.05779",
    "title": "Evaluation of Delay Uncertainty in PCB Interconnects Due to Fiber Weave  Effect",
    "abstract": "Delay Deviation Exceedance (DDE) and Differential Skew Exceedance (DSE)\nmeasures are proposed to quantify delay uncertainty in single-ended and\ndifferential PCB interconnects arising from Fiber Weave Effect (FWE). DDE and\nDSE are constructed with 3D EM analysis of traces over inhomogeneous dielectric\nwith glass fiber bundles in resin. Measurements or 3D EM models for FWE are\nusually used for observations of delay or impedance dependency on position of\ntraces over fiber bundles with the purpose to find the worst case scenario.\nThis paper turns the results of 3D EM analysis into probabilistic measures of\npossible delay or skew uncertainty - probability to have delay deviation in\nsingle-ended interconnects over allowed limit (DDE measure) or to have skew\nover allowed limit in differential interconnects (DSE measure). The introduced\nmeasures allow formalizing the laminate selection process for parallel as well\nas for serial PCB interconnects.",
    "descriptor": "\nComments: 6 pages, 16 figures, 1 table\n",
    "authors": [
      "Alex Manukovsky",
      "Yuriy Shlepnev",
      "Shimon Mordooch"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05779"
  },
  {
    "id": "arXiv:2210.05787",
    "title": "Hypercontractivity Meets Random Convex Hulls: Analysis of Randomized  Multivariate Cubatures",
    "abstract": "Given a probability measure $\\mu$ on a set $\\mathcal{X}$ and a vector-valued\nfunction $\\varphi$, a common problem is to construct a discrete probability\nmeasure on $\\mathcal{X}$ such that the push-forward of these two probability\nmeasures under $\\varphi$ is the same. This construction is at the heart of\nnumerical integration methods that run under various names such as quadrature,\ncubature, or recombination. A natural approach is to sample points from $\\mu$\nuntil their convex hull of their image under $\\varphi$ includes the mean of\n$\\varphi$. Here we analyze the computational complexity of this approach when\n$\\varphi$ exhibits a graded structure by using so-called hypercontractivity.\nThe resulting theorem not only covers the classical cubature case of\nmultivariate polynomials, but also integration on pathspace, as well as kernel\nquadrature for product measures.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Satoshi Hayakawa",
      "Harald Oberhauser",
      "Terry Lyons"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05787"
  },
  {
    "id": "arXiv:2210.05796",
    "title": "Accurate computations up to break-down of quasi-periodic attractors in  the dissipative spin-orbit problem",
    "abstract": "In recent papers, we developed extremely accurate methods to compute\nquasi-periodic attractors in a model of Celestial Mechanics: the spin-orbit\nproblem with a dissipative tidal torque. This problem is a singular\nperturbation of a conservative system. The goal is to show that it is possible\nto maintain the accuracy and reliability of the computation of quasi-periodic\nattractors for parameter values extremely close to the breakdown and,\ntherefore, it is possible to obtain information on the mechanism of breakdown\nof these quasi-periodic attractors.\nThe result is obtained by implementing a method that uses at the same time\nnumerical and rigorous improvements, in particular, (i) the time-one map of the\nspin-orbit problem (so that the invariant objects we seek for have less\ndimensions), (ii) very accurate computations of the time-one map (high order\nmethods with extended precision arithmetic), (iii) very efficient KAM methods\nfor maps (they are quadratically convergent, the step has low storage\nrequirements and low operation count), (iv) the algorithms are backed by a\nrigorous a-posteriori KAM Theorem, that establishes that, if the algorithm is\nsuccessful and produces a small residual, then there is a true solution nearby,\nand (v) The algorithms are guaranteed to reach arbitrarily close to the border\nof existence, given enough computer resources. Indeed, monitoring the\nproperties of the solution, we obtain very effective criteria to compute the\nparameters for the breakdown to happen. We do not know of any other method\nwhich can compute even heuristically this level of accurate and reliable values\nfor this model.\nWe also study several scale invariant observables of the tori. The behavior\nat breakdown does not satisfy standard scaling relations. Hence, the breakdown\nphenomena of the spin-orbit problem, are not described by a hyperbolic fixed\npoint of the renormalization operator.",
    "descriptor": "\nComments: 37 pages, 13 figures\n",
    "authors": [
      "Renato Calleja",
      "Alessandra Celletti",
      "Joan Gimeno",
      "Rafael de la Llave"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Chaotic Dynamics (nlin.CD)"
    ],
    "url": "https://arxiv.org/abs/2210.05796"
  },
  {
    "id": "arXiv:2210.05816",
    "title": "Finding and Listing Front-door Adjustment Sets",
    "abstract": "Identifying the effects of new interventions from data is a significant\nchallenge found across a wide range of the empirical sciences. A well-known\nstrategy for identifying such effects is Pearl's front-door (FD) criterion\n(Pearl, 1995). The definition of the FD criterion is declarative, only allowing\none to decide whether a specific set satisfies the criterion. In this paper, we\npresent algorithms for finding and enumerating possible sets satisfying the FD\ncriterion in a given causal diagram. These results are useful in facilitating\nthe practical applications of the FD criterion for causal effects estimation\nand helping scientists to select estimands with desired properties, e.g., based\non cost, feasibility of measurement, or statistical power.",
    "descriptor": "\nComments: Pages: 18 (main paper 10, references 2, appendix 6), Figures: 9 (main paper 7, appendix 2), to be published in Proceedings of the 36th Annual Conference on Neural Information Processing Systems\n",
    "authors": [
      "Hyunchai Jeong",
      "Jin Tian",
      "Elias Bareinboim"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05816"
  },
  {
    "id": "arXiv:2210.05821",
    "title": "Short-term prediction of stream turbidity using surrogate data and a  meta-model approach",
    "abstract": "Many water-quality monitoring programs aim to measure turbidity to help guide\neffective management of waterways and catchments, yet distributing turbidity\nsensors throughout networks is typically cost prohibitive. To this end, we\nbuilt and compared the ability of dynamic regression (ARIMA), long short-term\nmemory neural nets (LSTM), and generalized additive models (GAM) to forecast\nstream turbidity one step ahead, using surrogate data from relatively low-cost\nin-situ sensors and publicly available databases. We iteratively trialled\ncombinations of four surrogate covariates (rainfall, water level, air\ntemperature and total global solar exposure) selecting a final model for each\ntype that minimised the corrected Akaike Information Criterion.\nCross-validation using a rolling time-window indicated that ARIMA, which\nincluded the rainfall and water-level covariates only, produced the most\naccurate predictions, followed closely by GAM, which included all four\ncovariates. We constructed a meta-model, trained on time-series features of\nturbidity, to take advantage of the strengths of each model over different time\npoints and predict the best model (that with the lowest forecast error one-step\nprior) for each time step. The meta-model outperformed all other models,\nindicating that this methodology can yield high accuracy and may be a viable\nalternative to using measurements sourced directly from turbidity-sensors where\ncosts prohibit their deployment and maintenance, and when predicting turbidity\nacross the short term. Our findings also indicated that temperature and\nlight-associated variables, for example underwater illuminance, may hold\npromise as cost-effective, high-frequency surrogates of turbidity, especially\nwhen combined with other covariates, like rainfall, that are typically measured\nat coarse levels of spatial resolution.",
    "descriptor": "",
    "authors": [
      "Bhargav Rele",
      "Caleb Hogan",
      "Sevvandi Kandanaarachchi",
      "Catherine Leigh"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2210.05821"
  },
  {
    "id": "arXiv:2210.05841",
    "title": "Towards Optimal Primary- and Secondary-control Design for Networks with  Generators and Inverters",
    "abstract": "For power grids predominantly featuring large synchronous generators (SGs),\nthere exists a significant body of work bridging optimization and control\ntasks. A generic workflow in such efforts entails: characterizing the steady\nstate of control algorithms and SG dynamics; assessing the optimality of the\nresulting operating point with respect to an optimal dispatch task; and\nprescribing control parameters to ensure that (under reasonable ambient\nperturbations) the considered control nudges the system steady state to\noptimality. Well studied instances of the aforementioned approach include\ndesigning: i) automatic generation control (AGC) participation factors to\nensure economic optimality, and ii) governor frequency-droop slopes to ensure\npower sharing. Recognizing that future power grids will feature a diverse mix\nof SGs and inverter-based resources (IBRs) with varying control structures,\nthis work examines the different steps of the optimization-control workflow for\nthis context. Considering a representative model of active power-frequency\ndynamics of IBRs and SGs, a characterization of steady state is put forth (with\nand without secondary frequency control). Conditions on active-power droop\nslopes and AGC participation factors are then derived to ascertain desired\npower sharing and ensure economically optimal operation under varying power\ndemands.",
    "descriptor": "\nComments: Presented at the Allerton conference 2022\n",
    "authors": [
      "Manish K. Singh",
      "D. Venkatramanan",
      "Sairaj Dhople"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.05841"
  },
  {
    "id": "arXiv:2210.05843",
    "title": "Cross-dataset COVID-19 Transfer Learning with Cough Detection, Cough  Segmentation, and Data Augmentation",
    "abstract": "This paper addresses issues on cough-based COVID-19 detection. We propose a\ncross-dataset transfer learning approach to improve the performance of COVID-19\ndetection by incorporating cough detection, cough segmentation, and data\naugmentation. The first aimed at removing non-cough signals and cough signals\nwith low probability. The second aimed at segregating several coughs in a\nwaveform into individual coughs. The third aimed at increasing the number of\nsamples for the deep learning model. These three processing blocks are\nimportant as our finding revealed a large margin of improvement relative to the\nbaseline methods without these blocks. An ablation study is conducted to\noptimize hyperparameters and it was found that alpha mixup is an important\nfactor among others in improving the model performance via this augmentation\nmethod. A summary of this study with previous studies on the same evaluation\nset was given to gain insights into different methods of cough-based COVID-19\ndetection.",
    "descriptor": "",
    "authors": [
      "Bagus Tris Atmaja",
      "Zanjabila",
      "Suyanto",
      "Akira Sasou"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.05843"
  },
  {
    "id": "arXiv:2210.05880",
    "title": "Pathology Steered Stratification Network for Subtype Identification in  Alzheimer's Disease",
    "abstract": "Alzheimer's disease (AD) is a heterogeneous, multifactorial neurodegenerative\ndisorder characterized by beta-amyloid, pathologic tau, and neurodegeneration.\nThe massive heterogeneity between neurobiological examinations and clinical\nassessment is the current biggest challenge in the early diagnosis of\nAlzheimer's disease, urging for a comprehensive stratification of the aging\npopulation that is defined by reliable neurobiological biomarkers and closely\nassociated with clinical outcomes. However, existing statistical inference\napproaches in neuroimaging studies of AD subtype identification fail to take\ninto account the neuropathological domain knowledge, which could lead to\nill-posed results that are sometimes inconsistent with neurological principles.\nTo fill this knowledge gap, we propose a novel pathology steered stratification\nnetwork (PSSN) that integrates mainstream AD pathology with multimodal\nlongitudinal neuroimaging data to categorize the aging population. By combining\ntheory-based biological modeling and data-driven deep learning, this\ncross-disciplinary approach can not only generate long-term biomarker\nprediction consistent with the end-state of individuals but also stratifies\nsubjects into fine-grained subtypes with distinct neurological underpinnings,\nwhere ag-ing brains within the same subtype share com-mon biological behaviors\nthat emerge as similar trajectories of cognitive decline. Our stratification\noutperforms K-means and SuStaIn in both inter-cluster heterogeneity and\nintra-cluster homogeneity of various clinical scores. Importantly, we identify\nsix subtypes spanning AD spectrum, where each subtype exhibits a distinctive\nbiomarker pattern that is consistent with its clinical outcome. A disease\nevolutionary graph is further provided by quantifying subtype transition\nprobabilities, which may assist pre-symptomatic diagnosis and guide therapeutic\ntreatments.",
    "descriptor": "",
    "authors": [
      "Enze Xu",
      "Jingwen Zhang",
      "Jiadi Li",
      "Defu Yang",
      "Guorong Wu",
      "Minghan Chen"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05880"
  },
  {
    "id": "arXiv:2210.05885",
    "title": "Unitary property testing lower bounds by polynomials",
    "abstract": "We study unitary property testing, where a quantum algorithm is given query\naccess to a black-box unitary and has to decide whether it satisfies some\nproperty. In addition to containing the standard quantum query complexity model\n(where the unitary encodes a binary string) as a special case, this model\ncontains \"inherently quantum\" problems that have no classical analogue.\nCharacterizing the query complexity of these problems requires new algorithmic\ntechniques and lower bound methods.\nOur main contribution is a generalized polynomial method for unitary property\ntesting problems. By leveraging connections with invariant theory, we apply\nthis method to obtain lower bounds on problems such as determining recurrence\ntimes of unitaries, approximating the dimension of a marked subspace, and\napproximating the entanglement entropy of a marked state. We also present a\nunitary property testing-based approach towards an oracle separation between\n$\\mathsf{QMA}$ and $\\mathsf{QMA(2)}$, a long standing question in quantum\ncomplexity theory.",
    "descriptor": "\nComments: 54 pages\n",
    "authors": [
      "Adrian She",
      "Henry Yuen"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2210.05885"
  },
  {
    "id": "arXiv:2210.05893",
    "title": "The Power of Two Matrices in Spectral Algorithms",
    "abstract": "Spectral algorithms are some of the main tools in optimization and inference\nproblems on graphs. Typically, the graph is encoded as a matrix and\neigenvectors and eigenvalues of the matrix are then used to solve the given\ngraph problem. Spectral algorithms have been successfully used for graph\npartitioning, hidden clique recovery and graph coloring. In this paper, we\nstudy the power of spectral algorithms using two matrices in a graph\npartitioning problem. We use two different matrices resulting from two\ndifferent encodings of the same graph and then combine the spectral information\ncoming from these two matrices.\nWe analyze a two matrix spectral algorithm for the problem of identifying\nlatent community structure in large random graphs. In particular, we consider\nthe problem of recovering community assignments exactly in the censored\nstochastic block model, where each edge status is revealed independently with\nsome probability. We show that spectral algorithms based on two matrices are\noptimal and succeed in recovering communities up to the information theory\nthreshold. On the other hand, we show that for most choices of the parameters,\nany spectral algorithm based on one matrix is suboptimal. This is in contrast\nto our prior works (2022a, 2022b) which showed that for the symmetric\nStochastic Block Model and the Planted Dense Subgraph problem, spectral\nalgorithm based on one matrix achieve the information theory threshold. Of\nindependent interest, we provide more general geometric conditions for the\n(sub)-optimality of spectral algorithms, that are also applicable to cases when\nthere are more than two communities.",
    "descriptor": "\nComments: 28 pages, 1 figure\n",
    "authors": [
      "Souvik Dhara",
      "Julia Gaudio",
      "Elchanan Mossel",
      "Colin Sandon"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.05893"
  },
  {
    "id": "arXiv:2210.05946",
    "title": "Self-Supervised Equivariant Regularization Reconciles Multiple Instance  Learning: Joint Referable Diabetic Retinopathy Classification and Lesion  Segmentation",
    "abstract": "Lesion appearance is a crucial clue for medical providers to distinguish\nreferable diabetic retinopathy (rDR) from non-referable DR. Most existing\nlarge-scale DR datasets contain only image-level labels rather than pixel-based\nannotations. This motivates us to develop algorithms to classify rDR and\nsegment lesions via image-level labels. This paper leverages self-supervised\nequivariant learning and attention-based multi-instance learning (MIL) to\ntackle this problem. MIL is an effective strategy to differentiate positive and\nnegative instances, helping us discard background regions (negative instances)\nwhile localizing lesion regions (positive ones). However, MIL only provides\ncoarse lesion localization and cannot distinguish lesions located across\nadjacent patches. Conversely, a self-supervised equivariant attention mechanism\n(SEAM) generates a segmentation-level class activation map (CAM) that can guide\npatch extraction of lesions more accurately. Our work aims at integrating both\nmethods to improve rDR classification accuracy. We conduct extensive validation\nexperiments on the Eyepacs dataset, achieving an area under the receiver\noperating characteristic curve (AU ROC) of 0.958, outperforming current\nstate-of-the-art algorithms.",
    "descriptor": "\nComments: 7 pages, 2 tables, 3 figures. 18th International Symposium on Medical Information Processing and Analysis\n",
    "authors": [
      "Wenhui Zhu",
      "Peijie Qiu",
      "Natasha Lepore",
      "Oana M. Dumitrascu",
      "Yalin Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.05946"
  },
  {
    "id": "arXiv:2210.05952",
    "title": "3D Brain and Heart Volume Generative Models: A Survey",
    "abstract": "Generative models such as generative adversarial networks and autoencoders\nhave gained a great deal of attention in the medical field due to their\nexcellent data generation capability. This paper provides a comprehensive\nsurvey of generative models for three-dimensional (3D) volumes, focusing on the\nbrain and heart. A new and elaborate taxonomy of unconditional and conditional\ngenerative models is proposed to cover diverse medical tasks for the brain and\nheart: unconditional synthesis, classification, conditional synthesis,\nsegmentation, denoising, detection, and registration. We provide relevant\nbackground, examine each task and also suggest potential future directions. A\nlist of the latest publications will be updated on Github to keep up with the\nrapid influx of papers at\n\\url{https://github.com/csyanbin/3D-Medical-Generative-Survey}.",
    "descriptor": "\nComments: A survey on the 3D brain and heart volume generative models\n",
    "authors": [
      "Yanbin Liu",
      "Girish Dwivedi",
      "Farid Boussaid",
      "Mohammed Bennamoun"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05952"
  },
  {
    "id": "arXiv:2210.05955",
    "title": "Identifiability and Asymptotics in Learning Homogeneous Linear ODE  Systems from Discrete Observations",
    "abstract": "Ordinary Differential Equations (ODEs) have recently gained a lot of\nattention in machine learning. However, the theoretical aspects, e.g.,\nidentifiability and asymptotic properties of statistical estimation are still\nobscure. This paper derives a sufficient condition for the identifiability of\nhomogeneous linear ODE systems from a sequence of equally-spaced error-free\nobservations sampled from a single trajectory. When observations are disturbed\nby measurement noise, we prove that under mild conditions, the parameter\nestimator based on the Nonlinear Least Squares (NLS) method is consistent and\nasymptotic normal with $n^{-1/2}$ convergence rate. Based on the asymptotic\nnormality property, we construct confidence sets for the unknown system\nparameters and propose a new method to infer the causal structure of the ODE\nsystem, i.e., inferring whether there is a causal link between system\nvariables. Furthermore, we extend the results to degraded observations,\nincluding aggregated and time-scaled ones. To the best of our knowledge, our\nwork is the first systematic study of the identifiability and asymptotic\nproperties in learning linear ODE systems. We also construct simulations with\nvarious system dimensions to illustrate the established theoretical results.",
    "descriptor": "",
    "authors": [
      "Yuanyuan Wang",
      "Wei Huang",
      "Mingming Gong",
      "Xi Geng",
      "Tongliang Liu",
      "Kun Zhang",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05955"
  },
  {
    "id": "arXiv:2210.05960",
    "title": "Efficient Image Super-Resolution using Vast-Receptive-Field Attention",
    "abstract": "The attention mechanism plays a pivotal role in designing advanced\nsuper-resolution (SR) networks. In this work, we design an efficient SR network\nby improving the attention mechanism. We start from a simple pixel attention\nmodule and gradually modify it to achieve better super-resolution performance\nwith reduced parameters. The specific approaches include: (1) increasing the\nreceptive field of the attention branch, (2) replacing large dense convolution\nkernels with depth-wise separable convolutions, and (3) introducing pixel\nnormalization. These approaches paint a clear evolutionary roadmap for the\ndesign of attention mechanisms. Based on these observations, we propose VapSR,\nthe VAst-receptive-field Pixel attention network. Experiments demonstrate the\nsuperior performance of VapSR. VapSR outperforms the present lightweight\nnetworks with even fewer parameters. And the light version of VapSR can use\nonly 21.68% and 28.18% parameters of IMDB and RFDN to achieve similar\nperformances to those networks. The code and models are available at\nurl{https://github.com/zhoumumu/VapSR.",
    "descriptor": "",
    "authors": [
      "Lin Zhou",
      "Haoming Cai",
      "Jinjin Gu",
      "Zheyuan Li",
      "Yingqi Liu",
      "Xiangyu Chen",
      "Yu Qiao",
      "Chao Dong"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05960"
  },
  {
    "id": "arXiv:2210.05978",
    "title": "From the Hardness of Detecting Superpositions to Cryptography: Quantum  Public Key Encryption and Commitments",
    "abstract": "Recently, Aaronson et al. (arXiv:2009.07450) showed that detecting\ninterference between two orthogonal states is as hard as swapping these states.\nWhile their original motivation was from quantum gravity, we show its\napplications in quantum cryptography.\n1. We construct the first public key encryption scheme from cryptographic\n\\emph{non-abelian} group actions. Interestingly, the ciphertexts of our scheme\nare quantum even if messages are classical. This resolves an open question\nposed by Ji et al. (TCC '19). We construct the scheme through a new abstraction\ncalled swap-trapdoor function pairs, which may be of independent interest.\n2. We give a simple and efficient compiler that converts the flavor of\nquantum bit commitments. More precisely, for any prefix X,Y $\\in$\n{computationally,statistically,perfectly}, if the base scheme is X-hiding and\nY-binding, then the resulting scheme is Y-hiding and X-binding. Our compiler\ncalls the base scheme only once. Previously, all known compilers call the base\nschemes polynomially many times (Cr\\'epeau et al., Eurocrypt '01 and Yan,\nAsiacrypt '22). For the security proof of the conversion, we generalize the\nresult of Aaronson et al. by considering quantum auxiliary inputs.",
    "descriptor": "\nComments: 51 pages\n",
    "authors": [
      "Minki Hhan",
      "Tomoyuki Morimae",
      "Takashi Yamakawa"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.05978"
  },
  {
    "id": "arXiv:2210.05979",
    "title": "Adversarial Speaker-Consistency Learning Using Untranscribed Speech Data  for Zero-Shot Multi-Speaker Text-to-Speech",
    "abstract": "Several recently proposed text-to-speech (TTS) models achieved to generate\nthe speech samples with the human-level quality in the single-speaker and\nmulti-speaker TTS scenarios with a set of pre-defined speakers. However,\nsynthesizing a new speaker's voice with a single reference audio, commonly\nknown as zero-shot multi-speaker text-to-speech (ZSM-TTS), is still a very\nchallenging task. The main challenge of ZSM-TTS is the speaker domain shift\nproblem upon the speech generation of a new speaker. To mitigate this problem,\nwe propose adversarial speaker-consistency learning (ASCL). The proposed method\nfirst generates an additional speech of a query speaker using the external\nuntranscribed datasets at each training iteration. Then, the model learns to\nconsistently generate the speech sample of the same speaker as the\ncorresponding speaker embedding vector by employing an adversarial learning\nscheme. The experimental results show that the proposed method is effective\ncompared to the baseline in terms of the quality and speaker similarity in\nZSM-TTS.",
    "descriptor": "\nComments: Accepted to APSIPA 2022\n",
    "authors": [
      "Byoung Jin Choi",
      "Myeonghun Jeong",
      "Minchan Kim",
      "Sung Hwan Mun",
      "Nam Soo Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.05979"
  },
  {
    "id": "arXiv:2210.05987",
    "title": "A Momentum Accelerated Adaptive Cubic Regularization Method for  Nonconvex Optimization",
    "abstract": "The cubic regularization method (CR) and its adaptive version (ARC) are\npopular Newton-type methods in solving unconstrained non-convex optimization\nproblems, due to its global convergence to local minima under mild conditions.\nThe main aim of this paper is to develop a momentum-accelerated adaptive cubic\nregularization method (ARCm) to improve the convergent performance. With the\nproper choice of momentum step size, we show the global convergence of ARCm and\nthe local convergence can also be guaranteed under the \\KL property. Such\nglobal and local convergence can also be established when inexact solvers with\nlow computational costs are employed in the iteration procedure. Numerical\nresults for non-convex logistic regression and robust linear regression models\nare reported to demonstrate that the proposed ARCm significantly outperforms\nstate-of-the-art cubic regularization methods (e.g., CR, momentum-based CR,\nARC) and the trust region method. In particular, the number of iterations\nrequired by ARCm is less than 10\\% to 50\\% required by the most competitive\nmethod (ARC) in the experiments.",
    "descriptor": "",
    "authors": [
      "Yihang Gao",
      "Michael K. Ng"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05987"
  },
  {
    "id": "arXiv:2210.05988",
    "title": "CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG  Reconstruction",
    "abstract": "Human electroencephalography (EEG) is a brain monitoring modality that senses\ncortical neuroelectrophysiological activity in high-temporal resolution. One of\nthe greatest challenges posed in applications of EEG is the unstable signal\nquality susceptible to inevitable artifacts during recordings. To date, most\nexisting techniques for EEG artifact removal and reconstruction are applicable\nto offline analysis solely, or require individualized training data to\nfacilitate online reconstruction. We have proposed CLEEGN, a novel\nconvolutional neural network for plug-and-play automatic EEG reconstruction.\nCLEEGN is based on a subject-independent pre-trained model using existing data\nand can operate on a new user without any further calibration. The performance\nof CLEEGN was validated using multiple evaluations including waveform\nobservation, reconstruction error assessment, and decoding accuracy on\nwell-studied labeled datasets. The results of simulated online validation\nsuggest that, even without any calibration, CLEEGN can largely preserve\ninherent brain activity and outperforms leading online/offline artifact removal\nmethods in the decoding accuracy of reconstructed EEG data. In addition,\nvisualization of model parameters and latent features exhibit the model\nbehavior and reveal explainable insights related to existing knowledge of\nneuroscience. We foresee pervasive applications of CLEEGN in prospective works\nof online plug-and-play EEG decoding and analysis.",
    "descriptor": "",
    "authors": [
      "Pin-Hua Lai",
      "Wei-Chun Yang",
      "Hsiang-Chieh Tsou",
      "Chun-Shu Wei"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2210.05988"
  },
  {
    "id": "arXiv:2210.05994",
    "title": "SARAH-based Variance-reduced Algorithm for Stochastic Finite-sum  Cocoercive Variational Inequalities",
    "abstract": "Variational inequalities are a broad formalism that encompasses a vast number\nof applications. Motivated by applications in machine learning and beyond,\nstochastic methods are of great importance. In this paper we consider the\nproblem of stochastic finite-sum cocoercive variational inequalities. For this\nclass of problems, we investigate the convergence of the method based on the\nSARAH variance reduction technique. We show that for strongly monotone problems\nit is possible to achieve linear convergence to a solution using this method.\nExperiments confirm the importance and practical applicability of our approach.",
    "descriptor": "\nComments: 11 pages, 1 algorithm, 1 figure, 1 theorem\n",
    "authors": [
      "Aleksandr Beznosikov",
      "Alexander Gasnikov"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05994"
  },
  {
    "id": "arXiv:2210.06043",
    "title": "Predicting the vascular adhesion of deformable drug carriers in narrow  capillaries traversed by blood cell",
    "abstract": "In vascular targeted therapies, blood-borne carriers should realize sustained\ndrug release from the luminal side towards the diseased tissue. In this\ncontext, such carriers are required to firmly adhere to the vessel walls for a\nsufficient period of time while resisting force perturbations induced by the\nblood flow and circulating cells. Here, a hybrid computational model, combining\na Lattice Boltzmann (LBM) and Immersed Boundary Methods (IBM), is proposed for\npredicting the strength of adhesion of particles in narrow capillaries (7.5\n$\\mu \\mathrm{m})$ traversed by blood cells. While flowing down the capillary,\nglobular and biconcave deformable cells ( $7 \\mu \\mathrm{m}$ ) encounter $2 \\mu\n\\mathrm{m}$ discoidal particles, adhering to the vessel walls. Particles\npresent aspect ratios ranging from $0.25$ to $1.0$ and a mechanical stiffness\nvarying from rigid $(\\mathrm{Ca}=0)$ to soft\n$\\left(\\mathrm{Ca}=10^{-3}\\right)$. Cell-particle interactions are\nquantitatively predicted over time via three independent parameters: the cell\nmembrane stretching $\\delta p$; the cell-to-particle distance $r$, and the\nnumber of engaged ligand-receptor bonds $N_{\\mathrm{L}}$.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1907.00080\n",
    "authors": [
      "A. Coclite",
      "G. Pascazio",
      "M. D. de Tullio",
      "P. Decuzzi"
    ],
    "subjectives": [
      "Fluid Dynamics (physics.flu-dyn)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.06043"
  },
  {
    "id": "arXiv:2210.06061",
    "title": "Non-smooth and H\u00f6lder-smooth Submodular Maximization",
    "abstract": "We study the problem of maximizing a continuous DR-submodular function that\nis not necessarily smooth. We prove that the continuous greedy algorithm\nachieves an $[(1-1/e)\\text{OPT}-\\epsilon]$ guarantee when the function is\nmonotone and H\\\"older-smooth, meaning that it admits a H\\\"older-continuous\ngradient. For functions that are non-differentiable or non-smooth, we propose a\nvariant of the mirror-prox algorithm that attains an\n$[(1/2)\\text{OPT}-\\epsilon]$ guarantee. We apply our algorithmic frameworks to\nrobust submodular maximization and distrbituionally robust submodular\nmaximization under Wasserstein ambiguity. In particular, the mirror-prox method\napplies to robust submodular maximization to obtain a single feasible solution\nwhose value is at least $(1/2)\\text{OPT}-\\epsilon$. For distributionally robust\nmaximization under Wasserstein ambiguity, we deduce and work over a\nsubmodular-convex maximin reformulation whose objective function is\nH\\\"older-smooth, for which we may apply both the continuous greedy method and\nthe mirror-prox method.",
    "descriptor": "",
    "authors": [
      "Duksang Lee",
      "Nam Ho-Nguyen",
      "Dabeen Lee"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06061"
  },
  {
    "id": "arXiv:2210.06069",
    "title": "E3Bind: An End-to-End Equivariant Network for Protein-Ligand Docking",
    "abstract": "In silico prediction of the ligand binding pose to a given protein target is\na crucial but challenging task in drug discovery. This work focuses on blind\nflexible selfdocking, where we aim to predict the positions, orientations and\nconformations of docked molecules. Traditional physics-based methods usually\nsuffer from inaccurate scoring functions and high inference cost. Recently,\ndata-driven methods based on deep learning techniques are attracting growing\ninterest thanks to their efficiency during inference and promising performance.\nThese methods usually either adopt a two-stage approach by first predicting the\ndistances between proteins and ligands and then generating the final\ncoordinates based on the predicted distances, or directly predicting the global\nroto-translation of ligands. In this paper, we take a different route. Inspired\nby the resounding success of AlphaFold2 for protein structure prediction, we\npropose E3Bind, an end-to-end equivariant network that iteratively updates the\nligand pose. E3Bind models the protein-ligand interaction through careful\nconsideration of the geometric constraints in docking and the local context of\nthe binding site. Experiments on standard benchmark datasets demonstrate the\nsuperior performance of our end-to-end trainable model compared to traditional\nand recently-proposed deep learning methods.",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Yangtian Zhang",
      "Huiyu Cai",
      "Chence Shi",
      "Bozitao Zhong",
      "Jian Tang"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06069"
  },
  {
    "id": "arXiv:2210.06083",
    "title": "Outlier-Insensitive Kalman Filtering Using NUV Priors",
    "abstract": "The Kalman filter (KF) is a widely-used algorithm for tracking the latent\nstate of a dynamical system from noisy observations. For systems that are\nwell-described by linear Gaussian state space models, the KF minimizes the\nmean-squared error (MSE). However, in practice, observations are corrupted by\noutliers, severely impairing the KFs performance. In this work, an\noutlier-insensitive KF is proposed, where robustness is achieved by modeling\neach potential outlier as a normally distributed random variable with unknown\nvariance (NUV). The NUVs variances are estimated online, using both\nexpectation-maximization (EM) and alternating maximization (AM). The former was\npreviously proposed for the task of smoothing with outliers and was adapted\nhere to filtering, while both EM and AM obtained the same performance and\noutperformed the other algorithms, the AM approach is less complex and thus\nrequires 40 percentage less run-time. Our empirical study demonstrates that the\nMSE of our proposed outlier-insensitive KF outperforms previously proposed\nalgorithms, and that for data clean of outliers, it reverts to the classic KF,\ni.e., MSE optimality is preserved",
    "descriptor": "",
    "authors": [
      "Shunit Truzman",
      "Guy Revach",
      "Nir Shlezinger",
      "Itzik Klein"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06083"
  },
  {
    "id": "arXiv:2210.06093",
    "title": "Post-Quantum Zero-Knowledge with Space-Bounded Simulation",
    "abstract": "The traditional definition of quantum zero-knowledge stipulates that the\nknowledge gained by any quantum polynomial-time verifier in an interactive\nprotocol can be simulated by a quantum polynomial-time algorithm. One drawback\nof this definition is that it allows the simulator to consume significantly\nmore computational resources than the verifier. We argue that this drawback\nrenders the existing notion of quantum zero-knowledge not viable for certain\nsettings, especially when dealing with near-term quantum devices.\nIn this work, we initiate a fine-grained notion of post-quantum\nzero-knowledge that is more compatible with near-term quantum devices. We\nintroduce the notion of $(s,f)$ space-bounded quantum zero-knowledge. In this\nnew notion, we require that an $s$-qubit malicious verifier can be simulated by\na quantum polynomial-time algorithm that uses at most $f(s)$-qubits, for some\nfunction $f(\\cdot)$, and no restriction on the amount of the classical memory\nconsumed by either the verifier or the simulator. We explore this notion and\nestablish both positive and negative results:\n- For verifiers with logarithmic quantum space $s$ and (arbitrary) polynomial\nclassical space, we show that $(s,f)$-space-bounded QZK, for $f(s)=2s$, can be\nachieved based on the existence of post-quantum one-way functions. Moreover,\nour protocol runs in constant rounds.\n- For verifiers with super-logarithmic quantum space $s$, assuming the\nexistence of post-quantum secure one-way functions, we show that\n$(s,f)$-space-bounded QZK protocols, with fully black-box simulation (classical\nanalogue of black-box simulation) can only be achieved for languages in BQP.",
    "descriptor": "",
    "authors": [
      "Prabhanjan Ananth",
      "Alex B. Grilo"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06093"
  },
  {
    "id": "arXiv:2210.06124",
    "title": "Elastic buildings: Calibrated district-scale simulation of  occupant-flexible campus operation for hybrid work optimization",
    "abstract": "Before 2020, the way occupants utilized the built environment had been\nchanging slowly towards scenarios in which occupants have more choice and\nflexibility in where and how they work. The global COVID-19 pandemic\naccelerated this phenomenon rapidly through lockdowns and hybrid work\narrangements. Many occupants and employers are considering keeping some of\nthese flexibility-based strategies due to their benefits and cost impacts. This\npaper simulates various scenarios related to the operational technologies and\npolicies of a real-world campus using a district-scale City Energy Analyst\n(CEA) model that is calibrated with measured energy and occupancy profiles\nextracted from WiFi data. These scenarios demonstrate the energy impact of\nramping building operations up and down more rapidly and effectively to the\nflex-based work strategies that may solidify. The scenarios show a 4-12%\ndecrease in space cooling demand due to occupant absenteeism if centralized\nbuilding system operation is in place, but as high as 21-68% if\noccupancy-driven building controls are implemented. The paper discusses\ntechnologies and strategies that are important in this paradigm shift of\noperations.",
    "descriptor": "",
    "authors": [
      "Mart\u00edn Mosteiro-Romero",
      "Clayton Miller",
      "Adrian Chong",
      "Rudi Stouffs"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Multiagent Systems (cs.MA)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06124"
  },
  {
    "id": "arXiv:2210.06140",
    "title": "Differentially Private Bootstrap: New Privacy Analysis and Inference  Strategies",
    "abstract": "Differential private (DP) mechanisms protect individual-level information by\nintroducing randomness into the statistical analysis procedure. While there are\nnow many DP tools for various statistical problems, there is still a lack of\ngeneral techniques to understand the sampling distribution of a DP estimator,\nwhich is crucial for uncertainty quantification in statistical inference. We\nanalyze a DP bootstrap procedure that releases multiple private bootstrap\nestimates to infer the sampling distribution and construct confidence\nintervals. Our privacy analysis includes new results on the privacy cost of a\nsingle DP bootstrap estimate applicable to incorporate arbitrary DP mechanisms\nand identifies some misuses of the bootstrap in the existing literature. We\nshow that the release of $B$ DP bootstrap estimates from mechanisms satisfying\n$(\\mu/\\sqrt{(2-2/\\mathrm{e})B})$-Gaussian DP asymptotically satisfies\n$\\mu$-Gaussian DP as $B$ goes to infinity. We also develop a statistical\nprocedure based on the DP bootstrap estimates to correctly infer the sampling\ndistribution using techniques related to the deconvolution of probability\nmeasures, an approach which is novel in analyzing DP procedures. From our\ndensity estimate, we construct confidence intervals and compare them to\nexisting methods through simulations and real-world experiments using the 2016\nCanada Census Public Use Microdata. The coverage of our private confidence\nintervals achieves the nominal confidence level, while other methods fail to\nmeet this guarantee.",
    "descriptor": "",
    "authors": [
      "Zhanyu Wang",
      "Guang Cheng",
      "Jordan Awan"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Cryptography and Security (cs.CR)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06140"
  },
  {
    "id": "arXiv:2210.06144",
    "title": "Digital twins of nonlinear dynamical systems",
    "abstract": "We articulate the design imperatives for machine-learning based digital twins\nfor nonlinear dynamical systems subject to external driving, which can be used\nto monitor the ``health'' of the target system and anticipate its future\ncollapse. We demonstrate that, with single or parallel reservoir computing\nconfigurations, the digital twins are capable of challenging forecasting and\nmonitoring tasks. Employing prototypical systems from climate, optics and\necology, we show that the digital twins can extrapolate the dynamics of the\ntarget system to certain parameter regimes never experienced before, make\ncontinual forecasting/monitoring with sparse real-time updates under\nnon-stationary external driving, infer hidden variables and accurately predict\ntheir dynamical evolution, adapt to different forms of external driving, and\nextrapolate the global bifurcation behaviors to systems of some different\nsizes. These features make our digital twins appealing in significant\napplications such as monitoring the health of critical systems and forecasting\ntheir potential collapse induced by environmental changes.",
    "descriptor": "\nComments: 21 pages, 14 figures\n",
    "authors": [
      "Ling-Wei Kong",
      "Yang Weng",
      "Bryan Glaz",
      "Mulugeta Haile",
      "Ying-Cheng Lai"
    ],
    "subjectives": [
      "Adaptation and Self-Organizing Systems (nlin.AO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06144"
  },
  {
    "id": "arXiv:2210.06170",
    "title": "Contrastive Neural Ratio Estimation",
    "abstract": "Likelihood-to-evidence ratio estimation is usually cast as either a binary\n(NRE-A) or a multiclass (NRE-B) classification task. In contrast to the binary\nclassification framework, the current formulation of the multiclass version has\nan intrinsic and unknown bias term, making otherwise informative diagnostics\nunreliable. We propose a multiclass framework free from the bias inherent to\nNRE-B at optimum, leaving us in the position to run diagnostics that\npractitioners depend on. It also recovers NRE-A in one corner case and NRE-B in\nthe limiting case. For fair comparison, we benchmark the behavior of all\nalgorithms in both familiar and novel training regimes: when jointly drawn data\nis unlimited, when data is fixed but prior draws are unlimited, and in the\ncommonplace fixed data and parameters setting. Our investigations reveal that\nthe highest performing models are distant from the competitors (NRE-A, NRE-B)\nin hyperparameter space. We make a recommendation for hyperparameters distinct\nfrom the previous models. We suggest a bound on the mutual information as a\nperformance metric for simulation-based inference methods, without the need for\nposterior samples, and provide experimental results.",
    "descriptor": "\nComments: 10 pages. 32 pages with references and supplemental material. Accepted at NeurIPS 2022. Code at this https URL\n",
    "authors": [
      "Benjamin Kurt Miller",
      "Christoph Weniger",
      "Patrick Forr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.06170"
  },
  {
    "id": "arXiv:2210.06175",
    "title": "Exploring Efficient-tuning Methods in Self-supervised Speech Models",
    "abstract": "In this study, we aim to explore efficient tuning methods for speech\nself-supervised learning. Recent studies show that self-supervised learning\n(SSL) can learn powerful representations for different speech tasks. However,\nfine-tuning pre-trained models for each downstream task is\nparameter-inefficient since SSL models are notoriously large with millions of\nparameters. Adapters are lightweight modules commonly used in NLP to solve this\nproblem. In downstream tasks, the parameters of SSL models are frozen, and only\nthe adapters are trained. Given the lack of studies generally exploring the\neffectiveness of adapters for self-supervised speech tasks, we intend to fill\nthis gap by adding various adapter modules in pre-trained speech SSL models. We\nshow that the performance parity can be achieved with over 90% parameter\nreduction, and discussed the pros and cons of efficient tuning techniques. This\nis the first comprehensive investigation of various adapter types across speech\ntasks.",
    "descriptor": "\nComments: SLT 2022\n",
    "authors": [
      "Zih-Ching Chen",
      "Chin-Lun Fu",
      "Chih-Ying Liu",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.06175"
  },
  {
    "id": "arXiv:2210.06179",
    "title": "Convolutional Neural Network-Based Image Watermarking using Discrete  Wavelet Transform",
    "abstract": "As the Internet becomes more popular, digital images are used and transferred\nmore frequently. Although this phenomenon facilitates easy access to\ninformation, it also creates security concerns and violates intellectual\nproperty rights by allowing illegal use, copying, and digital content theft.\nUsing watermarks (WMs) in digital images is one of the most common ways to\nmaintain security. Watermarking is proving and declaring ownership of an image\nby adding a digital watermark to the original image. Watermarks can be either\ntext or an image placed overtly or covertly in an image and are expected to be\nchallenging to remove. This paper proposes a combination of convolutional\nneural networks (CNNs) and wavelet transforms to obtain a watermarking network\nfor embedding and extracting watermarks. The network is independent of the host\nimage resolution, can accept all kinds of watermarks, and has only 11 CNN\nlayers while keeping performance. Two terms measure performance; the similarity\nbetween the extracted watermark and the original one and the similarity between\nthe host image and the watermarked one.",
    "descriptor": "",
    "authors": [
      "Alireza Tavakoli",
      "Zahra Honjani",
      "Hedieh Sajedi"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06179"
  },
  {
    "id": "arXiv:2210.06226",
    "title": "Alpha-divergence Variational Inference Meets Importance Weighted  Auto-Encoders: Methodology and Asymptotics",
    "abstract": "Several algorithms involving the Variational R\\'enyi (VR) bound have been\nproposed to minimize an alpha-divergence between a target posterior\ndistribution and a variational distribution. Despite promising empirical\nresults, those algorithms resort to biased stochastic gradient descent\nprocedures and thus lack theoretical guarantees. In this paper, we formalize\nand study the VR-IWAE bound, a generalization of the Importance Weighted\nAuto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several\ndesirable properties and notably leads to the same stochastic gradient descent\nprocedure as the VR bound in the reparameterized case, but this time by relying\non unbiased gradient estimators. We then provide two complementary theoretical\nanalyses of the VR-IWAE bound and thus of the standard IWAE bound. Those\nanalyses shed light on the benefits or lack thereof of these bounds. Lastly, we\nillustrate our theoretical claims over toy and real-data examples.",
    "descriptor": "",
    "authors": [
      "Kam\u00e9lia Daudel",
      "Joe Benton",
      "Yuyang Shi",
      "Arnaud Doucet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06226"
  },
  {
    "id": "arXiv:2210.06227",
    "title": "Quantum Optimisation for Continuous Multivariable Functions by a  Structured Search",
    "abstract": "Solving optimisation problems is a promising near-term application of quantum\ncomputers. Quantum variational algorithms leverage quantum superposition and\nentanglement to optimise over exponentially large solution spaces using an\nalternating sequence of classically tunable unitaries. However, prior work has\nprimarily addressed discrete optimisation problems. In addition, these\nalgorithms have been designed generally under the assumption of an unstructured\nsolution space, which constrains their speedup to the theoretical limits for\nthe unstructured Grover's quantum search algorithm. In this paper, we show that\nquantum variational algorithms can efficiently optimise continuous\nmultivariable functions by exploiting general structural properties of a\ndiscretised continuous solution space with a convergence that exceeds the\nlimits of an unstructured quantum search. We introduce the Quantum\nMultivariable Optimisation Algorithm (QMOA) and demonstrate its advantage over\npre-existing methods, particularly when optimising high-dimensional and\noscillatory functions.",
    "descriptor": "\nComments: 16 Pages, 9 Figures\n",
    "authors": [
      "Edric Matwiejew",
      "Jason Pye",
      "Jingbo B. Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06227"
  },
  {
    "id": "arXiv:2210.06268",
    "title": "On a Canonical Distributed Controller in the Behavioral Framework",
    "abstract": "Control in a classical transfer function or state-space setting typically\nviews a controller as a signal processor: sensor outputs are mapped to actuator\ninputs. In behavioral system theory, control is simply viewed as\ninterconnection; the interconnection of a plant with a controller. In this\npaper we consider the problem of control of interconnected systems in a\nbehavioral setting. The behavioral setting is especially fit for modelling\ninterconnected systems, because it allows for the interconnection of subsystems\nwithout imposing inputs and outputs. We introduce a so-called canonical\ndistributed controller that implements a given interconnected behavior that is\ndesired, provided that necessary and sufficient conditions hold true. The\ncontroller design can be performed in a decentralized manner, in the sense that\na local controller only depends on the local system behavior. Regularity of\ninterconnections is an important property in behavioral control that yields\nfeedback interconnections. We provide conditions under which the\ninterconnection of this distributed controller with the plant is regular.\nFurthermore, we show that the interconnections of subsystems of the canonical\ndistributed controller are regular if and only if the interconnections of the\nplant and desired behavior are regular.",
    "descriptor": "",
    "authors": [
      "Tom R. V. Steentjes",
      "Mircea Lazar",
      "Paul M. J. Van den Hof"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2210.06268"
  },
  {
    "id": "arXiv:2210.06285",
    "title": "Smart Cup: An impedance sensing based fluid intake monitoring system for  beverages classification and freshness detection",
    "abstract": "This paper presents a novel beverage intake monitoring system that can\naccurately recognize beverage kinds and freshness. By mounting carbon\nelectrodes on the commercial cup, the system measures the electrochemical\nimpedance spectrum of the fluid in the cup. We studied the frequency\nsensitivity of the electrochemical impedance spectrum regarding distinct\nbeverages and the importance of features like amplitude, phase, and real and\nimaginary components for beverage classification. The results show that\nfeatures from a low-frequency domain (100 Hz to 1000 Hz) provide more\nmeaningful information for beverage classification than the higher frequency\ndomain. Twenty beverages, including carbonated drinks and juices, were\nclassified with nearly perfect accuracy using a supervised machine learning\napproach. The same performance was also observed in the freshness recognition,\nwhere four different kinds of milk and fruit juice were studied.",
    "descriptor": "\nComments: 3 pages, 3 figures, 3 tables\n",
    "authors": [
      "Mengxi Liu",
      "Sizhen Bian",
      "Bo Zhou",
      "Agnes Gr\u00fcnerbl",
      "Paul Lukowicz"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06285"
  },
  {
    "id": "arXiv:2210.06286",
    "title": "Self-supervised Learning for Label-Efficient Sleep Stage Classification:  A Comprehensive Evaluation",
    "abstract": "The past few years have witnessed a remarkable advance in deep learning for\nEEG-based sleep stage classification (SSC). However, the success of these\nmodels is attributed to possessing a massive amount of labeled data for\ntraining, limiting their applicability in real-world scenarios. In such\nscenarios, sleep labs can generate a massive amount of data, but labeling these\ndata can be expensive and time-consuming. Recently, the self-supervised\nlearning (SSL) paradigm has shined as one of the most successful techniques to\novercome the scarcity of labeled data. In this paper, we evaluate the efficacy\nof SSL to boost the performance of existing SSC models in the few-labels\nregime. We conduct a thorough study on three SSC datasets, and we find that\nfine-tuning the pretrained SSC models with only 5% of labeled data can achieve\ncompetitive performance to the supervised training with full labels. Moreover,\nself-supervised pretraining helps SSC models to be more robust to data\nimbalance and domain shift problems. The code is publicly available at\n\\url{https://github.com/emadeldeen24/eval_ssl_ssc}.",
    "descriptor": "\nComments: Under Review. This document contains 9 pages as a main text, and 2 pages for supplementary materials\n",
    "authors": [
      "Emadeldeen Eldele",
      "Mohamed Ragab",
      "Zhenghua Chen",
      "Min Wu",
      "Chee-Keong Kwoh",
      "Xiaoli Li"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06286"
  },
  {
    "id": "arXiv:2210.06287",
    "title": "An Energy-Efficient Spiking Neural Network for Finger Velocity Decoding  for Implantable Brain-Machine Interface",
    "abstract": "Brain-machine interfaces (BMIs) are promising for motor rehabilitation and\nmobility augmentation. High-accuracy and low-power algorithms are required to\nachieve implantable BMI systems. In this paper, we propose a novel spiking\nneural network (SNN) decoder for implantable BMI regression tasks. The SNN is\ntrained with enhanced spatio-temporal backpropagation to fully leverage its\nability in handling temporal problems. The proposed SNN decoder achieves the\nsame level of correlation coefficient as the state-of-the-art ANN decoder in\noffline finger velocity decoding tasks, while it requires only 6.8% of the\ncomputation operations and 9.4% of the memory access.",
    "descriptor": "",
    "authors": [
      "Jiawei Liao",
      "Lars Widmer",
      "Xiaying Wang",
      "Alfio Di Mauro",
      "Samuel R. Nason-Tomaszewski",
      "Cynthia A. Chestek",
      "Luca Benini",
      "Taekwang Jang"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06287"
  },
  {
    "id": "arXiv:2210.06288",
    "title": "fAux: Testing Individual Fairness via Gradient Alignment",
    "abstract": "Machine learning models are vulnerable to biases that result in unfair\ntreatment of individuals from different populations. Recent work that aims to\ntest a model's fairness at the individual level either relies on domain\nknowledge to choose metrics, or on input transformations that risk generating\nout-of-domain samples. We describe a new approach for testing individual\nfairness that does not have either requirement. We propose a novel criterion\nfor evaluating individual fairness and develop a practical testing method based\non this criterion which we call fAux (pronounced fox). This is based on\ncomparing the derivatives of the predictions of the model to be tested with\nthose of an auxiliary model, which predicts the protected variable from the\nobserved data. We show that the proposed method effectively identifies\ndiscrimination on both synthetic and real-world datasets, and has quantitative\nand qualitative advantages over contemporary methods.",
    "descriptor": "",
    "authors": [
      "Giuseppe Castiglione",
      "Ga Wu",
      "Christopher Srinivasa",
      "Simon Prince"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06288"
  },
  {
    "id": "arXiv:2210.06290",
    "title": "The evolution of AI approaches for motor imagery EEG-based BCIs",
    "abstract": "The Motor Imagery (MI) electroencephalography (EEG) based Brain Computer\nInterfaces (BCIs) allow the direct communication between humans and machines by\nexploiting the neural pathways connected to motor imagination. Therefore, these\nsystems open the possibility of developing applications that could span from\nthe medical field to the entertainment industry. In this context, Artificial\nIntelligence (AI) approaches become of fundamental importance especially when\nwanting to provide a correct and coherent feedback to BCI users. Moreover,\npublicly available datasets in the field of MI EEG-based BCIs have been widely\nexploited to test new techniques from the AI domain. In this work, AI\napproaches applied to datasets collected in different years and with different\ndevices but with coherent experimental paradigms are investigated with the aim\nof providing a concise yet sufficiently comprehensive survey on the evolution\nand influence of AI techniques on MI EEG-based BCI data.",
    "descriptor": "\nComments: Submitted to Italian Workshop on Artificial Intelligence for Human Machine Interaction (AIxHMI 2022), December 02, 2022, Udine, Italy\n",
    "authors": [
      "Aurora Saibene",
      "Silvia Corchs",
      "Mirko Caglioni",
      "Francesca Gasparini"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06290"
  },
  {
    "id": "arXiv:2210.06291",
    "title": "ECG for high-throughput screening of multiple diseases: Proof-of-concept  using multi-diagnosis deep learning from population-based datasets",
    "abstract": "Electrocardiogram (ECG) abnormalities are linked to cardiovascular diseases,\nbut may also occur in other non-cardiovascular conditions such as mental,\nneurological, metabolic and infectious conditions. However, most of the recent\nsuccess of deep learning (DL) based diagnostic predictions in selected patient\ncohorts have been limited to a small set of cardiac diseases. In this study, we\nuse a population-based dataset of >250,000 patients with >1000 medical\nconditions and >2 million ECGs to identify a wide range of diseases that could\nbe accurately diagnosed from the patient's first in-hospital ECG. Our DL models\nuncovered 128 diseases and 68 disease categories with strong discriminative\nperformance.",
    "descriptor": "\nComments: Accepted in Medical Imaging meets NeurIPS 2021 this https URL\n",
    "authors": [
      "Weijie Sun",
      "Sunil Vasu Kalmady",
      "Amir Salimi",
      "Nariman Sepehrvand",
      "Eric Ly",
      "Abram Hindle",
      "Russell Greiner",
      "Padma Kaul"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06291"
  },
  {
    "id": "arXiv:2210.06292",
    "title": "A review on Epileptic Seizure Detection using Machine Learning",
    "abstract": "Epilepsy is a neurological brain disorder which life threatening and gives\nrise to recurrent seizures that are unprovoked. It occurs due to the abnormal\nchemical changes in our brain. Over the course of many years, studies have been\nconducted to support automatic diagnosis of epileptic seizures for the ease of\nclinicians. For that, several studies entail the use of machine learning\nmethods for the early prediction of epileptic seizures. Mainly, feature\nextraction methods have been used to extract the right features from the EEG\ndata generated by the EEG machine and then various machine learning classifiers\nare used for the classification process. This study provides a systematic\nliterature review of feature selection process as well as the classification\nperformance. This study was limited to the finding of most used feature\nextraction methods and the classifiers used for accurate classification of\nnormal to epileptic seizures. The existing literature was examined from\nwell-known repositories such as MPDI, IEEEXplore, Wiley, Elsevier, ACM,\nSpringerlink and others. Furthermore, a taxonomy was created that recapitulates\nthe state-of-the-art used solutions for this problem. We also studied the\nnature of different benchmark and unbiased datasets and gave a rigorous\nanalysis of the working of classifiers. Finally, we concluded the research by\npresenting the gaps, challenges and opportunities which can further help\nresearchers in prediction of epileptic seizure",
    "descriptor": "",
    "authors": [
      "Muhammad Shoaib Farooq",
      "Aimen Zulfiqar",
      "Shamyla Riaz"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06292"
  },
  {
    "id": "arXiv:2210.06293",
    "title": "Two-stream Network for ECG Signal Classification",
    "abstract": "Electrocardiogram (ECG), a technique for medical monitoring of cardiac\nactivity, is an important method for identifying cardiovascular disease.\nHowever, analyzing the increasing quantity of ECG data consumes a lot of\nmedical resources. This paper explores an effective algorithm for automatic\nclassifications of multi-classes of heartbeat types based on ECG. Most neural\nnetwork based methods target the individual heartbeats, ignoring the secrets\nembedded in the temporal sequence. And the ECG signal has temporal variation\nand unique individual characteristics, which means that the same type of ECG\nsignal varies among patients under different physical conditions. A two-stream\narchitecture is used in this paper and presents an enhanced version of ECG\nrecognition based on this. The architecture achieves classification of holistic\nECG signal and individual heartbeat and incorporates identified and temporal\nstream networks. Identified networks are used to extract features of individual\nheartbeats, while temporal networks aim to extract temporal correlations\nbetween heartbeats. Results on the MIT-BIH Arrhythmia Database demonstrate that\nthe proposed algorithm performs an accuracy of 99.38\\%. In addition, the\nproposed algorithm reaches an 88.07\\% positive accuracy on massive data in real\nlife, showing that the proposed algorithm can efficiently categorize different\nclasses of heartbeat with high diagnostic performance.",
    "descriptor": "",
    "authors": [
      "Xinyao Hou",
      "Shengmei Qin",
      "Jianbo Su"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06293"
  },
  {
    "id": "arXiv:2210.06294",
    "title": "Indoor Localization with Robust Global Channel Charting: A  Time-Distance-Based Approach",
    "abstract": "Fingerprinting-based positioning significantly improves the indoor\nlocalization performance in non-line-of-sight-dominated areas. However, its\ndeployment and maintenance is cost-intensive as it needs ground-truth reference\nsystems for both the initial training and the adaption to environmental\nchanges. In contrast, channel charting (CC) works without explicit reference\ninformation and only requires the spatial correlations of channel state\ninformation (CSI). While CC has shown promising results in modelling the\ngeometry of the radio environment, a deeper insight into CC for localization\nusing multi-anchor large-bandwidth measurements is still pending. We contribute\na novel distance metric for time-synchronized single-input/single-output CSIs\nthat approaches a linear correlation to the Euclidean distance. This allows to\nlearn the environment's global geometry without annotations. To efficiently\noptimize the global channel chart we approximate the metric with a Siamese\nneural network. This enables full CC-assisted fingerprinting and positioning\nonly using a linear transformation from the chart to the real-world\ncoordinates. We compare our approach to the state-of-the-art of CC on two\ndifferent real-world data sets recorded with a 5G and UWB radio setup. Our\napproach outperforms others with localization accuracies of 0.69m for the UWB\nand 1.4m for the 5G setup. We show that CC-assisted fingerprinting enables\nhighly accurate localization and reduces (or eliminates) the need for annotated\ntraining data.",
    "descriptor": "\nComments: Submitted to IEEE Transactions on Machine Learning in Communications and Networking\n",
    "authors": [
      "Maximilian Stahlke",
      "George Yammine",
      "Tobias Feigl",
      "Bjoern M. Eskofier",
      "Christopher Mutschler"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06294"
  },
  {
    "id": "arXiv:2210.06295",
    "title": "Transfer learning on electromyography (EMG) tasks: approaches and beyond",
    "abstract": "Machine learning on electromyography (EMG) has recently achieved remarkable\nsuccess on a variety of tasks, while such success relies heavily on the\nassumption that the training and future data must be of the same data\ndistribution. However, this assumption may not hold in many real-world\napplications. Model calibration is required via data re-collection and label\nannotation, which is generally very expensive and time-consuming. To address\nthis problem, transfer learning (TL), which aims to improve target learners'\nperformance by transferring the knowledge from related source domains, is\nemerging as a new paradigm to reduce the amount of calibration effort. In this\nsurvey, we assess the eligibility of more than fifty published peer-reviewed\nrepresentative transfer learning approaches for EMG applications. Unlike\nprevious surveys on purely transfer learning or EMG-based machine learning,\nthis survey aims to provide an insight into the biological foundations of\nexisting transfer learning methods on EMG-related analysis. In specific, we\nfirst introduce the physiological structure of the muscles and the EMG\ngenerating mechanism, and the recording of EMG to provide biological insights\nbehind existing transfer learning approaches. Further, we categorize existing\nresearch endeavors into data based, model based, training scheme based, and\nadversarial based. This survey systematically summarizes and categorizes\nexisting transfer learning approaches for EMG related machine learning\napplications. In addition, we discuss possible drawbacks of existing works and\npoint out the future direction of better EMG transfer learning algorithms to\nenhance practicality for real-world applications.",
    "descriptor": "",
    "authors": [
      "Di Wu",
      "Jie Yang",
      "Mohamad Sawan"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06295"
  },
  {
    "id": "arXiv:2210.06297",
    "title": "Multimodality Multi-Lead ECG Arrhythmia Classification using  Self-Supervised Learning",
    "abstract": "Electrocardiogram (ECG) signal is one of the most effective sources of\ninformation mainly employed for the diagnosis and prediction of cardiovascular\ndiseases (CVDs) connected with the abnormalities in heart rhythm. Clearly,\nsingle modality ECG (i.e. time series) cannot convey its complete\ncharacteristics, thus, exploiting both time and time-frequency modalities in\nthe form of time-series data and spectrogram is needed. Leveraging the\ncutting-edge self-supervised learning (SSL) technique on unlabeled data, we\npropose SSL-based multimodality ECG classification. Our proposed network\nfollows SSL learning paradigm and consists of two modules corresponding to\npre-stream task, and down-stream task, respectively. In the SSL-pre-stream\ntask, we utilize self-knowledge distillation (KD) techniques with no labeled\ndata, on various transformations and in both time and frequency domains. In the\ndown-stream task, which is trained on labeled data, we propose a gate fusion\nmechanism to fuse information from multimodality.To evaluate the effectiveness\nof our approach, ten-fold cross validation on the 12-lead PhysioNet 2020\ndataset has been conducted.",
    "descriptor": "",
    "authors": [
      "Thinh Phan",
      "Duc Le",
      "Patel Brijesh",
      "Donald Adjeroh",
      "Jingxian Wu",
      "Morten Olgaard Jensen",
      "Ngan Le"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06297"
  },
  {
    "id": "arXiv:2210.06298",
    "title": "Cross Task Neural Architecture Search for EEG Signal Classifications",
    "abstract": "Electroencephalograms (EEGs) are brain dynamics measured outside the brain,\nwhich have been widely utilized in non-invasive brain-computer interface\napplications. Recently, various neural network approaches have been proposed to\nimprove the accuracy of EEG signal recognition. However, these approaches\nseverely rely on manually designed network structures for different tasks which\ngenerally are not sharing the same empirical design cross-task-wise. In this\npaper, we propose a cross-task neural architecture search (CTNAS-EEG) framework\nfor EEG signal recognition, which can automatically design the network\nstructure across tasks and improve the recognition accuracy of EEG signals.\nSpecifically, a compatible search space for cross-task searching and an\nefficient constrained searching method is proposed to overcome challenges\nbrought by EEG signals. By unifying structure search on different EEG tasks,\nthis work is the first to explore and analyze the searched structure difference\ncross-task-wise. Moreover, by introducing architecture search, this work is the\nfirst to analyze model performance by customizing model structure for each\nhuman subject. Detailed experimental results suggest that the proposed\nCTNAS-EEG could reach state-of-the-art performance on different EEG tasks, such\nas Motor Imagery (MI) and Emotion recognition. Extensive experiments and\ndetailed analysis are provided as a good reference for follow-up researchers.",
    "descriptor": "",
    "authors": [
      "Yiqun Duan",
      "Zhen Wang",
      "Yi Li",
      "Jianhang Tang",
      "Yu-Kai Wang",
      "Chin-Teng Lin"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06298"
  },
  {
    "id": "arXiv:2210.06300",
    "title": "Generalised Mutual Information for Discriminative Clustering",
    "abstract": "In the last decade, recent successes in deep clustering majorly involved the\nmutual information (MI) as an unsupervised objective for training neural\nnetworks with increasing regularisations. While the quality of the\nregularisations have been largely discussed for improvements, little attention\nhas been dedicated to the relevance of MI as a clustering objective. In this\npaper, we first highlight how the maximisation of MI does not lead to\nsatisfying clusters. We identified the Kullback-Leibler divergence as the main\nreason of this behaviour. Hence, we generalise the mutual information by\nchanging its core distance, introducing the generalised mutual information\n(GEMINI): a set of metrics for unsupervised neural network training. Unlike MI,\nsome GEMINIs do not require regularisations when training. Some of these\nmetrics are geometry-aware thanks to distances or kernels in the data space.\nFinally, we highlight that GEMINIs can automatically select a relevant number\nof clusters, a property that has been little studied in deep clustering context\nwhere the number of clusters is a priori unknown.",
    "descriptor": "\nComments: To be published in Neural Information Processing Systems 2022\n",
    "authors": [
      "Ohl Louis",
      "Mattei Pierre-Alexandre",
      "Bouveyron Charles",
      "Harchaoui Warith",
      "Leclercq Micka\u00ebl",
      "Droit Arnaud",
      "Precioso Frederic"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2210.06300"
  },
  {
    "id": "arXiv:2210.06303",
    "title": "Dynamic neuronal networks efficiently achieve classification in robotic  interactions with real-world objects",
    "abstract": "Biological cortical networks are potentially fully recurrent networks without\nany distinct output layer, where recognition may instead rely on the\ndistribution of activity across its neurons. Because such biological networks\ncan have rich dynamics, they are well-designed to cope with dynamical\ninteractions of the types that occur in nature, while traditional machine\nlearning networks may struggle to make sense of such data. Here we connected a\nsimple model neuronal network (based on the 'linear summation neuron model'\nfeaturing biologically realistic dynamics (LSM), consisting of 10 of excitatory\nand 10 inhibitory neurons, randomly connected) to a robot finger with multiple\ntypes of force sensors when interacting with materials of different levels of\ncompliance. Scope: to explore the performance of the network on classification\naccuracy. Therefore, we compared the performance of the network output with\nprincipal component analysis of statistical features of the sensory data as\nwell as its mechanical properties. Remarkably, even though the LSM was a very\nsmall and untrained network, and merely designed to provide rich internal\nnetwork dynamics while the neuron model itself was highly simplified, we found\nthat the LSM outperformed these other statistical approaches in terms of\naccuracy.",
    "descriptor": "\nComments: 9 pages, 8 figures.aim to use it for ph-coding reporting and further\n",
    "authors": [
      "Pakorn Uttayopas",
      "Xiaoxiao Cheng",
      "Udaya Bhaskar Rongala",
      "Henrik jorntell",
      "Etienne Burdet"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2210.06303"
  },
  {
    "id": "arXiv:2210.06310",
    "title": "Determining band structure parameters of two-dimensional materials by  deep learning",
    "abstract": "The field of two-dimensional materials has mastered the fabrication and\ncharacterisation of a broad range of novel high-quality compounds that feature\nincreasing complexity. Determination of the band structure parameters of such\ncomplex materials is a major ingredient required for quantitative theory. This\ntask currently presents a formidable challenge: ab initio methods often do not\nprovide quantitatively accurate values of parameters, whereas inferring band\nstructure parameters from experiments is hindered by the complexity of the band\nstructure and indirect nature of experimental probes. In this work we propose a\ngeneral framework for determination of band structure parameters from\nexperimental data based on deep neural networks. As a specific example we apply\nour method to the penetration field capacitance measurement of trilayer\ngraphene that effectively probes its density of states. First, we demonstrate\nthat a trained deep network gives accurate predictions for the penetration\nfield capacitance as a function of tight-binding parameters. Next, we use the\nfast and accurate predictions from the trained network to automatically\ndetermine tight-binding parameters directly from experimental data, with\nextracted parameters being in a good agreement with values in the literature.\nWe conclude by discussing potential applications of our method to other\nmaterials and experimental techniques beyond penetration field capacitance.",
    "descriptor": "\nComments: 11 pages, 4 figures\n",
    "authors": [
      "Paul Henderson",
      "Areg Ghazaryan",
      "Alexander A. Zibrov",
      "Andrea F. Young",
      "Maksym Serbyn"
    ],
    "subjectives": [
      "Mesoscale and Nanoscale Physics (cond-mat.mes-hall)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06310"
  },
  {
    "id": "arXiv:2210.06330",
    "title": "CoRRECT: A Deep Unfolding Framework for Motion-Corrected Quantitative  R2* Mapping",
    "abstract": "Quantitative MRI (qMRI) refers to a class of MRI methods for quantifying the\nspatial distribution of biological tissue parameters. Traditional qMRI methods\nusually deal separately with artifacts arising from accelerated data\nacquisition, involuntary physical motion, and magnetic-field inhomogeneities,\nleading to suboptimal end-to-end performance. This paper presents CoRRECT, a\nunified deep unfolding (DU) framework for qMRI consisting of a model-based\nend-to-end neural network, a method for motion-artifact reduction, and a\nself-supervised learning scheme. The network is trained to produce R2* maps\nwhose k-space data matches the real data by also accounting for motion and\nfield inhomogeneities. When deployed, CoRRECT only uses the k-space data\nwithout any pre-computed parameters for motion or inhomogeneity correction. Our\nresults on experimentally collected multi-Gradient-Recalled Echo (mGRE) MRI\ndata show that CoRRECT recovers motion and inhomogeneity artifact-free R2* maps\nin highly accelerated acquisition settings. This work opens the door to DU\nmethods that can integrate physical measurement models, biophysical signal\nmodels, and learned prior models for high-quality qMRI.",
    "descriptor": "",
    "authors": [
      "Xiaojian Xu",
      "Weijie Gan",
      "Satya V.V.N. Kothapalli",
      "Dmitriy A. Yablonskiy",
      "Ulugbek S. Kamilov"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06330"
  },
  {
    "id": "arXiv:2210.06334",
    "title": "A Self-attention Guided Multi-scale Gradient GAN for Diversified X-ray  Image Synthesis",
    "abstract": "Imbalanced image datasets are commonly available in the domain of biomedical\nimage analysis. Biomedical images contain diversified features that are\nsignificant in predicting targeted diseases. Generative Adversarial Networks\n(GANs) are utilized to address the data limitation problem via the generation\nof synthetic images. Training challenges such as mode collapse,\nnon-convergence, and instability degrade a GAN's performance in synthesizing\ndiversified and high-quality images. In this work, SAMGAN, an attention-guided\nmulti-scale gradient GAN architecture is proposed to model the relationship\nbetween long-range dependencies of biomedical image features and improves the\ntraining performance using a flow of multi-scale gradients at multiple\nresolutions in the layers of generator and discriminator models. The intent is\nto reduce the impact of mode collapse and stabilize the training of GAN using\nan attention mechanism with multi-scale gradient learning for diversified X-ray\nimage synthesis. Multi-scale Structural Similarity Index Measure (MS-SSIM) and\nFrechet Inception Distance (FID) are used to identify the occurrence of mode\ncollapse and evaluate the diversity of synthetic images generated. The proposed\narchitecture is compared with the multi-scale gradient GAN (MSG-GAN) to assess\nthe diversity of generated synthetic images. Results indicate that the SAMGAN\noutperforms MSG-GAN in synthesizing diversified images as evidenced by the\nMS-SSIM and FID scores.",
    "descriptor": "\nComments: Submitted to the AICS-2022 Conference\n",
    "authors": [
      "Muhammad Muneeb Saad",
      "Mubashir Husain Rehmani",
      "Ruairi O'Reilly"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06334"
  },
  {
    "id": "arXiv:2210.06335",
    "title": "A deep learning network with differentiable dynamic programming for  retina OCT surface segmentation",
    "abstract": "Multiple-surface segmentation in Optical Coherence Tomography (OCT) images is\na challenge problem, further complicated by the frequent presence of weak image\nboundaries. Recently, many deep learning (DL) based methods have been developed\nfor this task and yield remarkable performance. Unfortunately, due to the\nscarcity of training data in medical imaging, it is challenging for DL networks\nto learn the global structure of the target surfaces, including surface\nsmoothness. To bridge this gap, this study proposes to seamlessly unify a U-Net\nfor feature learning with a constrained differentiable dynamic programming\nmodule to achieve an end-to-end learning for retina OCT surface segmentation to\nexplicitly enforce surface smoothness. It effectively utilizes the feedback\nfrom the downstream model optimization module to guide feature learning,\nyielding a better enforcement of global structures of the target surfaces.\nExperiments on Duke AMD (age-related macular degeneration) and JHU MS (multiple\nsclerosis) OCT datasets for retinal layer segmentation demonstrated very\npromising segmentation accuracy.",
    "descriptor": "",
    "authors": [
      "Hui Xie",
      "Weiyu Xu",
      "Xiaodong Wu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.06335"
  },
  {
    "id": "arXiv:2210.06362",
    "title": "A Comparative Study on 1.5T-3T MRI Conversion through Deep Neural  Network Models",
    "abstract": "In this paper, we explore the capabilities of a number of deep neural network\nmodels in generating whole-brain 3T-like MR images from clinical 1.5T MRIs. The\nmodels include a fully convolutional network (FCN) method and three\nstate-of-the-art super-resolution solutions, ESPCN [26], SRGAN [17] and PRSR\n[7]. The FCN solution, U-Convert-Net, carries out mapping of 1.5T-to-3T slices\nthrough a U-Net-like architecture, with 3D neighborhood information integrated\nthrough a multi-view ensemble. The pros and cons of the models, as well the\nassociated evaluation metrics, are measured with experiments and discussed in\ndepth. To the best of our knowledge, this study is the first work to evaluate\nmultiple deep learning solutions for whole-brain MRI conversion, as well as the\nfirst attempt to utilize FCN/U-Net-like structure for this purpose.",
    "descriptor": "\nComments: Accepted to ICMLA 2022\n",
    "authors": [
      "Binhua Liao",
      "Yani Chen",
      "Zhewei Wang",
      "Charles D. Smith",
      "Jundong Liu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06362"
  },
  {
    "id": "arXiv:2210.06370",
    "title": "Can we use Common Voice to train a Multi-Speaker TTS system?",
    "abstract": "Training of multi-speaker text-to-speech (TTS) systems relies on curated\ndatasets based on high-quality recordings or audiobooks. Such datasets often\nlack speaker diversity and are expensive to collect. As an alternative, recent\nstudies have leveraged the availability of large, crowdsourced automatic speech\nrecognition (ASR) datasets. A major problem with such datasets is the presence\nof noisy and/or distorted samples, which degrade TTS quality. In this paper, we\npropose to automatically select high-quality training samples using a\nnon-intrusive mean opinion score (MOS) estimator, WV-MOS. We show the viability\nof this approach for training a multi-speaker GlowTTS model on the Common Voice\nEnglish dataset. Our approach improves the overall quality of generated\nutterances by 1.26 MOS point with respect to training on all the samples and by\n0.35 MOS point with respect to training on the LibriTTS dataset. This opens the\ndoor to automatic TTS dataset curation for a wider range of languages.",
    "descriptor": "\nComments: To appear in Proc. SLT 2022, Jan 09-12, 2023, Doha, Qatar\n",
    "authors": [
      "Sewade Ogun",
      "Vincent Colotte",
      "Emmanuel Vincent"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2210.06370"
  },
  {
    "id": "arXiv:2210.06382",
    "title": "An Ensemble Teacher-Student Learning Approach with Poisson Sub-sampling  to Differential Privacy Preserving Speech Recognition",
    "abstract": "We propose an ensemble learning framework with Poisson sub-sampling to\neffectively train a collection of teacher models to issue some differential\nprivacy (DP) guarantee for training data. Through boosting under DP, a student\nmodel derived from the training data suffers little model degradation from the\nmodels trained with no privacy protection. Our proposed solution leverages upon\ntwo mechanisms, namely: (i) a privacy budget amplification via Poisson\nsub-sampling to train a target prediction model that requires less noise to\nachieve a same level of privacy budget, and (ii) a combination of the\nsub-sampling technique and an ensemble teacher-student learning framework that\nintroduces DP-preserving noise at the output of the teacher models and\ntransfers DP-preserving properties via noisy labels. Privacy-preserving student\nmodels are then trained with the noisy labels to learn the knowledge with\nDP-protection from the teacher model ensemble. Experimental evidences on spoken\ncommand recognition and continuous speech recognition of Mandarin speech show\nthat our proposed framework greatly outperforms existing DP-preserving\nalgorithms in both speech processing tasks.",
    "descriptor": "\nComments: Accepted to ISCA, ISCSLP 2022, Singapore. 5 Pages\n",
    "authors": [
      "Chao-Han Huck Yang",
      "Jun Qi",
      "Sabato Marco Siniscalchi",
      "Chin-Hui Lee"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2210.06382"
  },
  {
    "id": "arXiv:2210.06385",
    "title": "The Extreme Cardiac MRI Analysis Challenge under Respiratory Motion  (CMRxMotion)",
    "abstract": "The quality of cardiac magnetic resonance (CMR) imaging is susceptible to\nrespiratory motion artifacts. The model robustness of automated segmentation\ntechniques in face of real-world respiratory motion artifacts is unclear. This\nmanuscript describes the design of extreme cardiac MRI analysis challenge under\nrespiratory motion (CMRxMotion Challenge). The challenge aims to establish a\npublic benchmark dataset to assess the effects of respiratory motion on image\nquality and examine the robustness of segmentation models. The challenge\nrecruited 40 healthy volunteers to perform different breath-hold behaviors\nduring one imaging visit, obtaining paired cine imaging with artifacts.\nRadiologists assessed the image quality and annotated the level of respiratory\nmotion artifacts. For those images with diagnostic quality, radiologists\nfurther segmented the left ventricle, left ventricle myocardium and right\nventricle. The images of training set (20 volunteers) along with the\nannotations are released to the challenge participants, to develop an automated\nimage quality assessment model (Task 1) and an automated segmentation model\n(Task 2). The images of validation set (5 volunteers) are released to the\nchallenge participants but the annotations are withheld for online evaluation\nof submitted predictions. Both the images and annotations of the test set (15\nvolunteers) were withheld and only used for offline evaluation of submitted\ncontainerized dockers. The image quality assessment task is quantitatively\nevaluated by the Cohen's kappa statistics and the segmentation task is\nevaluated by the Dice scores and Hausdorff distances.",
    "descriptor": "\nComments: Summary of CMRxMotion Challenge Design\n",
    "authors": [
      "Shuo Wang",
      "Chen Qin",
      "Chengyan Wang",
      "Kang Wang",
      "Haoran Wang",
      "Chen Chen",
      "Cheng Ouyang",
      "Xutong Kuang",
      "Chengliang Dai",
      "Yuanhan Mo",
      "Zhang Shi",
      "Chenchen Dai",
      "Xinrong Chen",
      "He Wang",
      "Wenjia Bai"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2210.06385"
  },
  {
    "id": "arXiv:2210.06411",
    "title": "Generating approximate state preparation circuits for NISQ computers  with a genetic algorithm",
    "abstract": "We study the approximate state preparation problem on noisy\nintermediate-scale quantum (NISQ) computers by applying a genetic algorithm to\ngenerate quantum circuits for state preparation. The algorithm can account for\nthe specific characteristics of the physical machine in the evaluation of\ncircuits, such as the native gate set and qubit connectivity. We use our\ngenetic algorithm to optimize the circuits provided by the low-rank state\npreparation algorithm introduced by Araujo et al. (arXiv:2111.03132), and find\nsubstantial improvements to the fidelity in preparing Haar random states with a\nlimited number of CNOT gates. Moreover, we observe that already for a 5-qubit\nquantum processor with limited qubit connectivity and significant noise levels\n(IBM Falcon 5T), the maximal fidelity for Haar random states is achieved by a\nshort approximate state preparation circuit instead of the exact preparation\ncircuit. We also present a theoretical analysis of approximate state\npreparation circuit complexity to motivate our findings. Our genetic algorithm\nfor quantum circuit discovery is freely available at\nhttps://github.com/beratyenilen/qc-ga .",
    "descriptor": "\nComments: 15 pages, 4 figures\n",
    "authors": [
      "Tom Rindell",
      "Berat Yenilen",
      "Niklas Halonen",
      "Arttu P\u00f6nni",
      "Ilkka Tittonen",
      "Matti Raasakka"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2210.06411"
  },
  {
    "id": "arXiv:2210.06416",
    "title": "Quantifying Uncertainty with Probabilistic Machine Learning Modeling in  Wireless Sensing",
    "abstract": "The application of machine learning (ML) techniques in wireless communication\ndomain has seen a tremendous growth over the years especially in the wireless\nsensing domain. However, the questions surrounding the ML model's inference\nreliability, and uncertainty associated with its predictions are never answered\nor communicated properly. This itself raises a lot of questions on the\ntransparency of these ML systems. Developing ML systems with probabilistic\nmodeling can solve this problem easily, where one can quantify uncertainty\nwhether it is arising from the data (irreducible error or aleotoric\nuncertainty) or from the model itself (reducible or epistemic uncertainty).\nThis paper describes the idea behind these types of uncertainty quantification\nin detail and uses a real example of WiFi channel state information (CSI) based\nsensing for motion/no-motion cases to demonstrate the uncertainty modeling.\nThis work will serve as a template to model uncertainty in predictions not only\nfor WiFi sensing but for most wireless sensing applications ranging from WiFi\nto millimeter wave radar based sensing that utilizes AI/ML models.",
    "descriptor": "\nComments: 6 pages, 3 figures, Submitted to IEEE CCNC 2023\n",
    "authors": [
      "Amit Kachroo",
      "Sai Prashanth Chinnapalli"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.06416"
  },
  {
    "id": "arXiv:2210.06419",
    "title": "Quantum divide and conquer",
    "abstract": "The divide-and-conquer framework, used extensively in classical algorithm\ndesign, recursively breaks a problem of size $n$ into smaller subproblems (say,\n$a$ copies of size $n/b$ each), along with some auxiliary work of cost\n$C^{\\textrm{aux}}(n)$, to give a recurrence relation $$C(n) \\leq a \\, C(n/b) +\nC^{\\textrm{aux}}(n)$$ for the classical complexity $C(n)$. We describe a\nquantum divide-and-conquer framework that, in certain cases, yields an\nanalogous recurrence relation $$C_Q(n) \\leq \\sqrt{a} \\, C_Q(n/b) +\nO(C^{\\textrm{aux}}_Q(n))$$ that characterizes the quantum query complexity. We\napply this framework to obtain near-optimal quantum query complexities for\nvarious string problems, such as (i) recognizing regular languages; (ii)\ndecision versions of String Rotation and String Suffix; and natural\nparameterized versions of (iii) Longest Increasing Subsequence and (iv) Longest\nCommon Subsequence.",
    "descriptor": "\nComments: 24 pages, 8 figures\n",
    "authors": [
      "Andrew M. Childs",
      "Robin Kothari",
      "Matt Kovacs-Deak",
      "Aarthi Sundaram",
      "Daochen Wang"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.06419"
  },
  {
    "id": "arXiv:2210.06424",
    "title": "Computing Persistence Diagram Bundles",
    "abstract": "Persistence diagram (PD) bundles, a generalization of vineyards, were\nrecently introduced as a way of studying the persistent homology of a set of\nfiltrations parameterized by a topological space $\\mathcal{T}$. In this paper,\nI present an algorithm for computing piecewise-linear PD bundles, a wide class\nthat includes many of the PD bundles that one may encounter in practice. I give\nfull implementation details for the case in which $\\dim(\\mathcal{T}) \\leq 2$,\nand I outline the generalization to higher dimensions.",
    "descriptor": "\nComments: Working paper\n",
    "authors": [
      "Abigail Hickok"
    ],
    "subjectives": [
      "Algebraic Topology (math.AT)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2210.06424"
  },
  {
    "id": "arXiv:2210.06454",
    "title": "Quantum Depth in the Random Oracle Model",
    "abstract": "We give a comprehensive characterization of the computational power of\nshallow quantum circuits combined with classical computation. Specifically, for\nclasses of search problems, we show that the following statements hold,\nrelative to a random oracle:\n(a) $\\mathsf{BPP}^{\\mathsf{QNC}^{\\mathsf{BPP}}} \\neq \\mathsf{BQP}$. This\nrefutes Jozsa's conjecture [QIP 05] in the random oracle model. As a result,\nthis gives the first instantiatable separation between the classes by replacing\nthe oracle with a cryptographic hash function, yielding a resolution to one of\nAaronson's ten semi-grand challenges in quantum computing.\n(b) $\\mathsf{BPP}^{\\mathsf{QNC}} \\nsubseteq \\mathsf{QNC}^{\\mathsf{BPP}}$ and\n$\\mathsf{QNC}^{\\mathsf{BPP}} \\nsubseteq \\mathsf{BPP}^{\\mathsf{QNC}}$. This\nshows that there is a subtle interplay between classical computation and\nshallow quantum computation. In fact, for the second separation, we establish\nthat, for some problems, the ability to perform adaptive measurements in a\nsingle shallow quantum circuit, is more useful than the ability to perform\npolynomially many shallow quantum circuits without adaptive measurements.\n(c) There exists a 2-message proof of quantum depth protocol. Such a protocol\nallows a classical verifier to efficiently certify that a prover must be\nperforming a computation of some minimum quantum depth. Our proof of quantum\ndepth can be instantiated using the recent proof of quantumness construction by\nYamakawa and Zhandry [STOC 22].",
    "descriptor": "\nComments: 104 pages (+ 9 page Appendix), 10 figures\n",
    "authors": [
      "Atul Singh Arora",
      "Andrea Coladangelo",
      "Matthew Coudron",
      "Alexandru Gheorghiu",
      "Uttam Singh",
      "Hendrik Waldner"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.06454"
  },
  {
    "id": "arXiv:2210.06459",
    "title": "Concentration of the exponential mechanism and differentially private  multivariate medians",
    "abstract": "We prove concentration inequalities for the output of the exponential\nmechanism about the maximizer of the population objective function. This bound\napplies to objective functions that satisfy a mild regularity condition. To\nillustrate our result, we study the problem of differentially private\nmultivariate median estimation. We present novel finite-sample performance\nguarantees for differentially private multivariate depth-based medians which\nare essentially sharp. Our results cover commonly used depth functions, such as\nthe halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.\nWe show that under Cauchy marginals, the cost of heavy-tailed location\nestimation outweighs the cost of privacy. We demonstrate our results\nnumerically using a Gaussian contamination model in dimensions up to $d = 100$,\nand compare them to a state-of-the-art private mean estimation algorithm.",
    "descriptor": "\nComments: 33 pages, 2 figures, 2 tables\n",
    "authors": [
      "Kelly Ramsay",
      "Aukosh Jagannath",
      "Shoja'eddin Chenouri"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2210.06459"
  },
  {
    "id": "arXiv:1806.08664",
    "title": "Acyclicity in finite groups and groupoids",
    "abstract": "Comments: An error in (v1) invalidated a more direct reduction of the groupoidal case to the group case. The current version further corrects and expands some of the core technical arguments, and introduces some core concepts more systematically (partly in dedicated new sections). See acknowledgements for a more detailed account; (v5) contains minor corrections and updates",
    "descriptor": "\nComments: An error in (v1) invalidated a more direct reduction of the groupoidal case to the group case. The current version further corrects and expands some of the core technical arguments, and introduces some core concepts more systematically (partly in dedicated new sections). See acknowledgements for a more detailed account; (v5) contains minor corrections and updates\n",
    "authors": [
      "Martin Otto"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Group Theory (math.GR)"
    ],
    "url": "https://arxiv.org/abs/1806.08664"
  },
  {
    "id": "arXiv:1902.03223",
    "title": "Robust Streaming PCA",
    "abstract": "Comments: The authors are ordered alphabetically. 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: The authors are ordered alphabetically. 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Daniel Bienstock",
      "Minchan Jeong",
      "Apurv Shukla",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/1902.03223"
  },
  {
    "id": "arXiv:1906.05591",
    "title": "Finite Sample Analysis Of Dynamic Regression Parameter Learning",
    "abstract": "Finite Sample Analysis Of Dynamic Regression Parameter Learning",
    "descriptor": "",
    "authors": [
      "Mark Kozdoba",
      "Edward Moroshko",
      "Shie Mannor",
      "Koby Crammer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/1906.05591"
  },
  {
    "id": "arXiv:1907.00380",
    "title": "Dimension is polynomial in height for posets with planar cover graphs",
    "abstract": "Dimension is polynomial in height for posets with planar cover graphs",
    "descriptor": "",
    "authors": [
      "Jakub Kozik",
      "Piotr Micek",
      "William T. Trotter"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/1907.00380"
  },
  {
    "id": "arXiv:1910.03365",
    "title": "A Penalized Inequality-Constrained Approach for Robust Beamforming with  DoF Limitation",
    "abstract": "Comments: Accepted by Signal Processing",
    "descriptor": "\nComments: Accepted by Signal Processing\n",
    "authors": [
      "Wenqiang Pu",
      "Jinjun Xiao",
      "Tao Zhang",
      "Zhi-Quan Luo"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/1910.03365"
  },
  {
    "id": "arXiv:1911.07782",
    "title": "QMA-hardness of Consistency of Local Density Matrices with Applications  to Quantum Zero-Knowledge",
    "abstract": "Comments: Published at SICOMP. Retracted section regarding statistical zero-knowledge arguments for QMA",
    "descriptor": "\nComments: Published at SICOMP. Retracted section regarding statistical zero-knowledge arguments for QMA\n",
    "authors": [
      "Anne Broadbent",
      "Alex B. Grilo"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/1911.07782"
  },
  {
    "id": "arXiv:2002.06635",
    "title": "HPIM-DM: a fast and reliable dense-mode multicast routing protocol  (extended version)",
    "abstract": "HPIM-DM: a fast and reliable dense-mode multicast routing protocol  (extended version)",
    "descriptor": "",
    "authors": [
      "Pedro Oliveira",
      "Alexandre Silva",
      "Rui Valadas"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2002.06635"
  },
  {
    "id": "arXiv:2005.06156",
    "title": "Super-resolution and Robust Sparse Continuous Fourier Transform in Any  Constant Dimension: Nearly Linear Time and Sample Complexity",
    "abstract": "Comments: SODA 2023",
    "descriptor": "\nComments: SODA 2023\n",
    "authors": [
      "Yaonan Jin",
      "Daogao Liu",
      "Zhao Song"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2005.06156"
  },
  {
    "id": "arXiv:2005.14458",
    "title": "Distributional Random Forests: Heterogeneity Adjustment and Multivariate  Distributional Regression",
    "abstract": "Distributional Random Forests: Heterogeneity Adjustment and Multivariate  Distributional Regression",
    "descriptor": "",
    "authors": [
      "Domagoj \u0106evid",
      "Loris Michel",
      "Jeffrey N\u00e4f",
      "Nicolai Meinshausen",
      "Peter B\u00fchlmann"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2005.14458"
  },
  {
    "id": "arXiv:2006.00469",
    "title": "Contextuality in entanglement-assisted one-shot classical communication",
    "abstract": "Comments: 25 pages, 4 figures, Accepted in Quantum",
    "descriptor": "\nComments: 25 pages, 4 figures, Accepted in Quantum\n",
    "authors": [
      "Shiv Akshar Yadavalli",
      "Ravi Kunjwal"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2006.00469"
  },
  {
    "id": "arXiv:2007.12107",
    "title": "Few-Shot Object Detection and Viewpoint Estimation for Objects in the  Wild",
    "abstract": "Comments: Accepted by TPAMI, add experimental results and additional ablation studies",
    "descriptor": "\nComments: Accepted by TPAMI, add experimental results and additional ablation studies\n",
    "authors": [
      "Yang Xiao",
      "Vincent Lepetit",
      "Renaud Marlet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2007.12107"
  },
  {
    "id": "arXiv:2009.05683",
    "title": "MACE: A Flexible Framework for Membership Privacy Estimation in  Generative Models",
    "abstract": "MACE: A Flexible Framework for Membership Privacy Estimation in  Generative Models",
    "descriptor": "",
    "authors": [
      "Yixi Xu",
      "Sumit Mukherjee",
      "Xiyang Liu",
      "Shruti Tople",
      "Rahul Dodhia",
      "Juan Lavista Ferres"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2009.05683"
  },
  {
    "id": "arXiv:2012.04164",
    "title": "Learning Independent Instance Maps for Crowd Localization",
    "abstract": "Learning Independent Instance Maps for Crowd Localization",
    "descriptor": "",
    "authors": [
      "Junyu Gao",
      "Tao Han",
      "Qi Wang",
      "Yuan Yuan",
      "Xuelong Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2012.04164"
  },
  {
    "id": "arXiv:2101.01945",
    "title": "Fine-Grained Complexity of Regular Path Queries",
    "abstract": "Fine-Grained Complexity of Regular Path Queries",
    "descriptor": "",
    "authors": [
      "Katrin Casel",
      "Markus L. Schmid"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Databases (cs.DB)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2101.01945"
  },
  {
    "id": "arXiv:2101.11156",
    "title": "Fundamental limits and algorithms for sparse linear regression with  sublinear sparsity",
    "abstract": "Comments: Accepted for Publication in the Journal of Machine Learning Research (JMLR)",
    "descriptor": "\nComments: Accepted for Publication in the Journal of Machine Learning Research (JMLR)\n",
    "authors": [
      "Lan V. Truong"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2101.11156"
  },
  {
    "id": "arXiv:2102.01065",
    "title": "Do Question Answering Modeling Improvements Hold Across Benchmarks?",
    "abstract": "Comments: 31 pages, 13 figures",
    "descriptor": "\nComments: 31 pages, 13 figures\n",
    "authors": [
      "Nelson F. Liu",
      "Tony Lee",
      "Robin Jia",
      "Percy Liang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2102.01065"
  },
  {
    "id": "arXiv:2102.04263",
    "title": "Generalised correlated batched bandits via the ARC algorithm with  application to dynamic pricing",
    "abstract": "Generalised correlated batched bandits via the ARC algorithm with  application to dynamic pricing",
    "descriptor": "",
    "authors": [
      "Samuel Cohen",
      "Tanut Treetanthiploet"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Machine Learning (cs.LG)",
      "General Economics (econ.GN)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2102.04263"
  },
  {
    "id": "arXiv:2102.10271",
    "title": "Meta-Learning Dynamics Forecasting Using Task Inference",
    "abstract": "Meta-Learning Dynamics Forecasting Using Task Inference",
    "descriptor": "",
    "authors": [
      "Rui Wang",
      "Robin Walters",
      "Rose Yu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.10271"
  },
  {
    "id": "arXiv:2103.00558",
    "title": "Is Simple Uniform Sampling Effective for Center-Based Clustering with  Outliers: When and Why?",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1905.10143",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1905.10143\n",
    "authors": [
      "Jiawei Huang",
      "Wenjie Liu",
      "Hu Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Geometry (cs.CG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2103.00558"
  },
  {
    "id": "arXiv:2103.06801",
    "title": "Upward Planar Drawings with Three and More Slopes",
    "abstract": "Upward Planar Drawings with Three and More Slopes",
    "descriptor": "",
    "authors": [
      "Jonathan Klawitter",
      "Johannes Zink"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2103.06801"
  },
  {
    "id": "arXiv:2103.17104",
    "title": "Deep Image Harmonization by Bridging the Reality Gap",
    "abstract": "Comments: Accepted by BMVC2022",
    "descriptor": "\nComments: Accepted by BMVC2022\n",
    "authors": [
      "Junyan Cao",
      "Wenyan Cong",
      "Li Niu",
      "Jianfu Zhang",
      "Liqing Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.17104"
  },
  {
    "id": "arXiv:2104.03814",
    "title": "Algorithmic Obfuscation for LDPC Decoders",
    "abstract": "Algorithmic Obfuscation for LDPC Decoders",
    "descriptor": "",
    "authors": [
      "Jingbo Zhou",
      "Xinmiao Zhang"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2104.03814"
  },
  {
    "id": "arXiv:2104.05624",
    "title": "Self-Adjusting Population Sizes for Non-Elitist Evolutionary Algorithms:  Why Success Rates Matter",
    "abstract": "Comments: This is an extended version of a paper that appeared in the Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2021)",
    "descriptor": "\nComments: This is an extended version of a paper that appeared in the Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2021)\n",
    "authors": [
      "Mario Alejandro Hevia Fajardo",
      "Dirk Sudholt"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2104.05624"
  },
  {
    "id": "arXiv:2104.09850",
    "title": "A Polyhedral Abstraction for Petri nets and its Application to SMT-Based  Model Checking",
    "abstract": "A Polyhedral Abstraction for Petri nets and its Application to SMT-Based  Model Checking",
    "descriptor": "",
    "authors": [
      "Nicolas Amat",
      "Bernard Berthomieu",
      "Silvano Dal Zilio"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2104.09850"
  },
  {
    "id": "arXiv:2105.07289",
    "title": "A new mixed finite-element method for $H^2$ elliptic problems",
    "abstract": "A new mixed finite-element method for $H^2$ elliptic problems",
    "descriptor": "",
    "authors": [
      "Patrick E. Farrell",
      "Abdalaziz Hamdan",
      "Scott P. MacLachlan"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2105.07289"
  },
  {
    "id": "arXiv:2105.11605",
    "title": "TransLoc3D : Point Cloud based Large-scale Place Recognition using  Adaptive Receptive Fields",
    "abstract": "Comments: Appeared in Computational Visual Media 2022, poster. Communications in Information and Systems. Accepted",
    "descriptor": "\nComments: Appeared in Computational Visual Media 2022, poster. Communications in Information and Systems. Accepted\n",
    "authors": [
      "Tian-Xing Xu",
      "Yuan-Chen Guo",
      "Zhiqiang Li",
      "Ge Yu",
      "Yu-Kun Lai",
      "Song-Hai Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2105.11605"
  },
  {
    "id": "arXiv:2105.15013",
    "title": "SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning",
    "abstract": "Comments: Accepted paper for NeurIPS 2022",
    "descriptor": "\nComments: Accepted paper for NeurIPS 2022\n",
    "authors": [
      "Jianhong Wang",
      "Yuan Zhang",
      "Yunjie Gu",
      "Tae-Kyun Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2105.15013"
  },
  {
    "id": "arXiv:2105.15183",
    "title": "Efficient and Modular Implicit Differentiation",
    "abstract": "Comments: V3: added more related work and Jacobian precision figure",
    "descriptor": "\nComments: V3: added more related work and Jacobian precision figure\n",
    "authors": [
      "Mathieu Blondel",
      "Quentin Berthet",
      "Marco Cuturi",
      "Roy Frostig",
      "Stephan Hoyer",
      "Felipe Llinares-L\u00f3pez",
      "Fabian Pedregosa",
      "Jean-Philippe Vert"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Numerical Analysis (math.NA)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.15183"
  },
  {
    "id": "arXiv:2106.08068",
    "title": "An Analytical Theory of Curriculum Learning in Teacher-Student Networks",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Luca Saglietti",
      "Stefano Sarao Mannelli",
      "Andrew Saxe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2106.08068"
  },
  {
    "id": "arXiv:2107.00462",
    "title": "Deep Hierarchical Super Resolution for Scientific Data",
    "abstract": "Comments: Accepted by IEEE Transactions on Visualization and Computer Graphics (TVCG) 2022",
    "descriptor": "\nComments: Accepted by IEEE Transactions on Visualization and Computer Graphics (TVCG) 2022\n",
    "authors": [
      "Skylar W. Wurster",
      "Hanqi Guo",
      "Han-Wei Shen",
      "Thomas Peterka",
      "Jiayi Xu"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2107.00462"
  },
  {
    "id": "arXiv:2107.03930",
    "title": "BF-QC: Belief Functions on Quantum Circuits",
    "abstract": "BF-QC: Belief Functions on Quantum Circuits",
    "descriptor": "",
    "authors": [
      "Qianli Zhou",
      "Guojing Tian",
      "Yong Deng"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2107.03930"
  },
  {
    "id": "arXiv:2107.13681",
    "title": "Rate-Independent Computation in Continuous Chemical Reaction Networks",
    "abstract": "Comments: preliminary version appeared in ITCS 2014: this http URL",
    "descriptor": "\nComments: preliminary version appeared in ITCS 2014: this http URL\n",
    "authors": [
      "Ho-Lin Chen",
      "David Doty",
      "David Soloveichik",
      "Wyatt Reeves"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)",
      "Molecular Networks (q-bio.MN)"
    ],
    "url": "https://arxiv.org/abs/2107.13681"
  },
  {
    "id": "arXiv:2108.12637",
    "title": "Oh My Mistake!: Toward Realistic Dialogue State Tracking including  Turnback Utterances",
    "abstract": "Comments: SereTOD Workshop at EMNLP 2022",
    "descriptor": "\nComments: SereTOD Workshop at EMNLP 2022\n",
    "authors": [
      "Takyoung Kim",
      "Yukyung Lee",
      "Hoonsang Yoon",
      "Pilsung Kang",
      "Junseong Bang",
      "Misuk Kim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2108.12637"
  },
  {
    "id": "arXiv:2109.04954",
    "title": "Saliency Guided Experience Packing for Replay in Continual Learning",
    "abstract": "Comments: To appear in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023",
    "descriptor": "\nComments: To appear in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023\n",
    "authors": [
      "Gobinda Saha",
      "Kaushik Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2109.04954"
  },
  {
    "id": "arXiv:2109.08844",
    "title": "Near-Minimax Optimal Estimation With Shallow ReLU Neural Networks",
    "abstract": "Comments: IEEE Transactions on Information Theory (in press)",
    "descriptor": "\nComments: IEEE Transactions on Information Theory (in press)\n",
    "authors": [
      "Rahul Parhi",
      "Robert D. Nowak"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2109.08844"
  },
  {
    "id": "arXiv:2109.13392",
    "title": "The Tensor Brain: A Unified Theory of Perception, Memory and Semantic  Decoding",
    "abstract": "Comments: Accepted for publication at Neural Computation",
    "descriptor": "\nComments: Accepted for publication at Neural Computation\n",
    "authors": [
      "Volker Tresp",
      "Sahand Sharifzadeh",
      "Hang Li",
      "Dario Konopatzki",
      "Yunpu Ma"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2109.13392"
  },
  {
    "id": "arXiv:2110.00060",
    "title": "Automating Internet of Things Network Traffic Collection with Robotic  Arm Interactions",
    "abstract": "Comments: 10 pages, 5 figures, 3 tables; revised version for publication",
    "descriptor": "\nComments: 10 pages, 5 figures, 3 tables; revised version for publication\n",
    "authors": [
      "Xi Jiang",
      "Noah Apthorpe"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Cryptography and Security (cs.CR)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2110.00060"
  },
  {
    "id": "arXiv:2110.01230",
    "title": "Efficient Identification of Butterfly Sparse Matrix Factorizations",
    "abstract": "Efficient Identification of Butterfly Sparse Matrix Factorizations",
    "descriptor": "",
    "authors": [
      "L\u00e9on Zheng",
      "Elisa Riccietti",
      "R\u00e9mi Gribonval"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.01230"
  },
  {
    "id": "arXiv:2110.04191",
    "title": "The Parallel Reversible Pebbling Game: Analyzing the Post-Quantum  Security of iMHFs",
    "abstract": "Comments: 42 pages, 5 figures",
    "descriptor": "\nComments: 42 pages, 5 figures\n",
    "authors": [
      "Jeremiah Blocki",
      "Blake Holman",
      "Seunghoon Lee"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Complexity (cs.CC)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2110.04191"
  },
  {
    "id": "arXiv:2110.05679",
    "title": "Large Language Models Can Be Strong Differentially Private Learners",
    "abstract": "Comments: 31 pages; update ethics statement to clarify benefits and potential long-term harms",
    "descriptor": "\nComments: 31 pages; update ethics statement to clarify benefits and potential long-term harms\n",
    "authors": [
      "Xuechen Li",
      "Florian Tram\u00e8r",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2110.05679"
  },
  {
    "id": "arXiv:2110.08627",
    "title": "Achieving the Pareto Frontier of Regret Minimization and Best Arm  Identification in Multi-Armed Bandits",
    "abstract": "Comments: 44 pages, 10 figures",
    "descriptor": "\nComments: 44 pages, 10 figures\n",
    "authors": [
      "Zixin Zhong",
      "Wang Chi Cheung",
      "Vincent Y. F. Tan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.08627"
  },
  {
    "id": "arXiv:2110.14392",
    "title": "TaylorSwiftNet: Taylor Driven Temporal Modeling for Swift Future Frame  Prediction",
    "abstract": "Comments: BMVC 2022",
    "descriptor": "\nComments: BMVC 2022\n",
    "authors": [
      "Saber Pourheydari",
      "Emad Bahrami",
      "Mohsen Fayyaz",
      "Gianpiero Francesca",
      "Mehdi Noroozi",
      "Juergen Gall"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.14392"
  },
  {
    "id": "arXiv:2111.00655",
    "title": "Collage: Seamless Integration of Deep Learning Backends with Automatic  Placement",
    "abstract": "Collage: Seamless Integration of Deep Learning Backends with Automatic  Placement",
    "descriptor": "",
    "authors": [
      "Byungsoo Jeon",
      "Sunghyun Park",
      "Peiyuan Liao",
      "Sheng Xu",
      "Tianqi Chen",
      "Zhihao Jia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.00655"
  },
  {
    "id": "arXiv:2111.02921",
    "title": "Map-Assisted Constellation Design for mmWave WDM with OAM in Short-Range  LOS Environment",
    "abstract": "Map-Assisted Constellation Design for mmWave WDM with OAM in Short-Range  LOS Environment",
    "descriptor": "",
    "authors": [
      "Yuan Wang",
      "Chen Gong",
      "Nuo Huang",
      "Zhengyuan Xu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2111.02921"
  },
  {
    "id": "arXiv:2111.05814",
    "title": "SwAMP: Swapped Assignment of Multi-Modal Pairs for Cross-Modal Retrieval",
    "abstract": "SwAMP: Swapped Assignment of Multi-Modal Pairs for Cross-Modal Retrieval",
    "descriptor": "",
    "authors": [
      "Minyoung Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.05814"
  },
  {
    "id": "arXiv:2111.10734",
    "title": "Deep Probability Estimation",
    "abstract": "Comments: SL, AK, WZ, ML, SM contributed equally to this work; 36 pages, 17 figures, 12 tables",
    "descriptor": "\nComments: SL, AK, WZ, ML, SM contributed equally to this work; 36 pages, 17 figures, 12 tables\n",
    "authors": [
      "Sheng Liu",
      "Aakash Kaku",
      "Weicheng Zhu",
      "Matan Leibovich",
      "Sreyas Mohan",
      "Boyang Yu",
      "Haoxiang Huang",
      "Laure Zanna",
      "Narges Razavian",
      "Jonathan Niles-Weed",
      "Carlos Fernandez-Granda"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2111.10734"
  },
  {
    "id": "arXiv:2111.10962",
    "title": "Enhancing Multilingual Language Model with Massive Multilingual  Knowledge Triples",
    "abstract": "Comments: Accepted by EMNLP 2022",
    "descriptor": "\nComments: Accepted by EMNLP 2022\n",
    "authors": [
      "Linlin Liu",
      "Xin Li",
      "Ruidan He",
      "Lidong Bing",
      "Shafiq Joty",
      "Luo Si"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2111.10962"
  },
  {
    "id": "arXiv:2111.11994",
    "title": "Degree-preserving graph dynamics -- a versatile process to construct  random networks",
    "abstract": "Comments: 21 pages, 5 figures",
    "descriptor": "\nComments: 21 pages, 5 figures\n",
    "authors": [
      "P\u00e9ter L. Erd\u0151s",
      "Shubha R. Kharel",
      "Tam\u00e1s R. Mezei",
      "Zolt\u00e1n Toroczkai"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Physics and Society (physics.soc-ph)"
    ],
    "url": "https://arxiv.org/abs/2111.11994"
  },
  {
    "id": "arXiv:2112.08222",
    "title": "Guaranteed Nonlinear Tracking in the Presence of DNN-Learned Dynamics  With Contraction Metrics and Disturbance Estimation",
    "abstract": "Comments: Shorter version submitted to ACC 2023",
    "descriptor": "\nComments: Shorter version submitted to ACC 2023\n",
    "authors": [
      "Pan Zhao",
      "Ziyao Guo",
      "Aditya Gahlawat",
      "Hyungsoo Kang",
      "Naira Hovakimyan"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.08222"
  },
  {
    "id": "arXiv:2112.09237",
    "title": "PECO: Examining Single Sentence Label Leakage in Natural Language  Inference Datasets through Progressive Evaluation of Cluster Outliers",
    "abstract": "Comments: 12 pages, 7 figures, 4 tables",
    "descriptor": "\nComments: 12 pages, 7 figures, 4 tables\n",
    "authors": [
      "Michael Saxon",
      "Xinyi Wang",
      "Wenda Xu",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2112.09237"
  },
  {
    "id": "arXiv:2112.09387",
    "title": "Morphisms and minimisation of weighted automata",
    "abstract": "Morphisms and minimisation of weighted automata",
    "descriptor": "",
    "authors": [
      "Sylvain Lombardy",
      "Jacques Sakarovitch"
    ],
    "subjectives": [
      "Discrete Mathematics (cs.DM)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.09387"
  },
  {
    "id": "arXiv:2112.12299",
    "title": "A Robust Initialization of Residual Blocks for Effective ResNet Training  without Batch Normalization",
    "abstract": "Comments: 16 pages (4 pages of supplementary material), 9 figures, 2 table",
    "descriptor": "\nComments: 16 pages (4 pages of supplementary material), 9 figures, 2 table\n",
    "authors": [
      "Enrico Civitelli",
      "Alessio Sortino",
      "Matteo Lapucci",
      "Francesco Bagattini",
      "Giulio Galvan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2112.12299"
  },
  {
    "id": "arXiv:2112.13593",
    "title": "Multi-modal Attention Network for Stock Movements Prediction",
    "abstract": "Comments: The AAAI-22 Workshop on Knowledge Discovery from Unstructured Data in Financial Services (KDF 2022)",
    "descriptor": "\nComments: The AAAI-22 Workshop on Knowledge Discovery from Unstructured Data in Financial Services (KDF 2022)\n",
    "authors": [
      "Shwai He",
      "Shi Gu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2112.13593"
  },
  {
    "id": "arXiv:2201.03533",
    "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Uri Shaham",
      "Elad Segal",
      "Maor Ivgi",
      "Avia Efrat",
      "Ori Yoran",
      "Adi Haviv",
      "Ankit Gupta",
      "Wenhan Xiong",
      "Mor Geva",
      "Jonathan Berant",
      "Omer Levy"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.03533"
  },
  {
    "id": "arXiv:2201.04122",
    "title": "In Defense of the Unitary Scalarization for Deep Multi-Task Learning",
    "abstract": "Comments: NeurIPS 2022 camera-ready version",
    "descriptor": "\nComments: NeurIPS 2022 camera-ready version\n",
    "authors": [
      "Vitaly Kurin",
      "Alessandro De Palma",
      "Ilya Kostrikov",
      "Shimon Whiteson",
      "M. Pawan Kumar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2201.04122"
  },
  {
    "id": "arXiv:2201.04928",
    "title": "Chambolle-Pock's Primal-Dual Method with Mismatched Adjoint",
    "abstract": "Chambolle-Pock's Primal-Dual Method with Mismatched Adjoint",
    "descriptor": "",
    "authors": [
      "Dirk A. Lorenz",
      "Felix Schneppe"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2201.04928"
  },
  {
    "id": "arXiv:2201.05021",
    "title": "Robustness against Read Committed for Transaction Templates with  Functional Constraints",
    "abstract": "Comments: 49 pages, 8 figures",
    "descriptor": "\nComments: 49 pages, 8 figures\n",
    "authors": [
      "Brecht Vandevoort",
      "Bas Ketsman",
      "Christoph Koch",
      "Frank Neven"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2201.05021"
  },
  {
    "id": "arXiv:2201.07135",
    "title": "Synthesizing explainable counterfactual policies for algorithmic  recourse with program synthesis",
    "abstract": "Synthesizing explainable counterfactual policies for algorithmic  recourse with program synthesis",
    "descriptor": "",
    "authors": [
      "Giovanni De Toni",
      "Bruno Lepri",
      "Andrea Passerini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.07135"
  },
  {
    "id": "arXiv:2201.09636",
    "title": "Neural Implicit Surface Evolution using Differential Equations",
    "abstract": "Neural Implicit Surface Evolution using Differential Equations",
    "descriptor": "",
    "authors": [
      "Tiago Novello",
      "Vinicius da Silva",
      "Guilherme Schardong",
      "Luiz Schirmer",
      "Helio Lopes",
      "Luiz Velho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2201.09636"
  },
  {
    "id": "arXiv:2201.09785",
    "title": "Unifying and Boosting Gradient-Based Training-Free Neural Architecture  Search",
    "abstract": "Comments: Published as a conference paper at NeurIPS 2022",
    "descriptor": "\nComments: Published as a conference paper at NeurIPS 2022\n",
    "authors": [
      "Yao Shu",
      "Zhongxiang Dai",
      "Zhaoxuan Wu",
      "Bryan Kian Hsiang Low"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2201.09785"
  },
  {
    "id": "arXiv:2201.11059",
    "title": "Generalization Error Bounds on Deep Learning with Markov Datasets",
    "abstract": "Comments: To appear at 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: To appear at 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Lan V. Truong"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Probability (math.PR)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2201.11059"
  },
  {
    "id": "arXiv:2201.12032",
    "title": "Neural Approximation of Graph Topological Features",
    "abstract": "Comments: Accepted in NeurIPS 2022",
    "descriptor": "\nComments: Accepted in NeurIPS 2022\n",
    "authors": [
      "Zuoyu Yan",
      "Tengfei Ma",
      "Liangcai Gao",
      "Zhi Tang",
      "Yusu Wang",
      "Chao Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.12032"
  },
  {
    "id": "arXiv:2201.12680",
    "title": "Understanding Deep Contrastive Learning via Coordinate-wise Optimization",
    "abstract": "Understanding Deep Contrastive Learning via Coordinate-wise Optimization",
    "descriptor": "",
    "authors": [
      "Yuandong Tian"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2201.12680"
  },
  {
    "id": "arXiv:2201.13180",
    "title": "Learning on Arbitrary Graph Topologies via Predictive Coding",
    "abstract": "Comments: 15 pages, 11 figures",
    "descriptor": "\nComments: 15 pages, 11 figures\n",
    "authors": [
      "Tommaso Salvatori",
      "Luca Pinchetti",
      "Beren Millidge",
      "Yuhang Song",
      "Tianyi Bao",
      "Rafal Bogacz",
      "Thomas Lukasiewicz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2201.13180"
  },
  {
    "id": "arXiv:2201.13259",
    "title": "Trajectory balance: Improved credit assignment in GFlowNets",
    "abstract": "Comments: NeurIPS 2022; see footnotes for code",
    "descriptor": "\nComments: NeurIPS 2022; see footnotes for code\n",
    "authors": [
      "Nikolay Malkin",
      "Moksh Jain",
      "Emmanuel Bengio",
      "Chen Sun",
      "Yoshua Bengio"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2201.13259"
  },
  {
    "id": "arXiv:2202.01339",
    "title": "Understanding Cross-Domain Few-Shot Learning Based on Domain Similarity  and Few-Shot Difficulty",
    "abstract": "Comments: NeurIPS 2022 camera-ready",
    "descriptor": "\nComments: NeurIPS 2022 camera-ready\n",
    "authors": [
      "Jaehoon Oh",
      "Sungnyun Kim",
      "Namgyu Ho",
      "Jin-Hwa Kim",
      "Hwanjun Song",
      "Se-Young Yun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.01339"
  },
  {
    "id": "arXiv:2202.02363",
    "title": "Meta-Reinforcement Learning with Self-Modifying Networks",
    "abstract": "Comments: Published at Neurips 2022",
    "descriptor": "\nComments: Published at Neurips 2022\n",
    "authors": [
      "Mathieu Chalvidal",
      "Thomas Serre",
      "Rufin VanRullen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2202.02363"
  },
  {
    "id": "arXiv:2202.02365",
    "title": "MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural  Networks",
    "abstract": "MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural  Networks",
    "descriptor": "",
    "authors": [
      "Roger Waleffe",
      "Jason Mohoney",
      "Theodoros Rekatsinas",
      "Shivaram Venkataraman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2202.02365"
  },
  {
    "id": "arXiv:2202.03618",
    "title": "On Unbalanced Optimal Transport: Gradient Methods, Sparsity and  Approximation Error",
    "abstract": "On Unbalanced Optimal Transport: Gradient Methods, Sparsity and  Approximation Error",
    "descriptor": "",
    "authors": [
      "Quang Minh Nguyen",
      "Hoang H. Nguyen",
      "Yi Zhou",
      "Lam M. Nguyen"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.03618"
  },
  {
    "id": "arXiv:2202.04538",
    "title": "Generating Training Data with Language Models: Towards Zero-Shot  Language Understanding",
    "abstract": "Comments: NeurIPS 2022. (Code: this https URL)",
    "descriptor": "\nComments: NeurIPS 2022. (Code: this https URL)\n",
    "authors": [
      "Yu Meng",
      "Jiaxin Huang",
      "Yu Zhang",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.04538"
  },
  {
    "id": "arXiv:2202.05420",
    "title": "A Characterization of Semi-Supervised Adversarially-Robust PAC  Learnability",
    "abstract": "Comments: NeurIPS 2022 camera-ready",
    "descriptor": "\nComments: NeurIPS 2022 camera-ready\n",
    "authors": [
      "Idan Attias",
      "Steve Hanneke",
      "Yishay Mansour"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.05420"
  },
  {
    "id": "arXiv:2202.06083",
    "title": "Escaping Saddle Points with Bias-Variance Reduced Local Perturbed SGD  for Communication Efficient Nonconvex Distributed Learning",
    "abstract": "Comments: 50 pages",
    "descriptor": "\nComments: 50 pages\n",
    "authors": [
      "Tomoya Murata",
      "Taiji Suzuki"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.06083"
  },
  {
    "id": "arXiv:2202.06239",
    "title": "Supported Policy Optimization for Offline Reinforcement Learning",
    "abstract": "Comments: Code is available at this https URL",
    "descriptor": "\nComments: Code is available at this https URL\n",
    "authors": [
      "Jialong Wu",
      "Haixu Wu",
      "Zihan Qiu",
      "Jianmin Wang",
      "Mingsheng Long"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2202.06239"
  },
  {
    "id": "arXiv:2202.08087",
    "title": "Extended Unconstrained Features Model for Exploring Deep Neural Collapse",
    "abstract": "Comments: ICML 2022. Relaxed Theorem 4.2 and clarified proofs",
    "descriptor": "\nComments: ICML 2022. Relaxed Theorem 4.2 and clarified proofs\n",
    "authors": [
      "Tom Tirer",
      "Joan Bruna"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.08087"
  },
  {
    "id": "arXiv:2202.08266",
    "title": "Open-Ended Reinforcement Learning with Neural Reward Functions",
    "abstract": "Open-Ended Reinforcement Learning with Neural Reward Functions",
    "descriptor": "",
    "authors": [
      "Robert Meier",
      "Asier Mujika"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2202.08266"
  },
  {
    "id": "arXiv:2202.09387",
    "title": "A mixed-integer programming model for identifying intuitive ambulance  dispatching policies",
    "abstract": "A mixed-integer programming model for identifying intuitive ambulance  dispatching policies",
    "descriptor": "",
    "authors": [
      "Laura A. Albert"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2202.09387"
  },
  {
    "id": "arXiv:2202.10670",
    "title": "From Optimization Dynamics to Generalization Bounds via \u0141ojasiewicz  Gradient Inequality",
    "abstract": "From Optimization Dynamics to Generalization Bounds via \u0141ojasiewicz  Gradient Inequality",
    "descriptor": "",
    "authors": [
      "Fusheng Liu",
      "Haizhao Yang",
      "Soufiane Hayou",
      "Qianxiao Li"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.10670"
  },
  {
    "id": "arXiv:2202.10887",
    "title": "Policy Evaluation for Temporal and/or Spatial Dependent Experiments in  Ride-sourcing Platforms",
    "abstract": "Comments: Some of the derivations appear to be incorrect and need to be revised",
    "descriptor": "\nComments: Some of the derivations appear to be incorrect and need to be revised\n",
    "authors": [
      "Shikai Luo",
      "Ying Yang",
      "Chengchun Shi",
      "Fang Yao",
      "Jieping Ye",
      "Hongtu Zhu"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2202.10887"
  },
  {
    "id": "arXiv:2202.11234",
    "title": "A QUBO formulation for the Tree Containment problem",
    "abstract": "Comments: final version accepted for publication in Theoretical Computer Science",
    "descriptor": "\nComments: final version accepted for publication in Theoretical Computer Science\n",
    "authors": [
      "Michael J. Dinneen",
      "Pankaj S. Ghodla",
      "Simone Linz"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2202.11234"
  },
  {
    "id": "arXiv:2203.03631",
    "title": "Student Becomes Decathlon Master in Retinal Vessel Segmentation via  Dual-teacher Multi-target Domain Adaptation",
    "abstract": "Comments: To be published in MICCAI-MLMI 2022",
    "descriptor": "\nComments: To be published in MICCAI-MLMI 2022\n",
    "authors": [
      "Linkai Peng",
      "Li Lin",
      "Pujin Cheng",
      "Huaqing He",
      "Xiaoying Tang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.03631"
  },
  {
    "id": "arXiv:2203.04071",
    "title": "AdaPT: Fast Emulation of Approximate DNN Accelerators in PyTorch",
    "abstract": "Comments: Accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "descriptor": "\nComments: Accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems\n",
    "authors": [
      "Dimitrios Danopoulos",
      "Georgios Zervakis",
      "Kostas Siozios",
      "Dimitrios Soudris",
      "J\u00f6rg Henkel"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2203.04071"
  },
  {
    "id": "arXiv:2203.07180",
    "title": "A pressure-robust HHO method for the solution of the incompressible  Navier-Stokes equations on general meshes",
    "abstract": "A pressure-robust HHO method for the solution of the incompressible  Navier-Stokes equations on general meshes",
    "descriptor": "",
    "authors": [
      "Daniel Castanon Quiroz",
      "Daniele A. Di Pietro"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2203.07180"
  },
  {
    "id": "arXiv:2203.09081",
    "title": "Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a  Learnable Classifier at the End of Deep Neural Network?",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Yibo Yang",
      "Shixiang Chen",
      "Xiangtai Li",
      "Liang Xie",
      "Zhouchen Lin",
      "Dacheng Tao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.09081"
  },
  {
    "id": "arXiv:2203.09615",
    "title": "Canvas: Isolated and Adaptive Swapping for Multi-Applications on Remote  Memory",
    "abstract": "Canvas: Isolated and Adaptive Swapping for Multi-Applications on Remote  Memory",
    "descriptor": "",
    "authors": [
      "Chenxi Wang",
      "Yifan Qiao",
      "Haoran Ma",
      "Shi Liu",
      "Yiying Zhang",
      "Wenguang Chen",
      "Ravi Netravali",
      "Miryung Kim",
      "Guoqing Harry Xu"
    ],
    "subjectives": [
      "Operating Systems (cs.OS)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2203.09615"
  },
  {
    "id": "arXiv:2203.09940",
    "title": "Alleviating Adversarial Attacks on Variational Autoencoders with MCMC",
    "abstract": "Alleviating Adversarial Attacks on Variational Autoencoders with MCMC",
    "descriptor": "",
    "authors": [
      "Anna Kuzina",
      "Max Welling",
      "Jakub M. Tomczak"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.09940"
  },
  {
    "id": "arXiv:2203.11203",
    "title": "Reinforcement learning for automatic quadrilateral mesh generation: a  soft actor-critic approach",
    "abstract": "Reinforcement learning for automatic quadrilateral mesh generation: a  soft actor-critic approach",
    "descriptor": "",
    "authors": [
      "Jie Pan",
      "Jingwei Huang",
      "Gengdong Cheng",
      "Yong Zeng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2203.11203"
  },
  {
    "id": "arXiv:2203.11847",
    "title": "Spectral Algorithms Optimally Recover Planted Sub-structures",
    "abstract": "Comments: 28 pages, 2 figures; New content on submatrix localization",
    "descriptor": "\nComments: 28 pages, 2 figures; New content on submatrix localization\n",
    "authors": [
      "Souvik Dhara",
      "Julia Gaudio",
      "Elchanan Mossel",
      "Colin Sandon"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.11847"
  },
  {
    "id": "arXiv:2203.12786",
    "title": "Bellman Residual Orthogonalization for Offline Reinforcement Learning",
    "abstract": "Comments: Appears in NeurIPS 2022",
    "descriptor": "\nComments: Appears in NeurIPS 2022\n",
    "authors": [
      "Andrea Zanette",
      "Martin J. Wainwright"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.12786"
  },
  {
    "id": "arXiv:2203.12865",
    "title": "Multilingual CheckList: Generation and Evaluation",
    "abstract": "Comments: Accepted to Findings of AACL-IJCNLP 2022",
    "descriptor": "\nComments: Accepted to Findings of AACL-IJCNLP 2022\n",
    "authors": [
      "Karthikeyan K",
      "Shaily Bhatt",
      "Pankaj Singh",
      "Somak Aditya",
      "Sandipan Dandapat",
      "Sunayana Sitaram",
      "Monojit Choudhury"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.12865"
  },
  {
    "id": "arXiv:2203.13313",
    "title": "Deep learning for laboratory earthquake prediction and autoregressive  forecasting of fault zone stress",
    "abstract": "Comments: Published in this https URL",
    "descriptor": "\nComments: Published in this https URL\n",
    "authors": [
      "Laura Laurenti",
      "Elisa Tinti",
      "Fabio Galasso",
      "Luca Franco",
      "Chris Marone"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2203.13313"
  },
  {
    "id": "arXiv:2204.01005",
    "title": "Frequency and Multi-Scale Selective Kernel Attention for Speaker  Verification",
    "abstract": "Comments: Accepted by IEEE SLT 2022. 7 pages, 4 figures, 1 table. Code is available at this https URL",
    "descriptor": "\nComments: Accepted by IEEE SLT 2022. 7 pages, 4 figures, 1 table. Code is available at this https URL\n",
    "authors": [
      "Sung Hwan Mun",
      "Jee-weon Jung",
      "Min Hyun Han",
      "Nam Soo Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.01005"
  },
  {
    "id": "arXiv:2204.04061",
    "title": "\"Am I Private and If So, how Many?\" -- Using Risk Communication Formats  for Making Differential Privacy Understandable",
    "abstract": "Comments: Accepted at ACM Conference on Computer and Communications Security (CCS) 2022",
    "descriptor": "\nComments: Accepted at ACM Conference on Computer and Communications Security (CCS) 2022\n",
    "authors": [
      "Daniel Franzen",
      "Saskia Nu\u00f1ez von Voigt",
      "Peter S\u00f6rries",
      "Florian Tschorsch",
      "Claudia M\u00fcller-Birn"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2204.04061"
  },
  {
    "id": "arXiv:2204.04218",
    "title": "Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes  for Medical Image Super-Resolution",
    "abstract": "Comments: Accepted at WACV 2023 (main paper + supplementary)",
    "descriptor": "\nComments: Accepted at WACV 2023 (main paper + supplementary)\n",
    "authors": [
      "Mariana-Iuliana Georgescu",
      "Radu Tudor Ionescu",
      "Andreea-Iuliana Miron",
      "Olivian Savencu",
      "Nicolae-Catalin Ristea",
      "Nicolae Verga",
      "Fahad Shahbaz Khan"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.04218"
  },
  {
    "id": "arXiv:2204.05002",
    "title": "Linear-time algorithm for computing the Bernstein-B\u00e9zier  coefficients of B-spline basis functions",
    "abstract": "Linear-time algorithm for computing the Bernstein-B\u00e9zier  coefficients of B-spline basis functions",
    "descriptor": "",
    "authors": [
      "Filip Chudy",
      "Pawe\u0142 Wo\u017any"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2204.05002"
  },
  {
    "id": "arXiv:2204.08241",
    "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural  Networks for Dense Passage Retrieval",
    "abstract": "Comments: Findings of EMNLP2022",
    "descriptor": "\nComments: Findings of EMNLP2022\n",
    "authors": [
      "Jiduan Liu",
      "Jiahao Liu",
      "Yang Yang",
      "Jingang Wang",
      "Wei Wu",
      "Dongyan Zhao",
      "Rui Yan"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2204.08241"
  },
  {
    "id": "arXiv:2204.09179",
    "title": "On the Representation Collapse of Sparse Mixture of Experts",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Shaohan Huang",
      "Damai Dai",
      "Shuming Ma",
      "Barun Patra",
      "Saksham Singhal",
      "Payal Bajaj",
      "Xia Song",
      "Xian-Ling Mao",
      "Heyan Huang",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.09179"
  },
  {
    "id": "arXiv:2204.10022",
    "title": "Scalable Sensitivity and Uncertainty Analysis for Causal-Effect  Estimates of Continuous-Valued Interventions",
    "abstract": "Comments: 33 pages",
    "descriptor": "\nComments: 33 pages\n",
    "authors": [
      "Andrew Jesson",
      "Alyson Douglas",
      "Peter Manshausen",
      "Ma\u00eblys Solal",
      "Nicolai Meinshausen",
      "Philip Stier",
      "Yarin Gal",
      "Uri Shalit"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.10022"
  },
  {
    "id": "arXiv:2204.10290",
    "title": "Learning to Revise References for Faithful Summarization",
    "abstract": "Comments: Findings of EMNLP 2022",
    "descriptor": "\nComments: Findings of EMNLP 2022\n",
    "authors": [
      "Griffin Adams",
      "Han-Chin Shing",
      "Qing Sun",
      "Christopher Winestock",
      "Kathleen McKeown",
      "No\u00e9mie Elhadad"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.10290"
  },
  {
    "id": "arXiv:2204.11144",
    "title": "Competitive Physics Informed Networks",
    "abstract": "Competitive Physics Informed Networks",
    "descriptor": "",
    "authors": [
      "Qi Zeng",
      "Yash Kothari",
      "Spencer H. Bryngelson",
      "Florian Sch\u00e4fer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)",
      "Numerical Analysis (math.NA)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2204.11144"
  },
  {
    "id": "arXiv:2204.11448",
    "title": "High-Efficiency Lossy Image Coding Through Adaptive Neighborhood  Information Aggregation",
    "abstract": "High-Efficiency Lossy Image Coding Through Adaptive Neighborhood  Information Aggregation",
    "descriptor": "",
    "authors": [
      "Ming Lu",
      "Fangdong Chen",
      "Shiliang Pu",
      "Zhan Ma"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2204.11448"
  },
  {
    "id": "arXiv:2204.12965",
    "title": "Scalable particle-based alternatives to EM",
    "abstract": "Scalable particle-based alternatives to EM",
    "descriptor": "",
    "authors": [
      "Juan Kuntz",
      "Jen Ning Lim",
      "Adam M. Johansen"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.12965"
  },
  {
    "id": "arXiv:2204.12993",
    "title": "Counterfactual harm",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Jonathan G. Richens",
      "Rory Beard",
      "Daniel H. Thompson"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.12993"
  },
  {
    "id": "arXiv:2204.13215",
    "title": "Fairness and promptness in Muller formulas",
    "abstract": "Fairness and promptness in Muller formulas",
    "descriptor": "",
    "authors": [
      "Youssouf Oualhadj",
      "L\u00e9o Tible",
      "Daniele Varacca"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2204.13215"
  },
  {
    "id": "arXiv:2205.01666",
    "title": "DANBO: Disentangled Articulated Neural Body Representations via Graph  Neural Networks",
    "abstract": "Comments: ECCV 2022. Project website: this https URL",
    "descriptor": "\nComments: ECCV 2022. Project website: this https URL\n",
    "authors": [
      "Shih-Yang Su",
      "Timur Bagautdinov",
      "Helge Rhodin"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.01666"
  },
  {
    "id": "arXiv:2205.02177",
    "title": "Tangle 2.0 Leaderless Nakamoto Consensus on the Heaviest DAG",
    "abstract": "Comments: revised version, to appear in IEEE Access",
    "descriptor": "\nComments: revised version, to appear in IEEE Access\n",
    "authors": [
      "Sebastian M\u00fcller",
      "Andreas Penzkofer",
      "Nikita Polyanskii",
      "Jonas Theis",
      "William Sanders",
      "Hans Moog"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2205.02177"
  },
  {
    "id": "arXiv:2205.02990",
    "title": "Linear-Complexity Black-Box Randomized Compression of Rank-Structured  Matrices",
    "abstract": "Linear-Complexity Black-Box Randomized Compression of Rank-Structured  Matrices",
    "descriptor": "",
    "authors": [
      "James Levitt",
      "Per-Gunnar Martinsson"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.02990"
  },
  {
    "id": "arXiv:2205.03017",
    "title": "Generative Adversarial Neural Operators",
    "abstract": "Comments: Transactions on Machine Learning Research 2022",
    "descriptor": "\nComments: Transactions on Machine Learning Research 2022\n",
    "authors": [
      "Md Ashiqur Rahman",
      "Manuel A. Florez",
      "Anima Anandkumar",
      "Zachary E. Ross",
      "Kamyar Azizzadenesheli"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Probability (math.PR)"
    ],
    "url": "https://arxiv.org/abs/2205.03017"
  },
  {
    "id": "arXiv:2205.05138",
    "title": "Efficient Risk-Averse Reinforcement Learning",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Ido Greenberg",
      "Yinlam Chow",
      "Mohammad Ghavamzadeh",
      "Shie Mannor"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.05138"
  },
  {
    "id": "arXiv:2205.05662",
    "title": "Deep Architecture Connectivity Matters for Its Convergence: A  Fine-Grained Analysis",
    "abstract": "Comments: Neurips 2022 accepted",
    "descriptor": "\nComments: Neurips 2022 accepted\n",
    "authors": [
      "Wuyang Chen",
      "Wei Huang",
      "Xinyu Gong",
      "Boris Hanin",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.05662"
  },
  {
    "id": "arXiv:2205.06846",
    "title": "Optimal Comparator Adaptive Online Learning with Switching Cost",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Zhiyu Zhang",
      "Ashok Cutkosky",
      "Ioannis Ch. Paschalidis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.06846"
  },
  {
    "id": "arXiv:2205.07146",
    "title": "Trajectory Inference via Mean-field Langevin in Path Space",
    "abstract": "Trajectory Inference via Mean-field Langevin in Path Space",
    "descriptor": "",
    "authors": [
      "L\u00e9na\u00efc Chizat",
      "Stephen Zhang",
      "Matthieu Heitz",
      "Geoffrey Schiebinger"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.07146"
  },
  {
    "id": "arXiv:2205.07211",
    "title": "GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain  Text-to-Speech",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Rongjie Huang",
      "Yi Ren",
      "Jinglin Liu",
      "Chenye Cui",
      "Zhou Zhao"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2205.07211"
  },
  {
    "id": "arXiv:2205.07313",
    "title": "Generalization Bounds on Multi-Kernel Learning with Mixed Datasets",
    "abstract": "Comments: Update Marton Coupling. Under review for possible publication",
    "descriptor": "\nComments: Update Marton Coupling. Under review for possible publication\n",
    "authors": [
      "Lan V. Truong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.07313"
  },
  {
    "id": "arXiv:2205.08397",
    "title": "Improved Utility Analysis of Private CountSketch",
    "abstract": "Comments: To appear at NeurIPS 2022",
    "descriptor": "\nComments: To appear at NeurIPS 2022\n",
    "authors": [
      "Rasmus Pagh",
      "Mikkel Thorup"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.08397"
  },
  {
    "id": "arXiv:2205.09067",
    "title": "Automatic Rule Induction for Efficient and Interpretable Semi-Supervised  Learning",
    "abstract": "Comments: EMNLP 2022",
    "descriptor": "\nComments: EMNLP 2022\n",
    "authors": [
      "Reid Pryzant",
      "Ziyi Yang",
      "Yichong Xu",
      "Chenguang Zhu",
      "Michael Zeng"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09067"
  },
  {
    "id": "arXiv:2205.09435",
    "title": "Adversarial random forests for density estimation and generative  modelling",
    "abstract": "Comments: 19 pages, 4 figures",
    "descriptor": "\nComments: 19 pages, 4 figures\n",
    "authors": [
      "David S. Watson",
      "Kristin Blesch",
      "Jan Kapar",
      "Marvin N. Wright"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2205.09435"
  },
  {
    "id": "arXiv:2205.09589",
    "title": "Learning Energy Networks with Generalized Fenchel-Young Losses",
    "abstract": "Learning Energy Networks with Generalized Fenchel-Young Losses",
    "descriptor": "",
    "authors": [
      "Mathieu Blondel",
      "Felipe Llinares-L\u00f3pez",
      "Robert Dadashi",
      "L\u00e9onard Hussenot",
      "Matthieu Geist"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09589"
  },
  {
    "id": "arXiv:2205.09661",
    "title": "Self-training with Two-phase Self-augmentation for Few-shot Dialogue  Generation",
    "abstract": "Comments: Findings of EMNLP2022",
    "descriptor": "\nComments: Findings of EMNLP2022\n",
    "authors": [
      "Wanyu Du",
      "Hanjie Chen",
      "Yangfeng Ji"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.09661"
  },
  {
    "id": "arXiv:2205.10130",
    "title": "Spiking Neural Operators for Scientific Machine Learning",
    "abstract": "Comments: 16 pages, 6 figures and 4 tables",
    "descriptor": "\nComments: 16 pages, 6 figures and 4 tables\n",
    "authors": [
      "Adar Kahana",
      "Qian Zhang",
      "Leonard Gleyzer",
      "George Em Karniadakis"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.10130"
  },
  {
    "id": "arXiv:2205.10677",
    "title": "Risk-Driven Design of Perception Systems",
    "abstract": "Comments: 17 pages, 10 figures",
    "descriptor": "\nComments: 17 pages, 10 figures\n",
    "authors": [
      "Anthony L. Corso",
      "Sydney M. Katz",
      "Craig Innes",
      "Xin Du",
      "Subramanian Ramamoorthy",
      "Mykel J. Kochenderfer"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.10677"
  },
  {
    "id": "arXiv:2205.11303",
    "title": "Real-time Collaborative Multi-Level Modeling by Conflict-Free Replicated  Data Types",
    "abstract": "Real-time Collaborative Multi-Level Modeling by Conflict-Free Replicated  Data Types",
    "descriptor": "",
    "authors": [
      "Istvan David",
      "Eugene Syriani"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2205.11303"
  },
  {
    "id": "arXiv:2205.11320",
    "title": "Active Learning Through a Covering Lens",
    "abstract": "Active Learning Through a Covering Lens",
    "descriptor": "",
    "authors": [
      "Ofer Yehuda",
      "Avihu Dekel",
      "Guy Hacohen",
      "Daphna Weinshall"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.11320"
  },
  {
    "id": "arXiv:2205.11610",
    "title": "uGLAD: Sparse graph recovery by optimizing deep unrolled networks",
    "abstract": "uGLAD: Sparse graph recovery by optimizing deep unrolled networks",
    "descriptor": "",
    "authors": [
      "Harsh Shrivastava",
      "Urszula Chajewska",
      "Robin Abraham",
      "Xinshi Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11610"
  },
  {
    "id": "arXiv:2205.11782",
    "title": "Fine-grained Poisoning Attack to Local Differential Privacy Protocols  for Mean and Variance Estimation",
    "abstract": "Fine-grained Poisoning Attack to Local Differential Privacy Protocols  for Mean and Variance Estimation",
    "descriptor": "",
    "authors": [
      "Xiaoguang Li",
      "Ninghui Li",
      "Wenhai Sun",
      "Neil Zhenqiang Gong",
      "Hui Li"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2205.11782"
  },
  {
    "id": "arXiv:2205.11894",
    "title": "Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs",
    "abstract": "Learning Interacting Dynamical Systems with Latent Gaussian Process ODEs",
    "descriptor": "",
    "authors": [
      "\u00c7a\u011fatay Y\u0131ld\u0131z",
      "Melih Kandemir",
      "Barbara Rakitsch"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.11894"
  },
  {
    "id": "arXiv:2205.12307",
    "title": "Approximate Euclidean lengths and distances beyond Johnson-Lindenstrauss",
    "abstract": "Comments: To appear NeurIPS 2022",
    "descriptor": "\nComments: To appear NeurIPS 2022\n",
    "authors": [
      "Aleksandros Sobczyk",
      "Mathieu Luisier"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.12307"
  },
  {
    "id": "arXiv:2205.12427",
    "title": "Non-stationary Bandits with Knapsacks",
    "abstract": "Non-stationary Bandits with Knapsacks",
    "descriptor": "",
    "authors": [
      "Shang Liu",
      "Jiashuo Jiang",
      "Xiaocheng Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.12427"
  },
  {
    "id": "arXiv:2205.12454",
    "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
    "abstract": "Comments: In Proceedings of NeurIPS 2022",
    "descriptor": "\nComments: In Proceedings of NeurIPS 2022\n",
    "authors": [
      "Ladislav Ramp\u00e1\u0161ek",
      "Mikhail Galkin",
      "Vijay Prakash Dwivedi",
      "Anh Tuan Luu",
      "Guy Wolf",
      "Dominique Beaini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12454"
  },
  {
    "id": "arXiv:2205.12468",
    "title": "Multiview Textured Mesh Recovery by Differentiable Rendering",
    "abstract": "Multiview Textured Mesh Recovery by Differentiable Rendering",
    "descriptor": "",
    "authors": [
      "Lixiang Lin",
      "Jianke Zhu",
      "Yisu Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.12468"
  },
  {
    "id": "arXiv:2205.12702",
    "title": "Detecting Label Errors using Pre-Trained Language Models",
    "abstract": "Comments: 17 pages, 10 figures, to be published in EMNLP",
    "descriptor": "\nComments: 17 pages, 10 figures, to be published in EMNLP\n",
    "authors": [
      "Derek Chong",
      "Jenny Hong",
      "Christopher D. Manning"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.12702"
  },
  {
    "id": "arXiv:2205.12751",
    "title": "Fast Stochastic Composite Minimization and an Accelerated Frank-Wolfe  Algorithm under Parallelization",
    "abstract": "Fast Stochastic Composite Minimization and an Accelerated Frank-Wolfe  Algorithm under Parallelization",
    "descriptor": "",
    "authors": [
      "Benjamin Dubois-Taine",
      "Francis Bach",
      "Quentin Berthet",
      "Adrien Taylor"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.12751"
  },
  {
    "id": "arXiv:2205.13503",
    "title": "Multi-layer State Evolution Under Random Convolutional Design",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Max Daniels",
      "C\u00e9dric Gerbelot",
      "Florent Krzakala",
      "Lenka Zdeborov\u00e1"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2205.13503"
  },
  {
    "id": "arXiv:2205.13532",
    "title": "Selective Classification Via Neural Network Training Dynamics",
    "abstract": "Selective Classification Via Neural Network Training Dynamics",
    "descriptor": "",
    "authors": [
      "Stephan Rabanser",
      "Anvith Thudi",
      "Kimia Hamidieh",
      "Adam Dziedzic",
      "Nicolas Papernot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.13532"
  },
  {
    "id": "arXiv:2205.13535",
    "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual  Recognition",
    "abstract": "Comments: Accepted by NeurIPS 2022. Code: this https URL",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. Code: this https URL\n",
    "authors": [
      "Shoufa Chen",
      "Chongjian Ge",
      "Zhan Tong",
      "Jiangliu Wang",
      "Yibing Song",
      "Jue Wang",
      "Ping Luo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.13535"
  },
  {
    "id": "arXiv:2205.13746",
    "title": "Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games",
    "abstract": "Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games",
    "descriptor": "",
    "authors": [
      "Sihan Zeng",
      "Thinh T. Doan",
      "Justin Romberg"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.13746"
  },
  {
    "id": "arXiv:2205.14014",
    "title": "What Dense Graph Do You Need for Self-Attention?",
    "abstract": "Comments: Accepted by ICML 2022. Code is available at this https URL",
    "descriptor": "\nComments: Accepted by ICML 2022. Code is available at this https URL\n",
    "authors": [
      "Yuxin Wang",
      "Chu-Tak Lee",
      "Qipeng Guo",
      "Zhangyue Yin",
      "Yunhua Zhou",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14014"
  },
  {
    "id": "arXiv:2205.14140",
    "title": "CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model  Behavior",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Eldar David Abraham",
      "Karel D'Oosterlinck",
      "Amir Feder",
      "Yair Ori Gat",
      "Atticus Geiger",
      "Christopher Potts",
      "Roi Reichart",
      "Zhengxuan Wu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14140"
  },
  {
    "id": "arXiv:2205.14790",
    "title": "Non-Stationary Bandits under Recharging Payoffs: Improved Planning with  Sublinear Regret",
    "abstract": "Comments: Accepted for publication to NeurIPS 2022",
    "descriptor": "\nComments: Accepted for publication to NeurIPS 2022\n",
    "authors": [
      "Orestis Papadigenopoulos",
      "Constantine Caramanis",
      "Sanjay Shakkottai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2205.14790"
  },
  {
    "id": "arXiv:2205.15129",
    "title": "On the SCD semismooth* Newton method for generalized equations with  application to a class of static contact problems with Coulomb friction",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2112.08080",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2112.08080\n",
    "authors": [
      "H. Gfrerer",
      "M. Mandlmayr",
      "J.V. Outrata",
      "J. Valdman"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2205.15129"
  },
  {
    "id": "arXiv:2205.15286",
    "title": "Robust and accelerated single-spike spiking neural network training with  applicability to challenging temporal tasks",
    "abstract": "Comments: 18 pages, 6 figures, under review at ICLR 2023",
    "descriptor": "\nComments: 18 pages, 6 figures, under review at ICLR 2023\n",
    "authors": [
      "Luke Taylor",
      "Andrew King",
      "Nicol Harper"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.15286"
  },
  {
    "id": "arXiv:2206.00152",
    "title": "Human-AI Shared Control via Policy Dissection",
    "abstract": "Human-AI Shared Control via Policy Dissection",
    "descriptor": "",
    "authors": [
      "Quanyi Li",
      "Zhenghao Peng",
      "Haibin Wu",
      "Lan Feng",
      "Bolei Zhou"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00152"
  },
  {
    "id": "arXiv:2206.00257",
    "title": "CoNSoLe: Convex Neural Symbolic Learning",
    "abstract": "Comments: 18 pages, 5 figures, conference for NeurIPS 2022",
    "descriptor": "\nComments: 18 pages, 5 figures, conference for NeurIPS 2022\n",
    "authors": [
      "Haoran Li",
      "Yang Weng",
      "Hanghang Tong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.00257"
  },
  {
    "id": "arXiv:2206.00466",
    "title": "An $\u03b1$-No-Regret Algorithm For Graphical Bilinear Bandits",
    "abstract": "An $\u03b1$-No-Regret Algorithm For Graphical Bilinear Bandits",
    "descriptor": "",
    "authors": [
      "Geovani Rizk",
      "Igor Colin",
      "Albert Thomas",
      "Rida Laraki",
      "Yann Chevaleyre"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00466"
  },
  {
    "id": "arXiv:2206.00517",
    "title": "One Positive Label is Sufficient: Single-Positive Multi-Label Learning  with Label Enhancement",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Ning Xu",
      "Congyu Qiao",
      "Jiaqi Lv",
      "Xin Geng",
      "Min-Ling Zhang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.00517"
  },
  {
    "id": "arXiv:2206.00665",
    "title": "MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface  Reconstruction",
    "abstract": "Comments: Project page: this https URL",
    "descriptor": "\nComments: Project page: this https URL\n",
    "authors": [
      "Zehao Yu",
      "Songyou Peng",
      "Michael Niemeyer",
      "Torsten Sattler",
      "Andreas Geiger"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.00665"
  },
  {
    "id": "arXiv:2206.00823",
    "title": "Beyond accuracy: generalization properties of bio-plausible temporal  credit assignment rules",
    "abstract": "Comments: To appear at NeurIPS 2022",
    "descriptor": "\nComments: To appear at NeurIPS 2022\n",
    "authors": [
      "Yuhan Helena Liu",
      "Arna Ghosh",
      "Blake A. Richards",
      "Eric Shea-Brown",
      "Guillaume Lajoie"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Artificial Intelligence (cs.AI)",
      "Neurons and Cognition (q-bio.NC)"
    ],
    "url": "https://arxiv.org/abs/2206.00823"
  },
  {
    "id": "arXiv:2206.01338",
    "title": "Biologically-plausible backpropagation through arbitrary timespans via  local neuromodulators",
    "abstract": "Comments: To appear at NeurIPS 2022",
    "descriptor": "\nComments: To appear at NeurIPS 2022\n",
    "authors": [
      "Yuhan Helena Liu",
      "Stephen Smith",
      "Stefan Mihalas",
      "Eric Shea-Brown",
      "Uygar S\u00fcmb\u00fcl"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.01338"
  },
  {
    "id": "arXiv:2206.02455",
    "title": "Mean Estimation in High-Dimensional Binary Markov Gaussian Mixture  Models",
    "abstract": "Mean Estimation in High-Dimensional Binary Markov Gaussian Mixture  Models",
    "descriptor": "",
    "authors": [
      "Yihan Zhang",
      "Nir Weinberger"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02455"
  },
  {
    "id": "arXiv:2206.02512",
    "title": "UTTS: Unsupervised TTS with Conditional Disentangled Sequential  Variational Auto-encoder",
    "abstract": "Comments: Under Review",
    "descriptor": "\nComments: Under Review\n",
    "authors": [
      "Jiachen Lian",
      "Chunlei Zhang",
      "Gopala Krishna Anumanchipalli",
      "Dong Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2206.02512"
  },
  {
    "id": "arXiv:2206.02626",
    "title": "Infinite Recommendation Networks: A Data-Centric Approach",
    "abstract": "Comments: Published at NeurIPS '22. $\\infty$-AE code available at this https URL and Distill-CF code available at this https URL",
    "descriptor": "\nComments: Published at NeurIPS '22. $\\infty$-AE code available at this https URL and Distill-CF code available at this https URL\n",
    "authors": [
      "Noveen Sachdeva",
      "Mehak Preet Dhaliwal",
      "Carole-Jean Wu",
      "Julian McAuley"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02626"
  },
  {
    "id": "arXiv:2206.02675",
    "title": "Effects of Safety State Augmentation on Safe Exploration",
    "abstract": "Comments: Published in Neurips 2022",
    "descriptor": "\nComments: Published in Neurips 2022\n",
    "authors": [
      "Aivar Sootla",
      "Alexander I. Cowen-Rivers",
      "Jun Wang",
      "Haitham Bou Ammar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.02675"
  },
  {
    "id": "arXiv:2206.03150",
    "title": "Group Meritocratic Fairness in Linear Contextual Bandits",
    "abstract": "Comments: NeurIPS 2022. Code for the experiments at this https URL",
    "descriptor": "\nComments: NeurIPS 2022. Code for the experiments at this https URL\n",
    "authors": [
      "Riccardo Grazzi",
      "Arya Akhavan",
      "John Isak Texas Falk",
      "Leonardo Cella",
      "Massimiliano Pontil"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03150"
  },
  {
    "id": "arXiv:2206.04045",
    "title": "STable: Table Generation Framework for Encoder-Decoder Models",
    "abstract": "STable: Table Generation Framework for Encoder-Decoder Models",
    "descriptor": "",
    "authors": [
      "Micha\u0142 Pietruszka",
      "Micha\u0142 Turski",
      "\u0141ukasz Borchmann",
      "Tomasz Dwojak",
      "Gabriela Pa\u0142ka",
      "Karolina Szyndler",
      "Dawid Jurkiewicz",
      "\u0141ukasz Garncarek"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04045"
  },
  {
    "id": "arXiv:2206.04199",
    "title": "Deep Surrogate Assisted Generation of Environments",
    "abstract": "Comments: 26 pages, 15 figures, supplemental website at this https URL",
    "descriptor": "\nComments: 26 pages, 15 figures, supplemental website at this https URL\n",
    "authors": [
      "Varun Bhatt",
      "Bryon Tjanaka",
      "Matthew C. Fontaine",
      "Stefanos Nikolaidis"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2206.04199"
  },
  {
    "id": "arXiv:2206.04531",
    "title": "ECLAD: Extracting Concepts with Local Aggregated Descriptors",
    "abstract": "Comments: 46 pages, under review",
    "descriptor": "\nComments: 46 pages, under review\n",
    "authors": [
      "Andres Felipe Posada-Moreno",
      "Nikita Surya",
      "Sebastian Trimpe"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04531"
  },
  {
    "id": "arXiv:2206.04910",
    "title": "NAGphormer: A Tokenized Graph Transformer for Node Classification in  Large Graphs",
    "abstract": "NAGphormer: A Tokenized Graph Transformer for Node Classification in  Large Graphs",
    "descriptor": "",
    "authors": [
      "Jinsong Chen",
      "Kaiyuan Gao",
      "Gaichao Li",
      "Kun He"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.04910"
  },
  {
    "id": "arXiv:2206.04916",
    "title": "PatchComplete: Learning Multi-Resolution Patch Priors for 3D Shape  Completion on Unseen Categories",
    "abstract": "Comments: Video link: this https URL ; Project page: this https URL ; Accepted to NeurIPS'22",
    "descriptor": "\nComments: Video link: this https URL ; Project page: this https URL ; Accepted to NeurIPS'22\n",
    "authors": [
      "Yuchen Rao",
      "Yinyu Nie",
      "Angela Dai"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.04916"
  },
  {
    "id": "arXiv:2206.05630",
    "title": "Mathematical Theory of Bayesian Statistics for Unknown Information  Source",
    "abstract": "Mathematical Theory of Bayesian Statistics for Unknown Information  Source",
    "descriptor": "",
    "authors": [
      "Sumio Watanabe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2206.05630"
  },
  {
    "id": "arXiv:2206.05836",
    "title": "GLIPv2: Unifying Localization and Vision-Language Understanding",
    "abstract": "Comments: NeurIPS 2022; updated with reviewers' comments addressed; Code is released at this https URL",
    "descriptor": "\nComments: NeurIPS 2022; updated with reviewers' comments addressed; Code is released at this https URL\n",
    "authors": [
      "Haotian Zhang",
      "Pengchuan Zhang",
      "Xiaowei Hu",
      "Yen-Chun Chen",
      "Liunian Harold Li",
      "Xiyang Dai",
      "Lijuan Wang",
      "Lu Yuan",
      "Jenq-Neng Hwang",
      "Jianfeng Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2206.05836"
  },
  {
    "id": "arXiv:2206.05985",
    "title": "Modeling the Machine Learning Multiverse",
    "abstract": "Comments: To appear in Advances in Neural Information Processing Systems (NeurIPS) 2022",
    "descriptor": "\nComments: To appear in Advances in Neural Information Processing Systems (NeurIPS) 2022\n",
    "authors": [
      "Samuel J. Bell",
      "Onno P. Kampman",
      "Jesse Dodge",
      "Neil D. Lawrence"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2206.05985"
  },
  {
    "id": "arXiv:2206.06679",
    "title": "Matching Pursuit Based Scheduling for Over-the-Air Federated Learning",
    "abstract": "Comments: 47 Pages and 10 Figures",
    "descriptor": "\nComments: 47 Pages and 10 Figures\n",
    "authors": [
      "Ali Bereyhi",
      "Adela Vagollari",
      "Saba Asaad",
      "Ralf R. M\u00fcller",
      "Wolfgang Gerstacker",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.06679"
  },
  {
    "id": "arXiv:2206.06922",
    "title": "Object Scene Representation Transformer",
    "abstract": "Comments: Accepted at NeurIPS '22. Project page: this https URL",
    "descriptor": "\nComments: Accepted at NeurIPS '22. Project page: this https URL\n",
    "authors": [
      "Mehdi S. M. Sajjadi",
      "Daniel Duckworth",
      "Aravindh Mahendran",
      "Sjoerd van Steenkiste",
      "Filip Paveti\u0107",
      "Mario Lu\u010di\u0107",
      "Leonidas J. Guibas",
      "Klaus Greff",
      "Thomas Kipf"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.06922"
  },
  {
    "id": "arXiv:2206.07307",
    "title": "VCT: A Video Compression Transformer",
    "abstract": "Comments: NeurIPS'22 Camera Ready Version. Code: this https URL",
    "descriptor": "\nComments: NeurIPS'22 Camera Ready Version. Code: this https URL\n",
    "authors": [
      "Fabian Mentzer",
      "George Toderici",
      "David Minnen",
      "Sung-Jin Hwang",
      "Sergi Caelles",
      "Mario Lucic",
      "Eirikur Agustsson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2206.07307"
  },
  {
    "id": "arXiv:2206.07681",
    "title": "Learning to Accelerate Partial Differential Equations via Latent Global  Evolution",
    "abstract": "Comments: NeurIPS 2022; 30 pages, 15 figures",
    "descriptor": "\nComments: NeurIPS 2022; 30 pages, 15 figures\n",
    "authors": [
      "Tailin Wu",
      "Takashi Maruyama",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2206.07681"
  },
  {
    "id": "arXiv:2206.08364",
    "title": "Interaction-Grounded Learning with Action-inclusive Feedback",
    "abstract": "Comments: Published in NeurIPS 2022",
    "descriptor": "\nComments: Published in NeurIPS 2022\n",
    "authors": [
      "Tengyang Xie",
      "Akanksha Saran",
      "Dylan J. Foster",
      "Lekan Molu",
      "Ida Momennejad",
      "Nan Jiang",
      "Paul Mineiro",
      "John Langford"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.08364"
  },
  {
    "id": "arXiv:2206.09491",
    "title": "On the Limitations of Stochastic Pre-processing Defenses",
    "abstract": "Comments: Accepted by Proceedings of the 36th Conference on Neural Information Processing Systems",
    "descriptor": "\nComments: Accepted by Proceedings of the 36th Conference on Neural Information Processing Systems\n",
    "authors": [
      "Yue Gao",
      "Ilia Shumailov",
      "Kassem Fawaz",
      "Nicolas Papernot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2206.09491"
  },
  {
    "id": "arXiv:2206.09682",
    "title": "SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous  Vehicles",
    "abstract": "Comments: NeurIPS 2022 (Track on Datasets and Benchmarks)",
    "descriptor": "\nComments: NeurIPS 2022 (Track on Datasets and Benchmarks)\n",
    "authors": [
      "Chejian Xu",
      "Wenhao Ding",
      "Weijie Lyu",
      "Zuxin Liu",
      "Shuai Wang",
      "Yihan He",
      "Hanjiang Hu",
      "Ding Zhao",
      "Bo Li"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.09682"
  },
  {
    "id": "arXiv:2206.10558",
    "title": "EnvPool: A Highly Parallel Reinforcement Learning Environment Execution  Engine",
    "abstract": "Comments: NeurIPS'22 camera-ready version",
    "descriptor": "\nComments: NeurIPS'22 camera-ready version\n",
    "authors": [
      "Jiayi Weng",
      "Min Lin",
      "Shengyi Huang",
      "Bo Liu",
      "Denys Makoviichuk",
      "Viktor Makoviychuk",
      "Zichen Liu",
      "Yufan Song",
      "Ting Luo",
      "Yukun Jiang",
      "Zhongwen Xu",
      "Shuicheng Yan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.10558"
  },
  {
    "id": "arXiv:2206.11251",
    "title": "Behavior Transformers: Cloning $k$ modes with one stone",
    "abstract": "Comments: Code and data available at this https URL",
    "descriptor": "\nComments: Code and data available at this https URL\n",
    "authors": [
      "Nur Muhammad Mahi Shafiullah",
      "Zichen Jeff Cui",
      "Ariuntuya Altanzaya",
      "Lerrel Pinto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2206.11251"
  },
  {
    "id": "arXiv:2206.11460",
    "title": "pyKT: A Python Library to Benchmark Deep Learning based Knowledge  Tracing Models",
    "abstract": "Comments: Accepted in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks",
    "descriptor": "\nComments: Accepted in 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks\n",
    "authors": [
      "Zitao Liu",
      "Qiongqiong Liu",
      "Jiahao Chen",
      "Shuyan Huang",
      "Jiliang Tang",
      "Weiqi Luo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2206.11460"
  },
  {
    "id": "arXiv:2206.11541",
    "title": "A Neuromorphic Vision-Based Measurement for Robust Relative Localization  in Future Space Exploration Missions",
    "abstract": "A Neuromorphic Vision-Based Measurement for Robust Relative Localization  in Future Space Exploration Missions",
    "descriptor": "",
    "authors": [
      "Mohammed Salah",
      "Mohammed Chehadah",
      "Muhammed Humais",
      "Mohammed Wahbah",
      "Abdulla Ayyad",
      "Rana Azzam",
      "Lakmal Seneviratne",
      "Yahya Zweiri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.11541"
  },
  {
    "id": "arXiv:2206.12314",
    "title": "Learning sparse features can lead to overfitting in neural networks",
    "abstract": "Learning sparse features can lead to overfitting in neural networks",
    "descriptor": "",
    "authors": [
      "Leonardo Petrini",
      "Francesco Cagnetta",
      "Eric Vanden-Eijnden",
      "Matthieu Wyart"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.12314"
  },
  {
    "id": "arXiv:2206.13477",
    "title": "Parametrically Retargetable Decision-Makers Tend To Seek Power",
    "abstract": "Comments: 10-page main paper, 36 pages total, poster at NeurIPS 2022",
    "descriptor": "\nComments: 10-page main paper, 36 pages total, poster at NeurIPS 2022\n",
    "authors": [
      "Alexander Matt Turner",
      "Prasad Tadepalli"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2206.13477"
  },
  {
    "id": "arXiv:2206.14476",
    "title": "Can Push-forward Generative Models Fit Multimodal Distributions?",
    "abstract": "Comments: In Thirty-sixth Conference on Neural Information Processing Systems",
    "descriptor": "\nComments: In Thirty-sixth Conference on Neural Information Processing Systems\n",
    "authors": [
      "Antoine Salmona",
      "Valentin de Bortoli",
      "Julie Delon",
      "Agn\u00e8s Desolneux"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.14476"
  },
  {
    "id": "arXiv:2206.15049",
    "title": "ZeroC: A Neuro-Symbolic Model for Zero-shot Concept Recognition and  Acquisition at Inference Time",
    "abstract": "Comments: NeurIPS 2022; 28 pages, 9 figures",
    "descriptor": "\nComments: NeurIPS 2022; 28 pages, 9 figures\n",
    "authors": [
      "Tailin Wu",
      "Megan Tjandrasuwita",
      "Zhengxuan Wu",
      "Xuelin Yang",
      "Kevin Liu",
      "Rok Sosi\u010d",
      "Jure Leskovec"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.15049"
  },
  {
    "id": "arXiv:2207.01848",
    "title": "TabPFN: A Transformer That Solves Small Tabular Classification Problems  in a Second",
    "abstract": "TabPFN: A Transformer That Solves Small Tabular Classification Problems  in a Second",
    "descriptor": "",
    "authors": [
      "Noah Hollmann",
      "Samuel M\u00fcller",
      "Katharina Eggensperger",
      "Frank Hutter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.01848"
  },
  {
    "id": "arXiv:2207.02449",
    "title": "Information Compression and Performance Evaluation of Tic-Tac-Toe's  Evaluation Function Using Singular Value Decomposition",
    "abstract": "Comments: 15 pages, 5 figures, Updated contents",
    "descriptor": "\nComments: 15 pages, 5 figures, Updated contents\n",
    "authors": [
      "Naoya Fujita",
      "Hiroshi Watanabe"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.02449"
  },
  {
    "id": "arXiv:2207.02803",
    "title": "Delving into Sequential Patches for Deepfake Detection",
    "abstract": "Comments: Accepted to NeurIPS 2022",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Jiazhi Guan",
      "Hang Zhou",
      "Zhibin Hong",
      "Errui Ding",
      "Jingdong Wang",
      "Chengbin Quan",
      "Youjian Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.02803"
  },
  {
    "id": "arXiv:2207.04396",
    "title": "Scalable and Privacy-enhanced Graph Generative Model for Graph Neural  Networks",
    "abstract": "Scalable and Privacy-enhanced Graph Generative Model for Graph Neural  Networks",
    "descriptor": "",
    "authors": [
      "Minji Yoon",
      "Yue Wu",
      "John Palowitch",
      "Bryan Perozzi",
      "Ruslan Salakhutdinov"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2207.04396"
  },
  {
    "id": "arXiv:2207.05997",
    "title": "Noise level free regularisation of general linear inverse problems under  unconstrained white noise",
    "abstract": "Comments: 29 pages, 16 tables",
    "descriptor": "\nComments: 29 pages, 16 tables\n",
    "authors": [
      "Tim Jahn"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2207.05997"
  },
  {
    "id": "arXiv:2207.06105",
    "title": "GriddlyJS: A Web IDE for Reinforcement Learning",
    "abstract": "GriddlyJS: A Web IDE for Reinforcement Learning",
    "descriptor": "",
    "authors": [
      "Christopher Bamford",
      "Minqi Jiang",
      "Mikayel Samvelyan",
      "Tim Rockt\u00e4schel"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.06105"
  },
  {
    "id": "arXiv:2207.06456",
    "title": "Graph Neural Network Bandits",
    "abstract": "Comments: Accepted to Neurips2022, 37 pages, 8 figures",
    "descriptor": "\nComments: Accepted to Neurips2022, 37 pages, 8 figures\n",
    "authors": [
      "Parnian Kassraie",
      "Andreas Krause",
      "Ilija Bogunovic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.06456"
  },
  {
    "id": "arXiv:2207.07783",
    "title": "Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection",
    "abstract": "Comments: ECCV 2022 camera ready (Supplementary videos: on ECVA soon). This paper supersedes arXiv:2112.01479",
    "descriptor": "\nComments: ECCV 2022 camera ready (Supplementary videos: on ECVA soon). This paper supersedes arXiv:2112.01479\n",
    "authors": [
      "Kyle Min",
      "Sourya Roy",
      "Subarna Tripathi",
      "Tanaya Guha",
      "Somdeb Majumdar"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.07783"
  },
  {
    "id": "arXiv:2207.10615",
    "title": "The Gift That Keeps on Giving: Generosity is Contagious in Multiplayer  Online Games",
    "abstract": "Comments: 22 pages, 6 figures, 6 tables. To appear in the Proceedings of the ACM on Human-Computer Interaction (PACM HCI), CSCW 2022",
    "descriptor": "\nComments: 22 pages, 6 figures, 6 tables. To appear in the Proceedings of the ACM on Human-Computer Interaction (PACM HCI), CSCW 2022\n",
    "authors": [
      "Alexander J. Bisberg",
      "Julie Jiang",
      "Yilei Zeng",
      "Emily Chen",
      "Emilio Ferrara"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2207.10615"
  },
  {
    "id": "arXiv:2207.11690",
    "title": "HouseX: A Fine-grained House Music Dataset and its Potential in the  Music Industry",
    "abstract": "Comments: 7 pages. Accepted by APSIPA ASC 2022 to be held during Nov. 2022",
    "descriptor": "\nComments: 7 pages. Accepted by APSIPA ASC 2022 to be held during Nov. 2022\n",
    "authors": [
      "Xinyu Li"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2207.11690"
  },
  {
    "id": "arXiv:2207.13877",
    "title": "p-Adic Statistical Field Theory and Deep Belief Networks",
    "abstract": "Comments: Some additional typos were corrected",
    "descriptor": "\nComments: Some additional typos were corrected\n",
    "authors": [
      "W. A. Z\u00fa\u00f1iga-Galindo"
    ],
    "subjectives": [
      "Mathematical Physics (math-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.13877"
  },
  {
    "id": "arXiv:2207.14742",
    "title": "Graph Neural Networks for Channel Decoding",
    "abstract": "Comments: Source code is available online this https URL",
    "descriptor": "\nComments: Source code is available online this https URL\n",
    "authors": [
      "Sebastian Cammerer",
      "Jakob Hoydis",
      "Fay\u00e7al A\u00eft Aoudia",
      "Alexander Keller"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.14742"
  },
  {
    "id": "arXiv:2208.00789",
    "title": "Self-supervised learning with rotation-invariant kernels",
    "abstract": "Self-supervised learning with rotation-invariant kernels",
    "descriptor": "",
    "authors": [
      "L\u00e9on Zheng",
      "Gilles Puy",
      "Elisa Riccietti",
      "Patrick P\u00e9rez",
      "R\u00e9mi Gribonval"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.00789"
  },
  {
    "id": "arXiv:2208.01190",
    "title": "Toward 6G TK$\u03bc$ Extreme Connectivity: Architecture, Key Technologies  and Experiments",
    "abstract": "Comments: 8 pages, 4 figures, in peer review with IEEE Wireless Communications Magazine",
    "descriptor": "\nComments: 8 pages, 4 figures, in peer review with IEEE Wireless Communications Magazine\n",
    "authors": [
      "Xiaohu You",
      "Yongming Huang",
      "Shengheng Liu",
      "Dongming Wang",
      "Junchao Ma",
      "Chuan Zhang",
      "Hang Zhan",
      "Cheng Zhang",
      "Jiao Zhang",
      "Jin Li",
      "Min Zhu",
      "Jianjie You",
      "Dongjie Liu",
      "Shiwen He",
      "Guanghui He",
      "Fengyi Yang",
      "Yang Liu",
      "Jianjun Wu",
      "Jianmin Lu",
      "Ge Li",
      "Xiaowu Chen",
      "Wenguang Chen",
      "Wen Gao"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2208.01190"
  },
  {
    "id": "arXiv:2208.02431",
    "title": "A Primal-Dual-Based Power Control Approach for Capacitated Edge Servers",
    "abstract": "A Primal-Dual-Based Power Control Approach for Capacitated Edge Servers",
    "descriptor": "",
    "authors": [
      "Qinghui Zhang",
      "Weidong Li",
      "Qian Su",
      "Xuejie Zhang"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2208.02431"
  },
  {
    "id": "arXiv:2208.02433",
    "title": "Simulation and application of COVID-19 compartment model using  physics-informed neural network",
    "abstract": "Simulation and application of COVID-19 compartment model using  physics-informed neural network",
    "descriptor": "",
    "authors": [
      "Jinhuan Ke",
      "Jiahao Ma",
      "Xiyu Yin",
      "Robin Singh"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Physics and Society (physics.soc-ph)",
      "Populations and Evolution (q-bio.PE)"
    ],
    "url": "https://arxiv.org/abs/2208.02433"
  },
  {
    "id": "arXiv:2208.02812",
    "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with  Point-to-Pixel Prompting",
    "abstract": "Comments: Accepted to NeurIPS 2022, project page: this https URL",
    "descriptor": "\nComments: Accepted to NeurIPS 2022, project page: this https URL\n",
    "authors": [
      "Ziyi Wang",
      "Xumin Yu",
      "Yongming Rao",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.02812"
  },
  {
    "id": "arXiv:2208.03848",
    "title": "Information bottleneck theory of high-dimensional regression: relevancy,  efficiency and optimality",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Vudtiwat Ngampruetikorn",
      "David J. Schwab"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Statistical Mechanics (cond-mat.stat-mech)",
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.03848"
  },
  {
    "id": "arXiv:2208.04950",
    "title": "BabyNet: A Lightweight Network for Infant Reaching Action Recognition in  Unconstrained Environments to Support Future Pediatric Rehabilitation  Applications",
    "abstract": "Comments: Accepted to RO-MAN 2021",
    "descriptor": "\nComments: Accepted to RO-MAN 2021\n",
    "authors": [
      "Amel Dechemi",
      "Vikarn Bhakri",
      "Ipsita Sahin",
      "Arjun Modi",
      "Julya Mestas",
      "Pamodya Peiris",
      "Dannya Enriquez Barrundia",
      "Elena Kokkoni",
      "Konstantinos Karydis"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2208.04950"
  },
  {
    "id": "arXiv:2208.05117",
    "title": "NOTE: Robust Continual Test-time Adaptation Against Temporal Correlation",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Taesik Gong",
      "Jongheon Jeong",
      "Taewon Kim",
      "Yewon Kim",
      "Jinwoo Shin",
      "Sung-Ju Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.05117"
  },
  {
    "id": "arXiv:2208.05592",
    "title": "Patching open-vocabulary models by interpolating weights",
    "abstract": "Comments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Gabriel Ilharco",
      "Mitchell Wortsman",
      "Samir Yitzhak Gadre",
      "Shuran Song",
      "Hannaneh Hajishirzi",
      "Simon Kornblith",
      "Ali Farhadi",
      "Ludwig Schmidt"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.05592"
  },
  {
    "id": "arXiv:2208.06195",
    "title": "Category-Level Pose Retrieval with Contrastive Features Learnt with  Occlusion Augmentation",
    "abstract": "Comments: 29 pages, 16 Figures, 14 tables, BMVC 2022",
    "descriptor": "\nComments: 29 pages, 16 Figures, 14 tables, BMVC 2022\n",
    "authors": [
      "Georgios Kouros",
      "Shubham Shrivastava",
      "C\u00e9dric Picron",
      "Sushruth Nagesh",
      "Punarjay Chakravarty",
      "Tinne Tuytelaars"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.06195"
  },
  {
    "id": "arXiv:2208.08012",
    "title": "Disentangled Speaker Representation Learning via Mutual Information  Minimization",
    "abstract": "Comments: Accepted by APSIPA ASC 2022. Camera-ready. 8 pages, 4 figures, and 1 table",
    "descriptor": "\nComments: Accepted by APSIPA ASC 2022. Camera-ready. 8 pages, 4 figures, and 1 table\n",
    "authors": [
      "Sung Hwan Mun",
      "Min Hyun Han",
      "Minchan Kim",
      "Dongjune Lee",
      "Nam Soo Kim"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2208.08012"
  },
  {
    "id": "arXiv:2208.10354",
    "title": "Real-world-robustness of tree-based classifiers",
    "abstract": "Comments: 8 pages, 5 figures",
    "descriptor": "\nComments: 8 pages, 5 figures\n",
    "authors": [
      "Christoph Schweimer",
      "Sebastian Scher"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.10354"
  },
  {
    "id": "arXiv:2208.10458",
    "title": "Minimax-Optimal Multi-Agent RL in Markov Games With a Generative Model",
    "abstract": "Comments: accepted in part to NeurIPS 2022",
    "descriptor": "\nComments: accepted in part to NeurIPS 2022\n",
    "authors": [
      "Gen Li",
      "Yuejie Chi",
      "Yuting Wei",
      "Yuxin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Science and Game Theory (cs.GT)",
      "Information Theory (cs.IT)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.10458"
  },
  {
    "id": "arXiv:2208.10478",
    "title": "Secret-Key Agreement Using Physical Identifiers for Degraded and Less  Noisy Authentication Channels",
    "abstract": "Comments: A shorter version of this work is accepted to be presented at ITW 2022",
    "descriptor": "\nComments: A shorter version of this work is accepted to be presented at ITW 2022\n",
    "authors": [
      "Vamoua Yachongka",
      "Hideki Yagi",
      "Hideki Ochiai"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2208.10478"
  },
  {
    "id": "arXiv:2208.11445",
    "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable  Extrapolation in Large Language Models",
    "abstract": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable  Extrapolation in Large Language Models",
    "descriptor": "",
    "authors": [
      "Mirelle Bueno",
      "Carlos Gemmel",
      "Jeffrey Dalton",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2208.11445"
  },
  {
    "id": "arXiv:2208.12591",
    "title": "On the Implicit Bias in Deep-Learning Algorithms",
    "abstract": "Comments: Added a section on dynamical stability, and made some other small changes",
    "descriptor": "\nComments: Added a section on dynamical stability, and made some other small changes\n",
    "authors": [
      "Gal Vardi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.12591"
  },
  {
    "id": "arXiv:2208.13066",
    "title": "SA: Sliding attack for synthetic speech detection with resistance to  clipping and self-splicing",
    "abstract": "Comments: Updated description and formula",
    "descriptor": "\nComments: Updated description and formula\n",
    "authors": [
      "Deng JiaCheng",
      "Dong Li",
      "Yan Diqun",
      "Wang Rangding",
      "Zeng Jiaming"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Cryptography and Security (cs.CR)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2208.13066"
  },
  {
    "id": "arXiv:2208.13701",
    "title": "Empirical Gateaux Derivatives for Causal Inference",
    "abstract": "Comments: Comments welcome; conference version accepted at Neurips 2022",
    "descriptor": "\nComments: Comments welcome; conference version accepted at Neurips 2022\n",
    "authors": [
      "Michael I. Jordan",
      "Yixin Wang",
      "Angela Zhou"
    ],
    "subjectives": [
      "Methodology (stat.ME)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.13701"
  },
  {
    "id": "arXiv:2209.00779",
    "title": "Optimal General Factor Problem and Jump System Intersection",
    "abstract": "Optimal General Factor Problem and Jump System Intersection",
    "descriptor": "",
    "authors": [
      "Yusuke Kobayashi"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.00779"
  },
  {
    "id": "arXiv:2209.05471",
    "title": "PATE: Property, Amenities, Traffic and Emotions Coming Together for Real  Estate Price Prediction",
    "abstract": "Comments: Accepted by IEEE DSAA 2022. 10 pages, 3 figures",
    "descriptor": "\nComments: Accepted by IEEE DSAA 2022. 10 pages, 3 figures\n",
    "authors": [
      "Yaping Zhao",
      "Ramgopal Ravi",
      "Shuhui Shi",
      "Zhongrui Wang",
      "Edmund Y. Lam",
      "Jichang Zhao"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.05471"
  },
  {
    "id": "arXiv:2209.07285",
    "title": "Identifying research supporting the United Nations Sustainable  Development Goals",
    "abstract": "Comments: 12 pages, 3 figures, 6 tables, 19 references",
    "descriptor": "\nComments: 12 pages, 3 figures, 6 tables, 19 references\n",
    "authors": [
      "Yury Kashnitsky",
      "Guillaume Roberge",
      "Jingwen Mu",
      "Kevin Kang",
      "Weiwei Wang",
      "Maurice Vanderfeesten",
      "Maxim Rivest",
      "Lennart Ke\u00dfler",
      "Robert Jaworek",
      "Ma\u00e9va Vignes",
      "Bamini Jayabalasingham",
      "Finne Boonen",
      "Chris James",
      "Marius Doornenbal",
      "Isabelle Labrosse"
    ],
    "subjectives": [
      "Digital Libraries (cs.DL)"
    ],
    "url": "https://arxiv.org/abs/2209.07285"
  },
  {
    "id": "arXiv:2209.07413",
    "title": "EZNAS: Evolving Zero Cost Proxies For Neural Architecture Scoring",
    "abstract": "EZNAS: Evolving Zero Cost Proxies For Neural Architecture Scoring",
    "descriptor": "",
    "authors": [
      "Yash Akhauri",
      "J. Pablo Munoz",
      "Nilesh Jain",
      "Ravi Iyer"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2209.07413"
  },
  {
    "id": "arXiv:2209.08514",
    "title": "Imbalanced Node Processing Method in Graph Neural Network Classification  Task",
    "abstract": "Comments: Insufficient experiments, will continue to study in depth",
    "descriptor": "\nComments: Insufficient experiments, will continue to study in depth\n",
    "authors": [
      "Min Liu",
      "Siwen Jin",
      "Luo Jin",
      "Shuohan Wang",
      "Yu Fang",
      "Yuliang Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.08514"
  },
  {
    "id": "arXiv:2209.10372",
    "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
    "abstract": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
    "descriptor": "",
    "authors": [
      "Hui Su",
      "Xiao Zhou",
      "Houjin Yu",
      "Yuwen Chen",
      "Zilin Zhu",
      "Yang Yu",
      "Jie Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.10372"
  },
  {
    "id": "arXiv:2209.12226",
    "title": "Re-contextualizing Fairness in NLP: The Case of India",
    "abstract": "Comments: Accepted to AACL-IJCNLP 2022",
    "descriptor": "\nComments: Accepted to AACL-IJCNLP 2022\n",
    "authors": [
      "Shaily Bhatt",
      "Sunipa Dev",
      "Partha Talukdar",
      "Shachi Dave",
      "Vinodkumar Prabhakaran"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.12226"
  },
  {
    "id": "arXiv:2209.12362",
    "title": "Multi-dataset Training of Transformers for Robust Action Recognition",
    "abstract": "Comments: Accepted by NeurIPS 2022. Camera-ready. Code and models are available at this https URL",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. Camera-ready. Code and models are available at this https URL\n",
    "authors": [
      "Junwei Liang",
      "Enwei Zhang",
      "Jun Zhang",
      "Chunhua Shen"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.12362"
  },
  {
    "id": "arXiv:2209.13020",
    "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial  Intelligence with Humans",
    "abstract": "Comments: Forthcoming in Northwestern Journal of Technology and Intellectual Property, Volume 20",
    "descriptor": "\nComments: Forthcoming in Northwestern Journal of Technology and Intellectual Property, Volume 20\n",
    "authors": [
      "John J. Nay"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13020"
  },
  {
    "id": "arXiv:2209.13394",
    "title": "Magnitude and Angle Dynamics in Training Single ReLU Neurons",
    "abstract": "Magnitude and Angle Dynamics in Training Single ReLU Neurons",
    "descriptor": "",
    "authors": [
      "Sangmin Lee",
      "Byeongsu Sim",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13394"
  },
  {
    "id": "arXiv:2209.13507",
    "title": "CrossDTR: Cross-view and Depth-guided Transformers for 3D Object  Detection",
    "abstract": "CrossDTR: Cross-view and Depth-guided Transformers for 3D Object  Detection",
    "descriptor": "",
    "authors": [
      "Ching-Yu Tseng",
      "Yi-Rong Chen",
      "Hsin-Ying Lee",
      "Tsung-Han Wu",
      "Wen-Chin Chen",
      "Winston H. Hsu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.13507"
  },
  {
    "id": "arXiv:2209.14369",
    "title": "Social Search: retrieving information in Online Social Platforms -- A  Survey",
    "abstract": "Social Search: retrieving information in Online Social Platforms -- A  Survey",
    "descriptor": "",
    "authors": [
      "Maddalena Amendola",
      "Andrea Passarella",
      "Raffaele Perego"
    ],
    "subjectives": [
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2209.14369"
  },
  {
    "id": "arXiv:2209.15190",
    "title": "Neural Integral Equations",
    "abstract": "Comments: 10 + 12 pages, 12 figures and 7 tables. Comments are welcome! v2: One reference has been fixed. Codes will be made public after the peer-review process",
    "descriptor": "\nComments: 10 + 12 pages, 12 figures and 7 tables. Comments are welcome! v2: One reference has been fixed. Codes will be made public after the peer-review process\n",
    "authors": [
      "Emanuele Zappala",
      "Antonio Henrique de Oliveira Fonseca",
      "Josue Ortega Caro",
      "David van Dijk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.15190"
  },
  {
    "id": "arXiv:2209.15565",
    "title": "Augmenting Operations Research with Auto-Formulation of Optimization  Models from Problem Descriptions",
    "abstract": "Comments: Accepted for presentation at the EMNLP 2022 Conference (industry track)",
    "descriptor": "\nComments: Accepted for presentation at the EMNLP 2022 Conference (industry track)\n",
    "authors": [
      "Rindranirina Ramamonjison",
      "Haley Li",
      "Timothy T. Yu",
      "Shiqi He",
      "Vishnu Rengan",
      "Amin Banitalebi-Dehkordi",
      "Zirui Zhou",
      "Yong Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15565"
  },
  {
    "id": "arXiv:2209.15618",
    "title": "Beyond Bayes-optimality: meta-learning what you know you don't know",
    "abstract": "Comments: 33 pages, 8 figures, technical report",
    "descriptor": "\nComments: 33 pages, 8 figures, technical report\n",
    "authors": [
      "Jordi Grau-Moya",
      "Gr\u00e9goire Del\u00e9tang",
      "Markus Kunesch",
      "Tim Genewein",
      "Elliot Catt",
      "Kevin Li",
      "Anian Ruoss",
      "Chris Cundy",
      "Joel Veness",
      "Jane Wang",
      "Marcus Hutter",
      "Christopher Summerfield",
      "Shane Legg",
      "Pedro Ortega"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15618"
  },
  {
    "id": "arXiv:2210.00208",
    "title": "Summing free unitary Brownian motions with applications to quantum  information",
    "abstract": "Comments: The characteristic curves are determined",
    "descriptor": "\nComments: The characteristic curves are determined\n",
    "authors": [
      "Nizar Demni",
      "Tarek Hamdi"
    ],
    "subjectives": [
      "Probability (math.PR)",
      "Information Theory (cs.IT)",
      "Operator Algebras (math.OA)"
    ],
    "url": "https://arxiv.org/abs/2210.00208"
  },
  {
    "id": "arXiv:2210.00421",
    "title": "Multiple Access Channel in Massive Multi-User MIMO Using Group Testing",
    "abstract": "Comments: 12 pages, 2 figures",
    "descriptor": "\nComments: 12 pages, 2 figures\n",
    "authors": [
      "George Vershinin",
      "Asaf Cohen",
      "Omer Gurewitz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2210.00421"
  },
  {
    "id": "arXiv:2210.00453",
    "title": "Neural Graphical Models",
    "abstract": "Neural Graphical Models",
    "descriptor": "",
    "authors": [
      "Harsh Shrivastava",
      "Urszula Chajewska"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.00453"
  },
  {
    "id": "arXiv:2210.02097",
    "title": "Teaching Yourself:Graph Self-Distillation on Neighborhood for Node  Classification",
    "abstract": "Teaching Yourself:Graph Self-Distillation on Neighborhood for Node  Classification",
    "descriptor": "",
    "authors": [
      "Lirong Wu",
      "Jun Xia",
      "Haitao Lin",
      "Zhangyang Gao",
      "Zicheng Liu",
      "Guojiang Zhao",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.02097"
  },
  {
    "id": "arXiv:2210.02191",
    "title": "On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks",
    "abstract": "On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks",
    "descriptor": "",
    "authors": [
      "Huimin Zeng",
      "Zhenrui Yue",
      "Yang Zhang",
      "Ziyi Kou",
      "Lanyu Shang",
      "Dong Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.02191"
  },
  {
    "id": "arXiv:2210.02534",
    "title": "Performing live time-traversal queries via SPARQL on RDF datasets",
    "abstract": "Comments: 26 pages, 10 figures, 3 tables, submitted to the Journal of the Association for Information Science and Technology (JASIST)",
    "descriptor": "\nComments: 26 pages, 10 figures, 3 tables, submitted to the Journal of the Association for Information Science and Technology (JASIST)\n",
    "authors": [
      "Arcangelo Massari",
      "Silvio Peroni"
    ],
    "subjectives": [
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.02534"
  },
  {
    "id": "arXiv:2210.02713",
    "title": "On Optimal Learning Under Targeted Data Poisoning",
    "abstract": "On Optimal Learning Under Targeted Data Poisoning",
    "descriptor": "",
    "authors": [
      "Steve Hanneke",
      "Amin Karbasi",
      "Mohammad Mahmoody",
      "Idan Mehalel",
      "Shay Moran"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2210.02713"
  },
  {
    "id": "arXiv:2210.02762",
    "title": "Vision Transformer Based Model for Describing a Set of Images as a Story",
    "abstract": "Comments: This paper has been accepted at the 35th Australasian Joint Conference on Artificial Intelligence 2022 (Camera-ready version is attached)",
    "descriptor": "\nComments: This paper has been accepted at the 35th Australasian Joint Conference on Artificial Intelligence 2022 (Camera-ready version is attached)\n",
    "authors": [
      "Zainy M. Malakan",
      "Ghulam Mubashar Hassan",
      "Ajmal Mian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.02762"
  },
  {
    "id": "arXiv:2210.04068",
    "title": "IcebergHT: High Performance PMEM Hash Tables Through Stability and Low  Associativity",
    "abstract": "IcebergHT: High Performance PMEM Hash Tables Through Stability and Low  Associativity",
    "descriptor": "",
    "authors": [
      "Prashant Pandey",
      "Michael A. Bender",
      "Alex Conway",
      "Mart\u00edn Farach-Colton",
      "William Kuszmaul",
      "Guido Tagliavini",
      "Rob Johnson"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Databases (cs.DB)"
    ],
    "url": "https://arxiv.org/abs/2210.04068"
  },
  {
    "id": "arXiv:2210.04074",
    "title": "Are All Steps Equally Important? Benchmarking Essentiality Detection of  Events",
    "abstract": "Are All Steps Equally Important? Benchmarking Essentiality Detection of  Events",
    "descriptor": "",
    "authors": [
      "Hongming Zhang",
      "Yueguan Wang",
      "Yuqian Deng",
      "Haoyu Wang",
      "Muhao Chen",
      "Dan Roth"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04074"
  },
  {
    "id": "arXiv:2210.04092",
    "title": "Advancing Model Pruning via Bi-level Optimization",
    "abstract": "Comments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)",
    "descriptor": "\nComments: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Yihua Zhang",
      "Yuguang Yao",
      "Parikshit Ram",
      "Pu Zhao",
      "Tianlong Chen",
      "Mingyi Hong",
      "Yanzhi Wang",
      "Sijia Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04092"
  },
  {
    "id": "arXiv:2210.04114",
    "title": "Towards Real-Time Temporal Graph Learning",
    "abstract": "Towards Real-Time Temporal Graph Learning",
    "descriptor": "",
    "authors": [
      "Deniz Gurevin",
      "Mohsin Shan",
      "Tong Geng",
      "Weiwen Jiang",
      "Caiwen Ding",
      "Omer Khan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04114"
  },
  {
    "id": "arXiv:2210.04318",
    "title": "Prediction intervals for neural network models using weighted asymmetric  loss functions",
    "abstract": "Comments: 14 pages, 4 figures, not submitted for conference yet as of 09-10-2022",
    "descriptor": "\nComments: 14 pages, 4 figures, not submitted for conference yet as of 09-10-2022\n",
    "authors": [
      "Milo Grillo",
      "Agnieszka Werpachowska"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04318"
  },
  {
    "id": "arXiv:2210.04443",
    "title": "Don't Copy the Teacher: Data and Model Challenges in Embodied Dialogue",
    "abstract": "Comments: To Appear in the Proceedings of EMNLP 2022",
    "descriptor": "\nComments: To Appear in the Proceedings of EMNLP 2022\n",
    "authors": [
      "So Yeon Min",
      "Hao Zhu",
      "Ruslan Salakhutdinov",
      "Yonatan Bisk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04443"
  },
  {
    "id": "arXiv:2210.04600",
    "title": "YFACC: A Yor\u00f9b\u00e1 speech-image dataset for cross-lingual keyword  localisation through visual grounding",
    "abstract": "Comments: Accepted to IEEE SLT 2022",
    "descriptor": "\nComments: Accepted to IEEE SLT 2022\n",
    "authors": [
      "Kayode Olaleye",
      "Dan Oneata",
      "Herman Kamper"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2210.04600"
  },
  {
    "id": "arXiv:2210.04633",
    "title": "CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models  for Programming Language Attend Code Structure",
    "abstract": "Comments: To appear in EMNLP 2022",
    "descriptor": "\nComments: To appear in EMNLP 2022\n",
    "authors": [
      "Nuo Chen",
      "Qiushi Sun",
      "Renyu Zhu",
      "Xiang Li",
      "Xuesong Lu",
      "Ming Gao"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2210.04633"
  },
  {
    "id": "arXiv:2210.04664",
    "title": "Depth-First Grover Search Algorithm on Hybrid Quantum-Classical Computer",
    "abstract": "Comments: 10 pages, 5 figures",
    "descriptor": "\nComments: 10 pages, 5 figures\n",
    "authors": [
      "Haoxiang Guo"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2210.04664"
  },
  {
    "id": "arXiv:2210.04705",
    "title": "Readability Controllable Biomedical Document Summarization",
    "abstract": "Comments: accepted to the Findings of EMNLP 2022",
    "descriptor": "\nComments: accepted to the Findings of EMNLP 2022\n",
    "authors": [
      "Zheheng Luo",
      "Qianqian Xie",
      "Sophia Ananiadou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04705"
  },
  {
    "id": "arXiv:2210.04776",
    "title": "Contrastive Learning Approach for Semi-Supervised Seismic Facies  Identification Using High-Confidence Representations",
    "abstract": "Contrastive Learning Approach for Semi-Supervised Seismic Facies  Identification Using High-Confidence Representations",
    "descriptor": "",
    "authors": [
      "Kewen Li",
      "Wenlong Liu",
      "Yimin Dou",
      "Zhifeng Xu",
      "Hongjie Duan",
      "Ruilin Jing"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2210.04776"
  },
  {
    "id": "arXiv:2210.04787",
    "title": "LMQFormer: A Laplace-Prior-Guided Mask Query Transformer for Lightweight  Snow Removal",
    "abstract": "Comments: 9 pages, 11 figures",
    "descriptor": "\nComments: 9 pages, 11 figures\n",
    "authors": [
      "Junhong Lin",
      "Nanfeng Jiang",
      "Zhentao Zhang",
      "Weiling Chen",
      "Tiesong Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.04787"
  },
  {
    "id": "arXiv:2210.04828",
    "title": "Assessing Neural Referential Form Selectors on a Realistic Multilingual  Dataset",
    "abstract": "Comments: Eval4NLP workshop",
    "descriptor": "\nComments: Eval4NLP workshop\n",
    "authors": [
      "Guanyi Chen",
      "Fahime Same",
      "Kees van Deemter"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.04828"
  },
  {
    "id": "arXiv:2210.04949",
    "title": "A Hybrid Active-Passive Approach to Imbalanced Nonstationary Data Stream  Classification",
    "abstract": "Comments: Keywords: incremental learning, concept drift, class imbalance, data streams, nonstationary environments",
    "descriptor": "\nComments: Keywords: incremental learning, concept drift, class imbalance, data streams, nonstationary environments\n",
    "authors": [
      "Kleanthis Malialis",
      "Manuel Roveri",
      "Cesare Alippi",
      "Christos G. Panayiotou",
      "Marios M. Polycarpou"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04949"
  },
  {
    "id": "arXiv:2210.04992",
    "title": "Extracting or Guessing? Improving Faithfulness of Event Temporal  Relation Extraction",
    "abstract": "Extracting or Guessing? Improving Faithfulness of Event Temporal  Relation Extraction",
    "descriptor": "",
    "authors": [
      "Haoyu Wang",
      "Hongming Zhang",
      "Yuqian Deng",
      "Jacob R. Gardner",
      "Dan Roth",
      "Muhao Chen"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.04992"
  },
  {
    "id": "arXiv:2210.04993",
    "title": "Learning with an Evolving Class Ontology",
    "abstract": "Comments: NeurIPS 2022; Website: this https URL",
    "descriptor": "\nComments: NeurIPS 2022; Website: this https URL\n",
    "authors": [
      "Zhiqiu Lin",
      "Deepak Pathak",
      "Yu-Xiong Wang",
      "Deva Ramanan",
      "Shu Kong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.04993"
  },
  {
    "id": "arXiv:2210.05152",
    "title": "TriangleNet: Edge Prior Augmented Network for Semantic Segmentation  through Cross-Task Consistency",
    "abstract": "Comments: 22 pages, 3 figures",
    "descriptor": "\nComments: 22 pages, 3 figures\n",
    "authors": [
      "Dan Zhang",
      "Rui Zheng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05152"
  },
  {
    "id": "arXiv:2210.05164",
    "title": "Tight Error Bounds for Nonnegative Orthogonality Constraints and Exact  Penalties",
    "abstract": "Tight Error Bounds for Nonnegative Orthogonality Constraints and Exact  Penalties",
    "descriptor": "",
    "authors": [
      "Xiaojun Chen",
      "Yifan He",
      "Zaikun Zhang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2210.05164"
  },
  {
    "id": "arXiv:2210.05225",
    "title": "A Formalisation of a Fast Fourier Transform",
    "abstract": "A Formalisation of a Fast Fourier Transform",
    "descriptor": "",
    "authors": [
      "Laurent Th\u00e9ry"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05225"
  },
  {
    "id": "arXiv:2210.05236",
    "title": "Planning Assembly Sequence with Graph Transformer",
    "abstract": "Comments: Submitted to ICRA2023",
    "descriptor": "\nComments: Submitted to ICRA2023\n",
    "authors": [
      "Lin Ma",
      "Jiangtao Gong",
      "Hao Xu",
      "Hao Chen",
      "Hao Zhao",
      "Wenbing Huang",
      "Guyue Zhou"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05236"
  },
  {
    "id": "arXiv:2210.05245",
    "title": "PatternRank: Leveraging Pretrained Language Models and Part of Speech  for Unsupervised Keyphrase Extraction",
    "abstract": "Comments: Accepted to 14th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - KDIR",
    "descriptor": "\nComments: Accepted to 14th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management - KDIR\n",
    "authors": [
      "Tim Schopf",
      "Simon Klimek",
      "Florian Matthes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2210.05245"
  },
  {
    "id": "arXiv:2210.05287",
    "title": "Revisiting and Advancing Chinese Natural Language Understanding with  Accelerated Heterogeneous Knowledge Pre-training",
    "abstract": "Comments: EMNLP 2022 industry track",
    "descriptor": "\nComments: EMNLP 2022 industry track\n",
    "authors": [
      "Taolin Zhang",
      "Junwei Dong",
      "Jianing Wang",
      "Chengyu Wang",
      "Ang Wang",
      "Yinghui Liu",
      "Jun Huang",
      "Yong Li",
      "Xiaofeng He"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2210.05287"
  },
  {
    "id": "arXiv:2210.05476",
    "title": "Medha: Microcoded Hardware Accelerator for computing on Encrypted Data",
    "abstract": "Comments: This paper will appear at IACR Transactions on Cryptographic Hardware and Embedded Systems 2023",
    "descriptor": "\nComments: This paper will appear at IACR Transactions on Cryptographic Hardware and Embedded Systems 2023\n",
    "authors": [
      "Ahmet Can Mert",
      "Aikata",
      "Sunmin Kwon",
      "Youngsam Shin",
      "Donghoon Yoo",
      "Yongwoo Lee",
      "Sujoy Sinha Roy"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2210.05476"
  },
  {
    "id": "arXiv:2210.05491",
    "title": "Strong negation in the theory of computable functionals TCF",
    "abstract": "Strong negation in the theory of computable functionals TCF",
    "descriptor": "",
    "authors": [
      "Nils K\u00f6pp",
      "Iosif Petrakis"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2210.05491"
  },
  {
    "id": "arXiv:2210.05553",
    "title": "Evaluating Unsupervised Denoising Requires Unsupervised Metrics",
    "abstract": "Evaluating Unsupervised Denoising Requires Unsupervised Metrics",
    "descriptor": "",
    "authors": [
      "Adria Marcos-Morales",
      "Matan Leibovich",
      "Sreyas Mohan",
      "Joshua Lawrence Vincent",
      "Piyush Haluai",
      "Mai Tan",
      "Peter Crozier",
      "Carlos Fernandez-Granda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05553"
  },
  {
    "id": "arXiv:2210.05567",
    "title": "Global Spectral Filter Memory Network for Video Object Segmentation",
    "abstract": "Comments: ECCV2022",
    "descriptor": "\nComments: ECCV2022\n",
    "authors": [
      "Yong Liu",
      "Ran Yu",
      "Jiahao Wang",
      "Xinyuan Zhao",
      "Yitong Wang",
      "Yansong Tang",
      "Yujiu Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05567"
  },
  {
    "id": "arXiv:2210.05574",
    "title": "Motion Aware Self-Supervision for Generic Event Boundary Detection",
    "abstract": "Comments: Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023",
    "descriptor": "\nComments: Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2023\n",
    "authors": [
      "Ayush K. Rai",
      "Tarun Krishna",
      "Julia Dietlmeier",
      "Kevin McGuinness",
      "Alan F. Smeaton",
      "Noel E. O'Connor"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2210.05574"
  },
  {
    "id": "arXiv:2210.05666",
    "title": "Point Transformer V2: Grouped Vector Attention and Partition-based  Pooling",
    "abstract": "Comments: Accepted at NeurIPS 2022",
    "descriptor": "\nComments: Accepted at NeurIPS 2022\n",
    "authors": [
      "Xiaoyang Wu",
      "Yixing Lao",
      "Li Jiang",
      "Xihui Liu",
      "Hengshuang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05666"
  },
  {
    "id": "arXiv:2210.05668",
    "title": "Understanding Embodied Reference with Touch-Line Transformer",
    "abstract": "Comments: Code: this https URL",
    "descriptor": "\nComments: Code: this https URL\n",
    "authors": [
      "Yang Li",
      "Xiaoxue Chen",
      "Hao Zhao",
      "Jiangtao Gong",
      "Guyue Zhou",
      "Federico Rossano",
      "Yixin Zhu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2210.05668"
  }
]