[
  {
    "id": "arXiv:2209.15010",
    "title": "A deep learning approach to the probabilistic numerical solution of  path-dependent partial differential equations",
    "abstract": "Recent work on Path-Dependent Partial Differential Equations (PPDEs) has\nshown that PPDE solutions can be approximated by a probabilistic\nrepresentation, implemented in the literature by the estimation of conditional\nexpectations using regression. However, a limitation of this approach is to\nrequire the selection of a basis in a function space. In this paper, we\novercome this limitation by the use of deep learning methods, and we show that\nthis setting allows for the derivation of error bounds on the approximation of\nconditional expectations. Numerical examples based on a two-person zero-sum\ngame, as well as on Asian and barrier option pricing, are presented. In\ncomparison with other deep learning approaches, our algorithm appears to be\nmore accurate, especially in large dimensions.",
    "descriptor": "",
    "authors": [
      "Jiang Yu Nguwi",
      "Nicolas Privault"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Analysis of PDEs (math.AP)",
      "Numerical Analysis (math.NA)",
      "Probability (math.PR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15010"
  },
  {
    "id": "arXiv:2209.15011",
    "title": "Does Collaborative Editing Help Mitigate Security Vulnerabilities in  Crowd-Shared IoT Code Examples?",
    "abstract": "Background: With the proliferation of crowd-sourced developer forums,\nsoftware developers are increasingly sharing more coding solutions to\nprogramming problems with others in forums. The decentralized nature of\nknowledge sharing on sites has raised the concern of sharing security\nvulnerable code, which then can be reused into mission critical software\nsystems - making those systems vulnerable in the process. Collaborative editing\nhas been introduced in forums like Stack Overflow to improve the quality of the\nshared contents. Aim: In this paper, we investigate whether code editing can\nmitigate shared vulnerable code examples by analyzing IoT code snippets and\ntheir revisions in three Stack Exchange sites: Stack Overflow, Arduino, and\nRaspberry Pi. Method:We analyze the vulnerabilities present in shared IoT C/C++\ncode snippets, as C/C++ is one of the most widely used languages in\nmission-critical devices and low-powered IoT devices. We further analyse the\nrevisions made to these code snippets, and their effects. Results: We find\nseveral vulnerabilities such as CWE 788 - Access of Memory Location After End\nof Buffer, in 740 code snippets . However, we find the vast majority of posts\nare not revised, or revisions are not made to the code snippets themselves (598\nout of 740). We also find that revisions are most likely to result in no change\nto the number of vulnerabilities in a code snippet rather than deteriorating or\nimproving the snippet. Conclusions: We conclude that the current collaborative\nediting system in the forums may be insufficient to help mitigate\nvulnerabilities in the shared code.",
    "descriptor": "\nComments: 10 pages, 14 figures, ESEM22\n",
    "authors": [
      "Madhu Selvaraj",
      "Gias Uddin"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.15011"
  },
  {
    "id": "arXiv:2209.15029",
    "title": "Multimodal analogs to infer humanities visualization requirements",
    "abstract": "Gaps and requirements for multi-modal interfaces for humanities can be\nexplored by observing the configuration of real-world environments and the\ntasks of visitors within them compared to digital environments. Examples\ninclude stores, museums, galleries, and stages with tasks similar to\nvisualization tasks such as overview, zoom and detail; multi-dimensional\nreduction; collaboration; and comparison; with real-world environments offering\nmuch richer interactions. Some of these capabilities exist with the technology\nand visualization research, but not routinely available in implementations.",
    "descriptor": "\nComments: 6 pages, 11 figures. Visualization for Digital Humanities 2022\n",
    "authors": [
      "Richard Brath"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.15029"
  },
  {
    "id": "arXiv:2209.15031",
    "title": "Automatic Data Augmentation via Invariance-Constrained Learning",
    "abstract": "Underlying data structures, such as symmetries or invariances to\ntransformations, are often exploited to improve the solution of learning tasks.\nHowever, embedding these properties in models or learning algorithms can be\nchallenging and computationally intensive. Data augmentation, on the other\nhand, induces these symmetries during training by applying multiple\ntransformations to the input data. Despite its ubiquity, its effectiveness\ndepends on the choices of which transformations to apply, when to do so, and\nhow often. In fact, there is both empirical and theoretical evidence that the\nindiscriminate use of data augmentation can introduce biases that outweigh its\nbenefits. This work tackles these issues by automatically adapting the data\naugmentation while solving the learning task. To do so, it formulates data\naugmentation as an invariance-constrained learning problem and leverages Monte\nCarlo Markov Chain (MCMC) sampling to solve it. The result is a practical\nalgorithm that not only does away with a priori searches for augmentation\ndistributions, but also dynamically controls if and when data augmentation is\napplied. Our experiments illustrate the performance of this method, which\nachieves state-of-the-art results in automatic data augmentation benchmarks for\nCIFAR datasets. Furthermore, this approach can be used to gather insights on\nthe actual symmetries underlying a learning task.",
    "descriptor": "",
    "authors": [
      "Ignacio Hounie",
      "Luiz F. O. Chamon",
      "Alejandro Ribeiro"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15031"
  },
  {
    "id": "arXiv:2209.15034",
    "title": "Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR  Image Retrieval",
    "abstract": "Spaceborne synthetic aperture radar (SAR) can provide accurate images of the\nocean surface roughness day-or-night in nearly all weather conditions, being an\nunique asset for many geophysical applications. Considering the huge amount of\ndata daily acquired by satellites, automated techniques for physical features\nextraction are needed. Even if supervised deep learning methods attain\nstate-of-the-art results, they require great amount of labeled data, which are\ndifficult and excessively expensive to acquire for ocean SAR imagery. To this\nend, we use the subaperture decomposition (SD) algorithm to enhance the\nunsupervised learning retrieval on the ocean surface, empowering ocean\nresearchers to search into large ocean databases. We empirically prove that SD\nimprove the retrieval precision with over 20% for an unsupervised transformer\nauto-encoder network. Moreover, we show that SD brings important performance\nboost when Doppler centroid images are used as input data, leading the way to\nnew unsupervised physics guided retrieval algorithms.",
    "descriptor": "",
    "authors": [
      "Nicolae-C\u0103t\u0103lin Ristea",
      "Andrei Anghel",
      "Mihai Datcu",
      "Bertrand Chapron"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15034"
  },
  {
    "id": "arXiv:2209.15040",
    "title": "Wafer-Scale Fast Fourier Transforms",
    "abstract": "We have implemented fast Fourier transforms for one, two, and\nthree-dimensional arrays on the Cerebras CS-2, a system whose memory and\nprocessing elements reside on a single silicon wafer. The wafer-scale engine\n(WSE) encompasses a two-dimensional mesh of roughly 850,000 processing elements\n(PEs) with fast local memory and equally fast nearest-neighbor\ninterconnections.\nOur wafer-scale FFT (wsFFT) parallelizes a $n^3$ problem with up to $n^2$\nPEs. At this point a PE processes only a single vector of the 3D domain (known\nas a pencil) per superstep, where each of the three supersteps performs FFT\nalong one of the three axes of the input array. Between supersteps, wsFFT\nredistributes (transposes) the data to bring all elements of each\none-dimensional pencil being transformed into the memory of a single PE. Each\nredistribution causes an all-to-all communication along one of the mesh\ndimensions. Given the level of parallelism, the size of the messages\ntransmitted between pairs of PEs can be as small as a single word. In theory, a\nmesh is not ideal for all-to-all communication due to its limited bisection\nbandwidth. However, the mesh interconnecting PEs on the WSE lies entirely\non-wafer and achieves nearly peak bandwidth even with tiny messages.\nThis high efficiency on fine-grain communication allow wsFFT to achieve\nunprecedented levels of parallelism and performance. We analyse in detail\ncomputation and communication time, as well as the weak and strong scaling,\nusing both FP16 and FP32 precision. With 32-bit arithmetic on the CS-2, we\nachieve 959 microseconds for 3D FFT of a $512^3$ complex input array using a\n512x512 subgrid of the on-wafer PEs. This is the largest ever parallelization\nfor this problem size and the first implementation that breaks the millisecond\nbarrier.",
    "descriptor": "",
    "authors": [
      "Marcelo Orenes-Vera",
      "Ilya Sharapov",
      "Robert Schreiber",
      "Mathias Jacquelin",
      "Philippe Vandermersch",
      "Sharan Chetlur"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2209.15040"
  },
  {
    "id": "arXiv:2209.15041",
    "title": "Summarizing text to embed qualitative data into visualizations",
    "abstract": "Qualitative data can be conveyed with strings of text. Fitting longer text\ninto visualizations requires a) space to place the text inside the\nvisualization; and b) appropriate text to fit the space available. For\nquantitative visualizations, space is available in area marks; or within\nvisualization layouts where the marks have an implied space (e.g. bar charts).\nFor qualitative visualizations, space is defined in common text layouts such as\nprose paragraphs. To fit text within these layouts is a function for emerging\nNLP capabilities such as summarization.",
    "descriptor": "\nComments: 6 pages, 8 figures, accepted at NLVIZ 2022: Exploring Research Opportunities for Natural Language, Text, and Data Visualization\n",
    "authors": [
      "Richard Brath"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.15041"
  },
  {
    "id": "arXiv:2209.15042",
    "title": "Generalizability of Adversarial Robustness Under Distribution Shifts",
    "abstract": "Recent progress in empirical and certified robustness promises to deliver\nreliable and deployable Deep Neural Networks (DNNs). Despite that success, most\nexisting evaluations of DNN robustness have been done on images sampled from\nthe same distribution that the model was trained on. Yet, in the real world,\nDNNs may be deployed in dynamic environments that exhibit significant\ndistribution shifts. In this work, we take a first step towards thoroughly\ninvestigating the interplay between empirical and certified adversarial\nrobustness on one hand and domain generalization on another. To do so, we train\nrobust models on multiple domains and evaluate their accuracy and robustness on\nan unseen domain. We observe that: (1) both empirical and certified robustness\ngeneralize to unseen domains, and (2) the level of generalizability does not\ncorrelate well with input visual similarity, measured by the FID between source\nand target domains. We also extend our study to cover a real-world medical\napplication, in which adversarial augmentation enhances both the robustness and\ngeneralization accuracy in unseen domains.",
    "descriptor": "",
    "authors": [
      "Kumail Alhamoud",
      "Hasan Abed Al Kader Hammoud",
      "Motasem Alfarra",
      "Bernard Ghanem"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15042"
  },
  {
    "id": "arXiv:2209.15050",
    "title": "On Second Order Rate Regions for the Static Scalar Gaussian Broadcast  Channel",
    "abstract": "This paper considers the single antenna, static Gaussian broadcast channel in\nthe finite blocklength regime. Second order achievable and converse rate\nregions are presented. Both a global reliability requirement and per-user\nreliability requirements are considered. The two-user case is analyzed in\ndetail, and generalizations to the $K$-user case are also discussed. The\nlargest second order achievable region presented here requires both\nsuperposition and rate splitting in the code construction, as opposed to the\n(infinite blocklength, first order) capacity region which does not require rate\nsplitting. Indeed, the finite blocklength penalty causes superposition alone to\nunder-perform other coding techniques in some parts of the region. In the\ntwo-user case with per-user reliability requirements, the capacity achieving\nsuperposition coding order (with the codeword of the user with the smallest SNR\nas cloud center) does not necessarily gives the largest second order region.\nInstead, the message of the user with the smallest point-to-point second order\ncapacity should be encoded in the cloud center in order to obtain the largest\nsecond order region for the proposed scheme.",
    "descriptor": "",
    "authors": [
      "Daniela Tuninetti",
      "Paul Sheldon",
      "Besma Smida",
      "Natasha Devroye"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.15050"
  },
  {
    "id": "arXiv:2209.15052",
    "title": "Start Small: Training Game Level Generators from Nothing by Learning at  Multiple Sizes",
    "abstract": "A procedural level generator is a tool that generates levels from noise. One\napproach to build generators is using machine learning, but given the training\ndata rarity, multiple methods have been proposed to train generators from\nnothing. However, level generation tasks tend to have sparse feedback, which is\ncommonly mitigated using game-specific supplemental rewards. This paper\nproposes a novel approach to train generators from nothing by learning at\nmultiple level sizes starting from a small size up to the desired sizes. This\napproach employs the observed phenomenon that feedback is denser at smaller\nsizes to avoid supplemental rewards. It also presents the benefit of training\ngenerators to output levels at various sizes. We apply this approach to train\ncontrollable generators using generative flow networks. We also modify\ndiversity sampling to be compatible with generative flow networks and to expand\nthe expressive range. The results show that our methods can generate\nhigh-quality diverse levels for Sokoban, Zelda and Danger Dave for a variety of\nsizes, after only 3h 29min up to 6h 11min (depending on the game) of training\non a single commodity machine. Also, the results show that our generators can\noutput levels for sizes that were unavailable during training.",
    "descriptor": "\nComments: 26 pages, 7 tables, 7 figures. Code: this https URL\n",
    "authors": [
      "Yahia Zakaria",
      "Magda Fayek",
      "Mayada Hadhoud"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15052"
  },
  {
    "id": "arXiv:2209.15056",
    "title": "Graph Attention Network for Camera Relocalization on Dynamic Scenes",
    "abstract": "We devise a graph attention network-based approach for learning a scene\ntriangle mesh representation in order to estimate an image camera position in a\ndynamic environment. Previous approaches built a scene-dependent model that\nexplicitly or implicitly embeds the structure of the scene. They use\nconvolution neural networks or decision trees to establish 2D/3D-3D\ncorrespondences. Such a mapping overfits the target scene and does not\ngeneralize well to dynamic changes in the environment. Our work introduces a\nnovel approach to solve the camera relocalization problem by using the\navailable triangle mesh. Our 3D-3D matching framework consists of three blocks:\n(1) a graph neural network to compute the embedding of mesh vertices, (2) a\nconvolution neural network to compute the embedding of grid cells defined on\nthe RGB-D image, and (3) a neural network model to establish the correspondence\nbetween the two embeddings. These three components are trained end-to-end. To\npredict the final pose, we run the RANSAC algorithm to generate camera pose\nhypotheses, and we refine the prediction using the point-cloud representation.\nOur approach significantly improves the camera pose accuracy of the\nstate-of-the-art method from $0.358$ to $0.506$ on the RIO10 benchmark for\ndynamic indoor camera relocalization.",
    "descriptor": "",
    "authors": [
      "Mohamed Amine Ouali",
      "Mohamed Bouguessa",
      "Riadh Ksantini"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15056"
  },
  {
    "id": "arXiv:2209.15059",
    "title": "Provably expressive temporal graph networks",
    "abstract": "Temporal graph networks (TGNs) have gained prominence as models for embedding\ndynamic interactions, but little is known about their theoretical\nunderpinnings. We establish fundamental results about the representational\npower and limits of the two main categories of TGNs: those that aggregate\ntemporal walks (WA-TGNs), and those that augment local message passing with\nrecurrent memory modules (MP-TGNs). Specifically, novel constructions reveal\nthe inadequacy of MP-TGNs and WA-TGNs, proving that neither category subsumes\nthe other. We extend the 1-WL (Weisfeiler-Leman) test to temporal graphs, and\nshow that the most powerful MP-TGNs should use injective updates, as in this\ncase they become as expressive as the temporal WL. Also, we show that\nsufficiently deep MP-TGNs cannot benefit from memory, and MP/WA-TGNs fail to\ncompute graph properties such as girth.\nThese theoretical insights lead us to PINT -- a novel architecture that\nleverages injective temporal message passing and relative positional features.\nImportantly, PINT is provably more expressive than both MP-TGNs and WA-TGNs.\nPINT significantly outperforms existing TGNs on several real-world benchmarks.",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Amauri H. Souza",
      "Diego Mesquita",
      "Samuel Kaski",
      "Vikas Garg"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15059"
  },
  {
    "id": "arXiv:2209.15060",
    "title": "Continuification-based control of large multiagent systems in a ring",
    "abstract": "In this paper, we propose a method to control large-scale multiagent systems\nswarming in a ring. Specifically, we use a continuification-based approach that\ntransforms the microscopic, agent-level description of the system dynamics into\na macroscopic, continuum-level representation, which we employ to synthesize a\ncontrol action towards a desired distribution of the agents. The\ncontinuum-level control action is then discretized at the agent-level in order\nto practically implement it. To confirm the effectiveness and the robustness of\nthe proposed approach, we complement theoretical derivations with a series of\nnumerical simulations.",
    "descriptor": "",
    "authors": [
      "Gian Carlo Maffettone",
      "Alain Boldini",
      "Mario di Bernardo",
      "Maurizio Porfiri"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15060"
  },
  {
    "id": "arXiv:2209.15067",
    "title": "Reasoning about Complex Networks: A Logic Programming Approach",
    "abstract": "Reasoning about complex networks has in recent years become an important\ntopic of study due to its many applications: the adoption of commercial\nproducts, spread of disease, the diffusion of an idea, etc. In this paper, we\npresent the MANCaLog language, a formalism based on logic programming that\nsatisfies a set of desiderata proposed in previous work as recommendations for\nthe development of approaches to reasoning in complex networks. To the best of\nour knowledge, this is the first formalism that satisfies all such criteria. We\nfirst focus on algorithms for finding minimal models (on which multi-attribute\nanalysis can be done), and then on how this formalism can be applied in certain\nreal world scenarios. Towards this end, we study the problem of deciding group\nmembership in social networks: given a social network and a set of groups where\ngroup membership of only some of the individuals in the network is known, we\nwish to determine a degree of membership for the remaining group-individual\npairs. We develop a prototype implementation that we use to obtain experimental\nresults on two real world datasets, including a current social network of\ncriminal gangs in a major U.S.\\ city. We then show how the assignment of degree\nof membership to nodes in this case allows for a better understanding of the\ncriminal gang problem when combined with other social network mining techniques\n-- including detection of sub-groups and identification of core group members\n-- which would not be possible without further identification of additional\ngroup members.",
    "descriptor": "\nComments: arXiv admin note: substantial text overlap with arXiv:1301.0302\n",
    "authors": [
      "Paulo Shakarian",
      "Gerardo I. Simari",
      "Devon Callahan"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15067"
  },
  {
    "id": "arXiv:2209.15069",
    "title": "Few-shot Text Classification with Dual Contrastive Consistency",
    "abstract": "In this paper, we explore how to utilize pre-trained language model to\nperform few-shot text classification where only a few annotated examples are\ngiven for each class. Since using traditional cross-entropy loss to fine-tune\nlanguage model under this scenario causes serious overfitting and leads to\nsub-optimal generalization of model, we adopt supervised contrastive learning\non few labeled data and consistency-regularization on vast unlabeled data.\nMoreover, we propose a novel contrastive consistency to further boost model\nperformance and refine sentence representation. After conducting extensive\nexperiments on four datasets, we demonstrate that our model (FTCC) can\noutperform state-of-the-art methods and has better robustness.",
    "descriptor": "\nComments: 8 pages, 2 figures, under review\n",
    "authors": [
      "Liwen Sun",
      "Jiawei Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15069"
  },
  {
    "id": "arXiv:2209.15072",
    "title": "Partially calibrated semi-generalized pose from hybrid point  correspondences",
    "abstract": "In this paper we study the problem of estimating the semi-generalized pose of\na partially calibrated camera, i.e., the pose of a perspective camera with\nunknown focal length w.r.t. a generalized camera, from a hybrid set of 2D-2D\nand 2D-3D point correspondences. We study all possible camera configurations\nwithin the generalized camera system. To derive practical solvers to previously\nunsolved challenging configurations, we test different parameterizations as\nwell as different solving strategies based on the state-of-the-art methods for\ngenerating efficient polynomial solvers. We evaluate the three most promising\nsolvers, i.e., the H51f solver with five 2D-2D correspondences and one 2D-3D\ncorrespondence viewed by the same camera inside generalized camera, the H32f\nsolver with three 2D-2D and two 2D-3D correspondences, and the H13f solver with\none 2D-2D and three 2D-3D correspondences, on synthetic and real data. We show\nthat in the presence of noise in the 3D points these solvers provide better\nestimates than the corresponding absolute pose solvers.",
    "descriptor": "",
    "authors": [
      "Snehal Bhayani",
      "Viktor Larsson",
      "Torsten Sattler",
      "Janne Heikkila",
      "Zuzana Kukelova"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15072"
  },
  {
    "id": "arXiv:2209.15073",
    "title": "A Benchmark Comparison of Imitation Learning-based Control Policies for  Autonomous Racing",
    "abstract": "Autonomous racing with scaled race cars has gained increasing attention as an\neffective approach for developing perception, planning and control algorithms\nfor safe autonomous driving at the limits of the vehicle's handling. To train\nagile control policies for autonomous racing, learning-based approaches largely\nutilize reinforcement learning, albeit with mixed results. In this study, we\nbenchmark a variety of imitation learning policies for racing vehicles that are\napplied directly or for bootstrapping reinforcement learning both in simulation\nand on scaled real-world environments. We show that interactive imitation\nlearning techniques outperform traditional imitation learning methods and can\ngreatly improve the performance of reinforcement learning policies by\nbootstrapping thanks to its better sample efficiency. Our benchmarks provide a\nfoundation for future research on autonomous racing using Imitation Learning\nand Reinforcement Learning.",
    "descriptor": "",
    "authors": [
      "Xiatao Sun",
      "Mingyan Zhou",
      "Zhijun Zhuang",
      "Shuo Yang",
      "Johannes Betz",
      "Rahul Mangharam"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15073"
  },
  {
    "id": "arXiv:2209.15075",
    "title": "What UAE Software Students Think about Software Testing: A Replicated  Study",
    "abstract": "Software testing is vital to improve software quality. However, software\ntester role is stigmatized, partly due to misperception and partly due to the\ntreatment of the testing process within the software industry. The present\nstudy analyses this situation aiming to explore what might inhibit an\nindividual from taking up a software testing career. In order to investigate\nthis issue, we surveyed 132 senior students pursuing degrees in information\nsystems, information and communication technology, computer science, computer\nengineering, software engineering, and other closely-related disciplines at\nthree universities in the United Arab Emirates: two publicly funded and one\ntop-notch private university. The students were asked to describe the PROs and\nCONs of taking up a career in software testing and to ponder the likelihood\nthat they would take up the career themselves. The study identified 7 main PROs\nand 9 main CONSs for pursuing a testing career, and indicated that the role of\nsoftware tester is perceived as a social role, which may require as many soft\nskills as technical prowess. The results also show that UAE software-related\nstudents have a stronger negative attitude towards software testing compared to\ntheir counterparts in other countries where similar investigations have been\ncarried out in the past three years.",
    "descriptor": "\nComments: 9 pagges. arXiv admin note: text overlap with arXiv:2205.07781, arXiv:2007.12632\n",
    "authors": [
      "Luiz Fernando Capretz",
      "Saad Harous",
      "Ali Bou Nassif"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.15075"
  },
  {
    "id": "arXiv:2209.15076",
    "title": "3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical  Transformer for Medical Image Segmentation",
    "abstract": "Vision transformers (ViTs) have quickly superseded convolutional networks\n(ConvNets) as the current state-of-the-art (SOTA) models for medical image\nsegmentation. Hierarchical transformers (e.g., Swin Transformers) reintroduced\nseveral ConvNet priors and further enhanced the practical viability of adapting\nvolumetric segmentation in 3D medical datasets. The effectiveness of hybrid\napproaches is largely credited to the large receptive field for non-local\nself-attention and the large number of model parameters. In this work, we\npropose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the\nhierarchical transformer using ConvNet modules for robust volumetric\nsegmentation. Specifically, we revisit volumetric depth-wise convolutions with\nlarge kernel size (e.g. starting from $7\\times7\\times7$) to enable the larger\nglobal receptive fields, inspired by Swin Transformer. We further substitute\nthe multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise\ndepth convolutions and enhance model performances with fewer normalization and\nactivation layers, thus reducing the number of model parameters. 3D UX-Net\ncompetes favorably with current SOTA transformers (e.g. SwinUNETR) using three\nchallenging public datasets on volumetric brain and abdominal imaging: 1)\nMICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI\nChallenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with\nimprovement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice\n(Feta2021). We further evaluate the transfer learning capability of 3D UX-Net\nwith AMOS2022 and demonstrates another improvement of $2.27\\%$ Dice (from 0.880\nto 0.900). The source code with our proposed model are available at\nhttps://github.com/MASILab/3DUX-Net.",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Ho Hin Lee",
      "Shunxing Bao",
      "Yuankai Huo",
      "Bennett A. Landman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15076"
  },
  {
    "id": "arXiv:2209.15078",
    "title": "Online Weighted Q-Ensembles for Reduced Hyperparameter Tuning in  Reinforcement Learning",
    "abstract": "Reinforcement learning is a promising paradigm for learning robot control,\nallowing complex control policies to be learned without requiring a dynamics\nmodel. However, even state of the art algorithms can be difficult to tune for\noptimum performance. We propose employing an ensemble of multiple reinforcement\nlearning agents, each with a different set of hyperparameters, along with a\nmechanism for choosing the best performing set(s) on-line. In the literature,\nthe ensemble technique is used to improve performance in general, but the\ncurrent work specifically addresses decreasing the hyperparameter tuning\neffort. Furthermore, our approach targets on-line learning on a single robotic\nsystem, and does not require running multiple simulators in parallel. Although\nthe idea is generic, the Deep Deterministic Policy Gradient was the model\nchosen, being a representative deep learning actor-critic method with good\nperformance in continuous action settings but known high variance. We compare\nour online weighted q-ensemble approach to q-average ensemble strategies\naddressed in literature using alternate policy training, as well as online\ntraining, demonstrating the advantage of the new approach in eliminating\nhyperparameter tuning. The applicability to real-world systems was validated in\ncommon robotic benchmark environments: the bipedal robot half cheetah and the\nswimmer. Online Weighted Q-Ensemble presented overall lower variance and\nsuperior results when compared with q-average ensembles using randomized\nparameterizations.",
    "descriptor": "",
    "authors": [
      "Renata Garcia",
      "Wouter Caarls"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15078"
  },
  {
    "id": "arXiv:2209.15084",
    "title": "Automatic satellite building construction monitoring",
    "abstract": "One of the promising applications of satellite images is building\nconstruction monitoring. It allows to control the construction progress around\nthe world even in the locations that are hard to reach. One of the main hurdles\nof this approach is the interpretation of the image data. In this paper, we\nhave employed several novel deep learning techniques to tackle the problem.\nVarious image segmentation and object detection networks were combined into a\nunified pipeline, which was then used to determine the building construction\nprogress.",
    "descriptor": "\nComments: 10 pages, 5 figures, 3 tables\n",
    "authors": [
      "Insaf Ashrapov",
      "Dmitriy Malakhov",
      "Anton Marchenkov",
      "Anton Lulin",
      "Dani El-Ayyass"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15084"
  },
  {
    "id": "arXiv:2209.15087",
    "title": "Zero-shot visual reasoning through probabilistic analogical mapping",
    "abstract": "Human reasoning is grounded in an ability to identify highly abstract\ncommonalities governing superficially dissimilar visual inputs. Recent efforts\nto develop algorithms with this capacity have largely focused on approaches\nthat require extensive direct training on visual reasoning tasks, and yield\nlimited generalization to problems with novel content. In contrast, a long\ntradition of research in cognitive science has focused on elucidating the\ncomputational principles underlying human analogical reasoning; however, this\nwork has generally relied on manually constructed representations. Here we\npresent visiPAM (visual Probabilistic Analogical Mapping), a model of visual\nreasoning that synthesizes these two approaches. VisiPAM employs learned\nrepresentations derived directly from naturalistic visual inputs, coupled with\na similarity-based mapping operation derived from cognitive theories of human\nreasoning. We show that without any direct training, visiPAM outperforms a\nstate-of-the-art deep learning model on an analogical mapping task. In\naddition, visiPAM closely matches the pattern of human performance on a novel\ntask involving mapping of 3D objects across disparate categories.",
    "descriptor": "",
    "authors": [
      "Taylor W. Webb",
      "Shuhao Fu",
      "Trevor Bihl",
      "Keith J. Holyoak",
      "Hongjing Lu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15087"
  },
  {
    "id": "arXiv:2209.15090",
    "title": "Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement  Learning in Unknown Stochastic Environments",
    "abstract": "It is quite challenging to ensure the safety of reinforcement learning (RL)\nagents in an unknown and stochastic environment under hard constraints that\nrequire the system state not to reach certain specified unsafe regions. Many\npopular safe RL methods such as those based on the Constrained Markov Decision\nProcess (CMDP) paradigm formulate safety violations in a cost function and try\nto constrain the expectation of cumulative cost under a threshold. However, it\nis often difficult to effectively capture and enforce hard reachability-based\nsafety constraints indirectly with such constraints on safety violation costs.\nIn this work, we leverage the notion of barrier function to explicitly encode\nthe hard safety constraints, and given that the environment is unknown, relax\nthem to our design of \\emph{generative-model-based soft barrier functions}.\nBased on such soft barriers, we propose a safe RL approach that can jointly\nlearn the environment and optimize the control policy, while effectively\navoiding unsafe regions with safety probability optimization. Experiments on a\nset of examples demonstrate that our approach can effectively enforce hard\nsafety constraints and significantly outperform CMDP-based baseline methods in\nsystem safe rate measured via simulations.",
    "descriptor": "\nComments: 13 pages, 7 figures\n",
    "authors": [
      "Yixuan Wang",
      "Simon Sinong Zhan",
      "Ruochen Jiao",
      "Zhilu Wang",
      "Wanxin Jin",
      "Zhuoran Yang",
      "Zhaoran Wang",
      "Chao Huang",
      "Qi Zhu"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15090"
  },
  {
    "id": "arXiv:2209.15091",
    "title": "L-SRR: Local Differential Privacy for Location-Based Services with  Staircase Randomized Response",
    "abstract": "Location-based services (LBS) have been significantly developed and widely\ndeployed in mobile devices. It is also well-known that LBS applications may\nresult in severe privacy concerns by collecting sensitive locations. A strong\nprivacy model ''local differential privacy'' (LDP) has been recently deployed\nin many different applications (e.g., Google RAPPOR, iOS, and Microsoft\nTelemetry) but not effective for LBS applications due to the low utility of\nexisting LDP mechanisms. To address such deficiency, we propose the first LDP\nframework for a variety of location-based services (namely ''L-SRR''), which\nprivately collects and analyzes user locations with high utility. Specifically,\nwe design a novel randomization mechanism ''Staircase Randomized Response''\n(SRR) and extend the empirical estimation to significantly boost the utility\nfor SRR in different LBS applications (e.g., traffic density estimation, and\nk-nearest neighbors). We have conducted extensive experiments on four real LBS\ndatasets by benchmarking with other LDP schemes in practical applications. The\nexperimental results demonstrate that L-SRR significantly outperforms them.",
    "descriptor": "\nComments: accepted to CCS'22; full version\n",
    "authors": [
      "Han Wang",
      "Hanbin Hong",
      "Li Xiong",
      "Zhan Qin",
      "Yuan Hong"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.15091"
  },
  {
    "id": "arXiv:2209.15092",
    "title": "Improving Generative Flow Networks with Path Regularization",
    "abstract": "Generative Flow Networks (GFlowNets) are recently proposed models for\nlearning stochastic policies that generate compositional objects by sequences\nof actions with the probability proportional to a given reward function. The\ncentral problem of GFlowNets is to improve their exploration and\ngeneralization. In this work, we propose a novel path regularization method\nbased on optimal transport theory that places prior constraints on the\nunderlying structure of the GFlowNets. The prior is designed to help the\nGFlowNets better discover the latent structure of the target distribution or\nenhance its ability to explore the environment in the context of active\nlearning. The path regularization controls the flow in GFlowNets to generate\nmore diverse and novel candidates via maximizing the optimal transport\ndistances between two forward policies or to improve the generalization via\nminimizing the optimal transport distances. In addition, we derive an efficient\nimplementation of the regularization by finding its closed form solutions in\nspecific cases and a meaningful upper bound that can be used as an\napproximation to minimize the regularization term. We empirically demonstrate\nthe advantage of our path regularization on a wide range of tasks, including\nsynthetic hypergrid environment modeling, discrete probabilistic modeling, and\nbiological sequence design.",
    "descriptor": "\nComments: 28 pages, 2 figures, 5 tables. Anh Do, Duy Dinh, and Tan Nguyen contributed equally to this work\n",
    "authors": [
      "Anh Do",
      "Duy Dinh",
      "Tan Nguyen",
      "Khuong Nguyen",
      "Stanley Osher",
      "Nhat Ho"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15092"
  },
  {
    "id": "arXiv:2209.15093",
    "title": "Unpacking Large Language Models with Conceptual Consistency",
    "abstract": "If a Large Language Model (LLM) answers \"yes\" to the question \"Are mountains\ntall?\" then does it know what a mountain is? Can you rely on it responding\ncorrectly or incorrectly to other questions about mountains? The success of\nLarge Language Models (LLMs) indicates they are increasingly able to answer\nqueries like these accurately, but that ability does not necessarily imply a\ngeneral understanding of concepts relevant to the anchor query. We propose\nconceptual consistency to measure a LLM's understanding of relevant concepts.\nThis novel metric measures how well a model can be characterized by finding out\nhow consistent its responses to queries about conceptually relevant background\nknowledge are. To compute it we extract background knowledge by traversing\npaths between concepts in a knowledge base and then try to predict the model's\nresponse to the anchor query from the background knowledge. We investigate the\nperformance of current LLMs in a commonsense reasoning setting using the CSQA\ndataset and the ConceptNet knowledge base. While conceptual consistency, like\nother metrics, does increase with the scale of the LLM used, we find that\npopular models do not necessarily have high conceptual consistency. Our\nanalysis also shows significant variation in conceptual consistency across\ndifferent kinds of relations, concepts, and prompts. This serves as a step\ntoward building models that humans can apply a theory of mind to, and thus\ninteract with intuitively.",
    "descriptor": "",
    "authors": [
      "Pritish Sahu",
      "Michael Cogswell",
      "Yunye Gong",
      "Ajay Divakaran"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15093"
  },
  {
    "id": "arXiv:2209.15095",
    "title": "Integration factor combined with level set method for reaction-diffusion  systems with free boundary in high spatial dimensions",
    "abstract": "For reaction-diffusion equations in irregular domain with moving boundaries,\nthe numerical stability constraints from the reaction and diffusion terms often\nrequire very restricted time step size, while complex geometries may lead to\ndifficulties in accuracy when discretizing the high-order derivatives on grid\npoints near the boundary. It is very challenging to design numerical methods\nthat can efficiently and accurately handle both difficulties. Applying an\nimplicit scheme may be able to remove the stability constraints on the time\nstep, however, it usually requires solving a large global system of nonlinear\nequations for each time step, and the computational cost could be significant.\nIntegration factor (IF) or exponential differencing time (ETD) methods are one\nof the popular methods for temporal partial differential equations (PDEs) among\nmany other methods. In our paper, we couple ETD methods with an embedded\nboundary method to solve a system of reaction-diffusion equations with complex\ngeometries. In particular, we rewrite all ETD schemes into a linear combination\nof specific {\\phi}-functions and apply one start-of-the-art algorithm to\ncompute the matrix-vector multiplications, which offers significant\ncomputational advantages with adaptive Krylov subspaces. In addition, we extend\nthis method by incorporating the level set method to solve the free boundary\nproblem. The accuracy, stability, and efficiency of the developed method are\ndemonstrated by numerical examples.",
    "descriptor": "\nComments: 20 pages, 6 figures, 2 tables\n",
    "authors": [
      "Shuang Liu",
      "Xinfeng Liu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.15095"
  },
  {
    "id": "arXiv:2209.15099",
    "title": "MUG: Interactive Multimodal Grounding on User Interfaces",
    "abstract": "We present MUG, a novel interactive task for multimodal grounding where a\nuser and an agent work collaboratively on an interface screen. Prior works\nmodeled multimodal UI grounding in one round: the user gives a command and the\nagent responds to the command. Yet, in a realistic scenario, a user command can\nbe ambiguous when the target action is inherently difficult to articulate in\nnatural language. MUG allows multiple rounds of interactions such that upon\nseeing the agent responses, the user can give further commands for the agent to\nrefine or even correct its actions. Such interaction is critical for improving\ngrounding performances in real-world use cases. To investigate the problem, we\ncreate a new dataset that consists of 77,820 sequences of human user-agent\ninteraction on mobile interfaces in which 20% involves multiple rounds of\ninteractions. To establish our benchmark, we experiment with a range of\nmodeling variants and evaluation strategies, including both offline and online\nevaluation-the online strategy consists of both human evaluation and automatic\nwith simulators. Our experiments show that allowing iterative interaction\nsignificantly improves the absolute task completion by 18% over the entire test\ndataset and 31% over the challenging subset. Our results lay the foundation for\nfurther investigation of the problem.",
    "descriptor": "",
    "authors": [
      "Tao Li",
      "Gang Li",
      "Jingjie Zheng",
      "Purple Wang",
      "Yang Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15099"
  },
  {
    "id": "arXiv:2209.15101",
    "title": "Improving Molecular Pretraining with Complementary Featurizations",
    "abstract": "Molecular pretraining, which learns molecular representations over massive\nunlabeled data, has become a prominent paradigm to solve a variety of tasks in\ncomputational chemistry and drug discovery. Recently, prosperous progress has\nbeen made in molecular pretraining with different molecular featurizations,\nincluding 1D SMILES strings, 2D graphs, and 3D geometries. However, the role of\nmolecular featurizations with their corresponding neural architectures in\nmolecular pretraining remains largely unexamined. In this paper, through two\ncase studies -- chirality classification and aromatic ring counting -- we first\ndemonstrate that different featurization techniques convey chemical information\ndifferently. In light of this observation, we propose a simple and effective\nMOlecular pretraining framework with COmplementary featurizations (MOCO). MOCO\ncomprehensively leverages multiple featurizations that complement each other\nand outperforms existing state-of-the-art models that solely relies on one or\ntwo featurizations on a wide range of molecular property prediction tasks.",
    "descriptor": "\nComments: 24 pages, work in progress\n",
    "authors": [
      "Yanqiao Zhu",
      "Dingshuo Chen",
      "Yuanqi Du",
      "Yingze Wang",
      "Qiang Liu",
      "Shu Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2209.15101"
  },
  {
    "id": "arXiv:2209.15103",
    "title": "Data Querying with Ciphertext Policy Attribute Based Encryption",
    "abstract": "Data encryption limits the power and efficiency of queries. Direct processing\nof encrypted data should ideally be possible to avoid the need for data\ndecryption, processing, and re-encryption. It is vital to keep the data\nsearchable and sortable. That is, some information is intentionally leaked.\nThis intentional leakage technology is known as \"querying over encrypted data\nschemes\", which offer confidentiality as well as querying over encrypted data,\nbut it is not meant to provide flexible access control. This paper suggests the\nuse of Ciphertext Policy Attributes Based Encryption (CP-ABE) to address three\nsecurity requirements, namely: confidentiality, queries over encrypted data,\nand flexible access control. By combining flexible access control and data\nconfidentiality, CP-ABE can authenticate who can access data and possess the\nsecret key. Thus, this paper identifies how much data leakage there is in order\nto figure out what kinds of operations are allowed when data is encrypted by\nCP-ABE.",
    "descriptor": "",
    "authors": [
      "Maryam Almarwani",
      "Boris Konev",
      "Alexei Lisitsa"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.15103"
  },
  {
    "id": "arXiv:2209.15104",
    "title": "OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence  for Digital Agriculture",
    "abstract": "Recent machine learning approaches have been effective in Artificial\nIntelligence (AI) applications. They produce robust results with a high level\nof accuracy. However, most of these techniques do not provide\nhuman-understandable explanations for supporting their results and decisions.\nThey usually act as black boxes, and it is not easy to understand how decisions\nhave been made. Explainable Artificial Intelligence (XAI), which has received\nmuch interest recently, tries to provide human-understandable explanations for\ndecision-making and trained AI models. For instance, in digital agriculture,\nrelated domains often present peculiar or input features with no link to\nbackground knowledge. The application of the data mining process on\nagricultural data leads to results (knowledge), which are difficult to explain.\nIn this paper, we propose a knowledge map model and an ontology design as an\nXAI framework (OAK4XAI) to deal with this issue. The framework does not only\nconsider the data analysis part of the process, but it takes into account the\nsemantics aspect of the domain knowledge via an ontology and a knowledge map\nmodel, provided as modules of the framework. Many ongoing XAI studies aim to\nprovide accurate and verbalizable accounts for how given feature values\ncontribute to model decisions. The proposed approach, however, focuses on\nproviding consistent information and definitions of concepts, algorithms, and\nvalues involved in the data mining models. We built an Agriculture Computing\nOntology (AgriComO) to explain the knowledge mined in agriculture. AgriComO has\na well-designed structure and includes a wide range of concepts and\ntransformations suitable for agriculture and computing domains.",
    "descriptor": "\nComments: AI-2022 Forty-second SGAI International Conference on Artificial Intelligence\n",
    "authors": [
      "Quoc Hung Ngo",
      "Tahar Kechadi",
      "Nhien-An Le-Khac"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15104"
  },
  {
    "id": "arXiv:2209.15106",
    "title": "Restricted Strong Convexity of Deep Learning Models with Smooth  Activations",
    "abstract": "We consider the problem of optimization of deep learning models with smooth\nactivation functions. While there exist influential results on the problem from\nthe ``near initialization'' perspective, we shed considerable new light on the\nproblem. In particular, we make two key technical contributions for such models\nwith $L$ layers, $m$ width, and $\\sigma_0^2$ initialization variance. First,\nfor suitable $\\sigma_0^2$, we establish a $O(\\frac{\\text{poly}(L)}{\\sqrt{m}})$\nupper bound on the spectral norm of the Hessian of such models, considerably\nsharpening prior results. Second, we introduce a new analysis of optimization\nbased on Restricted Strong Convexity (RSC) which holds as long as the squared\nnorm of the average gradient of predictors is\n$\\Omega(\\frac{\\text{poly}(L)}{\\sqrt{m}})$ for the square loss. We also present\nresults for more general losses. The RSC based analysis does not need the\n``near initialization\" perspective and guarantees geometric convergence for\ngradient descent (GD). To the best of our knowledge, ours is the first result\non establishing geometric convergence of GD based on RSC for deep learning\nmodels, thus becoming an alternative sufficient condition for convergence that\ndoes not depend on the widely-used Neural Tangent Kernel (NTK). We share\npreliminary experimental results supporting our theoretical advances.",
    "descriptor": "",
    "authors": [
      "Arindam Banerjee",
      "Pedro Cisneros-Velarde",
      "Libin Zhu",
      "Mikhail Belkin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.15106"
  },
  {
    "id": "arXiv:2209.15107",
    "title": "Hidden in Plain Sight: Exploring Encrypted Channels in Android apps",
    "abstract": "As privacy features in Android operating system improve, privacy-invasive\napps may gradually shift their focus to non-standard and covert channels for\nleaking private user/device information. Such leaks also remain largely\nundetected by state-of-the-art privacy analysis tools, which are very effective\nin uncovering privacy exposures via regular HTTP and HTTPS channels. In this\nstudy, we design and implement, ThirdEye, to significantly extend the\nvisibility of current privacy analysis tools, in terms of the exposures that\nhappen across various non-standard and covert channels, i.e., via any protocol\nover TCP/UDP (beyond HTTP/S), and using multi-layer custom encryption over\nHTTP/S and non-HTTP protocols. Besides network exposures, we also consider\ncovert channels via storage media that also leverage custom encryption layers.\nUsing ThirdEye, we analyzed 12,598 top-apps in various categories from\nAndroidrank, and found that 2887/12,598 (22.92%) apps used custom\nencryption/decryption for network transmission and storing content in shared\ndevice storage, and 2465/2887 (85.38%) of those apps sent device information\n(e.g., advertising ID, list of installed apps) over the network that can\nfingerprint users. Besides, 299 apps transmitted insecure encrypted content\nover HTTP/non-HTTP protocols; 22 apps that used authentication tokens over\nHTTPS, happen to expose them over insecure (albeit custom encrypted)\nHTTP/non-HTTP channels. We found non-standard and covert channels with multiple\nlevels of obfuscation (e.g., encrypted data over HTTPS, encryption at nested\nlevels), and the use of vulnerable keys and cryptographic algorithms. Our\nfindings can provide valuable insights into the evolving field of non-standard\nand covert channels, and help spur new countermeasures against such privacy\nleakage and security issues.",
    "descriptor": "\nComments: Extended version of an ACM CCS 2022 paper\n",
    "authors": [
      "Sajjad Pourali",
      "Nayanamana Samarasinghe",
      "Mohammad Mannan"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.15107"
  },
  {
    "id": "arXiv:2209.15108",
    "title": "How to tackle an emerging topic? Combining strong and weak labels for  Covid news NER",
    "abstract": "Being able to train Named Entity Recognition (NER) models for emerging topics\nis crucial for many real-world applications especially in the medical domain\nwhere new topics are continuously evolving out of the scope of existing models\nand datasets. For a realistic evaluation setup, we introduce a novel COVID-19\nnews NER dataset (COVIDNEWS-NER) and release 3000 entries of hand annotated\nstrongly labelled sentences and 13000 auto-generated weakly labelled sentences.\nBesides the dataset, we propose CONTROSTER, a recipe to strategically combine\nweak and strong labels in improving NER in an emerging topic through transfer\nlearning. We show the effectiveness of CONTROSTER on COVIDNEWS-NER while\nproviding analysis on combining weak and strong labels for training. Our key\nfindings are: (1) Using weak data to formulate an initial backbone before\ntuning on strong data outperforms methods trained on only strong or weak data.\n(2) A combination of out-of-domain and in-domain weak label training is crucial\nand can overcome saturation when being training on weak labels from a single\nsource.",
    "descriptor": "\nComments: AACL 2022\n",
    "authors": [
      "Aleksander Ficek",
      "Fangyu Liu",
      "Nigel Collier"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15108"
  },
  {
    "id": "arXiv:2209.15109",
    "title": "ConceptNet infused DialoGPT for Underlying Commonsense Understanding and  Reasoning in Dialogue Response Generation",
    "abstract": "The pre-trained conversational models still fail to capture the implicit\ncommonsense (CS) knowledge hidden in the dialogue interaction, even though they\nwere pre-trained with an enormous dataset. In order to build a dialogue agent\nwith CS capability, we firstly inject external knowledge into a pre-trained\nconversational model to establish basic commonsense through efficient Adapter\ntuning (Section 4). Secondly, we propose the ``two-way learning'' method to\nenable the bidirectional relationship between CS knowledge and sentence pairs\nso that the model can generate a sentence given the CS triplets, also generate\nthe underlying CS knowledge given a sentence (Section 5). Finally, we leverage\nthis integrated CS capability to improve open-domain dialogue response\ngeneration so that the dialogue agent is capable of understanding the CS\nknowledge hidden in dialogue history on top of inferring related other\nknowledge to further guide response generation (Section 6). The experiment\nresults demonstrate that CS\\_Adapter fusion helps DialoGPT to be able to\ngenerate series of CS knowledge. And the DialoGPT+CS\\_Adapter response model\nadapted from CommonGen training can generate underlying CS triplets that fits\nbetter to dialogue context.",
    "descriptor": "\nComments: this is a long paper, the short version was accepted by SemDial 2022\n",
    "authors": [
      "Ye Liu",
      "Wolfgang Maier",
      "Wolfgang Minker",
      "Stefan Ultes"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15109"
  },
  {
    "id": "arXiv:2209.15111",
    "title": "A Quantitative Account of Harm",
    "abstract": "In a companion paper (Beckers et al. 2022), we defined a qualitative notion\nof harm: either harm is caused, or it is not. For practical applications, we\noften need to quantify harm; for example, we may want to choose the lest\nharmful of a set of possible interventions. We first present a quantitative\ndefinition of harm in a deterministic context involving a single individual,\nthen we consider the issues involved in dealing with uncertainty regarding the\ncontext and going from a notion of harm for a single individual to a notion of\n\"societal harm\", which involves aggregating the harm to individuals. We show\nthat the \"obvious\" way of doing this (just taking the expected harm for an\nindividual and then summing the expected harm over all individuals can lead to\ncounterintuitive or inappropriate answers, and discuss alternatives, drawing on\nwork from the decision-theory literature.",
    "descriptor": "\nComments: 17 pages, under submissions\n",
    "authors": [
      "Sander Beckers",
      "Hana Chockler",
      "Joseph Y. Halpern"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15111"
  },
  {
    "id": "arXiv:2209.15123",
    "title": "Understanding Interventional TreeSHAP : How and Why it Works",
    "abstract": "Shapley values are ubiquitous in interpretable Machine Learning due to their\nstrong theoretical background and efficient implementation in the SHAP library.\nComputing these values used to induce an exponential cost with respect to the\nnumber of input features of an opaque model. Now, with efficient\nimplementations such as Interventional TreeSHAP, this exponential burden is\nalleviated assuming one is explaining ensembles of decision trees. Although\nInterventional TreeSHAP has risen in popularity, it still lacks a formal proof\nof how/why it works. We provide such proof with the aim of not only increasing\nthe transparency of the algorithm but also to encourage further development of\nthese ideas. Notably, our proof for Interventional TreeSHAP is easily adapted\nto Shapley-Taylor indices.",
    "descriptor": "",
    "authors": [
      "Gabriel Laberge",
      "Yann Pequignot"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15123"
  },
  {
    "id": "arXiv:2209.15132",
    "title": "Dynamic Inference on Graphs using Structured Transition Models",
    "abstract": "Enabling robots to perform complex dynamic tasks such as picking up an object\nin one sweeping motion or pushing off a wall to quickly turn a corner is a\nchallenging problem. The dynamic interactions implicit in these tasks are\ncritical towards the successful execution of such tasks. Graph neural networks\n(GNNs) provide a principled way of learning the dynamics of interactive systems\nbut can suffer from scaling issues as the number of interactions increases.\nFurthermore, the problem of using learned GNN-based models for optimal control\nis insufficiently explored. In this work, we present a method for efficiently\nlearning the dynamics of interacting systems by simultaneously learning a\ndynamic graph structure and a stable and locally linear forward model of the\nsystem. The dynamic graph structure encodes evolving contact modes along a\ntrajectory by making probabilistic predictions over the edges of the graph.\nAdditionally, we introduce a temporal dependence in the learned graph structure\nwhich allows us to incorporate contact measurement updates during execution\nthus enabling more accurate forward predictions. The learned stable and locally\nlinear dynamics enable the use of optimal control algorithms such as iLQR for\nlong-horizon planning and control for complex interactive tasks. Through\nexperiments in simulation and in the real world, we evaluate the performance of\nour method by using the learned interaction dynamics for control and\ndemonstrate generalization to more objects and interactions not seen during\ntraining. We introduce a control scheme that takes advantage of contact\nmeasurement updates and hence is robust to prediction inaccuracies during\nexecution.",
    "descriptor": "",
    "authors": [
      "Saumya Saxena",
      "Oliver Kroemer"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15132"
  },
  {
    "id": "arXiv:2209.15133",
    "title": "Modeling driver's evasive behavior during safety-critical lane  changes:Two-dimensional time-to-collision and deep reinforcement learning",
    "abstract": "Lane changes are complex driving behaviors and frequently involve\nsafety-critical situations. This study aims to develop a lane-change-related\nevasive behavior model, which can facilitate the development of safety-aware\ntraffic simulations and predictive collision avoidance systems. Large-scale\nconnected vehicle data from the Safety Pilot Model Deployment (SPMD) program\nwere used for this study. A new surrogate safety measure, two-dimensional\ntime-to-collision (2D-TTC), was proposed to identify the safety-critical\nsituations during lane changes. The validity of 2D-TTC was confirmed by showing\na high correlation between the detected conflict risks and the archived\ncrashes. A deep deterministic policy gradient (DDPG) algorithm, which could\nlearn the sequential decision-making process over continuous action spaces, was\nused to model the evasive behaviors in the identified safety-critical\nsituations. The results showed the superiority of the proposed model in\nreplicating both the longitudinal and lateral evasive behaviors.",
    "descriptor": "",
    "authors": [
      "Hongyu Guo",
      "Kun Xie",
      "Mehdi Keyvan-Ekbatani"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15133"
  },
  {
    "id": "arXiv:2209.15135",
    "title": "Learning an Efficient Terrain Representation for Haptic Localization of  a Legged Robot",
    "abstract": "Although haptic sensing has recently been used for legged robot localization\nin extreme environments where a camera or LiDAR might fail, the problem of\nefficiently representing the haptic signatures in a learned prior map is still\nopen. This paper introduces an approach to terrain representation for haptic\nlocalization inspired by recent trends in machine learning. It combines this\napproach with the proven Monte Carlo algorithm to obtain an accurate,\ncomputation-efficient, and practical method for localizing legged robots under\nadversarial environmental conditions. We apply the triplet loss concept to\nlearn highly descriptive embeddings in a transformer-based neural network. As\nthe training haptic data are not labeled, the positive and negative examples\nare discriminated by their geometric locations discovered while training. We\ndemonstrate experimentally that the proposed approach outperforms by a large\nmargin the previous solutions to haptic localization of legged robots\nconcerning the accuracy, inference time, and the amount of data stored in the\nmap. As far as we know, this is the first approach that completely removes the\nneed to use a dense terrain map for accurate haptic localization, thus paving\nthe way to practical applications.",
    "descriptor": "\nComments: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible\n",
    "authors": [
      "Damian S\u00f3jka",
      "Micha\u0142 R. Nowicki",
      "Piotr Skrzypczy\u0144ski"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15135"
  },
  {
    "id": "arXiv:2209.15137",
    "title": "Machine Learning for Stress Monitoring from Wearable Devices: A  Systematic Literature Review",
    "abstract": "Introduction. The stress response has both subjective, psychological and\nobjectively measurable, biological components. Both of them can be expressed\ndifferently from person to person, complicating the development of a generic\nstress measurement model. This is further compounded by the lack of large,\nlabeled datasets that can be utilized to build machine learning models for\naccurately detecting periods and levels of stress. The aim of this review is to\nprovide an overview of the current state of stress detection and monitoring\nusing wearable devices, and where applicable, machine learning techniques\nutilized.\nMethods. This study reviewed published works contributing and/or using\ndatasets designed for detecting stress and their associated machine learning\nmethods, with a systematic review and meta-analysis of those that utilized\nwearable sensor data as stress biomarkers. The electronic databases of Google\nScholar, Crossref, DOAJ and PubMed were searched for relevant articles and a\ntotal of 24 articles were identified and included in the final analysis. The\nreviewed works were synthesized into three categories of publicly available\nstress datasets, machine learning, and future research directions.\nResults. A wide variety of study-specific test and measurement protocols were\nnoted in the literature. A number of public datasets were identified that are\nlabeled for stress detection. In addition, we discuss that previous works show\nshortcomings in areas such as their labeling protocols, lack of statistical\npower, validity of stress biomarkers, and generalization ability.\nConclusion. Generalization of existing machine learning models still require\nfurther study, and research in this area will continue to provide improvements\nas newer and more substantial datasets become available for study.",
    "descriptor": "\nComments: 50 pages, 8 figures\n",
    "authors": [
      "Gideon Vos",
      "Kelly Trinh",
      "Zoltan Sarnyai",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15137"
  },
  {
    "id": "arXiv:2209.15139",
    "title": "Augmentation Backdoors",
    "abstract": "Data augmentation is used extensively to improve model generalisation.\nHowever, reliance on external libraries to implement augmentation methods\nintroduces a vulnerability into the machine learning pipeline. It is well known\nthat backdoors can be inserted into machine learning models through serving a\nmodified dataset to train on. Augmentation therefore presents a perfect\nopportunity to perform this modification without requiring an initially\nbackdoored dataset. In this paper we present three backdoor attacks that can be\ncovertly inserted into data augmentation. Our attacks each insert a backdoor\nusing a different type of computer vision augmentation transform, covering\nsimple image transforms, GAN-based augmentation, and composition-based\naugmentation. By inserting the backdoor using these augmentation transforms, we\nmake our backdoors difficult to detect, while still supporting arbitrary\nbackdoor functionality. We evaluate our attacks on a range of computer vision\nbenchmarks and demonstrate that an attacker is able to introduce backdoors\nthrough just a malicious augmentation routine.",
    "descriptor": "\nComments: 12 pages, 8 figures\n",
    "authors": [
      "Joseph Rance",
      "Yiren Zhao",
      "Ilia Shumailov",
      "Robert Mullins"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.15139"
  },
  {
    "id": "arXiv:2209.15140",
    "title": "Fully Proprioceptive Slip-Velocity-Aware State Estimation for Mobile  Robots via Invariant Kalman Filtering and Disturbance Observer",
    "abstract": "This paper develops a novel slip estimator using the invariant observer\ndesign theory and Disturbance Observer (DOB). The proposed state estimator for\nmobile robots is fully proprioceptive and combines data from an inertial\nmeasurement unit and body velocity within a Right Invariant Extended Kalman\nFilter (RI-EKF). By embedding the slip velocity into $\\mathrm{SE}_3(3)$ Lie\ngroup, the developed DOB-based RI-EKF provides real-time accurate velocity and\nslip velocity estimates on different terrains. Experimental results using a\nHusky wheeled robot confirm the mathematical derivations and show better\nperformance than a standard RI-EKF baseline. Open source software is available\nfor download and reproducing the presented results.",
    "descriptor": "\nComments: github repository at this https URL arXiv admin note: text overlap with arXiv:1805.10410 by other authors\n",
    "authors": [
      "Xihang Yu",
      "Sangli Teng",
      "Theodor Chakhachiro",
      "Wenzhe Tong",
      "Tingjun Li",
      "Tzu-Yuan Lin",
      "Sarah Koehler",
      "Manuel Ahumada",
      "Jeffrey M. Walls",
      "Maani Ghaffari"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15140"
  },
  {
    "id": "arXiv:2209.15141",
    "title": "On Convergence of Average-Reward Off-Policy Control Algorithms in  Weakly-Communicating MDPs",
    "abstract": "We show two average-reward off-policy control algorithms, Differential Q\nLearning (Wan, Naik, \\& Sutton 2021a) and RVI Q Learning (Abounadi Bertsekas \\&\nBorkar 2001), converge in weakly-communicating MDPs. Weakly-communicating MDPs\nare the most general class of MDPs that a learning algorithm with a single\nstream of experience can guarantee obtaining a policy achieving optimal reward\nrate. The original convergence proofs of the two algorithms require that all\noptimal policies induce unichains, which is not necessarily true for\nweakly-communicating MDPs. To the best of our knowledge, our results are the\nfirst showing average-reward off-policy control algorithms converge in\nweakly-communicating MDPs. As a direct extension, we show that average-reward\noptions algorithms introduced by (Wan, Naik, \\& Sutton 2021b) converge if the\nSemi-MDP induced by options is weakly-communicating.",
    "descriptor": "",
    "authors": [
      "Yi Wan",
      "Richard S. Sutton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15141"
  },
  {
    "id": "arXiv:2209.15143",
    "title": "Double Graphs Regularized Multi-view Subspace Clustering",
    "abstract": "Recent years have witnessed a growing academic interest in multi-view\nsubspace clustering. In this paper, we propose a novel Double Graphs\nRegularized Multi-view Subspace Clustering (DGRMSC) method, which aims to\nharness both global and local structural information of multi-view data in a\nunified framework. Specifically, DGRMSC firstly learns a latent representation\nto exploit the global complementary information of multiple views. Based on the\nlearned latent representation, we learn a self-representation to explore its\nglobal cluster structure. Further, Double Graphs Regularization (DGR) is\nperformed on both latent representation and self-representation to take\nadvantage of their local manifold structures simultaneously. Then, we design an\niterative algorithm to solve the optimization problem effectively. Extensive\nexperimental results on real-world datasets demonstrate the effectiveness of\nthe proposed method.",
    "descriptor": "",
    "authors": [
      "Longlong Chen",
      "Yulong Wang",
      "Youheng Liu",
      "Yutao Hu",
      "Libin Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15143"
  },
  {
    "id": "arXiv:2209.15145",
    "title": "Batch Multivalid Conformal Prediction",
    "abstract": "We develop fast distribution-free conformal prediction algorithms for\nobtaining multivalid coverage on exchangeable data in the batch setting.\nMultivalid coverage guarantees are stronger than marginal coverage guarantees\nin two ways: (1) They hold even conditional on group membership -- that is, the\ntarget coverage level $1-\\alpha$ holds conditionally on membership in each of\nan arbitrary (potentially intersecting) group in a finite collection\n$\\mathcal{G}$ of regions in the feature space. (2) They hold even conditional\non the value of the threshold used to produce the prediction set on a given\nexample. In fact multivalid coverage guarantees hold even when conditioning on\ngroup membership and threshold value simultaneously.\nWe give two algorithms: both take as input an arbitrary non-conformity score\nand an arbitrary collection of possibly intersecting groups $\\mathcal{G}$, and\nthen can equip arbitrary black-box predictors with prediction sets. Our first\nalgorithm (BatchGCP) is a direct extension of quantile regression, needs to\nsolve only a single convex minimization problem, and produces an estimator\nwhich has group-conditional guarantees for each group in $\\mathcal{G}$. Our\nsecond algorithm (BatchMVP) is iterative, and gives the full guarantees of\nmultivalid conformal prediction: prediction sets that are valid conditionally\nboth on group membership and non-conformity threshold. We evaluate the\nperformance of both of our algorithms in an extensive set of experiments. Code\nto replicate all of our experiments can be found at\nhttps://github.com/ProgBelarus/BatchMultivalidConformal",
    "descriptor": "\nComments: Code to replicate all of our experiments can be found at this https URL\n",
    "authors": [
      "Christopher Jung",
      "Georgy Noarov",
      "Ramya Ramalingam",
      "Aaron Roth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2209.15145"
  },
  {
    "id": "arXiv:2209.15146",
    "title": "Ensemble Machine Learning Model Trained on a New Synthesized Dataset  Generalizes Well for Stress Prediction Using Wearable Devices",
    "abstract": "Introduction. We investigate the generalization ability of models built on\ndatasets containing a small number of subjects, recorded in single study\nprotocols. Next, we propose and evaluate methods combining these datasets into\na single, large dataset. Finally, we propose and evaluate the use of ensemble\ntechniques by combining gradient boosting with an artificial neural network to\nmeasure predictive power on new, unseen data.\nMethods. Sensor biomarker data from six public datasets were utilized in this\nstudy. To test model generalization, we developed a gradient boosting model\ntrained on one dataset (SWELL), and tested its predictive power on two datasets\npreviously used in other studies (WESAD, NEURO). Next, we merged four small\ndatasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of\n99 subjects,. In addition, we utilized random sampling combined with another\ndataset (EXAM) to build a larger training dataset consisting of 200 synthesized\nsubjects,. Finally, we developed an ensemble model that combines our gradient\nboosting model with an artificial neural network, and tested it on two\nadditional, unseen publicly available stress datasets (WESAD and Toadstool).\nResults. Our method delivers a robust stress measurement system capable of\nachieving 85% predictive accuracy on new, unseen validation data, achieving a\n25% performance improvement over single models trained on small datasets.\nConclusion. Models trained on small, single study protocol datasets do not\ngeneralize well for use on new, unseen data and lack statistical power.\nMa-chine learning models trained on a dataset containing a larger number of\nvaried study subjects capture physiological variance better, resulting in more\nrobust stress detection.",
    "descriptor": "\nComments: 37 pages, 11 figures\n",
    "authors": [
      "Gideon Vos",
      "Kelly Trinh",
      "Zoltan Sarnyai",
      "Mostafa Rahimi Azghadi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15146"
  },
  {
    "id": "arXiv:2209.15147",
    "title": "Optimizing towards the best insertion-based error-tolerating joints",
    "abstract": "We present an optimization-based design process that can generate the best\ninsertion-based joints with respect to different errors, including manipulation\nerror, manufacturing error, and sensing error. We separate the analysis into\ntwo stages, the insertion and the after-insertion stability. Each sub-process\nis discretized into different modes of contacts. The transitions among the\ncontact modes form a directed graph and the connectivity of the graph is\nachieved and maintained through the manipulation of the socket edge-angle and\npeg contact-point locations. The analysis starts in 2D with the assumption of\npoint-edge contacts. During the optimization, the edges of the socket are\nrotated and the points on the peg are moved along the edges to ensure the\nsuccessful insertion and the stability after insertion. We show in simulation\nthat our proposed method can generate insertion-based joints that are tolerant\nto the given errors. and we present a few simple 3D projections to show that\nthe analysis is still effective beyond 2D cases.",
    "descriptor": "",
    "authors": [
      "Zhibin Zou"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15147"
  },
  {
    "id": "arXiv:2209.15148",
    "title": "Embedded System Performance Analysis for Implementing a Portable  Drowsiness Detection System for Drivers",
    "abstract": "Drowsiness on the road is a widespread problem with fatal consequences; thus,\na multitude of solutions implementing machine learning techniques have been\nproposed by researchers. Among existing methods, Ghoddoosian et al.'s\ndrowsiness detection method utilizes temporal blinking patterns to detect early\nsigns of drowsiness. Although the method reported promising results,\nGhoddoosian et al.'s algorithm was developed and tested only on a powerful\ndesktop computer, which is not practical to apply in a moving vehicle setting.\nIn this paper, we propose an embedded system that can process Ghoddoosian's\ndrowsiness detection algorithm on a small minicomputer and interact with the\nuser by phone; combined, the devices are powerful enough to run a web server\nand our drowsiness detection server. We used the AioRTC protocol on GitHub to\nconduct real-time transmission of video frames from the client to the server\nand evaluated the communication speed and processing times of the program on\nvarious platforms. Based on our results, we found that a Mini PC was most\nsuitable for our proposed system. Furthermore, we proposed an algorithm that\nconsiders the importance of sensitivity over specificity, specifically\nregarding drowsiness detection algorithms. Our algorithm optimizes the\nthreshold to adjust the false positive and false negative rates of the\ndrowsiness detection models. We anticipate our proposed platform can help many\nresearchers to advance their research on drowsiness detection solutions in\nembedded system settings.",
    "descriptor": "\nComments: 16 pages, 11 figures, 3 tables\n",
    "authors": [
      "Minjeong Kim",
      "Jimin Koo"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.15148"
  },
  {
    "id": "arXiv:2209.15149",
    "title": "Pure-Circuit: Strong Inapproximability for PPAD",
    "abstract": "The current state-of-the-art methods for showing inapproximability in PPAD\narise from the $\\varepsilon$-Generalized-Circuit ($\\varepsilon$-GCircuit)\nproblem. Rubinstein (2018) showed that there exists a small unknown constant\n$\\varepsilon$ for which $\\varepsilon$-GCircuit is PPAD-hard, and subsequent\nwork has shown hardness results for other problems in PPAD by using\n$\\varepsilon$-GCircuit as an intermediate problem.\nWe introduce Pure-Circuit, a new intermediate problem for PPAD, which can be\nthought of as $\\varepsilon$-GCircuit pushed to the limit as $\\varepsilon\n\\rightarrow 1$, and we show that the problem is PPAD-complete. We then prove\nthat $\\varepsilon$-GCircuit is PPAD-hard for all $\\varepsilon < 0.1$ by a\nreduction from Pure-Circuit, and thus strengthen all prior work that has used\nGCircuit as an intermediate problem from the existential-constant regime to the\nlarge-constant regime.\nWe show that stronger inapproximability results can be derived by reducing\ndirectly from Pure-Circuit. In particular, we prove tight inapproximability\nresults for computing $\\varepsilon$-well-supported Nash equilibria in\ntwo-action polymatrix games, as well as for finding approximate equilibria in\nthreshold games.",
    "descriptor": "",
    "authors": [
      "Argyrios Deligkas",
      "John Fearnley",
      "Alexandros Hollender",
      "Themistoklis Melissourgos"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2209.15149"
  },
  {
    "id": "arXiv:2209.15150",
    "title": "Spatio-temporal Motion Planning for Autonomous Vehicles with Trapezoidal  Prism Corridors and B\u00e9zier Curves",
    "abstract": "Safety-guaranteed motion planning is critical for self-driving cars to\ngenerate collision-free trajectories. A layered motion planning approach with\ndecoupled path and speed planning is widely used for this purpose. This\napproach is prone to be suboptimal in the presence of dynamic obstacles.\nSpatial-temporal approaches deal with path planning and speed planning\nsimultaneously; however, the existing methods only support simple-shaped\ncorridors like cuboids, which restrict the search space for optimization in\ncomplex scenarios. We propose to use trapezoidal prism-shaped corridors for\noptimization, which significantly enlarges the solution space compared to the\nexisting cuboidal corridors-based method. Finally, a piecewise B\\'{e}zier curve\noptimization is conducted in our proposed corridors. This formulation\ntheoretically guarantees the safety of the continuous-time trajectory. We\nvalidate the efficiency and effectiveness of the proposed approach in numerical\nand CommonRoad simulations.",
    "descriptor": "\nComments: Under Review at ACC 2023\n",
    "authors": [
      "Srujan Deolasee",
      "Qin Lin",
      "Jialun Li",
      "John M. Dolan"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15150"
  },
  {
    "id": "arXiv:2209.15151",
    "title": "Tight Inapproximability for Graphical Games",
    "abstract": "We provide a complete characterization for the computational complexity of\nfinding approximate equilibria in two-action graphical games. We consider the\ntwo most well-studied approximation notions: $\\varepsilon$-Nash equilibria\n($\\varepsilon$-NE) and $\\varepsilon$-well-supported Nash equilibria\n($\\varepsilon$-WSNE), where $\\varepsilon \\in [0,1]$. We prove that computing an\n$\\varepsilon$-NE is PPAD-complete for any constant $\\varepsilon < 1/2$, while a\nvery simple algorithm (namely, letting all players mix uniformly between their\ntwo actions) yields a $1/2$-NE. On the other hand, we show that computing an\n$\\varepsilon$-WSNE is PPAD-complete for any constant $\\varepsilon < 1$, while a\n$1$-WSNE is trivial to achieve, because any strategy profile is a $1$-WSNE. All\nof our lower bounds immediately also apply to graphical games with more than\ntwo actions per player.",
    "descriptor": "",
    "authors": [
      "Argyrios Deligkas",
      "John Fearnley",
      "Alexandros Hollender",
      "Themistoklis Melissourgos"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2209.15151"
  },
  {
    "id": "arXiv:2209.15153",
    "title": "MonoNeuralFusion: Online Monocular Neural 3D Reconstruction with  Geometric Priors",
    "abstract": "High-fidelity 3D scene reconstruction from monocular videos continues to be\nchallenging, especially for complete and fine-grained geometry reconstruction.\nThe previous 3D reconstruction approaches with neural implicit representations\nhave shown a promising ability for complete scene reconstruction, while their\nresults are often over-smooth and lack enough geometric details. This paper\nintroduces a novel neural implicit scene representation with volume rendering\nfor high-fidelity online 3D scene reconstruction from monocular videos. For\nfine-grained reconstruction, our key insight is to incorporate geometric priors\ninto both the neural implicit scene representation and neural volume rendering,\nthus leading to an effective geometry learning mechanism based on volume\nrendering optimization. Benefiting from this, we present MonoNeuralFusion to\nperform the online neural 3D reconstruction from monocular videos, by which the\n3D scene geometry is efficiently generated and optimized during the on-the-fly\n3D monocular scanning. The extensive comparisons with state-of-the-art\napproaches show that our MonoNeuralFusion consistently generates much better\ncomplete and fine-grained reconstruction results, both quantitatively and\nqualitatively.",
    "descriptor": "\nComments: 12 pages, 12 figures\n",
    "authors": [
      "Zi-Xin Zou",
      "Shi-Sheng Huang",
      "Yan-Pei Cao",
      "Tai-Jiang Mu",
      "Ying Shan",
      "Hongbo Fu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15153"
  },
  {
    "id": "arXiv:2209.15154",
    "title": "Variable-Based Calibration for Machine Learning Classifiers",
    "abstract": "The deployment of machine learning classifiers in high-stakes domains\nrequires well-calibrated confidence scores for model predictions. In this paper\nwe introduce the notion of variable-based calibration to characterize\ncalibration properties of a model with respect to a variable of interest,\ngeneralizing traditional score-based calibration and metrics such as expected\ncalibration error (ECE). In particular, we find that models with near-perfect\nECE can exhibit significant variable-based calibration error as a function of\nfeatures of the data. We demonstrate this phenomenon both theoretically and in\npractice on multiple well-known datasets, and show that it can persist after\nthe application of existing recalibration methods. To mitigate this issue, we\npropose strategies for detection, visualization, and quantification of\nvariable-based calibration error. We then examine the limitations of current\nscore-based recalibration methods and explore potential modifications. Finally,\nwe discuss the implications of these findings, emphasizing that an\nunderstanding of calibration beyond simple aggregate measures is crucial for\nendeavors such as fairness and model interpretability.",
    "descriptor": "",
    "authors": [
      "Markelle Kelly",
      "Padhraic Smyth"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15154"
  },
  {
    "id": "arXiv:2209.15156",
    "title": "Cooperative Beamforming Design for Multiple RIS-Assisted Communication  Systems",
    "abstract": "Reconfigurable intelligent surface (RIS) provides a promising way to build\nprogrammable wireless transmission environments. Owing to the massive number of\ncontrollable reflecting elements on the surface, RIS is capable of providing\nconsiderable passive beamforming gains. At present, most related works mainly\nconsider the modeling, design, performance analysis and optimization of\nsingle-RIS-assisted systems. Although there are a few of works that investigate\nmultiple RISs individually serving their associated users, the cooperation\namong multiple RISs is not well considered as yet. To fill the gap, this paper\nstudies a cooperative beamforming design for multi-RIS-assisted communication\nsystems, where multiple RISs are deployed to assist the downlink communications\nfrom a base station to its users. To do so, we first model the general channel\nfrom the base station to the users for arbitrary number of reflection links.\nThen, we formulate an optimization problem to maximize the sum rate of all\nusers. Analysis shows that the formulated problem is difficult to solve due to\nits non-convexity and the interactions among the decision variables. To solve\nit effectively, we first decouple the problem into three disjoint subproblems.\nThen, by introducing appropriate auxiliary variables, we derive the closed-form\nexpressions for the decision variables and propose a low-complexity cooperative\nbeamforming algorithm. Simulation results have verified the effectiveness of\nthe proposed algorithm through comparison with various baseline methods.\nFurthermore, these results also unveil that, for the sum rate maximization,\ndistributing the reflecting elements among multiple RISs is superior to\ndeploying them at one single RIS.",
    "descriptor": "",
    "authors": [
      "Xiaoyan Ma",
      "Yuguang Fang",
      "Haixia Zhang",
      "Shuaishuai Guo",
      "Dongfeng Yuan"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15156"
  },
  {
    "id": "arXiv:2209.15157",
    "title": "Rethinking and Recomputing the Value of ML Models",
    "abstract": "In this paper, we argue that the way we have been training and evaluating ML\nmodels has largely forgotten the fact that they are applied in an organization\nor societal context as they provide value to people. We show that with this\nperspective we fundamentally change how we evaluate, select and deploy ML\nmodels - and to some extent even what it means to learn. Specifically, we\nstress that the notion of value plays a central role in learning and\nevaluating, and different models may require different learning practices and\nprovide different values based on the application context they are applied. We\nalso show that this concretely impacts how we select and embed models into\nhuman workflows based on experimental datasets. Nothing of what is presented\nhere is hard: to a large extent is a series of fairly trivial observations with\nmassive practical implications.",
    "descriptor": "",
    "authors": [
      "Burcu Sayin",
      "Fabio Casati",
      "Andrea Passerini",
      "Jie Yang",
      "Xinyue Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15157"
  },
  {
    "id": "arXiv:2209.15159",
    "title": "MobileViTv3: Mobile-Friendly Vision Transformer with Simple and  Effective Fusion of Local, Global and Input Features",
    "abstract": "MobileViT (MobileViTv1) combines convolutional neural networks (CNNs) and\nvision transformers (ViTs) to create light-weight models for mobile vision\ntasks. Though the main MobileViTv1-block helps to achieve competitive\nstate-of-the-art results, the fusion block inside MobileViTv1-block, creates\nscaling challenges and has a complex learning task. We propose changes to the\nfusion block that are simple and effective to create MobileViTv3-block, which\naddresses the scaling and simplifies the learning task. Our proposed\nMobileViTv3-block used to create MobileViTv3-XXS, XS and S models outperform\nMobileViTv1 on ImageNet-1k, ADE20K, COCO and PascalVOC2012 datasets. On\nImageNet-1K, MobileViTv3-XXS and MobileViTv3-XS surpasses MobileViTv1-XXS and\nMobileViTv1-XS by 2% and 1.9% respectively. Recently published MobileViTv2\narchitecture removes fusion block and uses linear complexity transformers to\nperform better than MobileViTv1. We add our proposed fusion block to\nMobileViTv2 to create MobileViTv3-0.5, 0.75 and 1.0 models. These new models\ngive better accuracy numbers on ImageNet-1k, ADE20K, COCO and PascalVOC2012\ndatasets as compared to MobileViTv2. MobileViTv3-0.5 and MobileViTv3-0.75\noutperforms MobileViTv2-0.5 and MobileViTv2-0.75 by 2.1% and 1.0% respectively\non ImageNet-1K dataset. For segmentation task, MobileViTv3-1.0 achieves 2.07%\nand 1.1% better mIOU compared to MobileViTv2-1.0 on ADE20K dataset and\nPascalVOC2012 dataset respectively. Our code and the trained models are\navailable at: https://github.com/micronDLA/MobileViTv3",
    "descriptor": "\nComments: 20 pages, 7 figures\n",
    "authors": [
      "Shakti N. Wadekar",
      "Abhishek Chaurasia"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15159"
  },
  {
    "id": "arXiv:2209.15161",
    "title": "Geography-aware Optimal UAV 3D Placement for LOS Relaying: A Geometry  Approach",
    "abstract": "Many emerging technologies for the next generation wireless network prefer\nline-of-sight (LOS) propagation conditions to fully release their performance\nadvantages. This paper studies 3D unmanned aerial vehicle (UAV) placement to\nestablish LOS links for two ground terminals in deep shadow in a dense urban\nenvironment. The challenge is that the LOS region for the feasible UAV\npositions can be arbitrary due to the complicated structure of the environment.\nWhile most existing works rely on simplified stochastic LOS models and problem\nrelaxations, this paper focuses on establishing theoretical guarantees for the\noptimal UAV placement to ensure LOS conditions for two ground users in an\nactual propagation environment. It is found that it suffices to search a\nbounded 2D area for the globally optimal 3D UAV position. Thus, this paper\ndevelops an exploration-exploitation algorithm with a linear trajectory length\nand achieves above 99% global optimality over several real city environments\nbeing tested in our experiments. To further enhance the search capability in an\nultra-dense environment, a dynamic multi-stage algorithm is developed and\ntheoretically shown to find an $\\epsilon$-optimal UAV position with a search\nlength $O(1/\\epsilon)$. Significant performance advantages are demonstrated in\nseveral numerical experiments for wireless communication relaying and wireless\npower transfer.",
    "descriptor": "",
    "authors": [
      "Yuanshuai Zheng",
      "Junting Chen"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15161"
  },
  {
    "id": "arXiv:2209.15162",
    "title": "Linearly Mapping from Image to Text Space",
    "abstract": "The extent to which text-only language models (LMs) learn to represent the\nphysical, non-linguistic world is an open question. Prior work has shown that\npretrained LMs can be taught to ``understand'' visual inputs when the models'\nparameters are updated on image captioning tasks. We test a stronger\nhypothesis: that the conceptual representations learned by text-only models are\nfunctionally equivalent (up to a linear transformation) to those learned by\nmodels trained on vision tasks. Specifically, we show that the image\nrepresentations from vision models can be transferred as continuous prompts to\nfrozen LMs by training only a single linear projection. Using these to prompt\nthe LM achieves competitive performance on captioning and visual question\nanswering tasks compared to models that tune both the image encoder and text\ndecoder (such as the MAGMA model). We compare three image encoders with\nincreasing amounts of linguistic supervision seen during pretraining: BEIT (no\nlinguistic information), NF-ResNET (lexical category information), and CLIP\n(full natural language descriptions). We find that all three encoders perform\nequally well at transferring visual property information to the language model\n(e.g., whether an animal is large or small), but that image encoders pretrained\nwith linguistic supervision more saliently encode category information (e.g.,\ndistinguishing hippo vs.\\ elephant) and thus perform significantly better on\nbenchmark language-and-vision tasks. Our results indicate that LMs encode\nconceptual information structurally similarly to vision-based models, even\nthose that are solely trained on images.",
    "descriptor": "",
    "authors": [
      "Jack Merullo",
      "Louis Castricato",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15162"
  },
  {
    "id": "arXiv:2209.15164",
    "title": "Blur the Linguistic Boundary: Interpreting Chinese Buddhist Sutra in  English via Neural Machine Translation",
    "abstract": "Buddhism is an influential religion with a long-standing history and profound\nphilosophy. Nowadays, more and more people worldwide aspire to learn the\nessence of Buddhism, attaching importance to Buddhism dissemination. However,\nBuddhist scriptures written in classical Chinese are obscure to most people and\nmachine translation applications. For instance, general Chinese-English neural\nmachine translation (NMT) fails in this domain. In this paper, we proposed a\nnovel approach to building a practical NMT model for Buddhist scriptures. The\nperformance of our translation pipeline acquired highly promising results in\nablation experiments under three criteria.",
    "descriptor": "\nComments: This paper is accepted by ICTAI 2022. The 34th IEEE International Conference on Tools with Artificial Intelligence (ICTAI)\n",
    "authors": [
      "Denghao Li",
      "Yuqiao Zeng",
      "Jianzong Wang",
      "Lingwei Kong",
      "Zhangcheng Huang",
      "Ning Cheng",
      "Xiaoyang Qu",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15164"
  },
  {
    "id": "arXiv:2209.15165",
    "title": "Distilling Style from Image Pairs for Global Forward and Inverse Tone  Mapping",
    "abstract": "Many image enhancement or editing operations, such as forward and inverse\ntone mapping or color grading, do not have a unique solution, but instead a\nrange of solutions, each representing a different style. Despite this, existing\nlearning-based methods attempt to learn a unique mapping, disregarding this\nstyle. In this work, we show that information about the style can be distilled\nfrom collections of image pairs and encoded into a 2- or 3-dimensional vector.\nThis gives us not only an efficient representation but also an interpretable\nlatent space for editing the image style. We represent the global color mapping\nbetween a pair of images as a custom normalizing flow, conditioned on a\npolynomial basis of the pixel color. We show that such a network is more\neffective than PCA or VAE at encoding image style in low-dimensional space and\nlets us obtain an accuracy close to 40 dB, which is about 7-10 dB improvement\nover the state-of-the-art methods.",
    "descriptor": "\nComments: Published in European Conference on Visual Media Production (CVMP '22)\n",
    "authors": [
      "Aamir Mustafa",
      "Param Hanji",
      "Rafal K. Mantiuk"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15165"
  },
  {
    "id": "arXiv:2209.15166",
    "title": "Reward Shaping for User Satisfaction in a REINFORCE Recommender",
    "abstract": "How might we design Reinforcement Learning (RL)-based recommenders that\nencourage aligning user trajectories with the underlying user satisfaction?\nThree research questions are key: (1) measuring user satisfaction, (2)\ncombatting sparsity of satisfaction signals, and (3) adapting the training of\nthe recommender agent to maximize satisfaction. For measurement, it has been\nfound that surveys explicitly asking users to rate their experience with\nconsumed items can provide valuable orthogonal information to the\nengagement/interaction data, acting as a proxy to the underlying user\nsatisfaction. For sparsity, i.e, only being able to observe how satisfied users\nare with a tiny fraction of user-item interactions, imputation models can be\nuseful in predicting satisfaction level for all items users have consumed. For\nlearning satisfying recommender policies, we postulate that reward shaping in\nRL recommender agents is powerful for driving satisfying user experiences.\nPutting everything together, we propose to jointly learn a policy network and a\nsatisfaction imputation network: The role of the imputation network is to learn\nwhich actions are satisfying to the user; while the policy network, built on\ntop of REINFORCE, decides which items to recommend, with the reward utilizing\nthe imputed satisfaction. We use both offline analysis and live experiments in\nan industrial large-scale recommendation platform to demonstrate the promise of\nour approach for satisfying user experiences.",
    "descriptor": "\nComments: Accepted in Reinforcement Learning for Real Life (RL4RealLife) Workshop in the 38th International Conference on Machine Learning, 2021\n",
    "authors": [
      "Konstantina Christakopoulou",
      "Can Xu",
      "Sai Zhang",
      "Sriraj Badam",
      "Trevor Potter",
      "Daniel Li",
      "Hao Wan",
      "Xinyang Yi",
      "Ya Le",
      "Chris Berg",
      "Eric Bencomo Dixon",
      "Ed H. Chi",
      "Minmin Chen"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15166"
  },
  {
    "id": "arXiv:2209.15167",
    "title": "An empirical study of weakly supervised audio tagging embeddings for  general audio representations",
    "abstract": "We study the usability of pre-trained weakly supervised audio tagging (AT)\nmodels as feature extractors for general audio representations. We mainly\nanalyze the feasibility of transferring those embeddings to other tasks within\nthe speech and sound domains. Specifically, we benchmark weakly supervised\npre-trained models (MobileNetV2 and EfficientNet-B0) against modern\nself-supervised learning methods (BYOL-A) as feature extractors. Fourteen\ndownstream tasks are used for evaluation ranging from music instrument\nclassification to language classification. Our results indicate that AT\npre-trained models are an excellent transfer learning choice for music, event,\nand emotion recognition tasks. Further, finetuning AT models can also benefit\nspeech-related tasks such as keyword spotting and intent classification.",
    "descriptor": "\nComments: Odyssey 2022\n",
    "authors": [
      "Heinrich Dinkel",
      "Zhiyong Yan",
      "Yongqing Wang",
      "Junbo Zhang",
      "Yujun Wang"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15167"
  },
  {
    "id": "arXiv:2209.15168",
    "title": "Depth-Wise Attention (DWAtt): A Layer Fusion Method for Data-Efficient  Classification",
    "abstract": "Language Models pretrained on large textual data have been shown to encode\ndifferent types of knowledge simultaneously. Traditionally, only the features\nfrom the last layer are used when adapting to new tasks or data. We put forward\nthat, when using or finetuning deep pretrained models, intermediate layer\nfeatures that may be relevant to the downstream task are buried too deep to be\nused efficiently in terms of needed samples or steps. To test this, we propose\na new layer fusion method: Depth-Wise Attention (DWAtt), to help re-surface\nsignals from non-final layers. We compare DWAtt to a basic concatenation-based\nlayer fusion method (Concat), and compare both to a deeper model baseline --\nall kept within a similar parameter budget. Our findings show that DWAtt and\nConcat are more step- and sample-efficient than the baseline, especially in the\nfew-shot setting. DWAtt outperforms Concat on larger data sizes. On CoNLL-03\nNER, layer fusion shows 3.68-9.73% F1 gain at different few-shot sizes. The\nlayer fusion models presented significantly outperform the baseline in various\ntraining scenarios with different data sizes, architectures, and training\nconstraints.",
    "descriptor": "\nComments: 7 pages, 7 figures\n",
    "authors": [
      "Muhammad ElNokrashy",
      "Badr AlKhamissi",
      "Mona Diab"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15168"
  },
  {
    "id": "arXiv:2209.15169",
    "title": "Handle Anywhere: A Mobile Robot Arm for Providing Bodily Support to  Elderly Persons",
    "abstract": "Age-related loss of mobility and increased risk of falling remain important\nobstacles toward facilitating aging-in-place. Many elderly people lack the\ncoordination and strength necessary to perform common movements around their\nhome, such as getting out of bed or stepping into a bathtub. The traditional\nsolution has been to install grab bars on various surfaces; however, these are\noften not placed in optimal locations due to feasibility constraints in room\nlayout. In this paper, we present a mobile robot that provides an older adult\nwith a handle anywhere in space - \"handle anywhere\". The robot consists of an\nomnidirectional mobile base attached to a repositionable handle. We analyze the\npostural changes in four activities of daily living and determine, in each, the\nbody pose that requires the maximal muscle effort. Using a simple model of the\nhuman body, we develop a methodology to optimally place the handle to provide\nthe maximum support for the elderly person at the point of most effort. Our\nmodel is validated with experimental trials. We discuss how the robotic device\ncould be used to enhance patient mobility and reduce the incidence of falls.",
    "descriptor": "\nComments: 8 pages, 10 figures\n",
    "authors": [
      "Roberto Bolli, Jr.",
      "Paolo Bonato",
      "Harry Asada"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15169"
  },
  {
    "id": "arXiv:2209.15170",
    "title": "Securing Large-Scale D2D Networks Using Covert Communication and  Friendly Jamming",
    "abstract": "We exploit both covert communication and friendly jamming to propose a\nfriendly jamming-assisted covert communication and use it to doubly secure a\nlarge-scale device-to-device (D2D) network against eavesdroppers (i.e.,\nwardens). The D2D transmitters defend against the wardens by: 1) hiding their\ntransmissions with enhanced covert communication, and 2) leveraging friendly\njamming to ensure information secrecy even if the D2D transmissions are\ndetected. We model the combat between the wardens and the D2D network (the\ntransmitters and the friendly jammers) as a two-stage Stackelberg game.\nTherein, the wardens are the followers at the lower stage aiming to minimize\ntheir detection errors, and the D2D network is the leader at the upper stage\naiming to maximize its utility (in terms of link reliability and communication\nsecurity) subject to the constraint on communication covertness. We apply\nstochastic geometry to model the network spatial configuration so as to conduct\na system-level study. We develop a bi-level optimization algorithm to search\nfor the equilibrium of the proposed Stackelberg game based on the successive\nconvex approximation (SCA) method and Rosenbrock method. Numerical results\nreveal interesting insights. We observe that without the assistance from the\njammers, it is difficult to achieve covert communication on D2D transmission.\nMoreover, we illustrate the advantages of the proposed friendly\njamming-assisted covert communication by comparing it with the\ninformation-theoretical secrecy approach in terms of the secure communication\nprobability and network utility.",
    "descriptor": "",
    "authors": [
      "Shaohan Feng",
      "Xiao Lu",
      "Sumei Sun",
      "Dusit Niyato",
      "Ekram Hossain"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15170"
  },
  {
    "id": "arXiv:2209.15172",
    "title": "Understanding Pure CLIP Guidance for Voxel Grid NeRF Models",
    "abstract": "We explore the task of text to 3D object generation using CLIP. Specifically,\nwe use CLIP for guidance without access to any datasets, a setting we refer to\nas pure CLIP guidance. While prior work has adopted this setting, there is no\nsystematic study of mechanics for preventing adversarial generations within\nCLIP. We illustrate how different image-based augmentations prevent the\nadversarial generation problem, and how the generated results are impacted. We\ntest different CLIP model architectures and show that ensembling different\nmodels for guidance can prevent adversarial generations within bigger models\nand generate sharper results. Furthermore, we implement an implicit voxel grid\nmodel to show how neural networks provide an additional layer of\nregularization, resulting in better geometrical structure and coherency of\ngenerated objects. Compared to prior work, we achieve more coherent results\nwith higher memory efficiency and faster training speeds.",
    "descriptor": "",
    "authors": [
      "Han-Hung Lee",
      "Angel X. Chang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15172"
  },
  {
    "id": "arXiv:2209.15173",
    "title": "Construction of the Radio Map with Defective GPS Position Information",
    "abstract": "The basic idea of RSS-based indoor positioning is to estimate the receiver\nlocation by matching the measured received signal strength indicator (RSSI)\nwith preestablished RSSI collections with corresponding locations, known as the\nradio map. Hence, constructing an accurate radio map directly relates to\naccurate positioning performance in RSS-based indoor positioning. RSS-based\nindoor positioning can be easily conducted with a radio map that surveys every\nlocation, but a complete radio map cannot be constructed when the map area\nincludes locations that are physically impossible to reach or denied access. In\naddition, measurement errors or device problems can occur during the survey,\nresulting in degradation of the radio map. We analyzed incidents that occurred\nin actual RSSI surveys that could disrupt the construction of the radio map and\nproposed methods to construct a more accurate radio map.",
    "descriptor": "\nComments: 5 pages\n",
    "authors": [
      "Taewon Kang",
      "Joon Hyo Rhee"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15173"
  },
  {
    "id": "arXiv:2209.15176",
    "title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic  Speech Recognition",
    "abstract": "The Transformer architecture model, based on self-attention and multi-head\nattention, has achieved remarkable success in offline end-to-end Automatic\nSpeech Recognition (ASR). However, self-attention and multi-head attention\ncannot be easily applied for streaming or online ASR. For self-attention in\nTransformer ASR, the softmax normalization function-based attention mechanism\nmakes it impossible to highlight important speech information. For multi-head\nattention in Transformer ASR, it is not easy to model monotonic alignments in\ndifferent heads. To overcome these two limits, we integrate sparse attention\nand monotonic attention into Transformer-based ASR. The sparse mechanism\nintroduces a learned sparsity scheme to enable each self-attention structure to\nfit the corresponding head better. The monotonic attention deploys\nregularization to prune redundant heads for the multi-head attention structure.\nThe experiments show that our method can effectively improve the attention\nmechanism on widely used benchmarks of speech recognition.",
    "descriptor": "\nComments: Accepted to DSAA 2022\n",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Wen qi Wei",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15176"
  },
  {
    "id": "arXiv:2209.15177",
    "title": "Domain Generalization -- A Causal Perspective",
    "abstract": "Machine learning models have gained widespread success, from healthcare to\npersonalized recommendations. One of the preliminary assumptions of these\nmodels is the independent and identical distribution. Therefore, the train and\ntest data are sampled from the same observation per this assumption. However,\nthis assumption seldom holds in the real world due to distribution shifts.\nSince the models rely heavily on this assumption, they exhibit poor\ngeneralization capabilities. Over the recent years, dedicated efforts have been\nmade to improve the generalization capabilities of these models. The primary\nidea behind these methods is to identify stable features or mechanisms that\nremain invariant across the different distributions. Many generalization\napproaches employ causal theories to describe invariance since causality and\ninvariance are inextricably intertwined. However, current surveys deal with the\ncausality-aware domain generalization methods on a very high-level.\nFurthermore, none of the existing surveys categorize the causal domain\ngeneralization methods based on the problem and causal theories these methods\nleverage. To this end, we present a comprehensive survey on causal domain\ngeneralization models from the aspects of the problem and causal theories.\nFurthermore, this survey includes in-depth insights into publicly accessible\ndatasets and benchmarks for domain generalization in various domains. Finally,\nwe conclude the survey with insights and discussions on future research\ndirections. Finally, we conclude the survey with insights and discussions on\nfuture research directions.",
    "descriptor": "",
    "authors": [
      "Paras Sheth",
      "Raha Moraffah",
      "K. Sel\u00e7uk Candan",
      "Adrienne Raglin",
      "Huan Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15177"
  },
  {
    "id": "arXiv:2209.15179",
    "title": "Physical Adversarial Attack meets Computer Vision: A Decade Survey",
    "abstract": "Although Deep Neural Networks (DNNs) have achieved impressive results in\ncomputer vision, their exposed vulnerability to adversarial attacks remains a\nserious concern. A series of works has shown that by adding elaborate\nperturbations to images, DNNs could have catastrophic degradation in\nperformance metrics. And this phenomenon does not only exist in the digital\nspace but also in the physical space. Therefore, estimating the security of\nthese DNNs-based systems is critical for safely deploying them in the real\nworld, especially for security-critical applications, e.g., autonomous cars,\nvideo surveillance, and medical diagnosis. In this paper, we focus on physical\nadversarial attacks and provide a comprehensive survey of over 150 existing\npapers. We first clarify the concept of the physical adversarial attack and\nanalyze its characteristics. Then, we define the adversarial medium, essential\nto perform attacks in the physical world. Next, we present the physical\nadversarial attack methods in task order: classification, detection, and\nre-identification, and introduce their performance in solving the trilemma:\neffectiveness, stealthiness, and robustness. In the end, we discuss the current\nchallenges and potential future directions.",
    "descriptor": "\nComments: 32 pages. arXiv admin note: text overlap with arXiv:2207.04718, arXiv:2011.13375 by other authors\n",
    "authors": [
      "Hui Wei",
      "Hao Tang",
      "Xuemei Jia",
      "Hanxun Yu",
      "Zhubo Li",
      "Zhixiang Wang",
      "Shin'ichi Satoh",
      "Zheng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15179"
  },
  {
    "id": "arXiv:2209.15181",
    "title": "RL-MD: A Novel Reinforcement Learning Approach for DNA Motif Discovery",
    "abstract": "The extraction of sequence patterns from a collection of functionally linked\nunlabeled DNA sequences is known as DNA motif discovery, and it is a key task\nin computational biology. Several deep learning-based techniques have recently\nbeen introduced to address this issue. However, these algorithms can not be\nused in real-world situations because of the need for labeled data. Here, we\npresented RL-MD, a novel reinforcement learning based approach for DNA motif\ndiscovery task. RL-MD takes unlabelled data as input, employs a relative\ninformation-based method to evaluate each proposed motif, and utilizes these\ncontinuous evaluation results as the reward. The experiments show that RL-MD\ncan identify high-quality motifs in real-world data.",
    "descriptor": "\nComments: This paper is accepted by DSAA2022. The 9th IEEE International Conference on Data Science and Advanced Analytics\n",
    "authors": [
      "Wen Wang",
      "Jianzong Wang",
      "Shijing Si",
      "Zhangcheng Huang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Genomics (q-bio.GN)"
    ],
    "url": "https://arxiv.org/abs/2209.15181"
  },
  {
    "id": "arXiv:2209.15182",
    "title": "Husformer: A Multi-Modal Transformer for Multi-Modal Human State  Recognition",
    "abstract": "Human state recognition is a critical topic with pervasive and important\napplications in human-machine systems.Multi-modal fusion, the combination of\nmetrics from multiple data sources, has been shown as a sound method for\nimproving the recognition performance. However, while promising results have\nbeen reported by recent multi-modal-based models, they generally fail to\nleverage the sophisticated fusion strategies that would model sufficient\ncross-modal interactions when producing the fusion representation; instead,\ncurrent methods rely on lengthy and inconsistent data preprocessing and feature\ncrafting. To address this limitation, we propose an end-to-end multi-modal\ntransformer framework for multi-modal human state recognition called\nHusformer.Specifically, we propose to use cross-modal transformers, which\ninspire one modality to reinforce itself through directly attending to latent\nrelevance revealed in other modalities, to fuse different modalities while\nensuring sufficient awareness of the cross-modal interactions introduced.\nSubsequently, we utilize a self-attention transformer to further prioritize\ncontextual information in the fusion representation. Using two such attention\nmechanisms enables effective and adaptive adjustments to noise and\ninterruptions in multi-modal signals during the fusion process and in relation\nto high-level features. Extensive experiments on two human emotion corpora\n(DEAP and WESAD) and two cognitive workload datasets (MOCAS and CogLoad)\ndemonstrate that in the recognition of human state, our Husformer outperforms\nboth state-of-the-art multi-modal baselines and the use of a single modality by\na large margin, especially when dealing with raw multi-modal signals. We also\nconducted an ablation study to show the benefits of each component in\nHusformer/",
    "descriptor": "",
    "authors": [
      "Ruiqi Wang",
      "Wonse Jo",
      "Dezhong Zhao",
      "Weizheng Wang",
      "Baijian Yang",
      "Guohua Chen",
      "Byung-Cheol Min"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15182"
  },
  {
    "id": "arXiv:2209.15186",
    "title": "Leveraging Probabilistic Switching in Superparamagnets for Temporal  Information Encoding in Neuromorphic Systems",
    "abstract": "Brain-inspired computing - leveraging neuroscientific principles underpinning\nthe unparalleled efficiency of the brain in solving cognitive tasks - is\nemerging to be a promising pathway to solve several algorithmic and\ncomputational challenges faced by deep learning today. Nonetheless, current\nresearch in neuromorphic computing is driven by our well-developed notions of\nrunning deep learning algorithms on computing platforms that perform\ndeterministic operations. In this article, we argue that taking a different\nroute of performing temporal information encoding in probabilistic neuromorphic\nsystems may help solve some of the current challenges in the field. The article\nconsiders superparamagnetic tunnel junctions as a potential pathway to enable a\nnew generation of brain-inspired computing that combines the facets and\nassociated advantages of two complementary insights from computational\nneuroscience -- how information is encoded and how computing occurs in the\nbrain. Hardware-algorithm co-design analysis demonstrates $97.41\\%$ accuracy of\na state-compressed 3-layer spintronics enabled stochastic spiking network on\nthe MNIST dataset with high spiking sparsity due to temporal information\nencoding.",
    "descriptor": "",
    "authors": [
      "Kezhou Yang",
      "Dhuruva Priyan G M",
      "Abhronil Sengupta"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2209.15186"
  },
  {
    "id": "arXiv:2209.15189",
    "title": "Learning by Distilling Context",
    "abstract": "Language models significantly benefit from context tokens, such as prompts or\nscratchpads. They perform better when prompted with informative instructions,\nand they acquire new reasoning capabilities by generating a scratch-pad before\npredicting the final answers. However, they do not \\textit{internalize} these\nperformance gains, which disappear when the context tokens are gone. Our work\nproposes to apply context distillation so that a language model can improve\nitself by internalizing these gains. Concretely, given a synthetic unlabeled\ninput for the target task, we condition the model on ``[instructions] +\n[task-input]'' to predict ``[scratch-pad] + [final answer]''; then we fine-tune\nthe same model to predict its own ``[final answer]'' conditioned on the\n``[task-input]'', without seeing the ``[instructions]'' or using the\n``[scratch-pad]''.\nWe show that context distillation is a general method to train language\nmodels, and it can effectively internalize 3 types of training signals. First,\nit can internalize abstract task instructions and explanations, so we can\niteratively update the model parameters with new instructions and overwrite old\nones. Second, it can internalize step-by-step reasoning for complex tasks\n(e.g., 8-digit addition), and such a newly acquired capability proves to be\nuseful for other downstream tasks. Finally, it can internalize concrete\ntraining examples, and it outperforms directly learning with gradient descent\nby 9\\% on the SPIDER Text-to-SQL dataset; furthermore, combining context\ndistillation operations can internalize more training examples than the context\nwindow size allows.",
    "descriptor": "",
    "authors": [
      "Charlie Snell",
      "Dan Klein",
      "Ruiqi Zhong"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15189"
  },
  {
    "id": "arXiv:2209.15190",
    "title": "Neural Integral Equations",
    "abstract": "Integral equations (IEs) are functional equations defined through integral\noperators, where the unknown function is integrated over a possibly\nmultidimensional space. Important applications of IEs have been found\nthroughout theoretical and applied sciences, including in physics, chemistry,\nbiology, and engineering; often in the form of inverse problems. IEs are\nespecially useful since differential equations, e.g. ordinary differential\nequations (ODEs), and partial differential equations (PDEs) can be formulated\nin an integral version which is often more convenient to solve. Moreover,\nunlike ODEs and PDEs, IEs can model inherently non-local dynamical systems,\nsuch as ones with long distance spatiotemporal relations. While efficient\nalgorithms exist for solving given IEs, no method exists that can learn an\nintegral equation and its associated dynamics from data alone. In this article,\nwe introduce Neural Integral Equations (NIE), a method that learns an unknown\nintegral operator from data through a solver. We also introduce an attentional\nversion of NIE, called Attentional Neural Integral Equations (ANIE), where the\nintegral is replaced by self-attention, which improves scalability and provides\ninterpretability. We show that learning dynamics via integral equations is\nfaster than doing so via other continuous methods, such as Neural ODEs.\nFinally, we show that ANIE outperforms other methods on several benchmark tasks\nin ODE, PDE, and IE systems of synthetic and real-world data.",
    "descriptor": "\nComments: 10 + 12 pages, 12 figures and 7 tables. Comments are welcome!\n",
    "authors": [
      "Emanuele Zappala",
      "Antonio Henrique de Oliveira Fonseca",
      "Josue Ortega Caro",
      "David van Dijk"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Dynamical Systems (math.DS)",
      "Numerical Analysis (math.NA)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.15190"
  },
  {
    "id": "arXiv:2209.15195",
    "title": "RF-Transformer: A Unified Backscatter Radio Hardware Abstraction",
    "abstract": "This paper presents RF-Transformer, a unified backscatter radio hardware\nabstraction that allows a low-power IoT device to directly communicate with\nheterogeneous wireless receivers at the minimum power consumption. Unlike\nexisting backscatter systems that are tailored to a specific wireless\ncommunication protocol, RF-Transformer provides a programmable interface to the\nmicro-controller, allowing IoT devices to synthesize different types of\nprotocol-compliant backscatter signals sharing radically different PHY-layer\ndesigns. To show the efficacy of our design, we implement a PCB prototype of\nRF-Transformer on 2.4 GHz ISM band and showcase its capability on generating\nstandard ZigBee, Bluetooth, LoRa, and Wi-Fi 802.11b/g/n/ac packets. Our\nextensive field studies show that RF-Transformer achieves 23.8 Mbps, 247.1\nKbps, 986.5 Kbps, and 27.3 Kbps throughput when generating standard Wi-Fi,\nZigBee, Bluetooth, and LoRa signals while consuming 7.6-74.2 less power than\ntheir active counterparts. Our ASIC simulation based on the 65-nm CMOS process\nshows that the power gain of RF-Transformer can further grow to 92-678. We\nfurther integrate RF-Transformer with pressure sensors and present a case study\non detecting foot traffic density in hallways. Our 7-day case studies\ndemonstrate RFTransformer can reliably transmit sensor data to a commodity\ngateway by synthesizing LoRa packets on top of Wi-Fi signals. Our experimental\nresults also verify the compatibility of RF-Transformer with commodity\nreceivers. Code and hardware schematics can be found at:\nhttps://github.com/LeFsCC/RF-Transformer.",
    "descriptor": "",
    "authors": [
      "Xiuzhen Guo",
      "Yuan He",
      "Zihao Yu",
      "Jiacheng Zhang",
      "Yunhao Liu",
      "Longfei Shangguan"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15195"
  },
  {
    "id": "arXiv:2209.15196",
    "title": "Continuous Gaze Tracking With Implicit Saliency-Aware Calibration on  Mobile Devices",
    "abstract": "Gaze tracking is a useful human-to-computer interface, which plays an\nincreasingly important role in a range of mobile applications. Gaze calibration\nis an indispensable component of gaze tracking, which transforms the eye\ncoordinates to the screen coordinates. The existing approaches of gaze tracking\neither have limited accuracy or require the user's cooperation in calibration\nand in turn hurt the quality of experience. We in this paper propose vGaze,\ncontinuous gaze tracking with implicit saliency-aware calibration on mobile\ndevices. The design of vGaze stems from our insight on the temporal and spatial\ndependent relation between the visual saliency and the user's gaze. vGaze is\nimplemented as a light-weight software that identifies video frames with\n\"useful\" saliency information, sensing the user's head movement, performs\nopportunistic calibration using only those \"useful\" frames, and leverages\nhistorical information for accelerating saliency detection. We implement vGaze\non a commercial mobile device and evaluate its performance in various\nscenarios. The results show that vGaze can work at real time with video\nplayback applications. The average error of gaze tracking is 1.51 cm (2.884\ndegree) which decreases to 0.99 cm (1.891 degree) with historical information\nand 0.57 cm (1.089 degree) with an indicator.",
    "descriptor": "",
    "authors": [
      "Songzhou Yang",
      "Meng Jin",
      "Yuan He"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.15196"
  },
  {
    "id": "arXiv:2209.15197",
    "title": "Evaluation of taxonomic and neural embedding methods for calculating  semantic similarity",
    "abstract": "Modelling semantic similarity plays a fundamental role in lexical semantic\napplications. A natural way of calculating semantic similarity is to access\nhandcrafted semantic networks, but similarity prediction can also be\nanticipated in a distributional vector space. Similarity calculation continues\nto be a challenging task, even with the latest breakthroughs in deep neural\nlanguage models. We first examined popular methodologies in measuring taxonomic\nsimilarity, including edge-counting that solely employs semantic relations in a\ntaxonomy, as well as the complex methods that estimate concept specificity. We\nfurther extrapolated three weighting factors in modelling taxonomic similarity.\nTo study the distinct mechanisms between taxonomic and distributional\nsimilarity measures, we ran head-to-head comparisons of each measure with human\nsimilarity judgements from the perspectives of word frequency, polysemy degree\nand similarity intensity. Our findings suggest that without fine-tuning the\nuniform distance, taxonomic similarity measures can depend on the shortest path\nlength as a prime factor to predict semantic similarity; in contrast to\ndistributional semantics, edge-counting is free from sense distribution bias in\nuse and can measure word similarity both literally and metaphorically; the\nsynergy of retrofitting neural embeddings with concept relations in similarity\nprediction may indicate a new trend to leverage knowledge bases on transfer\nlearning. It appears that a large gap still exists on computing semantic\nsimilarity among different ranges of word frequency, polysemous degree and\nsimilarity intensity.",
    "descriptor": "",
    "authors": [
      "Dongqiang Yang",
      "Yanqin Yin"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15197"
  },
  {
    "id": "arXiv:2209.15198",
    "title": "FoVR: Attention-based VR Streaming through Bandwidth-limited Wireless  Networks",
    "abstract": "Consumer Virtual Reality (VR) has been widely used in various application\nareas, such as entertainment and medicine. In spite of the superb immersion\nexperience, to enable high-quality VR on untethered mobile devices remains an\nextremely challenging task. The high bandwidth demands of VR streaming\ngenerally overburden a conventional wireless connection, which affects the user\nexperience and in turn limits the usability of VR in practice. In this paper,\nwe propose FoVR, attention-based hierarchical VR streaming through\nbandwidth-limited wireless networks. The design of FoVR stems from the insight\nthat human's vision is hierarchical, so that different areas in the field of\nview (FoV) can be served with VR content of different qualities. By exploiting\nthe gaze tracking capacity of the VR devices, FoVR is able to accurately\npredict the user's attention so that the streaming of hierarchical VR can be\nappropriately scheduled. In this way, FoVR significantly reduces the bandwidth\ncost and computing cost while keeping high quality of user experience. We\nimplement FoVR on a commercial VR device and evaluate its performance in\nvarious scenarios. The experiment results show that FoVR reduces the bandwidth\ncost by 88.9% and 76.2%, respectively compared to the original VR streaming and\nthe state-of-the-art approach.",
    "descriptor": "",
    "authors": [
      "Songzhou Yang",
      "Yuan He",
      "Xiaolong Zheng"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2209.15198"
  },
  {
    "id": "arXiv:2209.15200",
    "title": "An efficient encoder-decoder architecture with top-down attention for  speech separation",
    "abstract": "Deep neural networks have shown excellent prospects in speech separation\ntasks. However, obtaining good results while keeping a low model complexity\nremains challenging in real-world applications. In this paper, we provide a\nbio-inspired efficient encoder-decoder architecture by mimicking the brain's\ntop-down attention, called TDANet, with decreased model complexity without\nsacrificing performance. The top-down attention in TDANet is extracted by the\nglobal attention (GA) module and the cascaded local attention (LA) layers. The\nGA module takes multi-scale acoustic features as input to extract global\nattention signal, which then modulates features of different scales by direct\ntop-down connections. The LA layers use features of adjacent layers as input to\nextract the local attention signal, which is used to modulate the lateral input\nin a top-down manner. On three benchmark datasets, TDANet consistently achieved\ncompetitive separation performance to previous state-of-the-art (SOTA) methods\nwith higher efficiency. Specifically, TDANet's multiply-accumulate operations\n(MACs) are only 5\\% of Sepformer, one of the previous SOTA models, and CPU\ninference time is only 10\\% of Sepformer. In addition, a large-size version of\nTDANet obtained SOTA results on three datasets, with MACs still only 10\\% of\nSepformer and the CPU inference time only 24\\% of Sepformer. Our study suggests\nthat top-down attention can be a more efficient strategy for speech separation.",
    "descriptor": "\nComments: 13 pages, 4 figures\n",
    "authors": [
      "Kai Li",
      "Runxuan Yang",
      "Xiaolin Hu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15200"
  },
  {
    "id": "arXiv:2209.15202",
    "title": "Synonym Detection Using Syntactic Dependency And Neural Embeddings",
    "abstract": "Recent advances on the Vector Space Model have significantly improved some\nNLP applications such as neural machine translation and natural language\ngeneration. Although word co-occurrences in context have been widely used in\ncounting-/predicting-based distributional models, the role of syntactic\ndependencies in deriving distributional semantics has not yet been thoroughly\ninvestigated. By comparing various Vector Space Models in detecting synonyms in\nTOEFL, we systematically study the salience of syntactic dependencies in\naccounting for distributional similarity. We separate syntactic dependencies\ninto different groups according to their various grammatical roles and then use\ncontext-counting to construct their corresponding raw and SVD-compressed\nmatrices. Moreover, using the same training hyperparameters and corpora, we\nstudy typical neural embeddings in the evaluation. We further study the\neffectiveness of injecting human-compiled semantic knowledge into neural\nembeddings on computing distributional similarity. Our results show that the\nsyntactically conditioned contexts can interpret lexical semantics better than\nthe unconditioned ones, whereas retrofitting neural embeddings with semantic\nknowledge can significantly improve synonym detection.",
    "descriptor": "",
    "authors": [
      "Dongqiang Yang",
      "Pikun Wang",
      "Xiaodong Sun",
      "Ning Li"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15202"
  },
  {
    "id": "arXiv:2209.15203",
    "title": "Downlink Compression Improves TopK Sparsification",
    "abstract": "Training large neural networks is time consuming. To speed up the process,\ndistributed training is often used. One of the largest bottlenecks in\ndistributed training is communicating gradients across different nodes.\nDifferent gradient compression techniques have been proposed to alleviate the\ncommunication bottleneck, including topK gradient sparsification, which\ntruncates the gradient to the largest K components before sending it to other\nnodes. While some authors have investigated topK gradient sparsification in the\nparameter-server framework by applying topK compression in both the\nworker-to-server (uplink) and server-to-worker (downlink) direction, the\ncurrently accepted belief says that adding extra compression degrades the\nconvergence of the model. We demonstrate, on the contrary, that adding downlink\ncompression can potentially improve the performance of topK sparsification: not\nonly does it reduce the amount of communication per step, but also,\ncounter-intuitively, can improve the upper bound in the convergence analysis.\nTo show this, we revisit non-convex convergence analysis of topK stochastic\ngradient descent (SGD) and extend it from the unidirectional to the\nbidirectional setting. We also remove a restriction of the previous analysis\nthat requires unrealistically large values of K. We experimentally evaluate\nbidirectional topK SGD against unidirectional topK SGD and show that models\ntrained with bidirectional topK SGD will perform as well as models trained with\nunidirectional topK SGD while yielding significant communication benefits for\nlarge numbers of workers.",
    "descriptor": "",
    "authors": [
      "William Zou",
      "Hans De Sterck",
      "Jun Liu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.15203"
  },
  {
    "id": "arXiv:2209.15205",
    "title": "ASPiRe:Adaptive Skill Priors for Reinforcement Learning",
    "abstract": "We introduce ASPiRe (Adaptive Skill Prior for RL), a new approach that\nleverages prior experience to accelerate reinforcement learning. Unlike\nexisting methods that learn a single skill prior from a large and diverse\ndataset, our framework learns a library of different distinction skill priors\n(i.e., behavior priors) from a collection of specialized datasets, and learns\nhow to combine them to solve a new task. This formulation allows the algorithm\nto acquire a set of specialized skill priors that are more reusable for\ndownstream tasks; however, it also brings up additional challenges of how to\neffectively combine these unstructured sets of skill priors to form a new prior\nfor new tasks. Specifically, it requires the agent not only to identify which\nskill prior(s) to use but also how to combine them (either sequentially or\nconcurrently) to form a new prior. To achieve this goal, ASPiRe includes\nAdaptive Weight Module (AWM) that learns to infer an adaptive weight assignment\nbetween different skill priors and uses them to guide policy learning for\ndownstream tasks via weighted Kullback-Leibler divergences. Our experiments\ndemonstrate that ASPiRe can significantly accelerate the learning of new\ndownstream tasks in the presence of multiple priors and show improvement on\ncompetitive baselines.",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Mengda Xu",
      "Manuela Veloso",
      "Shuran Song"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15205"
  },
  {
    "id": "arXiv:2209.15206",
    "title": "What Makes Pre-trained Language Models Better Zero/Few-shot Learners?",
    "abstract": "In this paper, we propose a theoretical framework to explain the efficacy of\nprompt learning in zero/few-shot scenarios. First, we prove that conventional\npre-training and fine-tuning paradigm fails in few-shot scenarios due to\noverfitting the unrepresentative labelled data. We then detail the assumption\nthat prompt learning is more effective because it empowers pre-trained language\nmodel that is built upon massive text corpora, as well as domain-related human\nknowledge to participate more in prediction and thereby reduces the impact of\nlimited label information provided by the small training set. We further\nhypothesize that language discrepancy can measure the quality of prompting.\nComprehensive experiments are performed to verify our assumptions. More\nremarkably, inspired by the theoretical framework, we propose an\nannotation-agnostic template selection method based on perplexity, which\nenables us to ``forecast'' the prompting performance in advance. This approach\nis especially encouraging because existing work still relies on development set\nto post-hoc evaluate templates. Experiments show that this method leads to\nsignificant prediction benefits compared to state-of-the-art zero-shot methods.",
    "descriptor": "",
    "authors": [
      "Jinghui Lu",
      "Rui Zhao",
      "Brian Mac Namee",
      "Dongsheng Zhu",
      "Weidong Han",
      "Fei Tan"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15206"
  },
  {
    "id": "arXiv:2209.15208",
    "title": "Scale-invariant Bayesian Neural Networks with Connectivity Tangent  Kernel",
    "abstract": "Explaining generalizations and preventing over-confident predictions are\ncentral goals of studies on the loss landscape of neural networks. Flatness,\ndefined as loss invariability on perturbations of a pre-trained solution, is\nwidely accepted as a predictor of generalization in this context. However, the\nproblem that flatness and generalization bounds can be changed arbitrarily\naccording to the scale of a parameter was pointed out, and previous studies\npartially solved the problem with restrictions: Counter-intuitively, their\ngeneralization bounds were still variant for the function-preserving parameter\nscaling transformation or limited only to an impractical network structure. As\na more fundamental solution, we propose new prior and posterior distributions\ninvariant to scaling transformations by \\textit{decomposing} the scale and\nconnectivity of parameters, thereby allowing the resulting generalization bound\nto describe the generalizability of a broad class of networks with the more\npractical class of transformations such as weight decay with batch\nnormalization. We also show that the above issue adversely affects the\nuncertainty calibration of Laplace approximation and propose a solution using\nour invariant posterior. We empirically demonstrate our posterior provides\neffective flatness and calibration measures with low complexity in such a\npractical parameter transformation case, supporting its practical effectiveness\nin line with our rationale.",
    "descriptor": "",
    "authors": [
      "SungYub Kim",
      "Sihwan Park",
      "Kyungsu Kim",
      "Eunho Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15208"
  },
  {
    "id": "arXiv:2209.15210",
    "title": "Multi-Prompt Alignment for Multi-source Unsupervised Domain Adaptation",
    "abstract": "Most existing methods for multi-source unsupervised domain adaptation (UDA)\nrely on a common feature encoder to extract domain-invariant features. However,\nlearning such an encoder involves updating the parameters of the entire\nnetwork, which makes the optimization computationally expensive, particularly\nwhen coupled with min-max objectives. Inspired by recent advances in prompt\nlearning that adapts high-capacity deep models for downstream tasks in a\ncomputationally economic way, we introduce Multi-Prompt Alignment (MPA), a\nsimple yet efficient two-stage framework for multi-source UDA. Given a source\nand target domain pair, MPA first trains an individual prompt to minimize the\ndomain gap through a contrastive loss, while tuning only a small set of\nparameters. Then, MPA derives a low-dimensional latent space through an\nauto-encoding process that maximizes the agreement of multiple learned prompts.\nThe resulting embedding further facilitates generalization to unseen domains.\nExtensive experiments show that our method achieves state-of-the-art results on\npopular benchmark datasets while requiring substantially fewer tunable\nparameters. To the best of our knowledge, we are the first to apply prompt\nlearning to the multi-source UDA problem and our method achieves the highest\nreported average accuracy of 54.1% on DomainNet, the most challenging UDA\ndataset to date, with only 15.9M parameters trained. More importantly, we\ndemonstrate that the learned embedding space can be easily adapted to novel\nunseen domains.",
    "descriptor": "",
    "authors": [
      "Haoran Chen",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15210"
  },
  {
    "id": "arXiv:2209.15211",
    "title": "Dual Progressive Transformations for Weakly Supervised Semantic  Segmentation",
    "abstract": "Weakly supervised semantic segmentation (WSSS), which aims to mine the object\nregions by merely using class-level labels, is a challenging task in computer\nvision. The current state-of-the-art CNN-based methods usually adopt\nClass-Activation-Maps (CAMs) to highlight the potential areas of the object,\nhowever, they may suffer from the part-activated issues. To this end, we try an\nearly attempt to explore the global feature attention mechanism of vision\ntransformer in WSSS task. However, since the transformer lacks the inductive\nbias as in CNN models, it can not boost the performance directly and may yield\nthe over-activated problems. To tackle these drawbacks, we propose a\nConvolutional Neural Networks Refined Transformer (CRT) to mine a globally\ncomplete and locally accurate class activation maps in this paper. To validate\nthe effectiveness of our proposed method, extensive experiments are conducted\non PASCAL VOC 2012 and CUB-200-2011 datasets. Experimental evaluations show\nthat our proposed CRT achieves the new state-of-the-art performance on both the\nweakly supervised semantic segmentation task the weakly supervised object\nlocalization task, which outperform others by a large margin.",
    "descriptor": "",
    "authors": [
      "Dongjian Huo",
      "Yukun Su",
      "Qingyao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15211"
  },
  {
    "id": "arXiv:2209.15214",
    "title": "Construction and Applications of Open Business Knowledge Graph",
    "abstract": "Business Knowledge Graph is important to many enterprises today, providing\nthe factual knowledge and structured data that steer many products and make\nthem more intelligent. Despite the welcome outcome, building business KG brings\nprohibitive issues of deficient structure, multiple modalities and unmanageable\nquality. In this paper, we advance the practical challenges related to building\nKG in non-trivial real-world systems. We introduce the process of building an\nopen business knowledge graph (OpenBG) derived from a well-known enterprise.\nSpecifically, we define a core ontology to cover various abstract products and\nconsumption demands, with fine-grained taxonomy and multi-modal facts in\ndeployed applications. OpenBG is ongoing, and the current version contains more\nthan 2.6 billion triples with more than 88 million entities and 2,681 types of\nrelations. We release all the open resources (OpenBG benchmark) derived from it\nfor the community. We also report benchmark results with best learned lessons\n\\url{https://github.com/OpenBGBenchmark/OpenBG}.",
    "descriptor": "\nComments: Work in Progress\n",
    "authors": [
      "Shumin Deng",
      "Hui Chen",
      "Zhoubo Li",
      "Feiyu Xiong",
      "Qiang Chen",
      "Mosha Chen",
      "Xiangwen Liu",
      "Jiaoyan Chen",
      "Jeff Z. Pan",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15214"
  },
  {
    "id": "arXiv:2209.15215",
    "title": "INT: Towards Infinite-frames 3D Detection with An Efficient Framework",
    "abstract": "It is natural to construct a multi-frame instead of a single-frame 3D\ndetector for a continuous-time stream. Although increasing the number of frames\nmight improve performance, previous multi-frame studies only used very limited\nframes to build their systems due to the dramatically increased computational\nand memory cost. To address these issues, we propose a novel on-stream training\nand prediction framework that, in theory, can employ an infinite number of\nframes while keeping the same amount of computation as a single-frame detector.\nThis infinite framework (INT), which can be used with most existing detectors,\nis utilized, for example, on the popular CenterPoint, with significant latency\nreductions and performance improvements. We've also conducted extensive\nexperiments on two large-scale datasets, nuScenes and Waymo Open Dataset, to\ndemonstrate the scheme's effectiveness and efficiency. By employing INT on\nCenterPoint, we can get around 7% (Waymo) and 15% (nuScenes) performance boost\nwith only 2~4ms latency overhead, and currently SOTA on the Waymo 3D Detection\nleaderboard.",
    "descriptor": "\nComments: accepted by ECCV2022\n",
    "authors": [
      "Jianyun Xu",
      "Zhenwei Miao",
      "Da Zhang",
      "Hongyu Pan",
      "Kaixuan Liu",
      "Peihan Hao",
      "Jun Zhu",
      "Zhengyang Sun",
      "Hongmin Li",
      "Xin Zhan"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15215"
  },
  {
    "id": "arXiv:2209.15216",
    "title": "The Role of Time Delay in Sim2real Transfer of Reinforcement Learning  for Cyber-Physical Systems",
    "abstract": "This paper analyzes the simulation to reality gap in reinforcement learning\n(RL) cyber-physical systems with fractional delays (i.e. delays that are\nnon-integer multiple of the sampling period). The consideration of fractional\ndelay has important implications on the nature of the cyber-physical system\nconsidered. Systems with delays are non-Markovian, and the system state vector\nneeds to be extended to make the system Markovian. We show that this is not\npossible when the delay is in the output, and the problem would always be\nnon-Markovian. Based on this analysis, a sampling scheme is proposed that\nresults in efficient RL training and agents that perform well in realistic\nmultirotor unmanned aerial vehicle simulations. We demonstrate that the\nresultant agents do not produce excessive oscillations, which is not the case\nwith RL agents that do not consider time delay in the model.",
    "descriptor": "\nComments: 6 pages,4 figures, Submitted to ICRA2023\n",
    "authors": [
      "Mohamad Chehadeh",
      "Igor Boiko",
      "Yahya Zweiri"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15216"
  },
  {
    "id": "arXiv:2209.15217",
    "title": "GM-VAE: Representation Learning with VAE on Gaussian Manifold",
    "abstract": "We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent\nspace consists of a set of diagonal Gaussian distributions. It is known that\nthe set of the diagonal Gaussian distributions with the Fisher information\nmetric forms a product hyperbolic space, which we call a Gaussian manifold. To\nlearn the VAE endowed with the Gaussian manifold, we first propose a pseudo\nGaussian manifold normal distribution based on the Kullback-Leibler divergence,\na local approximation of the squared Fisher-Rao distance, to define a density\nover the latent space. With the newly proposed distribution, we introduce\ngeometric transformations at the last and the first of the encoder and the\ndecoder of VAE, respectively to help the transition between the Euclidean and\nGaussian manifolds. Through the empirical experiments, we show competitive\ngeneralization performance of GM-VAE against other variants of hyperbolic- and\nEuclidean-VAEs. Our model achieves strong numerical stability, which is a\ncommon limitation reported with previous hyperbolic-VAEs.",
    "descriptor": "\nComments: 17 pages, 7 figures\n",
    "authors": [
      "Seunghyuk Cho",
      "Juyong Lee",
      "Dongwoo Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15217"
  },
  {
    "id": "arXiv:2209.15219",
    "title": "Optimal Query Complexities for Dynamic Trace Estimation",
    "abstract": "We consider the problem of minimizing the number of matrix-vector queries\nneeded for accurate trace estimation in the dynamic setting where our\nunderlying matrix is changing slowly, such as during an optimization process.\nSpecifically, for any $m$ matrices $A_1,...,A_m$ with consecutive differences\nbounded in Schatten-$1$ norm by $\\alpha$, we provide a novel binary tree\nsummation procedure that simultaneously estimates all $m$ traces up to\n$\\epsilon$ error with $\\delta$ failure probability with an optimal query\ncomplexity of $\\widetilde{O}\\left(m \\alpha\\sqrt{\\log(1/\\delta)}/\\epsilon +\nm\\log(1/\\delta)\\right)$, improving the dependence on both $\\alpha$ and $\\delta$\nfrom Dharangutte and Musco (NeurIPS, 2021). Our procedure works without\nadditional norm bounds on $A_i$ and can be generalized to a bound for the\n$p$-th Schatten norm for $p \\in [1,2]$, giving a complexity of\n$\\widetilde{O}\\left(m \\alpha\\left(\\sqrt{\\log(1/\\delta)}/\\epsilon\\right)^p +m\n\\log(1/\\delta)\\right)$.\nBy using novel reductions to communication complexity and\ninformation-theoretic analyses of Gaussian matrices, we provide matching lower\nbounds for static and dynamic trace estimation in all relevant parameters,\nincluding the failure probability. Our lower bounds (1) give the first tight\nbounds for Hutchinson's estimator in the matrix-vector product model with\nFrobenius norm error even in the static setting, and (2) are the first\nunconditional lower bounds for dynamic trace estimation, resolving open\nquestions of prior work.",
    "descriptor": "\nComments: 30 pages\n",
    "authors": [
      "David P. Woodruff",
      "Fred Zhang",
      "Qiuyi Zhang"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15219"
  },
  {
    "id": "arXiv:2209.15220",
    "title": "Assortment Optimization Under the Multivariate MNL Model",
    "abstract": "We study an assortment optimization problem under a multi-purchase choice\nmodel in which customers choose a bundle of up to one product from each of two\nproduct categories. Different bundles have different utilities and the bundle\nprice is the summation of the prices of products in it. For the uncapacitated\nsetting where any set of products can be offered, we prove that this problem is\nstrongly NP-hard. We show that an adjusted-revenue-ordered assortment provides\na 1/2-approximation. Furthermore, we develop an approximation framework based\non a linear programming relaxation of the problem and obtain a\n0.74-approximation algorithm. This approximation ratio almost matches the\nintegrality gap of the linear program, which is proven to be at most 0.75. For\nthe capacitated setting, we prove that there does not exist a constant-factor\napproximation algorithm assuming the Exponential Time Hypothesis. The same\nhardness result holds for settings with general bundle prices or more than two\ncategories. Finally, we conduct numerical experiments on randomly generated\nproblem instances. The average approximation ratios of our algorithms are over\n99%.",
    "descriptor": "",
    "authors": [
      "Xin Chen",
      "Jiachun Li",
      "Menglong Li",
      "Tiancheng Zhao",
      "Yuan Zhou"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.15220"
  },
  {
    "id": "arXiv:2209.15230",
    "title": "The Replicator Dynamic, Chain Components and the Response Graph",
    "abstract": "In this paper we examine the relationship between the flow of the replicator\ndynamic, the continuum limit of Multiplicative Weights Update, and a game's\nresponse graph. We settle an open problem establishing that under the\nreplicator, sink chain components -- a topological notion of long-run outcome\nof a dynamical system -- always exist and are approximated by the sink\nconnected components of the game's response graph. More specifically, each sink\nchain component contains a sink connected component of the response graph, as\nwell as all mixed strategy profiles whose support consists of pure profiles in\nthe same connected component, a set we call the content of the connected\ncomponent. As a corollary, all profiles are chain recurrent in games with\nstrongly connected response graphs. In any two-player game sharing a response\ngraph with a zero-sum game, the sink chain component is unique. In two-player\nzero-sum and potential games the sink chain components and sink connected\ncomponents are in a one-to-one correspondence, and we conjecture that this\nholds in all games.",
    "descriptor": "\nComments: 24 pages, 2 figures\n",
    "authors": [
      "Oliver Biggar",
      "Iman Shames"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15230"
  },
  {
    "id": "arXiv:2209.15236",
    "title": "Language-Family Adapters for Multilingual Neural Machine Translation",
    "abstract": "Massively multilingual models pretrained on abundant corpora with\nself-supervision achieve state-of-the-art results in a wide range of natural\nlanguage processing tasks. In machine translation, multilingual pretrained\nmodels are often fine-tuned on parallel data from one or multiple language\npairs. Multilingual fine-tuning improves performance on medium- and\nlow-resource languages but requires modifying the entire model and can be\nprohibitively expensive. Training a new set of adapters on each language pair\nor training a single set of adapters on all language pairs while keeping the\npretrained model's parameters frozen has been proposed as a parameter-efficient\nalternative. However, the former do not permit any sharing between languages,\nwhile the latter share parameters for all languages and have to deal with\nnegative interference. In this paper, we propose training language-family\nadapters on top of a pretrained multilingual model to facilitate cross-lingual\ntransfer. Our model consistently outperforms other adapter-based approaches. We\nalso demonstrate that language-family adapters provide an effective method to\ntranslate to languages unseen during pretraining.",
    "descriptor": "",
    "authors": [
      "Alexandra Chronopoulou",
      "Dario Stojanovski",
      "Alexander Fraser"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15236"
  },
  {
    "id": "arXiv:2209.15238",
    "title": "Efficient Graph based Recommender System with Weighted Averaging of  Messages",
    "abstract": "We showcase a novel solution to a recommendation system problem where we face\na perpetual soft item cold start issue. Our system aims to recommend demanded\nproducts to prospective sellers for listing in Amazon stores. These products\nalways have only few interactions thereby giving rise to a perpetual soft item\ncold start situation. Modern collaborative filtering methods solve cold start\nusing content attributes and exploit the existing implicit signals from warm\nstart items. This approach fails in our use-case since our entire item set\nfaces cold start issue always. Our Product Graph has over 500 Million nodes and\nover 5 Billion edges which makes training and inference using modern graph\nalgorithms very compute intensive. To overcome these challenges we propose a\nsystem which reduces the dataset size and employs an improved modelling\ntechnique to reduce storage and compute without loss in performance.\nParticularly, we reduce our graph size using a filtering technique and then\nexploit this reduced product graph using Weighted Averaging of Messages over\nLayers (WAML) algorithm. WAML simplifies training on large graphs and improves\nover previous methods by reducing compute time to 1/7 of LightGCN and 1/26 of\nGraph Attention Network (GAT) and increasing recall$@100$ by 66% over LightGCN\nand 2.3x over GAT.",
    "descriptor": "\nComments: Accepted to The Second International Conference on AI-ML Systems (AIMLSystems, October 12-15, 2022)\n",
    "authors": [
      "Faizan Ahemad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15238"
  },
  {
    "id": "arXiv:2209.15239",
    "title": "Smart Meters Integration in Distribution System State Estimation with  Collaborative Filtering and Deep Gaussian Process",
    "abstract": "The problem of state estimations for electric distribution system is\nconsidered. A collaborative filtering approach is proposed in this paper to\nintegrate the slow time-scale smart meter measurements in the distribution\nsystem state estimation, in which the deep Gaussian process is incorporated to\ninfer the fast time-scale pseudo measurements and avoid anomalies. Numerical\ntests have demonstrated the higher estimation accuracy of the proposed method.",
    "descriptor": "",
    "authors": [
      "Yifei Xu",
      "Ye Guo",
      "Wenjun Tang",
      "Hongbin Sun",
      "Shiming Li",
      "Yue Dai"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15239"
  },
  {
    "id": "arXiv:2209.15240",
    "title": "Prompt Tuning for Graph Neural Networks",
    "abstract": "In recent years, prompt tuning has set off a research boom in the adaptation\nof pre-trained models. In this paper, we propose Graph Prompt as an efficient\nand effective alternative to full fine-tuning for adapting the pre-trianed GNN\nmodels to downstream tasks. To the best of our knowledge, we are the first to\nexplore the effectiveness of prompt tuning on existing pre-trained GNN models.\nSpecifically, without tuning the parameters of the pre-trained GNN model, we\ntrain a task-specific graph prompt that provides graph-level transformations on\nthe downstream graphs during the adaptation stage. Then, we introduce a\nconcrete implementation of the graph prompt, called GP-Feature (GPF), which\nadds learnable perturbations to the feature space of the downstream graph. GPF\nhas a strong expressive ability that it can modify both the node features and\nthe graph structure implicitly. Accordingly, we demonstrate that GPF can\nachieve the approximately equivalent effect of any graph-level transformations\nunder most existing pre-trained GNN models. We validate the effectiveness of\nGPF on numerous pre-trained GNN models, and the experimental results show that\nwith a small amount (about 0.1% of that for fine-tuning ) of tunable\nparameters, GPF can achieve comparable performances as fine-tuning, and even\nobtain significant performance gains in some cases.",
    "descriptor": "",
    "authors": [
      "Taoran Fang",
      "Yunchao Zhang",
      "Yang Yang",
      "Chunping Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15240"
  },
  {
    "id": "arXiv:2209.15242",
    "title": "Review of Electric Vehicle Charging Technologies, Configurations, and  Architectures",
    "abstract": "Electric Vehicles (EVs) are projected to be one of the major contributors to\nenergy transition in the global transportation due to their rapid expansion.\nThe EVs will play a vital role in achieving a sustainable transportation system\nby reducing fossil fuel dependency and greenhouse gas (GHG) emissions. However,\nhigh level of EVs integration into the distribution grid has introduced many\nchallenges for the power grid operation, safety, and network planning due to\nthe increase in load demand, power quality impacts and power losses. An\nincreasing fleet of electric mobility requires the advanced charging systems to\nenhance charging efficiency and utility grid support. Innovative EV charging\ntechnologies are obtaining much attention in recent research studies aimed at\nstrengthening EV adoption while providing ancillary services. Therefore,\nanalysis of the status of EV charging technologies is significant to accelerate\nEV adoption with advanced control strategies to discover a remedial solution\nfor negative grid impacts, enhance desired charging efficiency and grid\nsupport. This paper presents a comprehensive review of the current deployment\nof EV charging systems, international standards, charging configurations, EV\nbattery technologies, architecture of EV charging stations, and emerging\ntechnical challenges. The charging systems require a dedicated converter\ntopology, a control strategy and international standards for charging and grid\ninterconnection to ensure optimum operation and enhance grid support. An\noverview of different charging systems in terms of onboard and off-board\nchargers, AC-DC and DC-DC converter topologies, and AC and DC-based charging\nstation architectures are evaluated.",
    "descriptor": "",
    "authors": [
      "Sithara S. G. Acharige",
      "Md Enamul Haque",
      "Mohammad Taufiqul Arif",
      "Nasser Hosseinzadeh"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15242"
  },
  {
    "id": "arXiv:2209.15245",
    "title": "Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated  Learning via Class-Imbalance Reduction",
    "abstract": "Due to limited communication capacities of edge devices, most existing\nfederated learning (FL) methods randomly select only a subset of devices to\nparticipate in training for each communication round. Compared with engaging\nall the available clients, the random-selection mechanism can lead to\nsignificant performance degradation on non-IID (independent and identically\ndistributed) data. In this paper, we show our key observation that the\nessential reason resulting in such performance degradation is the\nclass-imbalance of the grouped data from randomly selected clients. Based on\nour key observation, we design an efficient heterogeneity-aware client sampling\nmechanism, i.e., Federated Class-balanced Sampling (Fed-CBS), which can\neffectively reduce class-imbalance of the group dataset from the intentionally\nselected clients. In particular, we propose a measure of class-imbalance and\nthen employ homomorphic encryption to derive this measure in a\nprivacy-preserving way. Based on this measure, we also design a\ncomputation-efficient client sampling strategy, such that the actively selected\nclients will generate a more class-balanced grouped dataset with theoretical\nguarantees. Extensive experimental results demonstrate Fed-CBS outperforms the\nstatus quo approaches. Furthermore, it achieves comparable or even better\nperformance than the ideal setting where all the available clients participate\nin the FL training.",
    "descriptor": "",
    "authors": [
      "Jianyi Zhang",
      "Ang Li",
      "Minxue Tang",
      "Jingwei Sun",
      "Xiang Chen",
      "Fan Zhang",
      "Changyou Chen",
      "Yiran Chen",
      "Hai Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15245"
  },
  {
    "id": "arXiv:2209.15246",
    "title": "Your Out-of-Distribution Detection Method is Not Robust!",
    "abstract": "Out-of-distribution (OOD) detection has recently gained substantial attention\ndue to the importance of identifying out-of-domain samples in reliability and\nsafety. Although OOD detection methods have advanced by a great deal, they are\nstill susceptible to adversarial examples, which is a violation of their\npurpose. To mitigate this issue, several defenses have recently been proposed.\nNevertheless, these efforts remained ineffective, as their evaluations are\nbased on either small perturbation sizes, or weak attacks. In this work, we\nre-examine these defenses against an end-to-end PGD attack on in/out data with\nlarger perturbation sizes, e.g. up to commonly used $\\epsilon=8/255$ for the\nCIFAR-10 dataset. Surprisingly, almost all of these defenses perform worse than\na random detection under the adversarial setting. Next, we aim to provide a\nrobust OOD detection method. In an ideal defense, the training should expose\nthe model to almost all possible adversarial perturbations, which can be\nachieved through adversarial training. That is, such training perturbations\nshould based on both in- and out-of-distribution samples. Therefore, unlike OOD\ndetection in the standard setting, access to OOD, as well as in-distribution,\nsamples sounds necessary in the adversarial training setup. These tips lead us\nto adopt generative OOD detection methods, such as OpenGAN, as a baseline. We\nsubsequently propose the Adversarially Trained Discriminator (ATD), which\nutilizes a pre-trained robust model to extract robust features, and a generator\nmodel to create OOD samples. Using ATD with CIFAR-10 and CIFAR-100 as the\nin-distribution data, we could significantly outperform all previous methods in\nthe robust AUROC while maintaining high standard AUROC and classification\naccuracy. The code repository is available at https://github.com/rohban-lab/ATD .",
    "descriptor": "\nComments: Accepted to NeurIPS 2022\n",
    "authors": [
      "Mohammad Azizmalayeri",
      "Arshia Soltani Moakhar",
      "Arman Zarei",
      "Reihaneh Zohrabi",
      "Mohammad Taghi Manzuri",
      "Mohammad Hossein Rohban"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15246"
  },
  {
    "id": "arXiv:2209.15248",
    "title": "Hyperspectral and LiDAR data for the prediction via machine learning of  tree species, volume and biomass: a possible contribution for updating forest  management plans",
    "abstract": "This work intends to lay the foundations for identifying the prevailing\nforest types and the delineation of forest units within private forest\ninventories in the Autonomous Province of Trento (PAT), using currently\navailable remote sensing solutions. In particular, data from LiDAR and\nhyperspectral surveys of 2014 made available by PAT were acquired and\nprocessed. Such studies are very important in the context of forest management\nscenarios. The method includes defining tree species ground-truth by outlining\nsingle tree crowns with polygons and labeling them. Successively two supervised\nmachine learning classifiers, K-Nearest Neighborhood and Support Vector Machine\n(SVM) were used. The results show that, by setting specific hyperparameters,\nthe SVM methodology gave the best results in classification of tree species.\nBiomass was estimated using canopy parameters and the Jucker equation for the\nabove ground biomass (AGB) and that of Scrinzi for the tariff volume. Predicted\nvalues were compared with 11 field plots of fixed radius where volume and\nbiomass were field-estimated in 2017. Results show significant coefficients of\ncorrelation: 0.94 for stem volume and 0.90 for total aboveground tree biomass.",
    "descriptor": "",
    "authors": [
      "Daniele Michelini",
      "Michele Dalponte",
      "Angelo Carriero",
      "Erico Kutchart",
      "Salvatore Eugenio Pappalardo",
      "Massimo De Marchi",
      "Francesco Pirotti"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15248"
  },
  {
    "id": "arXiv:2209.15249",
    "title": "Experts in the Loop: Conditional Variable Selection for Accelerating  Post-Silicon Analysis Based on Deep Learning",
    "abstract": "Post-silicon validation is one of the most critical processes in modern\nsemiconductor manufacturing. Specifically, correct and deep understanding in\ntest cases of manufactured devices is key to enable post-silicon tuning and\ndebugging. This analysis is typically performed by experienced human experts.\nHowever, with the fast development in semiconductor industry, test cases can\ncontain hundreds of variables. The resulting high-dimensionality poses enormous\nchallenges to experts. Thereby, some recent prior works have introduced\ndata-driven variable selection algorithms to tackle these problems and achieved\nnotable success. Nevertheless, for these methods, experts are not involved in\ntraining and inference phases, which may lead to bias and inaccuracy due to the\nlack of prior knowledge. Hence, this work for the first time aims to design a\nnovel conditional variable selection approach while keeping experts in the\nloop. In this way, we expect that our algorithm can be more efficiently and\neffectively trained to identify the most critical variables under certain\nexpert knowledge. Extensive experiments on both synthetic and real-world\ndatasets from industry have been conducted and shown the effectiveness of our\nmethod.",
    "descriptor": "",
    "authors": [
      "Yiwen Liao",
      "Rapha\u00ebl Latty",
      "Bin Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15249"
  },
  {
    "id": "arXiv:2209.15251",
    "title": "Traffic Sign Classification Using Deep and Quantum Neural Networks",
    "abstract": "Quantum Neural Networks (QNNs) are an emerging technology that can be used in\nmany applications including computer vision. In this paper, we presented a\ntraffic sign classification system implemented using a hybrid quantum-classical\nconvolutional neural network. Experiments on the German Traffic Sign\nRecognition Benchmark dataset indicate that currently QNN do not outperform\nclassical DCNN (Deep Convolutuional Neural Networks), yet still provide an\naccuracy of over 90% and are a definitely promising solution for advanced\ncomputer vision.",
    "descriptor": "\nComments: Accepted for the ICCVG 2022 conference\n",
    "authors": [
      "Sylwia Kuros",
      "Tomasz Kryjak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15251"
  },
  {
    "id": "arXiv:2209.15252",
    "title": "PointPillars Backbone Type Selection For Fast and Accurate LiDAR Object  Detection",
    "abstract": "3D object detection from LiDAR sensor data is an important topic in the\ncontext of autonomous cars and drones. In this paper, we present the results of\nexperiments on the impact of backbone selection of a deep convolutional neural\nnetwork on detection accuracy and computation speed. We chose the PointPillars\nnetwork, which is characterised by a simple architecture, high speed, and\nmodularity that allows for easy expansion. During the experiments, we paid\nparticular attention to the change in detection efficiency (measured by the mAP\nmetric) and the total number of multiply-addition operations needed to process\none point cloud. We tested 10 different convolutional neural network\narchitectures that are widely used in image-based detection problems. For a\nbackbone like MobilenetV1, we obtained an almost 4x speedup at the cost of a\n1.13% decrease in mAP. On the other hand, for CSPDarknet we got an acceleration\nof more than 1.5x at an increase in mAP of 0.33%. We have thus demonstrated\nthat it is possible to significantly speed up a 3D object detector in LiDAR\npoint clouds with a small decrease in detection efficiency. This result can be\nused when PointPillars or similar algorithms are implemented in embedded\nsystems, including SoC FPGAs. The code is available at\nhttps://github.com/vision-agh/pointpillars\\_backbone.",
    "descriptor": "\nComments: Accepted for the ICCVG 2022 conference\n",
    "authors": [
      "Konrad Lis",
      "Tomasz Kryjak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.15252"
  },
  {
    "id": "arXiv:2209.15253",
    "title": "Cerberus: A Formal Approach to Secure and Efficient Enclave Memory  Sharing",
    "abstract": "Hardware enclaves rely on a disjoint memory model, which maps each physical\naddress to an enclave to achieve strong memory isolation. However, this\nseverely limits the performance and programmability of enclave programs. While\nsome prior work proposes enclave memory sharing, it does not provide a formal\nmodel or verification of their designs. This paper presents Cerberus, a formal\napproach to secure and efficient enclave memory sharing. To reduce the burden\nof formal verification, we compare different sharing models and choose a simple\nyet powerful sharing model. Based on the sharing model, Cerberus extends an\nenclave platform such that enclave memory can be made immutable and shareable\nacross multiple enclaves via additional operations. We use incremental\nverification starting with an existing formal model called the Trusted Abstract\nPlatform (TAP). Using our extended TAP model, we formally verify that Cerberus\ndoes not break or weaken the security guarantees of the enclaves despite\nallowing memory sharing. More specifically, we prove the Secure Remote\nExecution (SRE) property on our formal model. Finally, the paper shows the\nfeasibility of Cerberus by implementing it in an existing enclave platform,\nRISC-V Keystone.",
    "descriptor": "\nComments: Accepted by ACM CCS 2022\n",
    "authors": [
      "Dayeol Lee",
      "Kevin Cheang",
      "Alexander Thomas",
      "Catherine Lu",
      "Pranav Gaddamadugu",
      "Anjo Vahldiek-Oberwagner",
      "Mona Vij",
      "Dawn Song",
      "Sanjit A. Seshia",
      "Krste Asanovi\u0107"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.15253"
  },
  {
    "id": "arXiv:2209.15256",
    "title": "S2P: State-conditioned Image Synthesis for Data Augmentation in Offline  Reinforcement Learning",
    "abstract": "Offline reinforcement learning (Offline RL) suffers from the innate\ndistributional shift as it cannot interact with the physical environment during\ntraining. To alleviate such limitation, state-based offline RL leverages a\nlearned dynamics model from the logged experience and augments the predicted\nstate transition to extend the data distribution. For exploiting such benefit\nalso on the image-based RL, we firstly propose a generative model, S2P\n(State2Pixel), which synthesizes the raw pixel of the agent from its\ncorresponding state. It enables bridging the gap between the state and the\nimage domain in RL algorithms, and virtually exploring unseen image\ndistribution via model-based transition in the state space. Through\nexperiments, we confirm that our S2P-based image synthesis not only improves\nthe image-based offline RL performance but also shows powerful generalization\ncapability on unseen tasks.",
    "descriptor": "\nComments: NeurIPS 2022, first two authors contributed equally\n",
    "authors": [
      "Daesol Cho",
      "Dongseok Shim",
      "H. Jin Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15256"
  },
  {
    "id": "arXiv:2209.15257",
    "title": "Energy Efficient Hardware Acceleration of Neural Networks with  Power-of-Two Quantisation",
    "abstract": "Deep neural networks virtually dominate the domain of most modern vision\nsystems, providing high performance at a cost of increased computational\ncomplexity.Since for those systems it is often required to operate both in\nreal-time and with minimal energy consumption (e.g., for wearable devices or\nautonomous vehicles, edge Internet of Things (IoT), sensor networks), various\nnetwork optimisation techniques are used, e.g., quantisation, pruning, or\ndedicated lightweight architectures. Due to the logarithmic distribution of\nweights in neural network layers, a method providing high performance with\nsignificant reduction in computational precision (for 4-bit weights and less)\nis the Power-of-Two (PoT) quantisation (and therefore also with a logarithmic\ndistribution). This method introduces additional possibilities of replacing the\ntypical for neural networks Multiply and ACcumulate (MAC -- performing, e.g.,\nconvolution operations) units, with more energy-efficient Bitshift and\nACcumulate (BAC). In this paper, we show that a hardware neural network\naccelerator with PoT weights implemented on the Zynq UltraScale + MPSoC ZCU104\nSoC FPGA can be at least $1.4x$ more energy efficient than the uniform\nquantisation version. To further reduce the actual power requirement by\nomitting part of the computation for zero weights, we also propose a new\npruning method adapted to logarithmic quantisation.",
    "descriptor": "\nComments: Accepted for the ICCVG 2022 conference\n",
    "authors": [
      "Dominika Przewlocka-Rus",
      "Tomasz Kryjak"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.15257"
  },
  {
    "id": "arXiv:2209.15258",
    "title": "Transformers for Object Detection in Large Point Clouds",
    "abstract": "We present TransLPC, a novel detection model for large point clouds that is\nbased on a transformer architecture. While object detection with transformers\nhas been an active field of research, it has proved difficult to apply such\nmodels to point clouds that span a large area, e.g. those that are common in\nautonomous driving, with lidar or radar data. TransLPC is able to remedy these\nissues: The structure of the transformer model is modified to allow for larger\ninput sequence lengths, which are sufficient for large point clouds. Besides\nthis, we propose a novel query refinement technique to improve detection\naccuracy, while retaining a memory-friendly number of transformer decoder\nqueries. The queries are repositioned between layers, moving them closer to the\nbounding box they are estimating, in an efficient manner. This simple technique\nhas a significant effect on detection accuracy, which is evaluated on the\nchallenging nuScenes dataset on real-world lidar data. Besides this, the\nproposed method is compatible with existing transformer-based solutions that\nrequire object detection, e.g. for joint multi-object tracking and detection,\nand enables them to be used in conjunction with large point clouds.",
    "descriptor": "\nComments: Accepted for publication at the 2022 25th IEEE International Conference on Intelligent Transportation Systems (ITSC 2022), Sep 18- Oct 12, 2022, in Macau, China\n",
    "authors": [
      "Felicia Ruppel",
      "Florian Faion",
      "Claudius Gl\u00e4ser",
      "Klaus Dietmayer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15258"
  },
  {
    "id": "arXiv:2209.15259",
    "title": "SoK: On the Impossible Security of Very Large Foundation Models",
    "abstract": "Large machine learning models, or so-called foundation models, aim to serve\nas base-models for application-oriented machine learning. Although these models\nshowcase impressive performance, they have been empirically found to pose\nserious security and privacy issues. We may however wonder if this is a\nlimitation of the current models, or if these issues stem from a fundamental\nintrinsic impossibility of the foundation model learning problem itself. This\npaper aims to systematize our knowledge supporting the latter. More precisely,\nwe identify several key features of today's foundation model learning problem\nwhich, given the current understanding in adversarial machine learning, suggest\nincompatibility of high accuracy with both security and privacy. We begin by\nobserving that high accuracy seems to require (1) very high-dimensional models\nand (2) huge amounts of data that can only be procured through user-generated\ndatasets. Moreover, such data is fundamentally heterogeneous, as users\ngenerally have very specific (easily identifiable) data-generating habits. More\nimportantly, users' data is filled with highly sensitive information, and maybe\nheavily polluted by fake users. We then survey lower bounds on accuracy in\nprivacy-preserving and Byzantine-resilient heterogeneous learning that, we\nargue, constitute a compelling case against the possibility of designing a\nsecure and privacy-preserving high-accuracy foundation model. We further stress\nthat our analysis also applies to other high-stake machine learning\napplications, including content recommendation. We conclude by calling for\nmeasures to prioritize security and privacy, and to slow down the race for ever\nlarger models.",
    "descriptor": "\nComments: 13 pages\n",
    "authors": [
      "El-Mahdi El-Mhamdi",
      "Sadegh Farhadkhani",
      "Rachid Guerraoui",
      "Nirupam Gupta",
      "L\u00ea-Nguy\u00ean Hoang",
      "Rafael Pinot",
      "John Stephan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.15259"
  },
  {
    "id": "arXiv:2209.15260",
    "title": "A Multiple Criteria Decision Analysis based Approach to Remove  Uncertainty in SMP Models",
    "abstract": "Advanced AI technologies are serving humankind in a number of ways, from\nhealthcare to manufacturing. Advanced automated machines are quite expensive,\nbut the end output is supposed to be of the highest possible quality. Depending\non the agility of requirements, these automation technologies can change\ndramatically. The likelihood of making changes to automation software is\nextremely high, so it must be updated regularly. If maintainability is not\ntaken into account, it will have an impact on the entire system and increase\nmaintenance costs. Many companies use different programming paradigms in\ndeveloping advanced automated machines based on client requirements. Therefore,\nit is essential to estimate the maintainability of heterogeneous software. As a\nresult of the lack of widespread consensus on software maintainability\nprediction (SPM) methodologies, individuals and businesses are left perplexed\nwhen it comes to determining the appropriate model for estimating the\nmaintainability of software, which serves as the inspiration for this research.\nA structured methodology was designed, and the datasets were preprocessed and\nmaintainability index (MI) range was also found for all the datasets expect for\nUIMS and QUES, the metric CHANGE is used for UIMS and QUES. To remove the\nuncertainty among the aforementioned techniques, a popular multiple criteria\ndecision-making model, namely the technique for order preference by similarity\nto ideal solution (TOPSIS), is used in this work. TOPSIS revealed that GARF\noutperforms the other considered techniques in predicting the maintainability\nof heterogeneous automated software.",
    "descriptor": "\nComments: Submitted for peer review\n",
    "authors": [
      "Gokul Yenduri",
      "Thippa Reddy Gadekallu"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15260"
  },
  {
    "id": "arXiv:2209.15261",
    "title": "Minimalistic Unsupervised Learning with the Sparse Manifold Transform",
    "abstract": "We describe a minimalistic and interpretable method for unsupervised\nlearning, without resorting to data augmentation, hyperparameter tuning, or\nother engineering designs, that achieves performance close to the SOTA SSL\nmethods. Our approach leverages the sparse manifold transform, which unifies\nsparse coding, manifold learning, and slow feature analysis. With a one-layer\ndeterministic sparse manifold transform, one can achieve 99.3% KNN top-1\naccuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100.\nWith a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy\non CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap\nbetween simplistic ``white-box'' methods and the SOTA methods. Additionally, we\nprovide visualization to explain how an unsupervised representation transform\nis formed. The proposed method is closely connected to latent-embedding\nself-supervised methods and can be treated as the simplest form of VICReg.\nThough there remains a small performance gap between our simple constructive\nmodel and SOTA methods, the evidence points to this as a promising direction\nfor achieving a principled and white-box approach to unsupervised learning.",
    "descriptor": "",
    "authors": [
      "Yubei Chen",
      "Zeyu Yun",
      "Yi Ma",
      "Bruno Olshausen",
      "Yann LeCun"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15261"
  },
  {
    "id": "arXiv:2209.15264",
    "title": "Diffusion-based Image Translation using Disentangled Style and Content  Representation",
    "abstract": "Diffusion-based image translation guided by semantic texts or a single target\nimage has enabled flexible style transfer which is not limited to the specific\ndomains. Unfortunately, due to the stochastic nature of diffusion models, it is\noften difficult to maintain the original content of the image during the\nreverse diffusion. To address this, here we present a novel diffusion-based\nunsupervised image translation method using disentangled style and content\nrepresentation.\nSpecifically, inspired by the splicing Vision Transformer, we extract\nintermediate keys of multihead self attention layer from ViT model and used\nthem as the content preservation loss. Then, an image guided style transfer is\nperformed by matching the [CLS] classification token from the denoised samples\nand target image, whereas additional CLIP loss is used for the text-driven\nstyle transfer. To further accelerate the semantic change during the reverse\ndiffusion, we also propose a novel semantic divergence loss and resampling\nstrategy. Our experimental results show that the proposed method outperforms\nstate-of-the-art baseline models in both text-guided and image-guided\ntranslation tasks.",
    "descriptor": "",
    "authors": [
      "Gihyun Kwon",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15264"
  },
  {
    "id": "arXiv:2209.15265",
    "title": "ReLU Neural Networks Learn the Simplest Models: Neural Isometry and  Exact Recovery",
    "abstract": "The practice of deep learning has shown that neural networks generalize\nremarkably well even with an extreme number of learned parameters. This appears\nto contradict traditional statistical wisdom, in which a trade-off between\nmodel complexity and fit to the data is essential. We set out to resolve this\ndiscrepancy from a convex optimization and sparse recovery perspective. We\nconsider the training and generalization properties of two-layer ReLU networks\nwith standard weight decay regularization. Under certain regularity assumptions\non the data, we show that ReLU networks with an arbitrary number of parameters\nlearn only simple models that explain the data. This is analogous to the\nrecovery of the sparsest linear model in compressed sensing. For ReLU networks\nand their variants with skip connections or normalization layers, we present\nisometry conditions that ensure the exact recovery of planted neurons. For\nrandomly generated data, we show the existence of a phase transition in\nrecovering planted neural network models. The situation is simple: whenever the\nratio between the number of samples and the dimension exceeds a numerical\nthreshold, the recovery succeeds with high probability; otherwise, it fails\nwith high probability. Surprisingly, ReLU networks learn simple and sparse\nmodels even when the labels are noisy. The phase transition phenomenon is\nconfirmed through numerical experiments.",
    "descriptor": "",
    "authors": [
      "Yifei Wang",
      "Yixuan Hua",
      "Emmanuel Cand\u00e9s",
      "Mert Pilanci"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15265"
  },
  {
    "id": "arXiv:2209.15266",
    "title": "Data Poisoning Attacks Against Multimodal Encoders",
    "abstract": "Traditional machine learning (ML) models usually rely on large-scale labeled\ndatasets to achieve strong performance. However, such labeled datasets are\noften challenging and expensive to obtain. Also, the predefined categories\nlimit the model's ability to generalize to other visual concepts as additional\nlabeled data is required. On the contrary, the newly emerged multimodal model,\nwhich contains both visual and linguistic modalities, learns the concept of\nimages from the raw text. It is a promising way to solve the above problems as\nit can use easy-to-collect image-text pairs to construct the training dataset\nand the raw texts contain almost unlimited categories according to their\nsemantics. However, learning from a large-scale unlabeled dataset also exposes\nthe model to the risk of potential poisoning attacks, whereby the adversary\naims to perturb the model's training dataset to trigger malicious behaviors in\nit. Previous work mainly focuses on the visual modality. In this paper, we\ninstead focus on answering two questions: (1) Is the linguistic modality also\nvulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To\nanswer the two questions, we conduct three types of poisoning attacks against\nCLIP, the most representative multimodal contrastive learning framework.\nExtensive evaluations on different datasets and model architectures show that\nall three attacks can perform well on the linguistic modality with only a\nrelatively low poisoning rate and limited epochs. Also, we observe that the\npoisoning effect differs between different modalities, i.e., with lower MinRank\nin the visual modality and with higher Hit@K when K is small in the linguistic\nmodality. To mitigate the attacks, we propose both pre-training and\npost-training defenses. We empirically show that both defenses can\nsignificantly reduce the attack performance while preserving the model's\nutility.",
    "descriptor": "",
    "authors": [
      "Ziqing Yang",
      "Xinlei He",
      "Zheng Li",
      "Michael Backes",
      "Mathias Humbert",
      "Pascal Berrang",
      "Yang Zhang"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15266"
  },
  {
    "id": "arXiv:2209.15268",
    "title": "Generative Model Watermarking Based on Human Visual System",
    "abstract": "Intellectual property protection of deep neural networks is receiving\nattention from more and more researchers, and the latest research applies model\nwatermarking to generative models for image processing. However, the existing\nwatermarking methods designed for generative models do not take into account\nthe effects of different channels of sample images on watermarking. As a\nresult, the watermarking performance is still limited. To tackle this problem,\nin this paper, we first analyze the effects of embedding watermark information\non different channels. Then, based on the characteristics of human visual\nsystem (HVS), we introduce two HVS-based generative model watermarking methods,\nwhich are realized in RGB color space and YUV color space respectively. In RGB\ncolor space, the watermark is embedded into the R and B channels based on the\nfact that HVS is more sensitive to G channel. In YUV color space, the watermark\nis embedded into the DCT domain of U and V channels based on the fact that HVS\nis more sensitive to brightness changes. Experimental results demonstrate the\neffectiveness of the proposed work, which improves the fidelity of the model to\nbe protected and has good universality compared with previous methods.",
    "descriptor": "\nComments: this https URL&hl=en\n",
    "authors": [
      "Li Zhang",
      "Yong Liu",
      "Shaoteng Liu",
      "Tianshu Yang",
      "Yexin Wang",
      "Xinpeng Zhang",
      "Hanzhou Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)"
    ],
    "url": "https://arxiv.org/abs/2209.15268"
  },
  {
    "id": "arXiv:2209.15270",
    "title": "ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text  Pre-training",
    "abstract": "Recent Vision-Language Pre-trained (VLP) models based on dual encoder have\nattracted extensive attention from academia and industry due to their superior\nperformance on various cross-modal tasks and high computational efficiency.\nThey attempt to learn cross-modal representation using contrastive learning on\nimage-text pairs, however, the built inter-modal correlations only rely on a\nsingle view for each modality. Actually, an image or a text contains various\npotential views, just as humans could capture a real-world scene via diverse\ndescriptions or photos. In this paper, we propose ERNIE-ViL 2.0, a Multi-View\nContrastive learning framework to build intra-modal and inter-modal\ncorrelations between diverse views simultaneously, aiming at learning a more\nrobust cross-modal representation. Specifically, we construct multiple views\nwithin each modality to learn the intra-modal correlation for enhancing the\nsingle-modal representation. Besides the inherent visual/textual views, we\nconstruct sequences of object tags as a special textual view to narrow the\ncross-modal semantic gap on noisy image-text pairs. Pre-trained with 29M\npublicly available datasets, ERNIE-ViL 2.0 achieves competitive results on\nEnglish cross-modal retrieval. Additionally, to generalize our method to\nChinese cross-modal tasks, we train ERNIE-ViL 2.0 through scaling up the\npre-training datasets to 1.5B Chinese image-text pairs, resulting in\nsignificant improvements compared to previous SOTA results on Chinese\ncross-modal retrieval. We release our pre-trained models in\nhttps://github.com/PaddlePaddle/ERNIE.",
    "descriptor": "\nComments: 14 pages, 6 figures\n",
    "authors": [
      "Bin Shan",
      "Weichong Yin",
      "Yu Sun",
      "Hao Tian",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15270"
  },
  {
    "id": "arXiv:2209.15271",
    "title": "Application-Driven AI Paradigm for Human Action Recognition",
    "abstract": "Human action recognition in computer vision has been widely studied in recent\nyears. However, most algorithms consider only certain action specially with\neven high computational cost. That is not suitable for practical applications\nwith multiple actions to be identified with low computational cost. To meet\nvarious application scenarios, this paper presents a unified human action\nrecognition framework composed of two modules, i.e., multi-form human detection\nand corresponding action classification. Among them, an open-source dataset is\nconstructed to train a multi-form human detection model that distinguishes a\nhuman being's whole body, upper body or part body, and the followed action\nclassification model is adopted to recognize such action as falling, sleeping\nor on-duty, etc. Some experimental results show that the unified framework is\neffective for various application scenarios. It is expected to be a new\napplication-driven AI paradigm for human action recognition.",
    "descriptor": "",
    "authors": [
      "Zezhou Chen",
      "Yajie Cui",
      "Kaikai Zhao",
      "Zhaoxiang Liu",
      "Shiguo Lian"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15271"
  },
  {
    "id": "arXiv:2209.15274",
    "title": "Online Multi-Agent Decentralized Byzantine-robust Gradient Estimation",
    "abstract": "In this paper, we propose an iterative scheme for distributed\nByzantineresilient estimation of a gradient associated with a black-box model.\nOur algorithm is based on simultaneous perturbation, secure state estimation\nand two-timescale stochastic approximations. We also show the performance of\nour algorithm through numerical experiments.",
    "descriptor": "",
    "authors": [
      "Alexandre Reiffers-Masson",
      "Isabel Amigo"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15274"
  },
  {
    "id": "arXiv:2209.15275",
    "title": "A Multivariate Complexity Analysis of Qualitative Reasoning Problems",
    "abstract": "Qualitative reasoning is an important subfield of artificial intelligence\nwhere one describes relationships with qualitative, rather than numerical,\nrelations. Many such reasoning tasks, e.g., Allen's interval algebra, can be\nsolved in $2^{O(n \\cdot \\log n)}$ time, but single-exponential running times\n$2^{O(n)}$ are currently far out of reach. In this paper we consider\nsingle-exponential algorithms via a multivariate analysis consisting of a\nfine-grained parameter $n$ (e.g., the number of variables) and a coarse-grained\nparameter $k$ expected to be relatively small. We introduce the classes FPE and\nXE of problems solvable in $f(k) \\cdot 2^{O(n)}$, respectively $f(k)^n$, time,\nand prove several fundamental properties of these classes. We proceed by\nstudying temporal reasoning problems and (1) show that the Partially Ordered\nTime problem of effective width $k$ is solvable in $16^{kn}$ time and is thus\nincluded in XE, and (2) that the network consistency problem for Allen's\ninterval algebra with no interval overlapping with more than $k$ others is\nsolvable in $(2nk)^{2k} \\cdot 2^{n}$ time and is included in FPE. Our\nmultivariate approach is in no way limited to these to specific problems and\nmay be a generally useful approach for obtaining single-exponential algorithms.",
    "descriptor": "",
    "authors": [
      "Leif Eriksson",
      "Victor Lagerkvist"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15275"
  },
  {
    "id": "arXiv:2209.15276",
    "title": "Machine Unlearning Method Based On Projection Residual",
    "abstract": "Machine learning models (mainly neural networks) are used more and more in\nreal life. Users feed their data to the model for training. But these processes\nare often one-way. Once trained, the model remembers the data. Even when data\nis removed from the dataset, the effects of these data persist in the model.\nWith more and more laws and regulations around the world protecting data\nprivacy, it becomes even more important to make models forget this data\ncompletely through machine unlearning.\nThis paper adopts the projection residual method based on Newton iteration\nmethod. The main purpose is to implement machine unlearning tasks in the\ncontext of linear regression models and neural network models. This method\nmainly uses the iterative weighting method to completely forget the data and\nits corresponding influence, and its computational cost is linear in the\nfeature dimension of the data. This method can improve the current machine\nlearning method. At the same time, it is independent of the size of the\ntraining set. Results were evaluated by feature injection testing (FIT).\nExperiments show that this method is more thorough in deleting data, which is\nclose to model retraining.",
    "descriptor": "\nComments: This paper is accepted by DSAA2022. The 9th IEEE International Conference on Data Science and Advanced Analytics\n",
    "authors": [
      "Zihao Cao",
      "Jianzong Wang",
      "Shijing Si",
      "Zhangcheng Huang",
      "Jing Xiao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15276"
  },
  {
    "id": "arXiv:2209.15278",
    "title": "Rethinking skip connection model as a learnable Markov chain",
    "abstract": "Over past few years afterward the birth of ResNet, skip connection has become\nthe defacto standard for the design of modern architectures due to its\nwidespread adoption, easy optimization and proven performance. Prior work has\nexplained the effectiveness of the skip connection mechanism from different\nperspectives. In this work, we deep dive into the model's behaviors with skip\nconnections which can be formulated as a learnable Markov chain. An efficient\nMarkov chain is preferred as it always maps the input data to the target domain\nin a better way. However, while a model is explained as a Markov chain, it is\nnot guaranteed to be optimized following an efficient Markov chain by existing\nSGD-based optimizers which are prone to get trapped in local optimal points. In\norder to towards a more efficient Markov chain, we propose a simple routine of\npenal connection to make any residual-like model become a learnable Markov\nchain. Aside from that, the penal connection can also be viewed as a particular\nmodel regularization and can be easily implemented with one line of code in the\nmost popular deep learning frameworks~\\footnote{Source code:\n\\url{https://github.com/densechen/penal-connection}}. The encouraging\nexperimental results in multi-modal translation and image recognition\nempirically confirm our conjecture of the learnable Markov chain view and\ndemonstrate the superiority of the proposed penal connection.",
    "descriptor": "\nComments: 12 pages, 4 figures\n",
    "authors": [
      "Dengsheng Chen",
      "Jie Hu",
      "Wenwen Qiang",
      "Xiaoming Wei",
      "Enhua Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2209.15278"
  },
  {
    "id": "arXiv:2209.15279",
    "title": "Combining Theory of Mind and Abduction for Cooperation under Imperfect  Information",
    "abstract": "In this paper, we formalise and implement an agent model for cooperation\nunder imperfect information. It is based on Theory of Mind (the cognitive\nability to understand the mental state of others) and abductive reasoning (the\ninference paradigm that computes explanations from observations). The\ncombination of these two techniques allows agents to derive the motives behind\nthe actions of their peers, and incorporate this knowledge into their own\ndecision-making. We have implemented this model in a totally domain-independent\nfashion and successfully tested it for the cooperative card game Hanabi.",
    "descriptor": "",
    "authors": [
      "Nieves Montes",
      "Nardine Osman",
      "Carles Sierra"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2209.15279"
  },
  {
    "id": "arXiv:2209.15280",
    "title": "Learning Transferable Spatiotemporal Representations from Natural Script  Knowledge",
    "abstract": "Pre-training on large-scale video data has become a common recipe for\nlearning transferable spatiotemporal representations in recent years. Despite\nsome progress, existing methods are mostly limited to highly curated datasets\n(e.g., K400) and exhibit unsatisfactory out-of-the-box representations. We\nargue that it is due to the fact that they only capture pixel-level knowledge\nrather than spatiotemporal commonsense, which is far away from cognition-level\nvideo understanding. Inspired by the great success of image-text pre-training\n(e.g., CLIP), we take the first step to exploit language semantics to boost\ntransferable spatiotemporal representation learning. We introduce a new pretext\ntask, Turning to Video for Transcript Sorting (TVTS), which sorts shuffled ASR\nscripts by attending to learned video representations. We do not rely on\ndescriptive captions and learn purely from video, i.e., leveraging the natural\ntranscribed speech knowledge to provide noisy but useful semantics over time.\nFurthermore, rather than the simple concept learning in vision-caption\ncontrast, we encourage cognition-level temporal commonsense reasoning via\nnarrative reorganization. The advantages enable our model to contextualize what\nis happening like human beings and seamlessly apply to large-scale uncurated\nvideo data in the real world. Note that our method differs from ones designed\nfor video-text alignment (e.g., Frozen) and multimodal representation learning\n(e.g., Merlot). Our method demonstrates strong out-of-the-box spatiotemporal\nrepresentations on diverse video benchmarks, e.g., +13.6% gains over VideoMAE\non SSV2 via linear probing.",
    "descriptor": "",
    "authors": [
      "Ziyun Zeng",
      "Yuying Ge",
      "Xihui Liu",
      "Bin Chen",
      "Ping Luo",
      "Shu-Tao Xia",
      "Yixiao Ge"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15280"
  },
  {
    "id": "arXiv:2209.15285",
    "title": "QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural  Machine Translation",
    "abstract": "With the recent advance in neural machine translation demonstrating its\nimportance, research on quality estimation (QE) has been steadily progressing.\nQE aims to automatically predict the quality of machine translation (MT) output\nwithout reference sentences. Despite its high utility in the real world, there\nremain several limitations concerning manual QE data creation: inevitably\nincurred non-trivial costs due to the need for translation experts, and issues\nwith data scaling and language expansion. To tackle these limitations, we\npresent QUAK, a Korean-English synthetic QE dataset generated in a fully\nautomatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and\nQUAK-H, produced through three strategies that are relatively free from\nlanguage constraints. Since each strategy requires no human effort, which\nfacilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M\nfor QUAK-M. As an experiment, we quantitatively analyze word-level QE results\nin various ways while performing statistical analysis. Moreover, we show that\ndatasets scaled in an efficient way also contribute to performance improvements\nby observing meaningful performance gains in QUAK-M, P when adding data up to\n1.58M.",
    "descriptor": "",
    "authors": [
      "Sugyeong Eo",
      "Chanjun Park",
      "Hyeonseok Moon",
      "Jaehyung Seo",
      "Gyeongmin Kim",
      "Jungseob Lee",
      "Heuiseok Lim"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15285"
  },
  {
    "id": "arXiv:2209.15286",
    "title": "A refined first-order expansion formula in Rn: Application to  interpolation and finite element error estimates",
    "abstract": "The aim of this paper is to derive a refined first-order expansion formula in\nRn, the goal being to get an optimal reduced remainder, compared to the one\nobtained by usual Taylor's formula. For a given function, the formula we\nderived is obtained by introducing a linear combination of the first\nderivatives, computed at $n+1$ equally spaced points. We show how this formula\ncan be applied to two important applications: the interpolation error and the\nfinite elements error estimates. In both cases, we illustrate under which\nconditions a significant improvement of the errors can be obtained, namely how\nthe use of the refined expansion can reduce the upper bound of error estimates.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Joel Chaskalovic",
      "Franck Assous"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.15286"
  },
  {
    "id": "arXiv:2209.15287",
    "title": "Verifiable and Energy Efficient Medical Image Analysis with Quantised  Self-attentive Deep Neural Networks",
    "abstract": "Convolutional Neural Networks have played a significant role in various\nmedical imaging tasks like classification and segmentation. They provide\nstate-of-the-art performance compared to classical image processing algorithms.\nHowever, the major downside of these methods is the high computational\ncomplexity, reliance on high-performance hardware like GPUs and the inherent\nblack-box nature of the model. In this paper, we propose quantised stand-alone\nself-attention based models as an alternative to traditional CNNs. In the\nproposed class of networks, convolutional layers are replaced with stand-alone\nself-attention layers, and the network parameters are quantised after training.\nWe experimentally validate the performance of our method on classification and\nsegmentation tasks. We observe a $50-80\\%$ reduction in model size, $60-80\\%$\nlesser number of parameters, $40-85\\%$ fewer FLOPs and $65-80\\%$ more energy\nefficiency during inference on CPUs. The code will be available at \\href\n{https://github.com/Rakshith2597/Quantised-Self-Attentive-Deep-Neural-Network}{https://github.com/Rakshith2597/Quantised-Self-Attentive-Deep-Neural-Network}.",
    "descriptor": "\nComments: Accepted at MICCAI 2022 FAIR Workshop\n",
    "authors": [
      "Rakshith Sathish",
      "Swanand Khare",
      "Debdoot Sheet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15287"
  },
  {
    "id": "arXiv:2209.15288",
    "title": "A Survey: Implementations of Non-fungible Token System in Different  Fields",
    "abstract": "In the realm of digital art and collectibles, NFTs are sweeping the board.\nBecause of the massive sales to a new crypto audience, the livelihoods of\ndigital artists are being transformed. It is no surprise that celebs are\njumping on the bandwagon. It is a fact that NFTs can be used in multiple ways,\nincluding digital artwork such as animation, character design, digital\npainting, collection of selfies or vlogs, and many more digital entities. As a\nresult, they may be used to signify the possession of any specific object,\nwhether it be digital or physical. NFTs are digital tokens that may be used to\nindicate ownership of one of a-kind goods. For example, I can buy a shoe or T\nshirt from any store, and then if the store provides me the same 3D model of\nthat T-Shirt or shoe of the exact same design and color, it would be more\nconnected with my feelings. They enable us to tokenize items such as artwork,\nvaluables, and even real estate. NFTs can only be owned by one person at a\ntime, and they are protected by the Ethereum blockchain no one can alter the\nownership record or create a new NFT. The word non-fungible can be used to\ndescribe items like your furniture, a song file, or your computer. It is\nimpossible to substitute these goods with anything else because they each have\ntheir own distinct characteristics. The goal was to find all the existing\nimplementations of Non-fungible Tokens in different fields of recent\ntechnology, so that an overall overview of future implementations of NFT can be\nfound and how it can be used to enrich user experiences.",
    "descriptor": "\nComments: 14 pages, 3 figures, 3 tables\n",
    "authors": [
      "A. N. M. Sajedul Alam",
      "Junaid Bin Kibria",
      "Al Hasib Mahamud",
      "Arnob Kumar Dey",
      "Hasan Muhammed Zahidul Amin",
      "Md Sabbir Hossain",
      "Annajiat Alim Rasel"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.15288"
  },
  {
    "id": "arXiv:2209.15290",
    "title": "CDBB West Cambridge Digital Twin: Lessons Learned",
    "abstract": "The report describes the digital architecture developed for the West\nCambridge Digital Twin, particularly focussed on real-time sensor data\ncollection and analysis with a privacy framework allowing occupants of the\nbuildings to be first-class participants in the system. The implementation has\nsome notable characteristics. In particular 'push' technology is used\nthroughout such that information streams from the incoming asynchronous\nindividual sensor events through to the end-user web pages with the absolute\nminimum latency, including real-time generated simple and complex events\nderived from the the underlying sensor data and the updating of visualisations\nsuch as an in-building heatmap. We believe the ability of the entire system to\nrespond in the timescale of individual sensor messages to be unique. JSON\nstructures are used to represent all data types including sensor readings,\nsensor types, building objects, organisations and people, with the idea that\nJSON-LD may represent a more suitable way than XML/RDF for managing relations\nbetween those objects (such as the 'occupies' relationship of people to\noffices, or the 'type' relationship of sensors to sensor types).",
    "descriptor": "\nComments: 72 pages. arXiv admin note: substantial text overlap with arXiv:2103.04924, arXiv:2103.09169\n",
    "authors": [
      "Justas Brazauskas",
      "Matt Danish",
      "Vadim Safronov",
      "Rohit Verma",
      "Richard Mortier",
      "Ian Lewis"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.15290"
  },
  {
    "id": "arXiv:2209.15292",
    "title": "The Minority Matters: A Diversity-Promoting Collaborative Metric  Learning Algorithm",
    "abstract": "Collaborative Metric Learning (CML) has recently emerged as a popular method\nin recommendation systems (RS), closing the gap between metric learning and\nCollaborative Filtering. Following the convention of RS, existing methods\nexploit unique user representation in their model design. This paper focuses on\na challenging scenario where a user has multiple categories of interests. Under\nthis setting, we argue that the unique user representation might induce\npreference bias, especially when the item category distribution is imbalanced.\nTo address this issue, we propose a novel method called\n\\textit{Diversity-Promoting Collaborative Metric Learning} (DPCML), with the\nhope of considering the commonly ignored minority interest of the user. The key\nidea behind DPCML is to include a multiple set of representations for each user\nin the system. Based on this embedding paradigm, user preference toward an item\nis aggregated from different embeddings by taking the minimum item-user\ndistance among the user embedding set. Furthermore, we observe that the\ndiversity of the embeddings for the same user also plays an essential role in\nthe model. To this end, we propose a \\textit{diversity control regularization}\nterm to accommodate the multi-vector representation strategy better.\nTheoretically, we show that DPCML could generalize well to unseen test data by\ntackling the challenge of the annoying operation that comes from the minimum\nvalue. Experiments over a range of benchmark datasets speak to the efficacy of\nDPCML.",
    "descriptor": "",
    "authors": [
      "Shilong Bao",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Yuan He",
      "Xiaochun Cao",
      "Qingming Huang"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15292"
  },
  {
    "id": "arXiv:2209.15293",
    "title": "A Survey: Credit Sentiment Score Prediction",
    "abstract": "Manual approvals are still used by banks and other NGOs to approve loans. It\ntakes time and is prone to mistakes because it is controlled by a bank\nemployee. Several fields of machine learning mining technologies have been\nutilized to enhance various areas of credit rating forecast. A major goal of\nthis research is to look at current sentiment analysis techniques that are\nbeing used to generate creditworthiness.",
    "descriptor": "\nComments: 16 pages, 3 figures, 3 tables\n",
    "authors": [
      "A. N. M. Sajedul Alam",
      "Junaid Bin Kibria",
      "Arnob Kumar Dey",
      "Zawad Alam",
      "Shifat Zaman",
      "Motahar Mahtab",
      "Mohammed Julfikar Ali Mahbub",
      "Annajiat Alim Rasel"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15293"
  },
  {
    "id": "arXiv:2209.15296",
    "title": "Wake Word Detection Based on Res2Net",
    "abstract": "This letter proposes a new wake word detection system based on Res2Net. As a\nvariant of ResNet, Res2Net was first applied to objection detection. Res2Net\nrealizes multiple feature scales by increasing possible receptive fields. This\nmultiple scaling mechanism significantly improves the detection ability of wake\nwords with different durations. Compared with the ResNet-based model, Res2Net\nalso significantly reduces the model size and is more suitable for detecting\nwake words. The proposed system can determine the positions of wake words from\nthe audio stream without any additional assistance. The proposed method is\nverified on the Mobvoi dataset containing two wake words. At a false alarm rate\nof 0.5 per hour, the system reduced the false rejection of the two wake words\nby more than 12% over prior works.",
    "descriptor": "",
    "authors": [
      "Qiuchen Yu",
      "Ruohua Zhou"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15296"
  },
  {
    "id": "arXiv:2209.15300",
    "title": "Deterministic Performance Guarantees for Bidirectional BFS on Real-World  Networks",
    "abstract": "A common technique to speed up shortest path queries in graphs is to use a\nbidirectional search, i.e., performing a forward search from the start and a\nbackward search from the destination until a common vertex on a shortest path\nis found. In practice, this has a tremendous impact on the performance on some\nreal-world networks, while it only seems to save a constant factor on other\ntypes of networks. Even though finding shortest paths is a ubiquitous problem,\nthere are only few studies attempting to understand the apparently asymptotic\nspeedups on some networks, using average case analysis on certain models for\nreal-world networks.\nIn this paper we give a new perspective on this, by analyzing deterministic\nproperties that permit theoretical analysis and that can easily be checked on\nany particular instance. We prove that these parameters imply sublinear running\ntime for the bidirectional breadth-first search in several regimes, some of\nwhich are tight. Moreover, we perform experiments on a large set of real-world\nnetworks showing that our parameters capture the concept of practical running\ntime well.",
    "descriptor": "",
    "authors": [
      "Thomas Bl\u00e4sius",
      "Marcus Wilhelm"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.15300"
  },
  {
    "id": "arXiv:2209.15301",
    "title": "Medical Question Understanding and Answering with Knowledge Grounding  and Semantic Self-Supervision",
    "abstract": "Current medical question answering systems have difficulty processing long,\ndetailed and informally worded questions submitted by patients, called Consumer\nHealth Questions (CHQs). To address this issue, we introduce a medical question\nunderstanding and answering system with knowledge grounding and semantic\nself-supervision. Our system is a pipeline that first summarizes a long,\nmedical, user-written question, using a supervised summarization loss. Then,\nour system performs a two-step retrieval to return answers. The system first\nmatches the summarized user question with an FAQ from a trusted medical\nknowledge base, and then retrieves a fixed number of relevant sentences from\nthe corresponding answer document. In the absence of labels for question\nmatching or answer relevance, we design 3 novel, self-supervised and\nsemantically-guided losses. We evaluate our model against two strong\nretrieval-based question answering baselines. Evaluators ask their own\nquestions and rate the answers retrieved by our baselines and own system\naccording to their relevance. They find that our system retrieves more relevant\nanswers, while achieving speeds 20 times faster. Our self-supervised losses\nalso help the summarizer achieve higher scores in ROUGE, as well as in human\nevaluation metrics. We release our code to encourage further research.",
    "descriptor": "\nComments: Accepted as Main Conference Long paper at COLING 2022\n",
    "authors": [
      "Khalil Mrini",
      "Harpreet Singh",
      "Franck Dernoncourt",
      "Seunghyun Yoon",
      "Trung Bui",
      "Walter Chang",
      "Emilia Farcas",
      "Ndapa Nakashole"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15301"
  },
  {
    "id": "arXiv:2209.15304",
    "title": "Visual Privacy Protection Based on Type-I Adversarial Attack",
    "abstract": "With the development of online artificial intelligence systems, many deep\nneural networks (DNNs) have been deployed in cloud environments. In practical\napplications, developers or users need to provide their private data to DNNs,\nsuch as faces. However, data transmitted and stored in the cloud is insecure\nand at risk of privacy leakage. In this work, inspired by Type-I adversarial\nattack, we propose an adversarial attack-based method to protect visual privacy\nof data. Specifically, the method encrypts the visual information of private\ndata while maintaining them correctly predicted by DNNs, without modifying the\nmodel parameters. The empirical results on face recognition tasks show that the\nproposed method can deeply hide the visual information in face images and\nhardly affect the accuracy of the recognition models. In addition, we further\nextend the method to classification tasks and also achieve state-of-the-art\nperformance.",
    "descriptor": "",
    "authors": [
      "Zhigang Su",
      "Dawei Zhou",
      "Decheng Liu",
      "Nannan Wang",
      "Zhen Wang",
      "Xinbo Gao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15304"
  },
  {
    "id": "arXiv:2209.15305",
    "title": "Proportionally Fair Online Allocation of Public Goods with Predictions",
    "abstract": "We design online algorithms for the fair allocation of public goods to a set\nof $N$ agents over a sequence of $T$ rounds and focus on improving their\nperformance using predictions. In the basic model, a public good arrives in\neach round, the algorithm learns every agent's value for the good, and must\nirrevocably decide the amount of investment in the good without exceeding a\ntotal budget of $B$ across all rounds. The algorithm can utilize (potentially\ninaccurate) predictions of each agent's total value for all the goods to\narrive. We measure the performance of the algorithm using a proportional\nfairness objective, which informally demands that every group of agents be\nrewarded in proportion to its size and the cohesiveness of its preferences.\nIn the special case of binary agent preferences and a unit budget, we show\nthat $O(\\log N)$ proportional fairness can be achieved without using any\npredictions, and that this is optimal even if perfectly accurate predictions\nwere available. However, for general preferences and budget no algorithm can\nachieve better than $\\Theta(T/B)$ proportional fairness without predictions. We\nshow that algorithms with (reasonably accurate) predictions can do much better,\nachieving $\\Theta(\\log (T/B))$ proportional fairness. We also extend this\nresult to a general model in which a batch of $L$ public goods arrive in each\nround and achieve $O(\\log (\\min(N,L) \\cdot T/B))$ proportional fairness. Our\nexact bounds are parametrized as a function of the error in the predictions and\nthe performance degrades gracefully with increasing errors.",
    "descriptor": "",
    "authors": [
      "Siddhartha Banerjee",
      "Vasilis Gkatzelis",
      "Safwan Hossain",
      "Billy Jin",
      "Evi Micha",
      "Nisarg Shah"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.15305"
  },
  {
    "id": "arXiv:2209.15308",
    "title": "Effective Early Stopping of Point Cloud Neural Networks",
    "abstract": "Early stopping techniques can be utilized to decrease the time cost, however\ncurrently the ultimate goal of early stopping techniques is closely related to\nthe accuracy upgrade or the ability of the neural network to generalize better\non unseen data without being large or complex in structure and not directly\nwith its efficiency. Time efficiency is a critical factor in neural networks,\nespecially when dealing with the segmentation of 3D point cloud data, not only\nbecause a neural network itself is computationally expensive, but also because\npoint clouds are large and noisy data, making learning processes even more\ncostly. In this paper, we propose a new early stopping technique based on\nfundamental mathematics aiming to upgrade the trade-off between the learning\nefficiency and accuracy of neural networks dealing with 3D point clouds. Our\nresults show that by employing our early stopping technique in four distinct\nand highly utilized neural networks in segmenting 3D point clouds, the training\ntime efficiency of the models is greatly improved, with efficiency gain values\nreaching up to 94\\%, while the models achieving in just a few epochs\napproximately similar segmentation accuracy metric values like the ones that\nare obtained in the training of the neural networks in 200 epochs. Also, our\nproposal outperforms four conventional early stopping approaches in\nsegmentation accuracy, implying a promising innovative early stopping technique\nin point cloud segmentation.",
    "descriptor": "",
    "authors": [
      "Thanasis Zoumpekas",
      "Maria Salam\u00f3",
      "Anna Puig"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15308"
  },
  {
    "id": "arXiv:2209.15312",
    "title": "Strings And Colorings Of Topological Coding Towards Asymmetric Topology  Cryptography",
    "abstract": "We, for anti-quantum computing, will discuss various number-based strings,\nsuch as number-based super-strings, parameterized strings, set-based strings,\ngraph-based strings, integer-partitioned and integer-decomposed strings,\nHanzi-based strings, as well as algebraic operations based on number-based\nstrings. Moreover, we introduce number-based string-colorings, magic-constraint\ncolorings, and vector-colorings and set-colorings related with strings. For the\ntechnique of encrypting the entire network at once, we propose graphic lattices\nrelated with number-based strings, Hanzi-graphic lattices, string groups,\nall-tree-graphic lattices. We study some topics of asymmetric topology\ncryptography, such as topological signatures, Key-pair graphs, Key-pair\nstrings, one-encryption one-time and self-certification algorithms. Part of\ntopological techniques and algorithms introduced here are closely related with\nNP-complete problems or NP-hard problems.",
    "descriptor": "\nComments: Asymmetric topology encryption is a new topic of topological coding towards the certificateless public key cryptography\n",
    "authors": [
      "Bing Yao",
      "Chao Yang",
      "Xia Liu",
      "Fei Ma",
      "Jing Su",
      "Hui Sun",
      "Xiaohui Zhang",
      "Yarong Mu"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.15312"
  },
  {
    "id": "arXiv:2209.15314",
    "title": "Did You Get What You Paid For? Rethinking Annotation Cost of Deep  Learning Based Computer Aided Detection in Chest Radiographs",
    "abstract": "As deep networks require large amounts of accurately labeled training data, a\nstrategy to collect sufficiently large and accurate annotations is as important\nas innovations in recognition methods. This is especially true for building\nComputer Aided Detection (CAD) systems for chest X-rays where domain expertise\nof radiologists is required to annotate the presence and location of\nabnormalities on X-ray images. However, there lacks concrete evidence that\nprovides guidance on how much resource to allocate for data annotation such\nthat the resulting CAD system reaches desired performance. Without this\nknowledge, practitioners often fall back to the strategy of collecting as much\ndetail as possible on as much data as possible which is cost inefficient. In\nthis work, we investigate how the cost of data annotation ultimately impacts\nthe CAD model performance on classification and segmentation of chest\nabnormalities in frontal-view X-ray images. We define the cost of annotation\nwith respect to the following three dimensions: quantity, quality and\ngranularity of labels. Throughout this study, we isolate the impact of each\ndimension on the resulting CAD model performance on detecting 10 chest\nabnormalities in X-rays. On a large scale training data with over 120K X-ray\nimages with gold-standard annotations, we find that cost-efficient annotations\nprovide great value when collected in large amounts and lead to competitive\nperformance when compared to models trained with only gold-standard\nannotations. We also find that combining large amounts of cost efficient\nannotations with only small amounts of expensive labels leads to competitive\nCAD models at a much lower cost.",
    "descriptor": "\nComments: MICCAI 2022, Contains Supplemental Material\n",
    "authors": [
      "Tae Soo Kim",
      "Geonwoon Jang",
      "Sanghyup Lee",
      "Thijs Kooi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15314"
  },
  {
    "id": "arXiv:2209.15315",
    "title": "Metro: Memory-Enhanced Transformer for Retrosynthetic Planning via  Reaction Tree",
    "abstract": "Retrosynthetic planning plays a critical role in drug discovery and organic\nchemistry. Starting from a target molecule as the root node, it aims to find a\ncomplete reaction tree subject to the constraint that all leaf nodes belong to\na set of starting materials. The multi-step reactions are crucial because they\ndetermine the flow chart in the production of the Organic Chemical Industry.\nHowever, existing datasets lack curation of tree-structured multi-step\nreactions, and fail to provide such reaction trees, limiting models'\nunderstanding of organic molecule transformations. In this work, we first\ndevelop a benchmark curated for the retrosynthetic planning task, which\nconsists of 124,869 reaction trees retrieved from the public USPTO-full\ndataset. On top of that, we propose Metro: Memory-Enhanced Transformer for\nRetrOsynthetic planning. Specifically, the dependency among molecules in the\nreaction tree is captured as context information for multi-step retrosynthesis\npredictions through transformers with a memory module. Extensive experiments\nshow that Metro dramatically outperforms existing single-step retrosynthesis\nmodels by at least 10.7% in top-1 accuracy. The experiments demonstrate the\nsuperiority of exploiting context information in the retrosynthetic planning\ntask. Moreover, the proposed model can be directly used for synthetic\naccessibility analysis, as it is trained on reaction trees with the shortest\ndepths. Our work is the first step towards a brand new formulation for\nretrosynthetic planning in the aspects of data construction, model design, and\nevaluation. Code is available at https://github.com/SongtaoLiu0823/metro.",
    "descriptor": "",
    "authors": [
      "Songtao Liu",
      "Rex Ying",
      "Zuobai Zhang",
      "Peilin Zhao",
      "Jian Tang",
      "Lu Lin",
      "Dinghao Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Chemical Physics (physics.chem-ph)",
      "Biomolecules (q-bio.BM)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2209.15315"
  },
  {
    "id": "arXiv:2209.15317",
    "title": "Convolutional Neural Networks Quantization with Attention",
    "abstract": "It has been proven that, compared to using 32-bit floating-point numbers in\nthe training phase, Deep Convolutional Neural Networks (DCNNs) can operate with\nlow precision during inference, thereby saving memory space and power\nconsumption. However, quantizing networks is always accompanied by an accuracy\ndecrease. Here, we propose a method, double-stage Squeeze-and-Threshold\n(double-stage ST). It uses the attention mechanism to quantize networks and\nachieve state-of-art results. Using our method, the 3-bit model can achieve\naccuracy that exceeds the accuracy of the full-precision baseline model. The\nproposed double-stage ST activation quantization is easy to apply: inserting it\nbefore the convolution.",
    "descriptor": "\nComments: Preprint of an article published in International Journal of Neural Systems, [10.1142/S0129065722500514] \\c{opyright} [copyright World Scientific Publishing Company] [this https URL]\n",
    "authors": [
      "Binyi Wu",
      "Bernd Waschneck",
      "Christian Georg Mayr"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15317"
  },
  {
    "id": "arXiv:2209.15318",
    "title": "Extensions on `A Convex Scheme for the Secrecy Capacity of a MIMO  Wiretap Channel with a Single Antenna Eavesdropper'",
    "abstract": "One key metric for physical layer security is the secrecy capacity. This is\nthe maximum rate that a system can transmit with perfect secrecy. For a\nMultiple Input Multiple Output (MIMO) system (a newer technology for 5G, 6G and\nbeyond) the secrecy capacity is not fully understood. For a Gaussian MIMO\nchannel, the secrecy capacity is a non-convex optimisation problem for which a\ngeneral solution is not available.\nPrevious work by the authors showed that the secrecy capacity of a MIMO\nsystem with a single eavesdrop antenna is concave to a cut off point. In this\nwork, which extends the previous paper, results are given for the region beyond\nthis cut off point. It is shown that, for certain parameters, the presented\nscheme is concave to a point, and convex beyond it, and can therefore be solved\nefficiently using existing convex optimisation software.",
    "descriptor": "",
    "authors": [
      "Jennifer Chakravarty",
      "Oliver Johnson",
      "Robert Piechocki"
    ],
    "subjectives": [
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.15318"
  },
  {
    "id": "arXiv:2209.15320",
    "title": "Observational Robustness and Invariances in Reinforcement Learning via  Lexicographic Objectives",
    "abstract": "Policy robustness in Reinforcement Learning (RL) may not be desirable at any\nprice; the alterations caused by robustness requirements from otherwise optimal\npolicies should be explainable and quantifiable. Policy gradient algorithms\nthat have strong convergence guarantees are usually modified to obtain robust\npolicies in ways that do not preserve algorithm guarantees, which defeats the\npurpose of formal robustness requirements. In this work we study a notion of\nrobustness in partially observable MDPs where state observations are perturbed\nby a noise-induced stochastic kernel. We characterise the set of policies that\nare maximally robust by analysing how the policies are altered by this kernel.\nWe then establish a connection between such robust policies and certain\nproperties of the noise kernel, as well as with structural properties of the\nunderlying MDPs, constructing sufficient conditions for policy robustness. We\nuse these notions to propose a robustness-inducing scheme, applicable to any\npolicy gradient algorithm, to formally trade off the reward achieved by a\npolicy with its robustness level through lexicographic optimisation, which\npreserves convergence properties of the original algorithm. We test the the\nproposed approach through numerical experiments on safety-critical RL\nenvironments, and show how the proposed method helps achieve high robustness\nwhen state errors are introduced in the policy roll-out.",
    "descriptor": "",
    "authors": [
      "Daniel Jarne Ornia",
      "Licio Romao",
      "Lewis Hammond",
      "Manuel Mazo Jr.",
      "Alessandro Abate"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15320"
  },
  {
    "id": "arXiv:2209.15322",
    "title": "Wi-attack: Cross-technology Impersonation Attack against iBeacon  Services",
    "abstract": "iBeacon protocol is widely deployed to provide location-based services. By\nreceiving its BLE advertisements, nearby devices can estimate the proximity to\nthe iBeacon or calculate indoor positions. However, the open nature of these\nadvertisements brings vulnerability to impersonation attacks. Such attacks\ncould lead to spam, unreliable positioning, and even security breaches. In this\npaper, we propose Wi-attack, revealing the feasibility of using WiFi devices to\nconduct impersonation attacks on iBeacon services. Different from impersonation\nattacks using BLE compatible hardware, Wi-attack is not restricted by\nbroadcasting intervals and is able to impersonate multiple iBeacons at the same\ntime. Effective attacks can be launched on iBeacon services without\nmodifications to WiFi hardware or firmware. To enable direct communication from\nWiFi to BLE, we use the digital emulation technique of cross technology\ncommunication. To enhance the packet reception along with its stability, we add\nredundant packets to eliminate cyclic prefix error entirely. The emulation\nprovides an iBeacon packet reception rate up to 66.2%. We conduct attacks on\nthree iBeacon services scenarios, point deployment, multilateration, and\nfingerprint-based localization. The evaluation results show that Wi-attack can\nbring an average distance error of more than 20 meters on fingerprint-based\nlocalization using only 3 APs.",
    "descriptor": "\nComments: 9 pages; 26 figures; 2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), 2021\n",
    "authors": [
      "Xin Na",
      "Xiuzhen Guo",
      "Yuan He",
      "Rui Xi"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.15322"
  },
  {
    "id": "arXiv:2209.15323",
    "title": "SmallCap: Lightweight Image Captioning Prompted with Retrieval  Augmentation",
    "abstract": "Recent advances in image captioning have focused on scaling the data and\nmodel size, substantially increasing the cost of pre-training and finetuning.\nAs an alternative to large models, we present SmallCap, which generates a\ncaption conditioned on an input image and related captions retrieved from a\ndatastore. Our model is lightweight and fast to train as the only learned\nparameters are in newly introduced cross-attention layers between a pre-trained\nCLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without\nadditional finetuning and exploit large-scale data in a training-free fashion\nbecause the contents of the datastore can be readily replaced. Our experiments\nshow that SmallCap, trained only on COCO, has competitive performance on this\nbenchmark, and also transfers to other domains without retraining, solely\nthrough retrieval from target-domain data. Further improvement is achieved\nthrough the training-free exploitation of diverse human-labeled and web data,\nwhich proves effective for other domains, including the nocaps image captioning\nbenchmark, designed to test generalization to unseen visual concepts.",
    "descriptor": "",
    "authors": [
      "Rita Ramos",
      "Bruno Martins",
      "Desmond Elliott",
      "Yova Kementchedjhieva"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15323"
  },
  {
    "id": "arXiv:2209.15325",
    "title": "Symphony: Localizing Multiple Acoustic Sources with a Single Microphone  Array",
    "abstract": "Sound recognition is an important and popular function of smart devices. The\nlocation of sound is basic information associated with the acoustic source.\nApart from sound recognition, whether the acoustic sources can be localized\nlargely affects the capability and quality of the smart device's interactive\nfunctions. In this work, we study the problem of concurrently localizing\nmultiple acoustic sources with a smart device (e.g., a smart speaker like\nAmazon Alexa). The existing approaches either can only localize a single\nsource, or require deploying a distributed network of microphone arrays to\nfunction. Our proposal called Symphony is the first approach to tackle the\nabove problem with a single microphone array. The insight behind Symphony is\nthat the geometric layout of microphones on the array determines the unique\nrelationship among signals from the same source along the same arriving path,\nwhile the source's location determines the DoAs (direction-of-arrival) of\nsignals along different arriving paths. Symphony therefore includes a\ngeometry-based filtering module to distinguish signals from different sources\nalong different paths and a coherence-based module to identify signals from the\nsame source. We implement Symphony with different types of commercial\noff-the-shelf microphone arrays and evaluate its performance under different\nsettings. The results show that Symphony has a median localization error of\n0.694m, which is 68% less than that of the state-of-the-art approach.",
    "descriptor": "",
    "authors": [
      "Weiguo Wang",
      "Jinming Li",
      "Yuan He",
      "Yunhao Liu"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Networking and Internet Architecture (cs.NI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15325"
  },
  {
    "id": "arXiv:2209.15328",
    "title": "Sparse Random Networks for Communication-Efficient Federated Learning",
    "abstract": "One main challenge in federated learning is the large communication cost of\nexchanging weight updates from clients to the server at each round. While prior\nwork has made great progress in compressing the weight updates through gradient\ncompression methods, we propose a radically different approach that does not\nupdate the weights at all. Instead, our method freezes the weights at their\ninitial \\emph{random} values and learns how to sparsify the random network for\nthe best performance. To this end, the clients collaborate in training a\n\\emph{stochastic} binary mask to find the optimal sparse random network within\nthe original one. At the end of the training, the final model is a sparse\nnetwork with random weights -- or a subnetwork inside the dense random network.\nWe show improvements in accuracy, communication (less than $1$ bit per\nparameter (bpp)), convergence speed, and final model size (less than $1$ bpp)\nover relevant baselines on MNIST, EMNIST, CIFAR-10, and CIFAR-100 datasets, in\nthe low bitrate regime under various system configurations.",
    "descriptor": "\nComments: 20 pages\n",
    "authors": [
      "Berivan Isik",
      "Francesco Pase",
      "Deniz Gunduz",
      "Tsachy Weissman",
      "Michele Zorzi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15328"
  },
  {
    "id": "arXiv:2209.15329",
    "title": "SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data",
    "abstract": "How to boost speech pre-training with textual data is an unsolved problem due\nto the fact that speech and text are very different modalities with distinct\ncharacteristics. In this paper, we propose a cross-modal Speech and Language\nModel (SpeechLM) to explicitly align speech and text pre-training with a\npre-defined unified discrete representation. Specifically, we introduce two\nalternative discrete tokenizers to bridge the speech and text modalities,\nincluding phoneme-unit and hidden-unit tokenizers, which can be trained using a\nsmall amount of paired speech-text data. Based on the trained tokenizers, we\nconvert the unlabeled speech and text data into tokens of phoneme units or\nhidden units. The pre-training objective is designed to unify the speech and\nthe text into the same discrete semantic space with a unified Transformer\nnetwork. Leveraging only 10K text sentences, our SpeechLM gets a 16\\% relative\nWER reduction over the best base model performance (from 6.8 to 5.7) on the\npublic LibriSpeech ASR benchmark. Moreover, SpeechLM with fewer parameters even\noutperforms previous SOTA models on CoVoST-2 speech translation tasks. We also\nevaluate our SpeechLM on various spoken language processing tasks under the\nuniversal representation evaluation framework SUPERB, demonstrating significant\nimprovements on content-related tasks. Our code and models are available at\nhttps://aka.ms/SpeechLM.",
    "descriptor": "\nComments: 14 pages\n",
    "authors": [
      "Ziqiang Zhang",
      "Sanyuan Chen",
      "Long Zhou",
      "Yu Wu",
      "Shuo Ren",
      "Shujie Liu",
      "Zhuoyuan Yao",
      "Xun Gong",
      "Lirong Dai",
      "Jinyu Li",
      "Furu Wei"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15329"
  },
  {
    "id": "arXiv:2209.15334",
    "title": "ChordMics: Acoustic Signal Purification with Distributed Microphones",
    "abstract": "Acoustic signal acts as an essential input to many systems. However, the pure\nacoustic signal is very difficult to extract, especially in noisy environments.\nExisting beamforming systems are able to extract the signal transmitted from\ncertain directions. However, since microphones are centrally deployed, these\nsystems have limited coverage and low spatial resolution. We overcome the above\nlimitations and present ChordMics, a distributed beamforming system. By\nleveraging the spatial diversity of the distributed microphones, ChordMics is\nable to extract the acoustic signal from arbitrary points. To realize such a\nsystem, we further address the fundamental challenge in distributed\nbeamforming: aligning the signals captured by distributed and unsynchronized\nmicrophones. We implement ChordMics and evaluate its performance under both LOS\nand NLOS scenarios. The evaluation results tell that ChordMics can deliver\nhigher SINR than the centralized microphone array. The average performance gain\nis up to 15dB.",
    "descriptor": "",
    "authors": [
      "Weiguo Wang",
      "Jinming Li",
      "Meng Jin",
      "Yuan He"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Networking and Internet Architecture (cs.NI)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15334"
  },
  {
    "id": "arXiv:2209.15337",
    "title": "Towards Safe Landing of Falling Quadruped Robots Using a 3-DoF Morphable  Inertial Tail",
    "abstract": "Falling cat problem is well-known where cats show their super aerial\nreorientation capability and can land safely. For their robotic counterparts, a\nsimilar falling quadruped robot problem, has not been fully addressed, although\nachieving safe landing as the cats has been increasingly investigated. Unlike\nimposing the burden on landing control, we approach to safe landing of falling\nquadruped robots by effective flight phase control. Different from existing\nwork like swinging legs and attaching reaction wheels or simple tails, we\npropose to deploy a 3-DoF morphable inertial tail on a medium-size quadruped\nrobot. In the flight phase, the tail with its maximum length can self-right the\nbody orientation in 3D effectively; before touch-down, the tail length can be\nretracted to about 1/4 of its maximum for impressing the tail's side-effect on\nlanding. To enable aerial reorientation for safe landing in the quadruped\nrobots, we design a control architecture, which has been verified in a\nhigh-fidelity physics simulation environment with different initial conditions.\nExperimental results on a customized flight-phase test platform with comparable\ninertial properties are provided and show the tail's effectiveness on 3D body\nreorientation and its fast retractability before touch-down. An initial falling\nquadruped robot experiment is shown, where the robot Unitree A1 with the 3-DoF\ntail can land safely subject to non-negligible initial body angles.",
    "descriptor": "\nComments: 7 pages, 8 figures, submit to ICRA2023\n",
    "authors": [
      "Yunxi Tang",
      "Jiajun An",
      "Xiangyu Chu",
      "Shengzhi Wang",
      "Ching Yan Wong",
      "K. W. Samuel Au"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15337"
  },
  {
    "id": "arXiv:2209.15340",
    "title": "A Learnable Optimization and Regularization Approach to Massive MIMO CSI  Feedback",
    "abstract": "Channel state information (CSI) plays a critical role in achieving the\npotential benefits of massive multiple input multiple output (MIMO) systems. In\nfrequency division duplex (FDD) massive MIMO systems, the base station (BS)\nrelies on sustained and accurate CSI feedback from the users. However, due to\nthe large number of antennas and users being served in massive MIMO systems,\nfeedback overhead can become a bottleneck. In this paper, we propose a\nmodel-driven deep learning method for CSI feedback, called learnable\noptimization and regularization algorithm (LORA). Instead of using l1-norm as\nthe regularization term, a learnable regularization module is introduced in\nLORA to automatically adapt to the characteristics of CSI. We unfold the\nconventional iterative shrinkage-thresholding algorithm (ISTA) to a neural\nnetwork and learn both the optimization process and regularization term by\nend-toend training. We show that LORA improves the CSI feedback accuracy and\nspeed. Besides, a novel learnable quantization method and the corresponding\ntraining scheme are proposed, and it is shown that LORA can operate\nsuccessfully at different bit rates, providing flexibility in terms of the CSI\nfeedback overhead. Various realistic scenarios are considered to demonstrate\nthe effectiveness and robustness of LORA through numerical simulations.",
    "descriptor": "",
    "authors": [
      "Zhengyang Hu",
      "Guanzhang Liu",
      "Qi Xie",
      "Jiang Xue",
      "Deyu Meng",
      "Deniz Gunduz"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15340"
  },
  {
    "id": "arXiv:2209.15342",
    "title": "Emergent Communication: Generalization and Overfitting in Lewis Games",
    "abstract": "Lewis signaling games are a class of simple communication games for\nsimulating the emergence of language. In these games, two agents must agree on\na communication protocol in order to solve a cooperative task. Previous work\nhas shown that agents trained to play this game with reinforcement learning\ntend to develop languages that display undesirable properties from a linguistic\npoint of view (lack of generalization, lack of compositionality, etc). In this\npaper, we aim to provide better understanding of this phenomenon by\nanalytically studying the learning problem in Lewis games. As a core\ncontribution, we demonstrate that the standard objective in Lewis games can be\ndecomposed in two components: a co-adaptation loss and an information loss.\nThis decomposition enables us to surface two potential sources of overfitting,\nwhich we show may undermine the emergence of a structured communication\nprotocol. In particular, when we control for overfitting on the co-adaptation\nloss, we recover desired properties in the emergent languages: they are more\ncompositional and generalize better.",
    "descriptor": "\nComments: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)\n",
    "authors": [
      "Mathieu Rita",
      "Corentin Tallec",
      "Paul Michel",
      "Jean-Bastien Grill",
      "Olivier Pietquin",
      "Emmanuel Dupoux",
      "Florian Strub"
    ],
    "subjectives": [
      "Multiagent Systems (cs.MA)",
      "Computation and Language (cs.CL)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2209.15342"
  },
  {
    "id": "arXiv:2209.15348",
    "title": "Saiyan: Design and Implementation of a Low-power Demodulator for LoRa  Backscatter Systems",
    "abstract": "The radio range of backscatter systems continues growing as new wireless\ncommunication primitives are continuously invented. Nevertheless, both the bit\nerror rate and the packet loss rate of backscatter signals increase rapidly\nwith the radio range, thereby necessitating the cooperation between the access\npoint and the backscatter tags through a feedback loop. Unfortunately, the\nlow-power nature of backscatter tags limits their ability to demodulate\nfeedback signals from a remote access point and scales down to such\ncircumstances. This paper presents Saiyan, an ultra-low-power demodulator for\nlong-range LoRa backscatter systems. With Saiyan, a backscatter tag can\ndemodulate feedback signals from a remote access point with moderate power\nconsumption and then perform an immediate packet retransmission in the presence\nof packet loss. Moreover, Saiyan enables rate adaption and channel hopping-two\nPHY-layer operations that are important to channel efficiency yet unavailable\non long-range backscatter systems. We prototype Saiyan on a two-layer PCB board\nand evaluate its performance in different environments. Results show that\nSaiyan achieves 5 gain on the demodulation range, compared with\nstate-of-the-art systems. Our ASIC simulation shows that the power consumption\nof Saiyan is around 93.2 uW. Code and hardware schematics can be found at:\nhttps://github.com/ZangJac/Saiyan.",
    "descriptor": "",
    "authors": [
      "Xiuzhen Guo",
      "Longfei Shangguan",
      "Yuan He",
      "Nan Jing",
      "Jiacheng Zhang",
      "Haotian Jiang",
      "Yunhao Liu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15348"
  },
  {
    "id": "arXiv:2209.15349",
    "title": "Curing spurious magneto-mechanical coupling in soft non-magnetic  materials",
    "abstract": "The present work is concerned with the issue of spurious coupling effects\nthat are pervasive in fully coupled magneto-mechanical finite element\nsimulations involving very soft non-magnetic or air-like media. We first\naddress the characterization of the spurious magneto-mechanical effects and\ntheir intuitive interpretation based on energy considerations. Then, as main\ncontribution, we propose two new ways to prune the undesired spurious\nmagneto-mechanical coupling in non-magnetic media. The proposed methods are\ncompared with established methods in the context of magnetic bodies embedded in\n(i) air or vacuum and (ii) very soft elastic non-magnetic media. The comparison\nshows that the proposed approaches are accurate and effective. They,\nfurthermore, allow for a consistent linearization of the coupled boundary value\nproblems, which is crucial for the simulation of compliant structures. For\nreproducibility and accessibility of the proposed methods, we provide our\nimplementations with Netgen/NGSolve as well as all codes necessary for the\nreproduction of our results as supplementary material.",
    "descriptor": "\nComments: 31 pages, 17 figures, to be submitted to the International Journal of Numerical Methods in Engineering, supplementary material for this version hosted on zenodo with doi 10.5281/zenodo.7128677\n",
    "authors": [
      "Matthias Rambausek",
      "Joachim Sch\u00f6berl"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Soft Condensed Matter (cond-mat.soft)"
    ],
    "url": "https://arxiv.org/abs/2209.15349"
  },
  {
    "id": "arXiv:2209.15351",
    "title": "Efficient Ambient LoRa Backscatter with On-Off Keying Modulation",
    "abstract": "Backscatter communication holds potential for ubiquitous and low-cost\nconnectivity among low-power IoT devices. To avoid interference between the\ncarrier signal and the backscatter signal, recent works propose a\nfrequency-shifting technique to separate these two signals in the frequency\ndomain. Such proposals, however, have to occupy the precious wireless spectrum\nthat is already overcrowded, and increase the power, cost, and complexity of\nthe backscatter tag. In this paper, we revisit the classic ON-OFF Keying (OOK)\nmodulation and propose Aloba, a backscatter system that takes the ambient LoRa\ntransmissions as the excitation and piggybacks the in-band OOK modulated\nsignals over the LoRa transmissions. Our design enables the backsactter signal\nto work in the same frequency band of the carrier signal, meanwhile achieving\nflexible data rate at different transmission range. The key contributions of\nAloba include: (1) the design of a low-power backscatter tag that can pick up\nthe ambient LoRa signals from other signals. (2) a novel decoding algorithm to\ndemodulate both the carrier signal and the backscatter signal from their\nsuperposition. We further adopt link coding mechanism and interleave operation\nto enhance the reliability of backscatter signal decoding. We implement Aloba\nand conduct head-to-head comparison with the state-of-the-art LoRa backscatter\nsystem PLoRa in various settings. The experiment results show Aloba can achieve\n199.4 Kbps data rate at various distances, 52.4 times higher than PLoRa.",
    "descriptor": "",
    "authors": [
      "Xiuzhen Guo",
      "Longfei Shangguan",
      "Yuan He",
      "Jia Zhang",
      "Haotian Jiang",
      "Awais Ahmad Siddiqi",
      "Yunhao Liu"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15351"
  },
  {
    "id": "arXiv:2209.15352",
    "title": "AudioGen: Textually Guided Audio Generation",
    "abstract": "We tackle the problem of generating audio samples conditioned on descriptive\ntext captions. In this work, we propose AaudioGen, an auto-regressive\ngenerative model that generates audio samples conditioned on text inputs.\nAudioGen operates on a learnt discrete audio representation. The task of\ntext-to-audio generation poses multiple challenges. Due to the way audio\ntravels through a medium, differentiating ``objects'' can be a difficult task\n(e.g., separating multiple people simultaneously speaking). This is further\ncomplicated by real-world recording conditions (e.g., background noise,\nreverberation, etc.). Scarce text annotations impose another constraint,\nlimiting the ability to scale models. Finally, modeling high-fidelity audio\nrequires encoding audio at high sampling rate, leading to extremely long\nsequences. To alleviate the aforementioned challenges we propose an\naugmentation technique that mixes different audio samples, driving the model to\ninternally learn to separate multiple sources. We curated 10 datasets\ncontaining different types of audio and text annotations to handle the scarcity\nof text-audio data points. For faster inference, we explore the use of\nmulti-stream modeling, allowing the use of shorter sequences while maintaining\na similar bitrate and perceptual quality. We apply classifier-free guidance to\nimprove adherence to text. Comparing to the evaluated baselines, AudioGen\noutperforms over both objective and subjective metrics. Finally, we explore the\nability of the proposed method to generate audio continuation conditionally and\nunconditionally. Samples: https://tinyurl.com/audiogen-text2audio",
    "descriptor": "",
    "authors": [
      "Felix Kreuk",
      "Gabriel Synnaeve",
      "Adam Polyak",
      "Uriel Singer",
      "Alexandre D\u00e9fossez",
      "Jade Copet",
      "Devi Parikh",
      "Yaniv Taigman",
      "Yossi Adi"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15352"
  },
  {
    "id": "arXiv:2209.15362",
    "title": "Towards End-to-end Handwritten Document Recognition",
    "abstract": "Handwritten text recognition has been widely studied in the last decades for\nits numerous applications. Nowadays, the state-of-the-art approach consists in\na three-step process. The document is segmented into text lines, which are then\nordered and recognized. However, this three-step approach has many drawbacks.\nThe three steps are treated independently whereas they are closely related.\nErrors accumulate from one step to the other. The ordering step is based on\nheuristic rules which prevent its use for documents with a complex layouts or\nfor heterogeneous documents. The need for additional physical segmentation\nannotations for training the segmentation stage is inherent to this approach.\nIn this thesis, we propose to tackle these issues by performing the handwritten\ntext recognition of whole document in an end-to-end way. To this aim, we\ngradually increase the difficulty of the recognition task, moving from isolated\nlines to paragraphs, and then to whole documents. We proposed an approach at\nthe line level, based on a fully convolutional network, in order to design a\nfirst generic feature extraction step for the handwriting recognition task.\nBased on this preliminary work, we studied two different approaches to\nrecognize handwritten paragraphs. We reached state-of-the-art results at\nparagraph level on the RIMES 2011, IAM and READ 2016 datasets and outperformed\nthe line-level state of the art on these datasets. We finally proposed the\nfirst end-to-end approach dedicated to the recognition of both text and layout,\nat document level. Characters and layout tokens are sequentially predicted\nfollowing a learned reading order. We proposed two new metrics we used to\nevaluate this task on the RIMES 2009 and READ 2016 dataset, at page level and\ndouble-page level.",
    "descriptor": "\nComments: Ph.D Thesis\n",
    "authors": [
      "Denis Coquenet"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15362"
  },
  {
    "id": "arXiv:2209.15367",
    "title": "Efficient computation of the Knowledge Gradient for Bayesian  Optimization",
    "abstract": "Bayesian optimization is a powerful collection of methods for optimizing\nstochastic expensive black box functions. One key component of a Bayesian\noptimization algorithm is the acquisition function that determines which\nsolution should be evaluated in every iteration. A popular and very effective\nchoice is the Knowledge Gradient acquisition function, however there is no\nanalytical way to compute it. Several different implementations make different\napproximations. In this paper, we review and compare the spectrum of Knowledge\nGradient implementations and propose One-shot Hybrid KG, a new approach that\ncombines several of the previously proposed ideas and is cheap to compute as\nwell as powerful and efficient. We prove the new method preserves theoretical\nproperties of previous methods and empirically show the drastically reduced\ncomputational overhead with equal or improved performance. All experiments are\nimplemented in BOTorch and code is available on github.",
    "descriptor": "",
    "authors": [
      "Juan Ungredda",
      "Michael Pearce",
      "Juergen Branke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15367"
  },
  {
    "id": "arXiv:2209.15368",
    "title": "Inharmonious Region Localization by Magnifying Domain Discrepancy",
    "abstract": "Inharmonious region localization aims to localize the region in a synthetic\nimage which is incompatible with surrounding background. The inharmony issue is\nmainly attributed to the color and illumination inconsistency produced by image\nediting techniques. In this work, we tend to transform the input image to\nanother color space to magnify the domain discrepancy between inharmonious\nregion and background, so that the model can identify the inharmonious region\nmore easily. To this end, we present a novel framework consisting of a color\nmapping module and an inharmonious region localization network, in which the\nformer is equipped with a novel domain discrepancy magnification loss and the\nlatter could be an arbitrary localization network. Extensive experiments on\nimage harmonization dataset show the superiority of our designed framework. Our\ncode is available at\nhttps://github.com/bcmi/MadisNet-Inharmonious-Region-Localization.",
    "descriptor": "",
    "authors": [
      "Jing Liang",
      "Li Niu",
      "Penghao Wu",
      "Fengjun Guo",
      "Teng Long"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15368"
  },
  {
    "id": "arXiv:2209.15369",
    "title": "Towards effective assessment of steady state performance in Java  software: Are we there yet?",
    "abstract": "Microbenchmarking is a widely used form of performance testing in Java\nsoftware. A microbenchmark repeatedly executes a small chunk of code while\ncollecting measurements related to its performance. Due to Java Virtual Machine\noptimizations, microbenchmarks are usually subject to severe performance\nfluctuations in the first phase of their execution (also known as warmup). For\nthis reason, software developers typically discard measurements of this phase\nand focus their analysis when benchmarks reach a steady state of performance.\nDevelopers estimate the end of the warmup phase based on their expertise, and\nconfigure their benchmarks accordingly. Unfortunately, this approach is based\non two strong assumptions: (i) benchmarks always reach a steady state of\nperformance and (ii) developers accurately estimate warmup. In this paper, we\nshow that Java microbenchmarks do not always reach a steady state, and often\ndevelopers fail to accurately estimate the end of the warmup phase. We found\nthat a considerable portion of studied benchmarks do not hit the steady state,\nand warmup estimates provided by software developers are often inaccurate (with\na large error). This has significant implications both in terms of results\nquality and time-effort. Furthermore, we found that dynamic reconfiguration\nsignificantly improves warmup estimation accuracy, but still it induces\nsuboptimal warmup estimates and relevant side-effects. We envision this paper\nas a starting point for supporting the introduction of more sophisticated\nautomated techniques that can ensure results quality in a timely fashion.",
    "descriptor": "\nComments: Accepted for publication in Empirical Software Engineering (EMSE)\n",
    "authors": [
      "Luca Traini",
      "Vittorio Cortellessa",
      "Daniele Di Pompeo",
      "Michele Tucci"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.15369"
  },
  {
    "id": "arXiv:2209.15370",
    "title": "Automatic Context-Driven Inference of Engagement in HMI: A Survey",
    "abstract": "An integral part of seamless human-human communication is engagement, the\nprocess by which two or more participants establish, maintain, and end their\nperceived connection. Therefore, to develop successful human-centered\nhuman-machine interaction applications, automatic engagement inference is one\nof the tasks required to achieve engaging interactions between humans and\nmachines, and to make machines attuned to their users, hence enhancing user\nsatisfaction and technology acceptance. Several factors contribute to\nengagement state inference, which include the interaction context and\ninteractants' behaviours and identity. Indeed, engagement is a multi-faceted\nand multi-modal construct that requires high accuracy in the analysis and\ninterpretation of contextual, verbal and non-verbal cues. Thus, the development\nof an automated and intelligent system that accomplishes this task has been\nproven to be challenging so far. This paper presents a comprehensive survey on\nprevious work in engagement inference for human-machine interaction, entailing\ninterdisciplinary definition, engagement components and factors, publicly\navailable datasets, ground truth assessment, and most commonly used features\nand methods, serving as a guide for the development of future human-machine\ninteraction interfaces with reliable context-aware engagement inference\ncapability. An in-depth review across embodied and disembodied interaction\nmodes, and an emphasis on the interaction context of which engagement\nperception modules are integrated sets apart the presented survey from existing\nsurveys.",
    "descriptor": "",
    "authors": [
      "Hanan Salam",
      "Oya Celiktutan",
      "Hatice Gunes",
      "Mohamed Chetouani"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15370"
  },
  {
    "id": "arXiv:2209.15373",
    "title": "PART: Pre-trained Authorship Representation Transformer",
    "abstract": "Authors writing documents imprint identifying information within their texts:\nvocabulary, registry, punctuation, misspellings, or even emoji usage. Finding\nthese details is very relevant to profile authors, relating back to their\ngender, occupation, age, and so on. But most importantly, repeating writing\npatterns can help attributing authorship to a text. Previous works use\nhand-crafted features or classification tasks to train their authorship models,\nleading to poor performance on out-of-domain authors. A better approach to this\ntask is to learn stylometric representations, but this by itself is an open\nresearch challenge. In this paper, we propose PART: a contrastively trained\nmodel fit to learn \\textbf{authorship embeddings} instead of semantics. By\ncomparing pairs of documents written by the same author, we are able to\ndetermine the proprietary of a text by evaluating the cosine similarity of the\nevaluated documents, a zero-shot generalization to authorship identification.\nTo this end, a pre-trained Transformer with an LSTM head is trained with the\ncontrastive training method. We train our model on a diverse set of authors,\nfrom literature, anonymous blog posters and corporate emails; a heterogeneous\nset with distinct and identifiable writing styles. The model is evaluated on\nthese datasets, achieving zero-shot 72.39\\% and 86.73\\% accuracy and top-5\naccuracy respectively on the joint evaluation dataset when determining\nauthorship from a set of 250 different authors. We qualitatively assess the\nrepresentations with different data visualizations on the available datasets,\nprofiling features such as book types, gender, age, or occupation of the\nauthor.",
    "descriptor": "",
    "authors": [
      "Javier Huertas-Tato",
      "Alvaro Huertas-Garcia",
      "Alejandro Martin",
      "David Camacho"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15373"
  },
  {
    "id": "arXiv:2209.15376",
    "title": "Viewpoint Planning based on Shape Completion for Fruit Mapping and  Reconstruction",
    "abstract": "Robotic systems in agriculture do not only enable increasing automation of\nfarming activities but also represent new challenges for robotics due to the\nunstructured environment and the non-rigid structures of crops. Especially,\nactive perception for fruit mapping and harvesting is a difficult task since\nocclusions frequently occur and image segmentation provides only limited\naccuracy on the actual shape of the fruits. In this paper, we present a\nviewpoint planning approach that explictly uses the shape prediction from\ncollected data to guide the sensor to view as yet unobserved parts of the\nfruits. We developed a novel pipeline for continuous interaction between\nprediction and observation to maximize the information gain about sweet pepper\nfruits. We adapted two different shape prediction approaches, namely parametric\nsuperellipsoid fitting and model based non-rigid latent space registration, and\nintegrated them into our Region of Interest (RoI) viewpoint planner.\nAdditionally, we used a new concept of viewpoint dissimilarity to aid the\nplanner to select good viewpoints and for shortening the planning times. Our\nsimulation experiments with a UR5e arm equipped with a Realsense L515 sensor\nprovide a quantitative demonstration of the efficacy of our iterative shape\ncompletion based viewpoint planning. In comparative experiments with a\nstate-of-the-art viewpoint planner, we demonstrate improvement not only in the\nestimation of the fruit sizes, but also in their reconstruction. Finally, we\nshow the viability of our approach for mapping sweet peppers with a real\nrobotic system in a commercial glasshouse.",
    "descriptor": "\nComments: Agricultural Automation, Viewpoint Planning, Active Perception\n",
    "authors": [
      "Rohit Menon",
      "Tobias Zaenker",
      "Maren Bennewitz"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15376"
  },
  {
    "id": "arXiv:2209.15380",
    "title": "Improve learning combining crowdsourced labels by weighting Areas Under  the Margin",
    "abstract": "In supervised learning -- for instance in image classification -- modern\nmassive datasets are commonly labeled by a crowd of workers. The obtained\nlabels in this crowdsourcing setting are then aggregated for training. The\naggregation step generally leverages a per worker trust score. Yet, such\nworker-centric approaches discard each task ambiguity. Some intrinsically\nambiguous tasks might even fool expert workers, which could eventually be\nharmful for the learning step. In a standard supervised learning setting --\nwith one label per task and balanced classes -- the Area Under the Margin (AUM)\nstatistic is tailored to identify mislabeled data. We adapt the AUM to identify\nambiguous tasks in crowdsourced learning scenarios, introducing the Weighted\nAUM (WAUM). The WAUM is an average of AUMs weighted by worker and task\ndependent scores. We show that the WAUM can help discarding ambiguous tasks\nfrom the training set, leading to better generalization or calibration\nperformance. We report improvements with respect to feature-blind aggregation\nstrategies both for simulated settings and for the CIFAR-10H crowdsourced\ndataset.",
    "descriptor": "",
    "authors": [
      "Tanguy Lefort",
      "Benjamin Charlier",
      "Alexis Joly",
      "Joseph Salmon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15380"
  },
  {
    "id": "arXiv:2209.15382",
    "title": "Linear Convergence for Natural Policy Gradient with Log-linear Policy  Parametrization",
    "abstract": "We analyze the convergence rate of the unregularized natural policy gradient\nalgorithm with log-linear policy parametrizations in infinite-horizon\ndiscounted Markov decision processes. In the deterministic case, when the\nQ-value is known and can be approximated by a linear combination of a known\nfeature function up to a bias error, we show that a geometrically-increasing\nstep size yields a linear convergence rate towards an optimal policy. We then\nconsider the sample-based case, when the best representation of the Q- value\nfunction among linear combinations of a known feature function is known up to\nan estimation error. In this setting, we show that the algorithm enjoys the\nsame linear guarantees as in the deterministic case up to an error term that\ndepends on the estimation error, the bias error, and the condition number of\nthe feature covariance matrix. Our results build upon the general framework of\npolicy mirror descent and extend previous findings for the softmax tabular\nparametrization to the log-linear policy class.",
    "descriptor": "",
    "authors": [
      "Carlo Alfano",
      "Patrick Rebeschini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2209.15382"
  },
  {
    "id": "arXiv:2209.15383",
    "title": "Semi-Supervised Single-View 3D Reconstruction via Prototype Shape Priors",
    "abstract": "The performance of existing single-view 3D reconstruction methods heavily\nrelies on large-scale 3D annotations. However, such annotations are tedious and\nexpensive to collect. Semi-supervised learning serves as an alternative way to\nmitigate the need for manual labels, but remains unexplored in 3D\nreconstruction. Inspired by the recent success of semi-supervised image\nclassification tasks, we propose SSP3D, a semi-supervised framework for 3D\nreconstruction. In particular, we introduce an attention-guided prototype shape\nprior module for guiding realistic object reconstruction. We further introduce\na discriminator-guided module to incentivize better shape generation, as well\nas a regularizer to tolerate noisy training samples. On the ShapeNet benchmark,\nthe proposed approach outperforms previous supervised methods by clear margins\nunder various labeling ratios, (i.e., 1%, 5% , 10% and 20%). Moreover, our\napproach also performs well when transferring to real-world Pix3D datasets\nunder labeling ratios of 10%. We also demonstrate our method could transfer to\nnovel categories with few novel supervised data. Experiments on the popular\nShapeNet dataset show that our method outperforms the zero-shot baseline by\nover 12% and we also perform rigorous ablations and analysis to validate our\napproach.",
    "descriptor": "\nComments: Accepeted by ECCV2022\n",
    "authors": [
      "Zhen Xing",
      "Hengduo Li",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15383"
  },
  {
    "id": "arXiv:2209.15390",
    "title": "Deploying a sharded MongoDB cluster as a queued job on a shared HPC  architecture",
    "abstract": "Data stores are the foundation on which data science, in all its variations,\nis built upon. They provide a queryable interface to structured and\nunstructured data. Data science often starts by leveraging these query features\nto perform initial data preparation. However, most data stores are designed to\nrun continuously to service disparate user requests with little or no downtime.\nMany HPC architectures process user requests by job queue scheduler and\nmaintain a shard filesystem to store a jobs persistent data. We deploy a\nMongoDB sharded cluster with a run script that is designed to run a data\nscience workload concurrently. As our test piece, we run data ingest and data\nqueries to measure the performance with different configurations on the Blue\nWaters supper computer.",
    "descriptor": "",
    "authors": [
      "Aaron Saxton",
      "Stephen Squaire"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2209.15390"
  },
  {
    "id": "arXiv:2209.15393",
    "title": "Programmable Control of Ultrasound Swarmbots through Reinforcement  Learning",
    "abstract": "Powered by acoustics, existing therapeutic and diagnostic procedures will\nbecome less invasive and new methods will become available that have never been\navailable before. Acoustically driven microrobot navigation based on\nmicrobubbles is a promising approach for targeted drug delivery. Previous\nstudies have used acoustic techniques to manipulate microbubbles in vitro and\nin vivo for the delivery of drugs using minimally invasive procedures. Even\nthough many advanced capabilities and sophisticated control have been achieved\nfor acoustically powered microrobots, there remain many challenges that remain\nto be solved. In order to develop the next generation of intelligent\nmicro/nanorobots, it is highly desirable to conduct accurate identification of\nthe micro-nanorobots and to control their dynamic motion autonomously. Here we\nuse reinforcement learning control strategies to learn the microrobot dynamics\nand manipulate them through acoustic forces. The result demonstrated for the\nfirst time autonomous acoustic navigation of microbubbles in a microfluidic\nenvironment. Taking advantage of the benefit of the second radiation force,\nmicrobubbles swarm to form a large swarm, which is then driven along the\ndesired trajectory. More than 100 thousand images were used for the training to\nstudy the unexpected dynamics of microbubbles. As a result of this work, the\nmicrorobots are validated to be controlled, illustrating a good level of\nrobustness and providing computational intelligence to the microrobots, which\nenables them to navigate independently in an unstructured environment without\nrequiring outside assistance.",
    "descriptor": "",
    "authors": [
      "Matthijs Schrage",
      "Mahmoud Medany",
      "Daniel Ahmed"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15393"
  },
  {
    "id": "arXiv:2209.15396",
    "title": "Broadening the Complexity-theoretic Analysis of Manipulative Attacks in  Group Identification",
    "abstract": "In the Group Identification problem, we are given a set of individuals and\nare asked to identify a socially qualified subset among them. Each individual\nin the set has an opinion about who should be considered socially qualified.\nThere are several different rules that can be used to determine the socially\nqualified subset based on these mutual opinions. In a manipulative attack, an\noutsider attempts to exploit the way the used rule works, with the goal of\nchanging the outcome of the selection process to their liking.\nIn recent years, the complexity of group control and bribery based\nmanipulative attacks in Group Identification has been the subject of intense\nresearch. However, the picture is far from complete, and there remain many open\nquestions related to what exactly makes certain problems hard, or certain rules\nimmune to some attacks.\nSupplementing previous results, we examine the complexity of group\nmicrobribery on so-called protective problem instances; that is, instances\nwhere all individuals from the constructive target set are already socially\nqualified initially. In addition, we study a relaxed variant of group control\nby deleting individuals for the consent rules, the consensus-start-respecting\nrule, and the liberal-start-respecting rule. Based on existing literature, we\nalso formalize three new social rules of the iterative consensus type, and we\nprovide a comprehensive complexity-theoretic analysis of group control and\nbribery problems for these rules.",
    "descriptor": "\nComments: 93 pages, 8 figures, 3 tables\n",
    "authors": [
      "Emil Junker"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2209.15396"
  },
  {
    "id": "arXiv:2209.15397",
    "title": "KISS-ICP: In Defense of Point-to-Point ICP -- Simple, Accurate, and  Robust Registration If Done the Right Way",
    "abstract": "Robust and accurate pose estimation of a robotic platform, so-called\nsensor-based odometry, is an essential part of many robotic applications. While\nmany sensor odometry systems made progress by adding more complexity to the\nego-motion estimation process, we move in the opposite direction. By removing a\nmajority of parts and focusing on the core elements, we obtain a surprisingly\neffective system that is simple to realize and can operate under various\nenvironmental conditions using different LiDAR sensors. Our odometry estimation\napproach relies on point-to-point ICP combined with adaptive thresholding for\ncorrespondence matching, a robust kernel, a simple but widely applicable motion\ncompensation approach, and a point cloud subsampling strategy. This yields a\nsystem with only a few parameters that in most cases do not even have to be\ntuned to a specific LiDAR sensor. Our system using the same parameters performs\non par with state-of-the-art methods under various operating conditions using\ndifferent platforms: automotive platforms, UAV-based operation, vehicles like\nsegways, or handheld LiDARs. We do not require integrating IMU information and\nsolely rely on 3D point cloud data obtained from a wide range of 3D LiDAR\nsensors, thus, enabling a broad spectrum of different applications and\noperating conditions. Our open-source system operates faster than the sensor\nframe rate in all presented datasets and is designed for real-world scenarios.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Ignacio Vizzo",
      "Tiziano Guadagnino",
      "Benedikt Mersch",
      "Louis Wiesmann",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15397"
  },
  {
    "id": "arXiv:2209.15398",
    "title": "Evaluation of importance estimators in deep learning classifiers for  Computed Tomography",
    "abstract": "Deep learning has shown superb performance in detecting objects and\nclassifying images, ensuring a great promise for analyzing medical imaging.\nTranslating the success of deep learning to medical imaging, in which doctors\nneed to understand the underlying process, requires the capability to interpret\nand explain the prediction of neural networks. Interpretability of deep neural\nnetworks often relies on estimating the importance of input features (e.g.,\npixels) with respect to the outcome (e.g., class probability). However, a\nnumber of importance estimators (also known as saliency maps) have been\ndeveloped and it is unclear which ones are more relevant for medical imaging\napplications. In the present work, we investigated the performance of several\nimportance estimators in explaining the classification of computed tomography\n(CT) images by a convolutional deep network, using three distinct evaluation\nmetrics. First, the model-centric fidelity measures a decrease in the model\naccuracy when certain inputs are perturbed. Second, concordance between\nimportance scores and the expert-defined segmentation masks is measured on a\npixel level by a receiver operating characteristic (ROC) curves. Third, we\nmeasure a region-wise overlap between a XRAI-based map and the segmentation\nmask by Dice Similarity Coefficients (DSC). Overall, two versions of SmoothGrad\ntopped the fidelity and ROC rankings, whereas both Integrated Gradients and\nSmoothGrad excelled in DSC evaluation. Interestingly, there was a critical\ndiscrepancy between model-centric (fidelity) and human-centric (ROC and DSC)\nevaluation. Expert expectation and intuition embedded in segmentation maps does\nnot necessarily align with how the model arrived at its prediction.\nUnderstanding this difference in interpretability would help harnessing the\npower of deep learning in medicine.",
    "descriptor": "\nComments: 4th International Workshop on EXplainable and TRAnsparent AI and Multi-Agent Systems (EXTRAAMAS 2022) - International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)\n",
    "authors": [
      "Lennart Brocki",
      "Wistan Marchadour",
      "Jonas Maison",
      "Bogdan Badic",
      "Panagiotis Papadimitroulas",
      "Mathieu Hatt",
      "Franck Vermet",
      "Neo Christopher Chung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15398"
  },
  {
    "id": "arXiv:2209.15399",
    "title": "Parea: multi-view ensemble clustering for cancer subtype discovery",
    "abstract": "Multi-view clustering methods are essential for the stratification of\npatients into sub-groups of similar molecular characteristics. In recent years,\na wide range of methods has been developed for this purpose. However, due to\nthe high diversity of cancer-related data, a single method may not perform\nsufficiently well in all cases. We present Parea, a multi-view hierarchical\nensemble clustering approach for disease subtype discovery. We demonstrate its\nperformance on several machine learning benchmark datasets. We apply and\nvalidate our methodology on real-world multi-view cancer patient data. Parea\noutperforms the current state-of-the-art on six out of seven analysed cancer\ntypes. We have integrated the Parea method into our developed Python package\nPyrea (https://github.com/mdbloice/Pyrea), which enables the effortless and\nflexible design of ensemble workflows while incorporating a wide range of\nfusion and clustering algorithms.",
    "descriptor": "",
    "authors": [
      "Bastian Pfeifer",
      "Marcus D. Bloice",
      "Michael G. Schimek"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Quantitative Methods (q-bio.QM)"
    ],
    "url": "https://arxiv.org/abs/2209.15399"
  },
  {
    "id": "arXiv:2209.15402",
    "title": "Rethinking the Learning Paradigm for Facial Expression Recognition",
    "abstract": "Due to the subjective crowdsourcing annotations and the inherent inter-class\nsimilarity of facial expressions, the real-world Facial Expression Recognition\n(FER) datasets usually exhibit ambiguous annotation. To simplify the learning\nparadigm, most previous methods convert ambiguous annotation results into\nprecise one-hot annotations and train FER models in an end-to-end supervised\nmanner. In this paper, we rethink the existing training paradigm and propose\nthat it is better to use weakly supervised strategies to train FER models with\noriginal ambiguous annotation.",
    "descriptor": "",
    "authors": [
      "Weijie Wang",
      "Nicu Sebe",
      "Bruno Lepri"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15402"
  },
  {
    "id": "arXiv:2209.15404",
    "title": "An information-theoretic approach to unsupervised keypoint  representation learning",
    "abstract": "Extracting informative representations from videos is fundamental for the\neffective learning of various downstream tasks. Inspired by classical works on\nsaliency, we present a novel information-theoretic approach to discover\nmeaningful representations from videos in an unsupervised fashion. We argue\nthat local entropy of pixel neighborhoods and its evolution in a video stream\nis a valuable intrinsic supervisory signal for learning to attend to salient\nfeatures. We, thus, abstract visual features into a concise representation of\nkeypoints that serve as dynamic information transporters. We discover in an\nunsupervised fashion spatio-temporally consistent keypoint representations that\ncarry the prominent information across video frames, thanks to two original\ninformation-theoretic losses. First, a loss that maximizes the information\ncovered by the keypoints in a frame. Second, a loss that encourages optimized\nkeypoint transportation over time, thus, imposing consistency of the\ninformation flow. We evaluate our keypoint-based representation compared to\nstate-of-the-art baselines in different downstream tasks such as learning\nobject dynamics. To evaluate the expressivity and consistency of the keypoints,\nwe propose a new set of metrics. Our empirical results showcase the superior\nperformance of our information-driven keypoints that resolve challenges like\nattendance to both static and dynamic objects, and to objects abruptly entering\nand leaving the scene.",
    "descriptor": "",
    "authors": [
      "Ali Younes",
      "Simone Schaub-Meyer",
      "Georgia Chalvatzaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15404"
  },
  {
    "id": "arXiv:2209.15406",
    "title": "Emulating On-Orbit Interactions Using Forward Dynamics Based Cartesian  Motion",
    "abstract": "The paper presents a novel Hardware-In-the-Loop (HIL) emulation framework of\non-orbit interactions using on-ground robotic manipulators. It combines Virtual\nForward Dynamic Model (VFDM) for Cartesian motion control of robotic\nmanipulators with an Orbital Dynamics Simulator (ODS) based on the Clohessy\nWiltshire (CW) Model. VFDM-based Inverse Kinematics (IK) solver is known to\nhave better motion tracking, path accuracy, and solver convergency than\ntraditional IK solvers. Therefore it provides a stable Cartesian motion for\nmanipulator-based HIL on-orbit emulations. The framework is tested on a\nROS-based robotics testbed to emulate two scenarios: free-floating satellite\nmotion and free-floating interaction (collision). Mock-ups of two satellites\nare mounted at the robots' end-effectors. Forces acting on the mock-ups are\nmeasured through an in-built F/T sensor on each robotic arm. During the tests,\nthe relative motion of the mock-ups is expressed with respect to a moving\nobserver rotating at a fixed angular velocity in a circular orbit rather than\ntheir motion in the inertial frame. The ODS incorporates the force and torque\nvalues on the fly and delivers the corresponding satellite motions to the\nvirtual forward dynamics model as online trajectories. Results are comparable\nto other free-floating HIL emulators. Fidelity between the simulated motion and\nrobot-mounted mock-up motion is confirmed.",
    "descriptor": "\nComments: Submitted to ICRA2023, for associated video, see: this https URL\n",
    "authors": [
      "Mohatashem Reyaz Makhdoomi",
      "Vivek Muralidharan",
      "Kuldeep R. Barad",
      "Juan Sandoval",
      "Miguel Olivares-Mendez",
      "Carol Martinez"
    ],
    "subjectives": [
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15406"
  },
  {
    "id": "arXiv:2209.15407",
    "title": "Crocs: Cross-Technology Clock Synchronization for WiFi and ZigBee",
    "abstract": "Clock synchronization is a key function in embedded wireless systems and\nnetworks. This issue is equally important and more challenging in IoT systems\nnowadays, which often include heterogeneous wireless devices that follow\ndifferent wireless standards. Conventional solutions to this problem employ\ngateway-based indirect synchronization, which suffers low accuracy. This paper\nfor the first time studies the problem of cross-technology clock\nsynchronization. Our proposal called Crocs synchronizes WiFi and ZigBee devices\nby direct cross-technology communication. Crocs decouples the synchronization\nsignal from the transmission of a timestamp. By incorporating a barker-code\nbased beacon for time alignment and cross-technology transmission of\ntimestamps, Crocs achieves robust and accurate synchronization among WiFi and\nZigBee devices, with the synchronization error lower than 1 millisecond. We\nfurther make attempts to implement different cross-technology communication\nmethods in Crocs and provide insight findings with regard to the achievable\naccuracy and expected overhead.",
    "descriptor": "",
    "authors": [
      "Zihao Yu",
      "Chengkun Jiang",
      "Yuan He",
      "Xiaolong Zheng",
      "Xiuzhen Guo"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.15407"
  },
  {
    "id": "arXiv:2209.15409",
    "title": "Higher-order Neural Additive Models: An Interpretable Machine Learning  Model with Feature Interactions",
    "abstract": "Black-box models, such as deep neural networks, exhibit superior predictive\nperformances, but understanding their behavior is notoriously difficult. Many\nexplainable artificial intelligence methods have been proposed to reveal the\ndecision-making processes of black box models. However, their applications in\nhigh-stakes domains remain limited. Recently proposed neural additive models\n(NAM) have achieved state-of-the-art interpretable machine learning. NAM can\nprovide straightforward interpretations with slight performance sacrifices\ncompared with multi-layer perceptron. However, NAM can only model\n1$^{\\text{st}}$-order feature interactions; thus, it cannot capture the\nco-relationships between input features. To overcome this problem, we propose a\nnovel interpretable machine learning method called higher-order neural additive\nmodels (HONAM) and a feature interaction method for high interpretability.\nHONAM can model arbitrary orders of feature interactions. Therefore, it can\nprovide the high predictive performance and interpretability that high-stakes\ndomains need. In addition, we propose a novel hidden unit to effectively learn\nsharp-shape functions. We conducted experiments using various real-world\ndatasets to examine the effectiveness of HONAM. Furthermore, we demonstrate\nthat HONAM can achieve fair AI with a slight performance sacrifice. The source\ncode for HONAM is publicly available.",
    "descriptor": "",
    "authors": [
      "Minkyu Kim",
      "Hyun-Soo Choi",
      "Jinho Kim"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15409"
  },
  {
    "id": "arXiv:2209.15410",
    "title": "P vs NP",
    "abstract": "In this paper, we discuss the $\\mathcal{NP}$ problem using the Henkin's\nTheory and the Herbrand Theory in the first-order logic, and prove that\n$\\mathcal{P}$ is a proper subset of $\\mathcal{NP}$.",
    "descriptor": "",
    "authors": [
      "Jian-Gang Tang"
    ],
    "subjectives": [
      "Computational Complexity (cs.CC)",
      "Logic (math.LO)"
    ],
    "url": "https://arxiv.org/abs/2209.15410"
  },
  {
    "id": "arXiv:2209.15416",
    "title": "Optimal Efficiency-Envy Trade-Off via Optimal Transport",
    "abstract": "We consider the problem of allocating a distribution of items to $n$\nrecipients where each recipient has to be allocated a fixed, prespecified\nfraction of all items, while ensuring that each recipient does not experience\ntoo much envy. We show that this problem can be formulated as a variant of the\nsemi-discrete optimal transport (OT) problem, whose solution structure in this\ncase has a concise representation and a simple geometric interpretation. Unlike\nexisting literature that treats envy-freeness as a hard constraint, our\nformulation allows us to \\emph{optimally} trade off efficiency and envy\ncontinuously. Additionally, we study the statistical properties of the space of\nour OT based allocation policies by showing a polynomial bound on the number of\nsamples needed to approximate the optimal solution from samples. Our approach\nis suitable for large-scale fair allocation problems such as the blood donation\nmatching problem, and we show numerically that it performs well on a prior\nrealistic data simulator.",
    "descriptor": "",
    "authors": [
      "Steven Yin",
      "Christian Kroer"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Data Structures and Algorithms (cs.DS)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.15416"
  },
  {
    "id": "arXiv:2209.15418",
    "title": "Equitable Marketplace Mechanism Design",
    "abstract": "We consider a trading marketplace that is populated by traders with diverse\ntrading strategies and objectives. The marketplace allows the suppliers to list\ntheir goods and facilitates matching between buyers and sellers. In return,\nsuch a marketplace typically charges fees for facilitating trade. The goal of\nthis work is to design a dynamic fee schedule for the marketplace that is\nequitable and profitable to all traders while being profitable to the\nmarketplace at the same time (from charging fees). Since the traders adapt\ntheir strategies to the fee schedule, we present a reinforcement learning\nframework for simultaneously learning a marketplace fee schedule and trading\nstrategies that adapt to this fee schedule using a weighted optimization\nobjective of profits and equitability. We illustrate the use of the proposed\napproach in detail on a simulated stock exchange with different types of\ninvestors, specifically market makers and consumer investors. As we vary the\nequitability weights across different investor classes, we see that the learnt\nexchange fee schedule starts favoring the class of investors with the highest\nweight. We further discuss the observed insights from the simulated stock\nexchange in light of the general framework of equitable marketplace mechanism\ndesign.",
    "descriptor": "",
    "authors": [
      "Kshama Dwarakanath",
      "Svitlana S Vyetrenko",
      "Tucker Balch"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Machine Learning (cs.LG)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2209.15418"
  },
  {
    "id": "arXiv:2209.15421",
    "title": "TabDDPM: Modelling Tabular Data with Diffusion Models",
    "abstract": "Denoising diffusion probabilistic models are currently becoming the leading\nparadigm of generative modeling for many important data modalities. Being the\nmost prevalent in the computer vision community, diffusion models have also\nrecently gained some attention in other domains, including speech, NLP, and\ngraph-like data. In this work, we investigate if the framework of diffusion\nmodels can be advantageous for general tabular problems, where datapoints are\ntypically represented by vectors of heterogeneous features. The inherent\nheterogeneity of tabular data makes it quite challenging for accurate modeling,\nsince the individual features can be of completely different nature, i.e., some\nof them can be continuous and some of them can be discrete. To address such\ndata types, we introduce TabDDPM -- a diffusion model that can be universally\napplied to any tabular dataset and handles any type of feature. We extensively\nevaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority\nover existing GAN/VAE alternatives, which is consistent with the advantage of\ndiffusion models in other fields. Additionally, we show that TabDDPM is\neligible for privacy-oriented setups, where the original datapoints cannot be\npublicly shared.",
    "descriptor": "\nComments: code this https URL\n",
    "authors": [
      "Akim Kotelnikov",
      "Dmitry Baranchuk",
      "Ivan Rubachev",
      "Artem Babenko"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15421"
  },
  {
    "id": "arXiv:2209.15425",
    "title": "Spikformer: When Spiking Neural Network Meets Transformer",
    "abstract": "We consider two biologically plausible structures, the Spiking Neural Network\n(SNN) and the self-attention mechanism. The former offers an energy-efficient\nand event-driven paradigm for deep learning, while the latter has the ability\nto capture feature dependencies, enabling Transformer to achieve good\nperformance. It is intuitively promising to explore the marriage between them.\nIn this paper, we consider leveraging both self-attention capability and\nbiological properties of SNNs, and propose a novel Spiking Self Attention (SSA)\nas well as a powerful framework, named Spiking Transformer (Spikformer). The\nSSA mechanism in Spikformer models the sparse visual feature by using\nspike-form Query, Key, and Value without softmax. Since its computation is\nsparse and avoids multiplication, SSA is efficient and has low computational\nenergy consumption. It is shown that Spikformer with SSA can outperform the\nstate-of-the-art SNNs-like frameworks in image classification on both\nneuromorphic and static datasets. Spikformer (66.3M parameters) with comparable\nsize to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on\nImageNet using 4 time steps, which is the state-of-the-art in directly trained\nSNNs models.",
    "descriptor": "",
    "authors": [
      "Zhaokun Zhou",
      "Yuesheng Zhu",
      "Chao He",
      "Yaowei Wang",
      "Shuicheng Yan",
      "Yonghong Tian",
      "Li Yuan"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15425"
  },
  {
    "id": "arXiv:2209.15427",
    "title": "Tuning of Mixture-of-Experts Mixed-Precision Neural Networks",
    "abstract": "Deep learning has become a useful data analysis method, however mainstream\nadaption in distributed computer software and embedded devices has been low so\nfar. Often, adding deep learning inference in mainstream applications and\ndevices requires new hardware with signal processors suited for convolutional\nneural networks. This work adds new data types (quantized 16-bit and 8-bit\ninteger, 16-bit floating point) to Caffe in order to save memory and increase\ninference speed on existing commodity graphics processors with OpenCL, common\nin everyday devices. Existing models can be executed effortlessly in\nmixed-precision mode. Additionally, we propose a variation of\nmixture-of-experts to increase inference speed on AlexNet for image\nclassification. We managed to decrease memory usage up to 3.29x while\nincreasing inference speed up to 3.01x on certain devices. We demonstrate with\nfive simple examples how the presented techniques can easily be applied to\ndifferent machine learning problems. The whole pipeline, consisting of models,\nexample python scripts and modified Caffe library, is available as Open Source\nsoftware.",
    "descriptor": "\nComments: 55 pages\n",
    "authors": [
      "Fabian Tschopp"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15427"
  },
  {
    "id": "arXiv:2209.15428",
    "title": "PyPose: A Library for Robot Learning with Physics-based Optimization",
    "abstract": "Deep learning has had remarkable success in robotic perception, but its\ndata-centric nature suffers when it comes to generalizing to ever-changing\nenvironments. By contrast, physics-based optimization generalizes better, but\nit does not perform as well in complicated tasks due to the lack of high-level\nsemantic information and the reliance on manual parametric tuning. To take\nadvantage of these two complementary worlds, we present PyPose: a\nrobotics-oriented, PyTorch-based library that combines deep perceptual models\nwith physics-based optimization techniques. Our design goal for PyPose is to\nmake it user-friendly, efficient, and interpretable with a tidy and\nwell-organized architecture. Using an imperative style interface, it can be\neasily integrated into real-world robotic applications. Besides, it supports\nparallel computing of any order gradients of Lie groups and Lie algebras and\n$2^{\\text{nd}}$-order optimizers, such as trust region methods. Experiments\nshow that PyPose achieves 3-20$\\times$ speedup in computation compared to\nstate-of-the-art libraries. To boost future research, we provide concrete\nexamples across several fields of robotics, including SLAM, inertial\nnavigation, planning, and control.",
    "descriptor": "",
    "authors": [
      "Chen Wang",
      "Dasong Gao",
      "Kuan Xu",
      "Junyi Geng",
      "Yaoyu Hu",
      "Yuheng Qiu",
      "Bowen Li",
      "Fan Yang",
      "Brady Moon",
      "Abhinav Pandey",
      "Aryan",
      "Jiahe Xu",
      "Tianhao Wu",
      "Haonan He",
      "Daning Huang",
      "Zhongqiang Ren",
      "Shibo Zhao",
      "Taimeng Fu",
      "Pranay Reddy",
      "Xiao Lin",
      "Wenshan Wang",
      "Jingnan Shi",
      "Rajat Talak",
      "Han Wang",
      "Huai Yu",
      "Shanzhao Wang",
      "Ananth Kashyap",
      "Rohan Bandaru",
      "Karthik Dantu",
      "Jiajun Wu",
      "Luca Carlone",
      "Marco Hutter",
      "Sebastian Scherer"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15428"
  },
  {
    "id": "arXiv:2209.15430",
    "title": "Relative representations enable zero-shot latent space communication",
    "abstract": "Neural networks embed the geometric structure of a data manifold lying in a\nhigh-dimensional space into latent representations. Ideally, the distribution\nof the data points in the latent space should depend only on the task, the\ndata, the loss, and other architecture-specific constraints. However, factors\nsuch as the random weights initialization, training hyperparameters, or other\nsources of randomness in the training phase may induce incoherent latent spaces\nthat hinder any form of reuse. Nevertheless, we empirically observe that, under\nthe same data and modeling choices, distinct latent spaces typically differ by\nan unknown quasi-isometric transformation: that is, in each space, the\ndistances between the encodings do not change. In this work, we propose to\nadopt pairwise similarities as an alternative data representation, that can be\nused to enforce the desired invariance without any additional training. We show\nhow neural architectures can leverage these relative representations to\nguarantee, in practice, latent isometry invariance, effectively enabling latent\nspace communication: from zero-shot model stitching to latent space comparison\nbetween diverse settings. We extensively validate the generalization capability\nof our approach on different datasets, spanning various modalities (images,\ntext, graphs), tasks (e.g., classification, reconstruction) and architectures\n(e.g., CNNs, GCNs, transformers).",
    "descriptor": "\nComments: 20 pages, 8 figures, 16 tables\n",
    "authors": [
      "Luca Moschella",
      "Valentino Maiorca",
      "Marco Fumero",
      "Antonio Norelli",
      "Francesco Locatello",
      "Emanuele Rodol\u00e0"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15430"
  },
  {
    "id": "arXiv:2209.15431",
    "title": "A discretization-convergent Level-Set-DEM",
    "abstract": "The recently developed level-set-DEM is able to seamlessly handle arbitrarily\nshaped grains and their contacts through a discrete level-set representation of\ngrains' volume and a node-based discretization of their bounding surfaces.\nHeretofore, the convergence properties of LS-DEM with refinement of these\ndiscretizations have not been examined. Here, we examine these properties and\nshow that the original LS-DEM diverges upon surface discretization refinement\ndue to its force-based discrete contact formulation. Next, we fix this issue by\nadopting a continuum-based contact formulation wherein the contact interactions\nare traction-based, and show that the adapted LS-DEM is fully discretization\nconvergent. Lastly, we discuss the significance of convergence in capturing the\nphysical response, as well as a few other convergence-related topics of\npractical importance.",
    "descriptor": "",
    "authors": [
      "Shai Feldfogel",
      "Konstantinos Karapiperis",
      "Jose Andrade",
      "David S. Kammer"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.15431"
  },
  {
    "id": "arXiv:2209.15438",
    "title": "Empowering the trustworthiness of ML-based critical systems through  engineering activities",
    "abstract": "This paper reviews the entire engineering process of trustworthy Machine\nLearning (ML) algorithms designed to equip critical systems with advanced\nanalytics and decision functions. We start from the fundamental principles of\nML and describe the core elements conditioning its trust, particularly through\nits design: namely domain specification, data engineering, design of the ML\nalgorithms, their implementation, evaluation and deployment. The latter\ncomponents are organized in an unique framework for the design of trusted ML\nsystems.",
    "descriptor": "\nComments: This work has been supported by the French government under the \"France 2030\" program, as part of the SystemX Technological Research Institute Research Institute\n",
    "authors": [
      "Juliette Mattioli",
      "Agnes Delaborde",
      "Souhaiel Khalfaoui",
      "Freddy Lecue",
      "Henri Sohier",
      "Frederic Jurie"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15438"
  },
  {
    "id": "arXiv:2209.15439",
    "title": "Exploiting Instance-based Mixed Sampling via Auxiliary Source Domain  Supervision for Domain-adaptive Action Detection",
    "abstract": "We propose a novel domain adaptive action detection approach and a new\nadaptation protocol that leverages the recent advancements in image-level\nunsupervised domain adaptation (UDA) techniques and handle vagaries of\ninstance-level video data. Self-training combined with cross-domain mixed\nsampling has shown remarkable performance gain in semantic segmentation in UDA\n(unsupervised domain adaptation) context. Motivated by this fact, we propose an\napproach for human action detection in videos that transfers knowledge from the\nsource domain (annotated dataset) to the target domain (unannotated dataset)\nusing mixed sampling and pseudo-label-based selftraining. The existing UDA\ntechniques follow a ClassMix algorithm for semantic segmentation. However,\nsimply adopting ClassMix for action detection does not work, mainly because\nthese are two entirely different problems, i.e., pixel-label classification vs.\ninstance-label detection. To tackle this, we propose a novel action instance\nmixed sampling technique that combines information across domains based on\naction instances instead of action classes. Moreover, we propose a new UDA\ntraining protocol that addresses the long-tail sample distribution and domain\nshift problem by using supervision from an auxiliary source domain (ASD). For\nthe ASD, we propose a new action detection dataset with dense frame-level\nannotations. We name our proposed framework as domain-adaptive action instance\nmixing (DA-AIM). We demonstrate that DA-AIM consistently outperforms prior\nworks on challenging domain adaptation benchmarks. The source code is available\nat https://github.com/wwwfan628/DA-AIM.",
    "descriptor": "",
    "authors": [
      "Yifan Lu",
      "Gurkirt Singh",
      "Suman Saha",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15439"
  },
  {
    "id": "arXiv:2209.15448",
    "title": "Blessing from Experts: Super Reinforcement Learning in Confounded  Environments",
    "abstract": "We introduce super reinforcement learning in the batch setting, which takes\nthe observed action as input for enhanced policy learning. In the presence of\nunmeasured confounders, the recommendations from human experts recorded in the\nobserved data allow us to recover certain unobserved information. Including\nthis information in the policy search, the proposed super reinforcement\nlearning will yield a super-policy that is guaranteed to outperform both the\nstandard optimal policy and the behavior one (e.g., the expert's\nrecommendation). Furthermore, to address the issue of unmeasured confounding in\nfinding super-policies, a number of non-parametric identification results are\nestablished. Finally, we develop two super-policy learning algorithms and\nderive their corresponding finite-sample regret guarantees.",
    "descriptor": "",
    "authors": [
      "Jiayi Wang",
      "Zhengling Qi",
      "Chengchun Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2209.15448"
  },
  {
    "id": "arXiv:2209.15450",
    "title": "Explainable Censored Learning: Finding Critical Features with Long Term  Prognostic Values for Survival Prediction",
    "abstract": "Interpreting critical variables involved in complex biological processes\nrelated to survival time can help understand prediction from survival models,\nevaluate treatment efficacy, and develop new therapies for patients. Currently,\nthe predictive results of deep learning (DL)-based models are better than or as\ngood as standard survival methods, they are often disregarded because of their\nlack of transparency and little interpretability, which is crucial to their\nadoption in clinical applications. In this paper, we introduce a novel, easily\ndeployable approach, called EXplainable CEnsored Learning (EXCEL), to\niteratively exploit critical variables and simultaneously implement (DL) model\ntraining based on these variables. First, on a toy dataset, we illustrate the\nprinciple of EXCEL; then, we mathematically analyze our proposed method, and we\nderive and prove tight generalization error bounds; next, on two semi-synthetic\ndatasets, we show that EXCEL has good anti-noise ability and stability;\nfinally, we apply EXCEL to a variety of real-world survival datasets including\nclinical data and genetic data, demonstrating that EXCEL can effectively\nidentify critical features and achieve performance on par with or better than\nthe original models. It is worth pointing out that EXCEL is flexibly deployed\nin existing or emerging models for explainable survival data in the presence of\nright censoring.",
    "descriptor": "\nComments: 39 pages\n",
    "authors": [
      "Xinxing Wu",
      "Chong Peng",
      "Richard Charnigo",
      "Qiang Cheng"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15450"
  },
  {
    "id": "arXiv:2209.15452",
    "title": "Safe Exploration Method for Reinforcement Learning under Existence of  Disturbance",
    "abstract": "Recent rapid developments in reinforcement learning algorithms have been\ngiving us novel possibilities in many fields. However, due to their exploring\nproperty, we have to take the risk into consideration when we apply those\nalgorithms to safety-critical problems especially in real environments. In this\nstudy, we deal with a safe exploration problem in reinforcement learning under\nthe existence of disturbance. We define the safety during learning as\nsatisfaction of the constraint conditions explicitly defined in terms of the\nstate and propose a safe exploration method that uses partial prior knowledge\nof a controlled object and disturbance. The proposed method assures the\nsatisfaction of the explicit state constraints with a pre-specified probability\neven if the controlled object is exposed to a stochastic disturbance following\na normal distribution. As theoretical results, we introduce sufficient\nconditions to construct conservative inputs not containing an exploring aspect\nused in the proposed method and prove that the safety in the above explained\nsense is guaranteed with the proposed method. Furthermore, we illustrate the\nvalidity and effectiveness of the proposed method through numerical simulations\nof an inverted pendulum and a four-bar parallel link robot manipulator.",
    "descriptor": "\nComments: Accepted to the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2022\n",
    "authors": [
      "Yoshihiro Okawa",
      "Tomotake Sasaki",
      "Hitoshi Yanami",
      "Toru Namerikawa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Systems and Control (eess.SY)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15452"
  },
  {
    "id": "arXiv:2209.15454",
    "title": "GPNet: Simplifying Graph Neural Networks via Multi-channel Geometric  Polynomials",
    "abstract": "Graph Neural Networks (GNNs) are a promising deep learning approach for\ncircumventing many real-world problems on graph-structured data. However, these\nmodels usually have at least one of four fundamental limitations:\nover-smoothing, over-fitting, difficult to train, and strong homophily\nassumption. For example, Simple Graph Convolution (SGC) is known to suffer from\nthe first and fourth limitations. To tackle these limitations, we identify a\nset of key designs including (D1) dilated convolution, (D2) multi-channel\nlearning, (D3) self-attention score, and (D4) sign factor to boost learning\nfrom different types (i.e. homophily and heterophily) and scales (i.e. small,\nmedium, and large) of networks, and combine them into a graph neural network,\nGPNet, a simple and efficient one-layer model. We theoretically analyze the\nmodel and show that it can approximate various graph filters by adjusting the\nself-attention score and sign factor. Experiments show that GPNet consistently\noutperforms baselines in terms of average rank, average accuracy, complexity,\nand parameters on semi-supervised and full-supervised tasks, and achieves\ncompetitive performance compared to state-of-the-art model with inductive\nlearning task.",
    "descriptor": "\nComments: 15 pages, 15 figures\n",
    "authors": [
      "Xun Liu",
      "Alex Hay-Man Ng",
      "Fangyuan Lei",
      "Yikuan Zhang",
      "Zhengmin Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15454"
  },
  {
    "id": "arXiv:2209.15455",
    "title": "Road Network Deterioration Monitoring Using Aerial Images and Computer  Vision",
    "abstract": "Road maintenance is an essential process for guaranteeing the quality of\ntransportation in any city. A crucial step towards effective road maintenance\nis the ability to update the inventory of the road network. We present a proof\nof concept of a protocol for maintaining said inventory based on the use of\nunmanned aerial vehicles to quickly collect images which are processed by a\ncomputer vision program that automatically identifies potholes and their\nseverity. Our protocol aims to provide information to local governments to\nprioritise the road network maintenance budget, and to be able to detect early\nstages of road deterioration so as to minimise maintenance expenditure.",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Nicolas Parra-A",
      "Vladimir Vargas-Calder\u00f3n",
      "Herbert Vinck-Posada",
      "Nicanor Vinck"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2209.15455"
  },
  {
    "id": "arXiv:2209.15457",
    "title": "Scheduling for Urban Air Mobility using Safe Learning",
    "abstract": "This work considers the scheduling problem for Urban Air Mobility (UAM)\nvehicles travelling between origin-destination pairs with both hard and soft\ntrip deadlines. Each route is described by a discrete probability distribution\nover trip completion times (or delay) and over inter-arrival times of requests\n(or demand) for the route along with a fixed hard or soft deadline. Soft\ndeadlines carry a cost that is incurred when the deadline is missed. An online,\nsafe scheduler is developed that ensures that hard deadlines are never missed,\nand that average cost of missing soft deadlines is minimized. The system is\nmodelled as a Markov Decision Process (MDP) and safe model-based learning is\nused to find the probabilistic distributions over route delays and demand.\nMonte Carlo Tree Search (MCTS) Earliest Deadline First (EDF) is used to safely\nexplore the learned models in an online fashion and develop a near-optimal\nnon-preemptive scheduling policy. These results are compared with Value\nIteration (VI) and MCTS (Random) scheduling solutions.",
    "descriptor": "\nComments: In Proceedings FMAS2022 ASYDE2022, arXiv:2209.13181\n",
    "authors": [
      "Surya Murthy",
      "Natasha A. Neogi",
      "Suda Bharadwaj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15457"
  },
  {
    "id": "arXiv:2209.15458",
    "title": "Towards General-Purpose Representation Learning of Polygonal Geometries",
    "abstract": "Neural network representation learning for spatial data is a common need for\ngeographic artificial intelligence (GeoAI) problems. In recent years, many\nadvancements have been made in representation learning for points, polylines,\nand networks, whereas little progress has been made for polygons, especially\ncomplex polygonal geometries. In this work, we focus on developing a\ngeneral-purpose polygon encoding model, which can encode a polygonal geometry\n(with or without holes, single or multipolygons) into an embedding space. The\nresult embeddings can be leveraged directly (or finetuned) for downstream tasks\nsuch as shape classification, spatial relation prediction, and so on. To\nachieve model generalizability guarantees, we identify a few desirable\nproperties: loop origin invariance, trivial vertex invariance, part permutation\ninvariance, and topology awareness. We explore two different designs for the\nencoder: one derives all representations in the spatial domain; the other\nleverages spectral domain representations. For the spatial domain approach, we\npropose ResNet1D, a 1D CNN-based polygon encoder, which uses circular padding\nto achieve loop origin invariance on simple polygons. For the spectral domain\napproach, we develop NUFTspec based on Non-Uniform Fourier Transformation\n(NUFT), which naturally satisfies all the desired properties. We conduct\nexperiments on two tasks: 1) shape classification based on MNIST; 2) spatial\nrelation prediction based on two new datasets - DBSR-46K and DBSR-cplx46K. Our\nresults show that NUFTspec and ResNet1D outperform multiple existing baselines\nwith significant margins. While ResNet1D suffers from model performance\ndegradation after shape-invariance geometry modifications, NUFTspec is very\nrobust to these modifications due to the nature of the NUFT.",
    "descriptor": "\nComments: 58 pages, 20 figures, Accepted to GeoInformatica\n",
    "authors": [
      "Gengchen Mai",
      "Chiyu Jiang",
      "Weiwei Sun",
      "Rui Zhu",
      "Yao Xuan",
      "Ling Cai",
      "Krzysztof Janowicz",
      "Stefano Ermon",
      "Ni Lao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15458"
  },
  {
    "id": "arXiv:2209.15465",
    "title": "Melanoma Skin Cancer and Nevus Mole Classification using Intensity Value  Estimation with Convolutional Neural Network",
    "abstract": "Melanoma skin cancer is one of the most dangerous and life-threatening\ncancer. Exposure to ultraviolet rays may damage the skin cell's DNA, which\ncauses melanoma skin cancer. However, it is difficult to detect and classify\nmelanoma and nevus mole at the immature stages. In this work, an automatic deep\nlearning system is developed based on the intensity value estimation with a\nconvolutional neural network model (CNN) to detect and classify melanoma and\nnevus mole more accurately. Since intensity levels are the most distinctive\nfeatures for object or region of interest identification, the high-intensity\npixel values are selected from the extracted lesion images. Incorporating those\nhigh-intensity features into the CNN improves the overall performance of the\nproposed model than the state-of-the-art methods for detecting melanoma skin\ncancer. To evaluate the system, we used 5-fold cross-validation. Experimental\nresults show that a superior percentage of accuracy (92.58%), sensitivity\n(93.76%), specificity (91.56%), and precision (90.68%) are achieved.",
    "descriptor": "\nComments: The paper has been accepted for publication in Computer Science journal: this http URL\n",
    "authors": [
      "N. I. Md. Ashafuddula",
      "Rafiqul Islam"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15465"
  },
  {
    "id": "arXiv:2209.15469",
    "title": "Zero-Shot Retrieval with Search Agents and Hybrid Environments",
    "abstract": "Learning to search is the task of building artificial agents that learn to\nautonomously use a search box to find information. So far, it has been shown\nthat current language models can learn symbolic query reformulation policies,\nin combination with traditional term-based retrieval, but fall short of\noutperforming neural retrievers. We extend the previous learning to search\nsetup to a hybrid environment, which accepts discrete query refinement\noperations, after a first-pass retrieval step performed by a dual encoder.\nExperiments on the BEIR task show that search agents, trained via behavioral\ncloning, outperform the underlying search system based on a combined dual\nencoder retriever and cross encoder reranker. Furthermore, we find that simple\nheuristic Hybrid Retrieval Environments (HRE) can improve baseline performance\nby several nDCG points. The search agent based on HRE (HARE) produces\nstate-of-the-art performance on both zero-shot and in-domain evaluations. We\ncarry out an extensive qualitative analysis to shed light on the agents\npolicies.",
    "descriptor": "",
    "authors": [
      "Michelle Chen Huebscher",
      "Christian Buck",
      "Massimiliano Ciaramita",
      "Sascha Rothe"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15469"
  },
  {
    "id": "arXiv:2209.15471",
    "title": "Two-headed eye-segmentation approach for biometric identification",
    "abstract": "Iris-based identification systems are among the most popular approaches for\nperson identification. Such systems require good-quality segmentation modules\nthat ideally identify the regions for different eye components. This paper\nintroduces the new two-headed architecture, where the eye components and\neyelashes are segmented using two separate decoding modules. Moreover, we\ninvestigate various training scenarios by adopting different training losses.\nThanks to the two-headed approach, we were also able to examine the quality of\nthe model with the convex prior, which enforces the convexity of the segmented\nshapes. We conducted an extensive evaluation of various learning scenarios on\nreal-life conditions high-resolution near-infrared iris images.",
    "descriptor": "",
    "authors": [
      "Wiktor Lazarski",
      "Maciej Zieba",
      "Tanguy Jeanneau",
      "Tobias Zillig",
      "Christian Brendel"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15471"
  },
  {
    "id": "arXiv:2209.15474",
    "title": "Reliable Face Morphing Attack Detection in On-The-Fly Border Control  Scenario with Variation in Image Resolution and Capture Distance",
    "abstract": "Face Recognition Systems (FRS) are vulnerable to various attacks performed\ndirectly and indirectly. Among these attacks, face morphing attacks are highly\npotential in deceiving automatic FRS and human observers and indicate a severe\nsecurity threat, especially in the border control scenario. This work presents\na face morphing attack detection, especially in the On-The-Fly (OTF) Automatic\nBorder Control (ABC) scenario. We present a novel Differential-MAD (D-MAD)\nalgorithm based on the spherical interpolation and hierarchical fusion of deep\nfeatures computed from six different pre-trained deep Convolutional Neural\nNetworks (CNNs). Extensive experiments are carried out on the newly generated\nface morphing dataset (SCFace-Morph) based on the publicly available SCFace\ndataset by considering the real-life scenario of Automatic Border Control (ABC)\ngates. Experimental protocols are designed to benchmark the proposed and\nstate-of-the-art (SOTA) D-MAD techniques for different camera resolutions and\ncapture distances. Obtained results have indicated the superior performance of\nthe proposed D-MAD method compared to the existing methods.",
    "descriptor": "\nComments: The paper is accepted at the International Joint Conference on Biometrics (IJCB) 2022\n",
    "authors": [
      "Jag Mohan Singh",
      "Raghavendra Ramachandra"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15474"
  },
  {
    "id": "arXiv:2209.15475",
    "title": "Point Cloud Quality Assessment using 3D Saliency Maps",
    "abstract": "Point cloud quality assessment (PCQA) has become an appealing research field\nin recent days. Considering the importance of saliency detection in quality\nassessment, we propose an effective full-reference PCQA metric which makes the\nfirst attempt to utilize the saliency information to facilitate quality\nprediction, called point cloud quality assessment using 3D saliency maps\n(PQSM). Specifically, we first propose a projection-based point cloud saliency\nmap generation method, in which depth information is introduced to better\nreflect the geometric characteristics of point clouds. Then, we construct point\ncloud local neighborhoods to derive three structural descriptors to indicate\nthe geometry, color and saliency discrepancies. Finally, a saliency-based\npooling strategy is proposed to generate the final quality score. Extensive\nexperiments are performed on four independent PCQA databases. The results\ndemonstrate that the proposed PQSM shows competitive performances compared to\nmultiple state-of-the-art PCQA metrics.",
    "descriptor": "",
    "authors": [
      "Zhengyu Wang",
      "Yujie Zhang",
      "Qi Yang",
      "Yiling Xu",
      "Jun Sun",
      "Shan Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.15475"
  },
  {
    "id": "arXiv:2209.15483",
    "title": "On The Robustness of Self-Supervised Representations for Spoken Language  Modeling",
    "abstract": "Self-supervised representations have been extensively studied for\ndiscriminative and generative tasks. However, their robustness capabilities\nhave not been extensively investigated. This work focuses on self-supervised\nrepresentations for spoken generative language models. First, we empirically\ndemonstrate how current state-of-the-art speech representation models lack\nrobustness to basic signal variations that do not alter the spoken information.\nTo overcome this, we propose an effective and efficient method to learn robust\nself-supervised speech representation for generative spoken language modeling.\nThe proposed approach is based on applying a set of signal transformations to\nthe speech signal and optimizing the model using an iterative pseudo-labeling\nscheme. Our method significantly improves over the evaluated baselines when\nconsidering encoding metrics. We additionally evaluate our method on the\nspeech-to-speech translation task. We consider Spanish-English and\nFrench-English conversions and empirically demonstrate the benefits of\nfollowing the proposed approach.",
    "descriptor": "",
    "authors": [
      "Itai Gat",
      "Felix Kreuk",
      "Ann Lee",
      "Jade Copet",
      "Gabriel Synnaeve",
      "Emmanuel Dupoux",
      "Yossi Adi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15483"
  },
  {
    "id": "arXiv:2209.15486",
    "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
    "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple\nheuristics on Link Prediction (LP) tasks. This is due to limitations in\nexpressive power such as the inability to count triangles (the backbone of most\nLP heuristics) and because they can not distinguish automorphic nodes (those\nhaving identical structural roles). Both expressiveness issues can be\nalleviated by learning link (rather than node) representations and\nincorporating structural features such as triangle counts. Since explicit link\nrepresentations are often prohibitively expensive, recent works resorted to\nsubgraph-based methods, which have achieved state-of-the-art performance for\nLP, but suffer from poor efficiency due to high levels of redundancy between\nsubgraphs. We analyze the components of subgraph GNN (SGNN) methods for link\nprediction. Based on our analysis, we propose a novel full-graph GNN called\nELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as\nmessages to approximate the key components of SGNNs without explicit subgraph\nconstruction. ELPH is provably more expressive than Message Passing GNNs\n(MPNNs). It outperforms existing SGNN models on many standard LP benchmarks\nwhile being orders of magnitude faster. However, it shares the common GNN\nlimitation that it is only efficient when the dataset fits in GPU memory.\nAccordingly, we develop a highly scalable model, called BUDDY, which uses\nfeature precomputation to circumvent this limitation without sacrificing\npredictive performance. Our experiments show that BUDDY also outperforms SGNNs\non standard LP benchmarks while being highly scalable and faster than ELPH.",
    "descriptor": "\nComments: 9 pages, 6 figures, 6 appendices\n",
    "authors": [
      "Benjamin Paul Chamberlain",
      "Sergey Shirobokov",
      "Emanuele Ross",
      "Fabrizio Frasca",
      "Thomas Markovich",
      "Nils Hammerla",
      "Michael M. Bronstein",
      "Max Hansmire"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2209.15486"
  },
  {
    "id": "arXiv:2209.15489",
    "title": "Impact of Face Image Quality Estimation on Presentation Attack Detection",
    "abstract": "Non-referential face image quality assessment methods have gained popularity\nas a pre-filtering step on face recognition systems. In most of them, the\nquality score is usually designed with face matching in mind. However, a small\namount of work has been done on measuring their impact and usefulness on\nPresentation Attack Detection (PAD). In this paper, we study the effect of\nquality assessment methods on filtering bona fide and attack samples, their\nimpact on PAD systems, and how the performance of such systems is improved when\ntraining on a filtered (by quality) dataset. On a Vision Transformer PAD\nalgorithm, a reduction of 20% of the training dataset by removing lower quality\nsamples allowed us to improve the BPCER by 3% in a cross-dataset test.",
    "descriptor": "",
    "authors": [
      "Carlos Aravena",
      "Diego Pasmino",
      "Juan E. Tapia",
      "Christoph Busch"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15489"
  },
  {
    "id": "arXiv:2209.15490",
    "title": "Learning Second Order Local Anomaly for General Face Forgery Detection",
    "abstract": "In this work, we propose a novel method to improve the generalization ability\nof CNN-based face forgery detectors. Our method considers the feature anomalies\nof forged faces caused by the prevalent blending operations in face forgery\nalgorithms. Specifically, we propose a weakly supervised Second Order Local\nAnomaly (SOLA) learning module to mine anomalies in local regions using deep\nfeature maps. SOLA first decomposes the neighborhood of local features by\ndifferent directions and distances and then calculates the first and second\norder local anomaly maps which provide more general forgery traces for the\nclassifier. We also propose a Local Enhancement Module (LEM) to improve the\ndiscrimination between local features of real and forged regions, so as to\nensure accuracy in calculating anomalies. Besides, an improved Adaptive Spatial\nRich Model (ASRM) is introduced to help mine subtle noise features via\nlearnable high pass filters. With neither pixel level annotations nor external\nsynthetic data, our method using a simple ResNet18 backbone achieves\ncompetitive performances compared with state-of-the-art works when evaluated on\nunseen forgeries.",
    "descriptor": "",
    "authors": [
      "Jianwei Fei",
      "Yunshu Dai",
      "Peipeng Yu",
      "Tianrun Shen",
      "Zhihua Xia",
      "Jian Weng"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15490"
  },
  {
    "id": "arXiv:2209.15492",
    "title": "Formalized Class Group Computations and Integral Points on Mordell  Elliptic Curves",
    "abstract": "Diophantine equations are a popular and active area of research in number\ntheory. In this paper we consider Mordell equations, which are of the form\n$y^2=x^3+d$, where $d$ is a (given) nonzero integer number and all solutions in\nintegers $x$ and $y$ have to be determined. One non-elementary approach for\nthis problem is the resolution via descent and class groups. Along these lines\nwe formalized in Lean 3 the resolution of Mordell equations for several\ninstances of $d<0$. In order to achieve this, we needed to formalize several\nother theories from number theory that are interesting on their own as well,\nsuch as ideal norms, quadratic fields and rings, and explicit computations of\nthe class number. Moreover we introduced new computational tactics in order to\ncarry out efficiently computations in quadratic rings and beyond.",
    "descriptor": "\nComments: 14 pages. Submitted to CPP '23. Source code available at this https URL\n",
    "authors": [
      "Anne Baanen",
      "Alex J. Best",
      "Nirvana Coppola",
      "Sander R. Dahmen"
    ],
    "subjectives": [
      "Logic in Computer Science (cs.LO)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2209.15492"
  },
  {
    "id": "arXiv:2209.15496",
    "title": "Using Knowledge Distillation to improve interpretable models in a retail  banking context",
    "abstract": "This article sets forth a review of knowledge distillation techniques with a\nfocus on their applicability to retail banking contexts. Predictive machine\nlearning algorithms used in banking environments, especially in risk and\ncontrol functions, are generally subject to regulatory and technical\nconstraints limiting their complexity. Knowledge distillation gives the\nopportunity to improve the performances of simple models without burdening\ntheir application, using the results of other - generally more complex and\nbetter-performing - models. Parsing recent advances in this field, we highlight\nthree main approaches: Soft Targets, Sample Selection and Data Augmentation. We\nassess the relevance of a subset of such techniques by applying them to open\nsource datasets, before putting them to the test on the use cases of BPCE, a\nmajor French institution in the retail banking sector. As such, we demonstrate\nthe potential of knowledge distillation to improve the performance of these\nmodels without altering their form and simplicity.",
    "descriptor": "\nComments: 25 pages, 9 figures, 11 tables\n",
    "authors": [
      "Maxime Biehler",
      "Mohamed Guermazi",
      "C\u00e9lim Starck"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Risk Management (q-fin.RM)"
    ],
    "url": "https://arxiv.org/abs/2209.15496"
  },
  {
    "id": "arXiv:2209.15498",
    "title": "Towards remote fault detection by analyzing communication priorities",
    "abstract": "The ability to detect faults is an important safety feature for event-based\nmulti-agent systems. In most existing algorithms, each agent tries to detect\nfaults by checking its own behavior. But what if one agent becomes unable to\nrecognize misbehavior, for example due to failure in its onboard fault\ndetection? To improve resilience and avoid propagation of individual errors to\nthe multi-agent system, agents should check each other remotely for malfunction\nor misbehavior. In this paper, we build upon a recently proposed predictive\ntriggering architecture that involves communication priorities shared\nthroughout the network to manage limited bandwidth. We propose a fault\ndetection method that uses these priorities to detect errors in other agents.\nThe resulting algorithms is not only able to detect faults, but can also run on\na low-power microcontroller in real-time, as we demonstrate in hardware\nexperiments.",
    "descriptor": "",
    "authors": [
      "Alexander Gr\u00e4fe",
      "Dominik Baumann",
      "Sebastian Trimpe"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15498"
  },
  {
    "id": "arXiv:2209.15501",
    "title": "A Closer Look at Temporal Ordering in the Segmentation of Instructional  Videos",
    "abstract": "Understanding the steps required to perform a task is an important skill for\nAI systems. Learning these steps from instructional videos involves two\nsubproblems: (i) identifying the temporal boundary of sequentially occurring\nsegments and (ii) summarizing these steps in natural language. We refer to this\ntask as Procedure Segmentation and Summarization (PSS). In this paper, we take\na closer look at PSS and propose three fundamental improvements over current\nmethods. The segmentation task is critical, as generating a correct summary\nrequires the step to be identified first. However, current segmentation metrics\noften overestimate the segmentation quality because they do not incorporate the\ntemporal order of segments. We propose a new segmentation metric based on\ndynamic programming that takes into account the order of segments. Current PSS\nmethods are typically trained by proposing segments, matching them with the\nground truth and computing a loss. However, much like segmentation metrics,\nexisting matching algorithms do not consider the temporal order of the mapping\nbetween candidate segments and the ground truth. We propose a matching\nalgorithm that constrains the temporal order of segment mapping, and is also\ndifferentiable. Lastly, we introduce multi-modal feature training for PSS,\nwhich further improves segmentation. We evaluate our approach on two\ninstructional video datasets (YouCook2 and Tasty) and improve the state of the\nart by a margin of $\\sim7\\%$ and $\\sim2.5\\%$ for procedure segmentation and\nsummarization, respectively.",
    "descriptor": "\nComments: Accepted in BMVC 2022\n",
    "authors": [
      "Anil Batra",
      "Shreyank Gowda",
      "Laura Sevilla-Lara",
      "Frank Keller"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15501"
  },
  {
    "id": "arXiv:2209.15502",
    "title": "Efficient LSTM Training with Eligibility Traces",
    "abstract": "Training recurrent neural networks is predominantly achieved via\nbackpropagation through time (BPTT). However, this algorithm is not an optimal\nsolution from both a biological and computational perspective. A more efficient\nand biologically plausible alternative for BPTT is e-prop. We investigate the\napplicability of e-prop to long short-term memorys (LSTMs), for both supervised\nand reinforcement learning (RL) tasks. We show that e-prop is a suitable\noptimization algorithm for LSTMs by comparing it to BPTT on two benchmarks for\nsupervised learning. This proves that e-prop can achieve learning even for\nproblems with long sequences of several hundred timesteps. We introduce\nextensions that improve the performance of e-prop, which can partially be\napplied to other network architectures. With the help of these extensions we\nshow that, under certain conditions, e-prop can outperform BPTT for one of the\ntwo benchmarks for supervised learning. Finally, we deliver a proof of concept\nfor the integration of e-prop to RL in the domain of deep recurrent Q-learning.",
    "descriptor": "",
    "authors": [
      "Michael Hoyer",
      "Shahram Eivazi",
      "Sebastian Otte"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15502"
  },
  {
    "id": "arXiv:2209.15505",
    "title": "Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning  on Heterogeneous Data",
    "abstract": "SGD with momentum acceleration is one of the key components for improving the\nperformance of neural networks. For decentralized learning, a straightforward\napproach using momentum acceleration is Distributed SGD (DSGD) with momentum\nacceleration (DSGDm). However, DSGDm performs worse than DSGD when the data\ndistributions are statistically heterogeneous. Recently, several studies have\naddressed this issue and proposed methods with momentum acceleration that are\nmore robust to data heterogeneity than DSGDm, although their convergence rates\nremain dependent on data heterogeneity and decrease when the data distributions\nare heterogeneous. In this study, we propose Momentum Tracking, which is a\nmethod with momentum acceleration whose convergence rate is proven to be\nindependent of data heterogeneity. More specifically, we analyze the\nconvergence rate of Momentum Tracking in the standard deep learning setting,\nwhere the objective function is non-convex and the stochastic gradient is used.\nThen, we identify that it is independent of data heterogeneity for any momentum\ncoefficient $\\beta\\in [0, 1)$. Through image classification tasks, we\ndemonstrate that Momentum Tracking is more robust to data heterogeneity than\nthe existing decentralized learning methods with momentum acceleration and can\nconsistently outperform these existing methods when the data distributions are\nheterogeneous.",
    "descriptor": "",
    "authors": [
      "Yuki Takezawa",
      "Han Bao",
      "Kenta Niwa",
      "Ryoma Sato",
      "Makoto Yamada"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15505"
  },
  {
    "id": "arXiv:2209.15511",
    "title": "Sphere-Guided Training of Neural Implicit Surfaces",
    "abstract": "In recent years, surface modeling via neural implicit functions has become\none of the main techniques for multi-view 3D reconstruction. However, the\nstate-of-the-art methods rely on the implicit functions to model an entire\nvolume of the scene, leading to reduced reconstruction fidelity in the areas\nwith thin objects or high-frequency details. To address that, we present a\nmethod for jointly training neural implicit surfaces alongside an auxiliary\nexplicit shape representation, which acts as surface guide. In our approach,\nthis representation encapsulates the surface region of the scene and enables us\nto boost the efficiency of the implicit function training by only modeling the\nvolume in that region. We propose using a set of learnable spherical primitives\nas a learnable surface guidance since they can be efficiently trained alongside\nthe neural surface function using its gradients. Our training pipeline consists\nof iterative updates of the spheres' centers using the gradients of the\nimplicit function and then fine-tuning the latter to the updated surface region\nof the scene. We show that such modification to the training procedure can be\nplugged into several popular implicit reconstruction methods, improving the\nquality of the results over multiple 3D reconstruction benchmarks.",
    "descriptor": "",
    "authors": [
      "Andreea Dogaru",
      "Andrei Timotei Ardelean",
      "Savva Ignatyev",
      "Evgeny Burnaev",
      "Egor Zakharov"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2209.15511"
  },
  {
    "id": "arXiv:2209.15514",
    "title": "Learning with MISELBO: The Mixture Cookbook",
    "abstract": "Mixture models in variational inference (VI) is an active field of research.\nRecent works have established their connection to multiple importance sampling\n(MIS) through the MISELBO and advanced the use of ensemble approximations for\nlarge-scale problems. However, as we show here, an independent learning of the\nensemble components can lead to suboptimal diversity. Hence, we study the\neffect of instead using MISELBO as an objective function for learning mixtures,\nand we propose the first ever mixture of variational approximations for a\nnormalizing flow-based hierarchical variational autoencoder (VAE) with\nVampPrior and a PixelCNN decoder network. Two major insights led to the\nconstruction of this novel composite model. First, mixture models have\npotential to be off-the-shelf tools for practitioners to obtain more flexible\nposterior approximations in VAEs. Therefore, we make them more accessible by\ndemonstrating how to apply them to four popular architectures. Second, the\nmixture components cooperate in order to cover the target distribution while\ntrying to maximize their diversity when MISELBO is the objective function. We\nexplain this cooperative behavior by drawing a novel connection between VI and\nadaptive importance sampling. Finally, we demonstrate the superiority of the\nMixture VAEs' learned feature representations on both image and single-cell\ntranscriptome data, and obtain state-of-the-art results among VAE architectures\nin terms of negative log-likelihood on the MNIST and FashionMNIST datasets.\nCode available here: \\url{https://github.com/Lagergren-Lab/MixtureVAEs}.",
    "descriptor": "",
    "authors": [
      "Oskar Kviman",
      "Ricky Mol\u00e9n",
      "Alexandra Hotti",
      "Semih Kurt",
      "V\u00edctor Elvira",
      "Jens Lagergren"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15514"
  },
  {
    "id": "arXiv:2209.15517",
    "title": "Medical Image Understanding with Pretrained Vision Language Models: A  Comprehensive Study",
    "abstract": "The large-scale pre-trained vision language models (VLM) have shown\nremarkable domain transfer capability on natural images. However, it remains\nunknown whether this capability can also apply to the medical image domain.\nThis paper thoroughly studies the knowledge transferability of pre-trained VLMs\nto the medical domain, where we show that well-designed medical prompts are the\nkey to elicit knowledge from pre-trained VLMs. We demonstrate that by prompting\nwith expressive attributes that are shared between domains, the VLM can carry\nthe knowledge across domains and improve its generalization. This mechanism\nempowers VLMs to recognize novel objects with fewer or without image samples.\nFurthermore, to avoid the laborious manual designing process, we develop three\napproaches for automatic generation of medical prompts, which can inject\nexpert-level medical knowledge and image-specific information into the prompts\nfor fine-grained grounding. We conduct extensive experiments on thirteen\ndifferent medical datasets across various modalities, showing that our\nwell-designed prompts greatly improve the zero-shot performance compared to the\ndefault prompts, and our fine-tuned models surpass the supervised models by a\nsignificant margin.",
    "descriptor": "\nComments: 14 pages, 4 figures,\n",
    "authors": [
      "Ziyuan Qin",
      "Huahui Yi",
      "Qicheng Lao",
      "Kang Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15517"
  },
  {
    "id": "arXiv:2209.15520",
    "title": "Efficient hyperbolic-parabolic models on multi-dimensional unbounded  domains using an extended DG approach",
    "abstract": "We introduce an extended discontinuous Galerkin discretization of\nhyperbolic-parabolic problems on multidimensional semi-infinite domains.\nBuilding on previous work on the one-dimensional case, we split the\nstrip-shaped computational domain into a bounded region, discretized by means\nof discontinuous finite elements using Legendre basis functions, and an\nunbounded subdomain, where scaled Laguerre functions are used as a basis.\nNumerical fluxes at the interface allow for a seamless coupling of the two\nregions. The resulting coupling strategy is shown to produce accurate numerical\nsolutions in tests on both linear and non-linear scalar and vectorial model\nproblems. In addition, an efficient absorbing layer can be simulated in the\nsemi-infinite part of the domain in order to damp outgoing signals with\nnegligible spurious reflections at the interface. By tuning the scaling\nparameter of the Laguerre basis functions, the extended DG scheme simulates\ntransient dynamics over large spatial scales with a substantial reduction in\ncomputational cost at a given accuracy level compared to standard single-domain\ndiscontinuous finite element techniques.",
    "descriptor": "\nComments: 28 pages, 13 figures\n",
    "authors": [
      "Federico Vismara",
      "Tommaso Benacchio"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.15520"
  },
  {
    "id": "arXiv:2209.15525",
    "title": "Slimmable Networks for Contrastive Self-supervised Learning",
    "abstract": "Self-supervised learning makes great progress in large model pre-training but\nsuffers in training small models. Previous solutions to this problem mainly\nrely on knowledge distillation and indeed have a two-stage learning procedure:\nfirst train a large teacher model, then distill it to improve the\ngeneralization ability of small ones. In this work, we present a new one-stage\nsolution to obtain pre-trained small models without extra teachers: slimmable\nnetworks for contrastive self-supervised learning (\\emph{SlimCLR}). A slimmable\nnetwork contains a full network and several weight-sharing sub-networks. We can\npre-train for only one time and obtain various networks including small ones\nwith low computation costs. However, in self-supervised cases, the interference\nbetween weight-sharing networks leads to severe performance degradation. One\nevidence of the interference is \\emph{gradient imbalance}: a small proportion\nof parameters produces dominant gradients during backpropagation, and the main\nparameters may not be fully optimized. The divergence in gradient directions of\nvarious networks may also cause interference between networks. To overcome\nthese problems, we make the main parameters produce dominant gradients and\nprovide consistent guidance for sub-networks via three techniques: slow start\ntraining of sub-networks, online distillation, and loss re-weighting according\nto model sizes. Besides, a switchable linear probe layer is applied during\nlinear evaluation to avoid the interference of weight-sharing linear layers. We\ninstantiate SlimCLR with typical contrastive learning frameworks and achieve\nbetter performance than previous arts with fewer parameters and FLOPs.",
    "descriptor": "\nComments: preprint,work in progress\n",
    "authors": [
      "Shuai Zhao",
      "Xiaohan Wang",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15525"
  },
  {
    "id": "arXiv:2209.15529",
    "title": "TT-NF: Tensor Train Neural Fields",
    "abstract": "Learning neural fields has been an active topic in deep learning research,\nfocusing, among other issues, on finding more compact and easy-to-fit\nrepresentations. In this paper, we introduce a novel low-rank representation\ntermed Tensor Train Neural Fields (TT-NF) for learning neural fields on dense\nregular grids and efficient methods for sampling from them. Our representation\nis a TT parameterization of the neural field, trained with backpropagation to\nminimize a non-convex objective. We analyze the effect of low-rank compression\non the downstream task quality metrics in two settings. First, we demonstrate\nthe efficiency of our method in a sandbox task of tensor denoising, which\nadmits comparison with SVD-based schemes designed to minimize reconstruction\nerror. Furthermore, we apply the proposed approach to Neural Radiance Fields,\nwhere the low-rank structure of the field corresponding to the best quality can\nbe discovered only through learning.",
    "descriptor": "\nComments: Preprint, under review\n",
    "authors": [
      "Anton Obukhov",
      "Mikhail Usvyatsov",
      "Christos Sakaridis",
      "Konrad Schindler",
      "Luc Van Gool"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15529"
  },
  {
    "id": "arXiv:2209.15532",
    "title": "Joint Scheduling and Resource Allocation for Packets with Deadlines and  Priorities",
    "abstract": "Cellular networks provide communication for different applications. Some\napplications have strict and very short latency requirements, while others\nrequire high bandwidth with varying priorities. The challenge of satisfying the\nrequirements grows in congested traffic where some packets might miss their\ndeadlines. Unfortunately, we prove that the problem is NP-Hard. To overcome\nthis, we propose a new scheduling policy for packets with multiple priorities,\nlatency requirements, and strict deadlines. To alleviate the complexity, our\nsolution incorporates a novel time domain relaxation solved by linear\nprogramming. Simulation results show that this method outperforms existing\nscheduling strategies.",
    "descriptor": "",
    "authors": [
      "Li-on Raviv",
      "Amir Leshem"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15532"
  },
  {
    "id": "arXiv:2209.15533",
    "title": "A $\\star$-product solver with spectral accuracy for non-autonomous  ordinary differential equations",
    "abstract": "A new method for solving non-autonomous ordinary differential equations is\nproposed, the method achieves spectral accuracy. It is based on a new result\nwhich expresses the solution of such ODEs as an element in the so called\n$\\star$-algebra. This algebra is equipped with a product, the $\\star$-product,\nwhich is the integral over the usual product of two bivariate distributions.\nExpanding the bivariate distributions in bases of Legendre polynomials leads to\na discretization of the $\\star$-product and this allows for the solution to be\napproximated by a vector that is obtained by solving a linear system of\nequations. The effectiveness of this approach is illustrated with numerical\nexperiments.",
    "descriptor": "",
    "authors": [
      "Stefano Pozza",
      "Niel Van Buggenhout"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.15533"
  },
  {
    "id": "arXiv:2209.15536",
    "title": "Spike-based local synaptic plasticity: A survey of computational models  and neuromorphic circuits",
    "abstract": "Understanding how biological neural networks carry out learning using\nspike-based local plasticity mechanisms can lead to the development of\npowerful, energy-efficient, and adaptive neuromorphic processing systems. A\nlarge number of spike-based learning models have recently been proposed\nfollowing different approaches. However, it is difficult to assess if and how\nthey could be mapped onto neuromorphic hardware, and to compare their features\nand ease of implementation. To this end, in this survey, we provide a\ncomprehensive overview of representative brain-inspired synaptic plasticity\nmodels and mixed-signal \\acs{CMOS} neuromorphic circuits within a unified\nframework. We review historical, bottom-up, and top-down approaches to modeling\nsynaptic plasticity, and we identify computational primitives that can support\nlow-latency and low-power hardware implementations of spike-based learning\nrules. We provide a common definition of a locality principle based on pre- and\npost-synaptic neuron information, which we propose as a fundamental requirement\nfor physical implementations of synaptic plasticity. Based on this principle,\nwe compare the properties of these models within the same framework, and\ndescribe the mixed-signal electronic circuits that implement their computing\nprimitives, pointing out how these building blocks enable efficient on-chip and\nonline learning in neuromorphic processing systems.",
    "descriptor": "",
    "authors": [
      "Lyes Khacef",
      "Philipp Klein",
      "Matteo Cartiglia",
      "Arianna Rubino",
      "Giacomo Indiveri",
      "Elisabetta Chicca"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2209.15536"
  },
  {
    "id": "arXiv:2209.15539",
    "title": "Riemannian geometry as a unifying theory for robot motion learning and  control",
    "abstract": "Riemannian geometry is a mathematical field which has been the cornerstone of\nrevolutionary scientific discoveries such as the theory of general relativity.\nDespite early uses in robot design and recent applications for exploiting data\nwith specific geometries, it mostly remains overlooked in robotics. With this\nblue sky paper, we argue that Riemannian geometry provides the most suitable\ntools to analyze and generate well-coordinated, energy-efficient motions of\nrobots with many degrees of freedom. Via preliminary solutions and novel\nresearch directions, we discuss how Riemannian geometry may be leveraged to\ndesign and combine physically-meaningful synergies for robotics, and how this\ntheory also opens the door to coupling motion synergies with perceptual inputs.",
    "descriptor": "\nComments: Published as a blue sky paper at ISRR'22. 8 pages, 2 figures. Video at this https URL\n",
    "authors": [
      "No\u00e9mie Jaquier",
      "Tamim Asfour"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15539"
  },
  {
    "id": "arXiv:2209.15550",
    "title": "The More Secure, The Less Equally Usable: Gender and Ethnicity  (Un)fairness of Deep Face Recognition along Security Thresholds",
    "abstract": "Face biometrics are playing a key role in making modern smart city\napplications more secure and usable. Commonly, the recognition threshold of a\nface recognition system is adjusted based on the degree of security for the\nconsidered use case. The likelihood of a match can be for instance decreased by\nsetting a high threshold in case of a payment transaction verification. Prior\nwork in face recognition has unfortunately showed that error rates are usually\nhigher for certain demographic groups. These disparities have hence brought\ninto question the fairness of systems empowered with face biometrics. In this\npaper, we investigate the extent to which disparities among demographic groups\nchange under different security levels. Our analysis includes ten face\nrecognition models, three security thresholds, and six demographic groups based\non gender and ethnicity. Experiments show that the higher the security of the\nsystem is, the higher the disparities in usability among demographic groups\nare. Compelling unfairness issues hence exist and urge countermeasures in\nreal-world high-stakes environments requiring severe security levels.",
    "descriptor": "\nComments: Accepted as a full paper at the 2nd International Workshop on Artificial Intelligence Methods for Smart Cities (AISC 2022)\n",
    "authors": [
      "Andrea Atzori",
      "Gianni Fenu",
      "Mirko Marras"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15550"
  },
  {
    "id": "arXiv:2209.15555",
    "title": "Towards a Unified View of Affinity-Based Knowledge Distillation",
    "abstract": "Knowledge transfer between artificial neural networks has become an important\ntopic in deep learning. Among the open questions are what kind of knowledge\nneeds to be preserved for the transfer, and how it can be effectively achieved.\nSeveral recent work have shown good performance of distillation methods using\nrelation-based knowledge. These algorithms are extremely attractive in that\nthey are based on simple inter-sample similarities. Nevertheless, a proper\nmetric of affinity and use of it in this context is far from well understood.\nIn this paper, by explicitly modularising knowledge distillation into a\nframework of three components, i.e. affinity, normalisation, and loss, we give\na unified treatment of these algorithms as well as study a number of unexplored\ncombinations of the modules. With this framework we perform extensive\nevaluations of numerous distillation objectives for image classification, and\nobtain a few useful insights for effective design choices while demonstrating\nhow relation-based knowledge distillation could achieve comparable performance\nto the state of the art in spite of the simplicity.",
    "descriptor": "",
    "authors": [
      "Vladimir Li",
      "Atsuto Maki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15555"
  },
  {
    "id": "arXiv:2209.15557",
    "title": "Explaining Hierarchical Features in Dynamic Point Cloud Processing",
    "abstract": "This paper aims at bringing some light and understanding to the field of deep\nlearning for dynamic point cloud processing. Specifically, we focus on the\nhierarchical features learning aspect, with the ultimate goal of understanding\nwhich features are learned at the different stages of the process and what\ntheir meaning is. Last, we bring clarity on how hierarchical components of the\nnetwork affect the learned features and their importance for a successful\nlearning model. This study is conducted for point cloud prediction tasks,\nuseful for predicting coding applications.",
    "descriptor": "",
    "authors": [
      "Pedro Gomes",
      "Silvia Rossi",
      "Laura Toni"
    ],
    "subjectives": [
      "Multimedia (cs.MM)"
    ],
    "url": "https://arxiv.org/abs/2209.15557"
  },
  {
    "id": "arXiv:2209.15558",
    "title": "Out-of-Distribution Detection and Selective Generation for Conditional  Language Models",
    "abstract": "Machine learning algorithms typically assume independent and identically\ndistributed samples in training and at test time. Much work has shown that\nhigh-performing ML classifiers can degrade significantly and provide\noverly-confident, wrong classification predictions, particularly for\nout-of-distribution (OOD) inputs. Conditional language models (CLMs) are\npredominantly trained to classify the next token in an output sequence, and may\nsuffer even worse degradation on OOD inputs as the prediction is done\nauto-regressively over many steps. Furthermore, the space of potential\nlow-quality outputs is larger as arbitrary text can be generated and it is\nimportant to know when to trust the generated output. We present a highly\naccurate and lightweight OOD detection method for CLMs, and demonstrate its\neffectiveness on abstractive summarization and translation. We also show how\nour method can be used under the common and realistic setting of distribution\nshift for selective generation (analogous to selective prediction for\nclassification) of high-quality outputs, while automatically abstaining from\nlow-quality ones, enabling safer deployment of generative language models.",
    "descriptor": "",
    "authors": [
      "Jie Ren",
      "Jiaming Luo",
      "Yao Zhao",
      "Kundan Krishna",
      "Mohammad Saleh",
      "Balaji Lakshminarayanan",
      "Peter J. Liu"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2209.15558"
  },
  {
    "id": "arXiv:2209.15560",
    "title": "Designing and Training of Lightweight Neural Networks on Edge Devices  using Early Halting in Knowledge Distillation",
    "abstract": "Automated feature extraction capability and significant performance of Deep\nNeural Networks (DNN) make them suitable for Internet of Things (IoT)\napplications. However, deploying DNN on edge devices becomes prohibitive due to\nthe colossal computation, energy, and storage requirements. This paper presents\na novel approach for designing and training lightweight DNN using large-size\nDNN. The approach considers the available storage, processing speed, and\nmaximum allowable processing time to execute the task on edge devices. We\npresent a knowledge distillation based training procedure to train the\nlightweight DNN to achieve adequate accuracy. During the training of\nlightweight DNN, we introduce a novel early halting technique, which preserves\nnetwork resources; thus, speedups the training procedure. Finally, we present\nthe empirically and real-world evaluations to verify the effectiveness of the\nproposed approach under different constraints using various edge devices.",
    "descriptor": "\nComments: 13 pages, 7 figures, 11 tables\n",
    "authors": [
      "Rahul Mishra",
      "Hari Prabhat Gupta"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2209.15560"
  },
  {
    "id": "arXiv:2209.15562",
    "title": "On the optimization and generalization of overparameterized implicit  neural networks",
    "abstract": "Implicit neural networks have become increasingly attractive in the machine\nlearning community since they can achieve competitive performance but use much\nless computational resources. Recently, a line of theoretical works established\nthe global convergences for first-order methods such as gradient descent if the\nimplicit networks are over-parameterized. However, as they train all layers\ntogether, their analyses are equivalent to only studying the evolution of the\noutput layer. It is unclear how the implicit layer contributes to the training.\nThus, in this paper, we restrict ourselves to only training the implicit layer.\nWe show that global convergence is guaranteed, even if only the implicit layer\nis trained. On the other hand, the theoretical understanding of when and how\nthe training performance of an implicit neural network can be generalized to\nunseen data is still under-explored. Although this problem has been studied in\nstandard feed-forward networks, the case of implicit neural networks is still\nintriguing since implicit networks theoretically have infinitely many layers.\nTherefore, this paper investigates the generalization error for implicit neural\nnetworks. Specifically, we study the generalization of an implicit network\nactivated by the ReLU function over random initialization. We provide a\ngeneralization bound that is initialization sensitive. As a result, we show\nthat gradient flow with proper random initialization can train a sufficient\nover-parameterized implicit network to achieve arbitrarily small generalization\nerrors.",
    "descriptor": "",
    "authors": [
      "Tianxiang Gao",
      "Hongyang Gao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15562"
  },
  {
    "id": "arXiv:2209.15565",
    "title": "Augmenting Operations Research with Auto-Formulation of Optimization  Models from Problem Descriptions",
    "abstract": "We describe an augmented intelligence system for simplifying and enhancing\nthe modeling experience for operations research. Using this system, the user\nreceives a suggested formulation of an optimization problem based on its\ndescription. To facilitate this process, we build an intuitive user interface\nsystem that enables the users to validate and edit the suggestions. We\ninvestigate controlled generation techniques to obtain an automatic suggestion\nof formulation. Then, we evaluate their effectiveness with a newly created\ndataset of linear programming problems drawn from various application domains.",
    "descriptor": "\nComments: 6 pages text, 23 pages supplementary material\n",
    "authors": [
      "Rindranirina Ramamonjison",
      "Haley Li",
      "Timothy T. Yu",
      "Shiqi He",
      "Vishnu Rengan",
      "Amin Banitalebi-Dehkordi",
      "Zirui Zhou",
      "Yong Zhang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15565"
  },
  {
    "id": "arXiv:2209.15566",
    "title": "ContactNet: Online Multi-Contact Planning for Acyclic Legged Robot  Locomotion",
    "abstract": "Online trajectory optimization techniques generally depend on heuristic-based\ncontact planners in order to have low computation times and achieve high\nreplanning frequencies. In this work, we propose ContactNet, a fast acyclic\ncontact planner based on a multi-output regression neural network. ContactNet\nranks discretized stepping regions, allowing to quickly choose the best\nfeasible solution, even in complex environments. The low computation time, in\nthe order of 1 ms, makes possible the execution of the contact planner\nconcurrently with a trajectory optimizer in a Model Predictive Control (MPC)\nfashion. We demonstrate the effectiveness of the approach in simulation in\ndifferent complex scenarios with the quadruped robot Solo12.",
    "descriptor": "",
    "authors": [
      "Angelo Bratta",
      "Avadesh Meduri",
      "Michele Focchi",
      "Ludovic Righetti",
      "Claudio Semini"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15566"
  },
  {
    "id": "arXiv:2209.15567",
    "title": "Holographic-(V)AE: an end-to-end SO(3)-Equivariant (Variational)  Autoencoder in Fourier Space",
    "abstract": "Group-equivariant neural networks have emerged as a data-efficient approach\nto solve classification and regression tasks, while respecting the relevant\nsymmetries of the data. However, little work has been done to extend this\nparadigm to the unsupervised and generative domains. Here, we present\nHolographic-(V)AE (H-(V)AE), a fully end-to-end SO(3)-equivariant (variational)\nautoencoder in Fourier space, suitable for unsupervised learning and generation\nof data distributed around a specified origin. H-(V)AE is trained to\nreconstruct the spherical Fourier encoding of data, learning in the process a\nlatent space with a maximally informative invariant embedding alongside an\nequivariant frame describing the orientation of the data. We extensively test\nthe performance of H-(V)AE on diverse datasets and show that its latent space\nefficiently encodes the categorical features of spherical images and structural\nfeatures of protein atomic environments. Our work can further be seen as a case\nstudy for equivariant modeling of a data distribution by reconstructing its\nFourier encoding.",
    "descriptor": "",
    "authors": [
      "Gian Marco Visani",
      "Michael N. Pun",
      "Armita Nourmohammad"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Biological Physics (physics.bio-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.15567"
  },
  {
    "id": "arXiv:2209.15569",
    "title": "Credible Decentralized Exchange Design via Verifiable Sequencing Rules",
    "abstract": "Trading on decentralized exchanges has been one of the primary use cases for\npermissionless blockchains with daily trading volume exceeding billions of\nU.S.~dollars. In the status quo, users broadcast transactions and miners are\nresponsible for composing a block of transactions and picking an execution\nordering -- the order in which transactions execute in the exchange. Due to the\nlack of a regulatory framework, it is common to observe miners exploiting their\nprivileged position by front-running transactions and obtaining risk-fee\nprofits. In this work, we propose to modify the interaction between miners and\nusers and initiate the study of {\\em verifiable sequencing rules}. As in the\nstatus quo, miners can determine the content of a block; however, they commit\nto respecting a sequencing rule that constrains the execution ordering and is\nverifiable (there is a polynomial time algorithm that can verify if the\nexecution ordering satisfies such constraints). Thus in the event a miner\ndeviates from the sequencing rule, anyone can generate a proof of\nnon-compliance.\nWe ask if there are sequencing rules that limit price manipulation from\nminers in a two-token liquidity pool exchange. Our first result is an\nimpossibility theorem: for any sequencing rule, there is an instance of user\ntransactions where the miner can obtain non-zero risk-free profits. In light of\nthis impossibility result, our main result is a verifiable sequencing rule that\nprovides execution price guarantees for users. In particular, for any user\ntransaction $A$, it ensures that either (1) the execution price of $A$ is at\nleast as good as if $A$ was the only transaction in the block, or (2) the\nexecution price of $A$ is worse than this ``standalone'' price and the miner\ndoes not gain (or lose) when including $A$ in the block.",
    "descriptor": "\nComments: 32 Pages\n",
    "authors": [
      "Matheus V. X. Ferreira",
      "David C. Parkes"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Cryptography and Security (cs.CR)",
      "Theoretical Economics (econ.TH)",
      "Trading and Market Microstructure (q-fin.TR)"
    ],
    "url": "https://arxiv.org/abs/2209.15569"
  },
  {
    "id": "arXiv:2209.15571",
    "title": "Building Normalizing Flows with Stochastic Interpolants",
    "abstract": "A simple generative model based on a continuous-time normalizing flow between\nany pair of base and target distributions is proposed. The velocity field of\nthis flow is inferred from the probability current of a time-dependent\ndistribution that interpolates between the base and the target in finite time.\nUnlike conventional normalizing flow inference methods based the maximum\nlikelihood principle, which require costly backpropagation through ODE solvers,\nour interpolant approach leads to a simple quadratic loss for the velocity\nitself which is expressed in terms of expectations that are readily amenable to\nempirical estimation. The flow can be used to generate samples from either the\nbase or target, and can be used to estimate the likelihood at any time along\nthe interpolant. The approach is contextualized in its relation to diffusions.\nIn particular, in situations where the base is a Gaussian distribution, we show\nthat the velocity of our normalizing flow can also be used to construct a\ndiffusion model to sample the target as well as estimating its score. This\nallows one to map methods based on stochastic differential equations to those\nof ordinary differential equations, simplifying the mechanics of the model, but\ncapturing equivalent dynamics. Benchmarking on density estimation tasks\nillustrates that the learned flow can match and surpass maximum likelihood\ncontinuous flows at a fraction of the conventional ODE training costs.",
    "descriptor": "",
    "authors": [
      "Michael S. Albergo",
      "Eric Vanden-Eijnden"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15571"
  },
  {
    "id": "arXiv:2209.15573",
    "title": "Convergence of weak-SINDy Surrogate Models",
    "abstract": "In this paper, we give an in-depth error analysis for surrogate models\ngenerated by a variant of the Sparse Identification of Nonlinear Dynamics\n(SINDy) method. We start with an overview of a variety of non-linear system\nidentification techniques, namely, SINDy, weak-SINDy, and the occupation kernel\nmethod. Under the assumption that the dynamics are a finite linear combination\nof a set of basis functions, these methods establish a matrix equation to\nrecover coefficients. We illuminate the structural similarities between these\ntechniques and establish a projection property for the weak-SINDy technique.\nFollowing the overview, we analyze the error of surrogate models generated by a\nsimplified version of weak-SINDy. In particular, under the assumption of\nboundedness of a composition operator given by the solution, we show that (i)\nthe surrogate dynamics converges towards the true dynamics and (ii) the\nsolution of the surrogate model is reasonably close to the true solution.\nFinally, as an application, we discuss the use of a combination of weak-SINDy\nsurrogate modeling and proper orthogonal decomposition (POD) to build a\nsurrogate model for partial differential equations (PDEs).",
    "descriptor": "",
    "authors": [
      "Benjamin Russo",
      "M. Paul Laiu"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Dynamical Systems (math.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.15573"
  },
  {
    "id": "arXiv:2209.15574",
    "title": "An improved algorithm for Generalized \u010cech complex construction",
    "abstract": "In this paper, we present an algorithm that computes the generalized \\v{C}ech\ncomplex for a finite set of disks where each may have a different radius in 2D\nspace. An extension of this algorithm is also proposed for a set of balls in 3D\nspace with different radius.\nTo compute a $k$-simplex, we leverage the computation performed in the round\nof $(k-1)$-simplices such that we can reduce the number of potential candidates\nto verify to improve the efficiency. An efficient verification method is\nproposed to confirm if a $k$-simplex can be constructed on the basis of the\n$(k-1)$-simplices. We demonstrate the performance with a comparison to some\nclosely related algorithms.",
    "descriptor": "",
    "authors": [
      "Jie Chu",
      "Mikael Vejdemo-Johansson",
      "Ping Ji"
    ],
    "subjectives": [
      "Computational Geometry (cs.CG)"
    ],
    "url": "https://arxiv.org/abs/2209.15574"
  },
  {
    "id": "arXiv:2209.15575",
    "title": "Match to Win: Analysing Sequences Lengths for Efficient Self-supervised  Learning in Speech and Audio",
    "abstract": "Self-supervised learning (SSL) has proven vital in speech and audio-related\napplications. The paradigm trains a general model on unlabeled data that can\nlater be used to solve specific downstream tasks. This type of model is costly\nto train as it requires manipulating long input sequences that can only be\nhandled by powerful centralised servers. Surprisingly, despite many attempts to\nincrease training efficiency through model compression, the effects of\ntruncating input sequence lengths to reduce computation have not been studied.\nIn this paper, we provide the first empirical study of SSL pre-training for\ndifferent specified sequence lengths and link this to various downstream tasks.\nWe find that training on short sequences can dramatically reduce resource costs\nwhile retaining a satisfactory performance for all tasks. This simple one-line\nchange would promote the migration of SSL training from data centres to\nuser-end edge devices for more realistic and personalised applications.",
    "descriptor": "",
    "authors": [
      "Yan Gao",
      "Javier Fernandez-Marques",
      "Titouan Parcollet",
      "Pedro P. B. de Gusmao",
      "Nicholas D. Lane"
    ],
    "subjectives": [
      "Sound (cs.SD)",
      "Machine Learning (cs.LG)",
      "Audio and Speech Processing (eess.AS)"
    ],
    "url": "https://arxiv.org/abs/2209.15575"
  },
  {
    "id": "arXiv:2209.15578",
    "title": "Dual-Modality Haptic Feedback Improves Dexterous Task Execution with  Virtual EMG-Controlled Gripper",
    "abstract": "Upper-extremity amputees who use myoelectric prostheses currently lack the\nhaptic sensory information needed to perform dexterous activities of daily\nliving. While considerable research has focused on restoring this haptic\ninformation, these approaches often rely on single-modality feedback schemes\nwhich are necessary but insufficient for the feedforward and feedback control\nstrategies employed by the central nervous system. Multi-modality feedback\napproaches have been gaining attention in several application domains, however,\nthe utility for myoelectric prosthesis use remains unclear. In this study, we\ninvestigated the utility of dual-modality haptic feedback in a virtual\nEMG-controlled grasp-and-hold task with a brittle object and variable load\nforce. We recruited N=20 non-amputee participants to perform the task in four\nconditions: no feedback, vibration feedback of incipient slip, squeezing\nfeedback of grip force, and dual (vibration + squeezing) feedback of incipient\nslip and grip force. Results suggest that receiving any feedback is better than\nreceiving none, however, dual-modality feedback is far superior to either\nsingle-modality feedback approach in terms of preventing the object from\nbreaking or dropping, even after it started slipping. Control with\ndual-modality feedback was also seen as more intuitive than with either of the\nsingle-modality feedback approaches.",
    "descriptor": "",
    "authors": [
      "Kezi Li",
      "Jeremy D. Brown"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15578"
  },
  {
    "id": "arXiv:2209.15579",
    "title": "Physically Meaningful Uncertainty Quantification in Probabilistic Wind  Turbine Power Curve Models as a Damage Sensitive Feature",
    "abstract": "A wind turbines' power curve is easily accessible damage sensitive data, and\nas such is a key part of structural health monitoring in wind turbines. Power\ncurve models can be constructed in a number of ways, but the authors argue that\nprobabilistic methods carry inherent benefits in this use case, such as\nuncertainty quantification and allowing uncertainty propagation analysis. Many\nprobabilistic power curve models have a key limitation in that they are not\nphysically meaningful - they return mean and uncertainty predictions outside of\nwhat is physically possible (the maximum and minimum power outputs of the wind\nturbine). This paper investigates the use of two bounded Gaussian Processes in\norder to produce physically meaningful probabilistic power curve models. The\nfirst model investigated was a warped heteroscedastic Gaussian process, and was\nfound to be ineffective due to specific shortcomings of the Gaussian Process in\nrelation to the warping function. The second model - an approximated Gaussian\nProcess with a Beta likelihood was highly successful and demonstrated that a\nworking bounded probabilistic model results in better predictive uncertainty\nthan a corresponding unbounded one without meaningful loss in predictive\naccuracy. Such a bounded model thus offers increased accuracy for performance\nmonitoring and increased operator confidence in the model due to guaranteed\nphysical plausibility.",
    "descriptor": "",
    "authors": [
      "J.H. Mclean",
      "M.R. Jones",
      "B.J. O'Connell",
      "A.E Maguire",
      "T.J. Rogers"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15579"
  },
  {
    "id": "arXiv:2209.15588",
    "title": "New Metric Formulas that Include Measurement Errors in Machine Learning  for Natural Sciences",
    "abstract": "The application of machine learning to physics problems is widely found in\nthe scientific literature. Both regression and classification problems are\naddressed by a large array of techniques that involve learning algorithms.\nUnfortunately, the measurement errors of the data used to train machine\nlearning models are almost always neglected. This leads to estimations of the\nperformance of the models (and thus their generalisation power) that is too\noptimistic since it is always assumed that the target variables (what one wants\nto predict) are correct. In physics, this is a dramatic deficiency as it can\nlead to the belief that theories or patterns exist where, in reality, they do\nnot. This paper addresses this deficiency by deriving formulas for commonly\nused metrics (both for regression and classification problems) that take into\naccount measurement errors of target variables. The new formulas give an\nestimation of the metrics which is always more pessimistic than what is\nobtained with the classical ones, not taking into account measurement errors.\nThe formulas given here are of general validity, completely model-independent,\nand can be applied without limitations. Thus, with statistical confidence, one\ncan analyze the existence of relationships when dealing with measurements with\nerrors of any kind. The formulas have wide applicability outside physics and\ncan be used in all problems where measurement errors are relevant to the\nconclusions of studies.",
    "descriptor": "",
    "authors": [
      "Umberto Michelucci",
      "Francesca Venturini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2209.15588"
  },
  {
    "id": "arXiv:2209.15589",
    "title": "Where Should I Spend My FLOPS? Efficiency Evaluations of Visual  Pre-training Methods",
    "abstract": "Self-supervised methods have achieved remarkable success in transfer\nlearning, often achieving the same or better accuracy than supervised\npre-training. Most prior work has done so by increasing pre-training\ncomputation by adding complex data augmentation, multiple views, or lengthy\ntraining schedules. In this work, we investigate a related, but orthogonal\nquestion: given a \\textit{fixed} FLOP budget, what are the best datasets,\nmodels, and (self-)supervised training methods for obtaining high accuracy on\nrepresentative visual tasks? Given the availability of large datasets, this\nsetting is often more relevant for both academic and industry labs alike. We\nexamine five large-scale datasets (JFT-300M, ALIGN, ImageNet-1K, ImageNet-21K,\nand COCO) and six pre-training methods (CLIP, DINO, SimCLR, BYOL, Masked\nAutoencoding, and supervised). In a like-for-like fashion, we characterize\ntheir FLOP and CO$_2$ footprints, relative to their accuracy when transferred\nto a canonical image segmentation task. Our analysis reveals strong disparities\nin the computational efficiency of pre-training methods and their dependence on\ndataset quality. In particular, our results call into question the\ncommonly-held assumption that self-supervised methods inherently scale to\nlarge, uncurated data. We therefore advocate for (1) paying closer attention to\ndataset curation and (2) reporting of accuracies in context of the total\ncomputational cost.",
    "descriptor": "",
    "authors": [
      "Skanda Koppula",
      "Yazhe Li",
      "Evan Shelhamer",
      "Andrew Jaegle",
      "Nikhil Parthasarathy",
      "Relja Arandjelovic",
      "Jo\u00e3o Carreira",
      "Olivier H\u00e9naff"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15589"
  },
  {
    "id": "arXiv:2209.15594",
    "title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of  Stability",
    "abstract": "Traditional analyses of gradient descent show that when the largest\neigenvalue of the Hessian, also known as the sharpness $S(\\theta)$, is bounded\nby $2/\\eta$, training is \"stable\" and the training loss decreases\nmonotonically. Recent works, however, have observed that this assumption does\nnot hold when training modern neural networks with full batch or large batch\ngradient descent. Most recently, Cohen et al. (2021) observed two important\nphenomena. The first, dubbed progressive sharpening, is that the sharpness\nsteadily increases throughout training until it reaches the instability cutoff\n$2/\\eta$. The second, dubbed edge of stability, is that the sharpness hovers at\n$2/\\eta$ for the remainder of training while the loss continues decreasing,\nalbeit non-monotonically.\nWe demonstrate that, far from being chaotic, the dynamics of gradient descent\nat the edge of stability can be captured by a cubic Taylor expansion: as the\niterates diverge in direction of the top eigenvector of the Hessian due to\ninstability, the cubic term in the local Taylor expansion of the loss function\ncauses the curvature to decrease until stability is restored. This property,\nwhich we call self-stabilization, is a general property of gradient descent and\nexplains its behavior at the edge of stability. A key consequence of\nself-stabilization is that gradient descent at the edge of stability implicitly\nfollows projected gradient descent (PGD) under the constraint $S(\\theta) \\le\n2/\\eta$. Our analysis provides precise predictions for the loss, sharpness, and\ndeviation from the PGD trajectory throughout training, which we verify both\nempirically in a number of standard settings and theoretically under mild\nconditions. Our analysis uncovers the mechanism for gradient descent's implicit\nbias towards stability.",
    "descriptor": "\nComments: First two authors contributed equally\n",
    "authors": [
      "Alex Damian",
      "Eshaan Nichani",
      "Jason D. Lee"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Theory (cs.IT)",
      "Optimization and Control (math.OC)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15594"
  },
  {
    "id": "arXiv:2209.15595",
    "title": "Rethinking Data Heterogeneity in Federated Learning: Introducing a New  Notion and Standard Benchmarks",
    "abstract": "Though successful, federated learning presents new challenges for machine\nlearning, especially when the issue of data heterogeneity, also known as\nNon-IID data, arises. To cope with the statistical heterogeneity, previous\nworks incorporated a proximal term in local optimization or modified the model\naggregation scheme at the server side or advocated clustered federated learning\napproaches where the central server groups agent population into clusters with\njointly trainable data distributions to take the advantage of a certain level\nof personalization. While effective, they lack a deep elaboration on what kind\nof data heterogeneity and how the data heterogeneity impacts the accuracy\nperformance of the participating clients. In contrast to many of the prior\nfederated learning approaches, we demonstrate not only the issue of data\nheterogeneity in current setups is not necessarily a problem but also in fact\nit can be beneficial for the FL participants. Our observations are intuitive:\n(1) Dissimilar labels of clients (label skew) are not necessarily considered\ndata heterogeneity, and (2) the principal angle between the agents' data\nsubspaces spanned by their corresponding principal vectors of data is a better\nestimate of the data heterogeneity. Our code is available at\nhttps://github.com/MMorafah/FL-SC-NIID.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2209.10526\n",
    "authors": [
      "Mahdi Morafah",
      "Saeed Vahidian",
      "Chen Chen",
      "Mubarak Shah",
      "Bill Lin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15595"
  },
  {
    "id": "arXiv:2209.15596",
    "title": "Individual Privacy Accounting with Gaussian Differential Privacy",
    "abstract": "Individual privacy accounting enables bounding differential privacy (DP) loss\nindividually for each participant involved in the analysis. This can be\ninformative as often the individual privacy losses are considerably smaller\nthan those indicated by the DP bounds that are based on considering worst-case\nbounds at each data access. In order to account for the individual privacy\nlosses in a principled manner, we need a privacy accountant for adaptive\ncompositions of randomised mechanisms, where the loss incurred at a given data\naccess is allowed to be smaller than the worst-case loss. This kind of analysis\nhas been carried out for the R\\'enyi differential privacy (RDP) by Feldman and\nZrnic (2021), however not yet for the so-called optimal privacy accountants. We\nmake first steps in this direction by providing a careful analysis using the\nGaussian differential privacy which gives optimal bounds for the Gaussian\nmechanism, one of the most versatile DP mechanisms. This approach is based on\ndetermining a certain supermartingale for the hockey-stick divergence and on\nextending the R\\'enyi divergence-based fully adaptive composition results by\nFeldman and Zrnic (2021). We also consider measuring the individual\n$(\\varepsilon,\\delta)$-privacy losses using the so-called privacy loss\ndistributions. With the help of the Blackwell theorem, we can then make use of\nthe RDP analysis to construct an approximative individual\n$(\\varepsilon,\\delta)$-accountant.",
    "descriptor": "\nComments: 27 pages, 10 figures\n",
    "authors": [
      "Antti Koskela",
      "Marlon Tobaben",
      "Antti Honkela"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15596"
  },
  {
    "id": "arXiv:2209.15597",
    "title": "MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for  Efficient and Expressive Link Prediction",
    "abstract": "Knowledge graph embedding aims to predict the missing relations between\nentities in knowledge graphs. Tensor-decomposition-based models, such as\nComplEx, provide a good trade-off between efficiency and expressiveness, that\nis crucial because of the large size of real world knowledge graphs. The recent\nmulti-partition embedding interaction (MEI) model subsumes these models by\nusing the block term tensor format and provides a systematic solution for the\ntrade-off. However, MEI has several drawbacks, some of which carried from its\nsubsumed tensor-decomposition-based models. In this paper, we address these\ndrawbacks and introduce the Multi-partition Embedding Interaction iMproved\nbeyond block term format (MEIM) model, with independent core tensor for\nensemble effects and soft orthogonality for max-rank mapping, in addition to\nmulti-partition embedding. MEIM improves expressiveness while still being\nhighly efficient, helping it to outperform strong baselines and achieve\nstate-of-the-art results on difficult link prediction benchmarks using fairly\nsmall embedding sizes. The source code is released at\nhttps://github.com/tranhungnghiep/MEIM-KGE.",
    "descriptor": "\nComments: Accepted at the International Joint Conference on Artificial Intelligence (IJCAI), 2022; add appendix with extra experiments\n",
    "authors": [
      "Hung-Nghiep Tran",
      "Atsuhiro Takasu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15597"
  },
  {
    "id": "arXiv:2209.15603",
    "title": "Modeling dispersive silver in the electrodynamic lattice-Boltzmann  method using complex-conjugate pole-residue pairs",
    "abstract": "The polarization density of a broadband electrodynamic lattice-Boltzmann\nmethod (ELBM) is generalized to represent frequency-dispersion of materials\ninteracting with electromagnetic waves. The frequency-dependent refractive\nindex and extinction coefficient are modeled using complex-conjugate\npole-residue pairs in an auxiliary-differential-equation (ADE). Electric and\nmagnetic fields are evaluated on a single lattice, ensuring a stable numerical\nsolution up to the Nyquist limit. The electric and magnetic fields from the\nELBM are compared with the electric and magnetic fields from the\nfinite-difference-time-domain (FDTD) method. Accurate transmittance of a 100 nm\nsilver slab is extracted from the transmitted power spectrum of a broadband\nDirac-delta wave-function for photon energies ranging from 0.125-5 eV. Given\nthis capability, the ELBM with an ADE is an accurate and computationally\nefficient method for modeling broadband frequency-dispersion of materials.",
    "descriptor": "\nComments: 13 pages, 5 figures, CodeOcean samples at doi: 10.24433/CO.5926359.v1\n",
    "authors": [
      "Cael Warner",
      "Lo\u00efc Markley",
      "Kenneth J. Chau"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15603"
  },
  {
    "id": "arXiv:2209.15605",
    "title": "Bias Mimicking: A Simple Sampling Approach for Bias Mitigation",
    "abstract": "Prior work has shown that Visual Recognition datasets frequently\nunder-represent sensitive groups (\\eg Female) within a category (\\eg\nProgrammers). This dataset bias can lead to models that learn spurious\ncorrelations between class labels and sensitive attributes such as age, gender,\nor race. Most of the recent methods that address this problem require\nsignificant architectural changes or expensive hyper-parameter tuning.\nAlternatively, data re-sampling baselines from the class imbalance literature\n(\\eg Undersampling, Upweighting), which can often be implemented in a single\nline of code and often have no hyperparameters, offer a cheaper and more\nefficient solution. However, we found that some of these baselines were missing\nfrom recent bias mitigation benchmarks. In this paper, we show that these\nsimple methods are strikingly competitive with state-of-the-art bias mitigation\nmethods on many datasets. Furthermore, we improve these methods by introducing\na new class conditioned sampling method: Bias Mimicking. In cases where the\nbaseline dataset re-sampling methods do not perform well, Bias Mimicking\neffectively bridges the performance gap and improves the total averaged\naccuracy of under-represented subgroups by over $3\\%$ compared to prior work.",
    "descriptor": "",
    "authors": [
      "Maan Qraitem",
      "Kate Saenko",
      "Bryan A. Plummer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15605"
  },
  {
    "id": "arXiv:2209.15614",
    "title": "TinyTurbo: Efficient Turbo Decoders on Edge",
    "abstract": "In this paper, we introduce a neural-augmented decoder for Turbo codes called\nTINYTURBO . TINYTURBO has complexity comparable to the classical max-log-MAP\nalgorithm but has much better reliability than the max-log-MAP baseline and\nperforms close to the MAP algorithm. We show that TINYTURBO exhibits strong\nrobustness on a variety of practical channels of interest, such as EPA and EVA\nchannels, which are included in the LTE standards. We also show that TINYTURBO\nstrongly generalizes across different rate, blocklengths, and trellises. We\nverify the reliability and efficiency of TINYTURBO via over-the-air\nexperiments.",
    "descriptor": "\nComments: 10 pages, 6 figures. Published at the 2022 IEEE International Symposium on Information Theory (ISIT)\n",
    "authors": [
      "S Ashwin Hebbar",
      "Rajesh K Mishra",
      "Sravan Kumar Ankireddy",
      "Ashok V Makkuva",
      "Hyeji Kim",
      "Pramod Viswanath"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15614"
  },
  {
    "id": "arXiv:2209.15616",
    "title": "Towards Multi-spatiotemporal-scale Generalized PDE Modeling",
    "abstract": "Partial differential equations (PDEs) are central to describing complex\nphysical system simulations. Their expensive solution techniques have led to an\nincreased interest in deep neural network based surrogates. However, the\npractical utility of training such surrogates is contingent on their ability to\nmodel complex multi-scale spatio-temporal phenomena. Various neural network\narchitectures have been proposed to target such phenomena, most notably Fourier\nNeural Operators (FNOs) which give a natural handle over local \\& global\nspatial information via parameterization of different Fourier modes, and U-Nets\nwhich treat local and global information via downsampling and upsampling paths.\nHowever, generalizing across different equation parameters or different\ntime-scales still remains a challenge. In this work, we make a comprehensive\ncomparison between various FNO and U-Net like approaches on fluid mechanics\nproblems in both vorticity-stream and velocity function form. For U-Nets, we\ntransfer recent architectural improvements from computer vision, most notably\nfrom object segmentation and generative modeling. We further analyze the design\nconsiderations for using FNO layers to improve performance of U-Net\narchitectures without major degradation of computational performance. Finally,\nwe show promising results on generalization to different PDE parameters and\ntime-scales with a single surrogate model.",
    "descriptor": "",
    "authors": [
      "Jayesh K. Gupta",
      "Johannes Brandstetter"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15616"
  },
  {
    "id": "arXiv:2209.15617",
    "title": "Technical Report on: Anchoring Sagittal Plane Templates in a Spatial  Quadruped",
    "abstract": "This technical report provides a more thorough treatment of the proofs and\nderivations contained in a recent conference paper submission. The description\nof the anchoring controller is reproduced here without abridgement, and\nadditional appendices provide a clearer account of the implementation details.",
    "descriptor": "\nComments: 5 pages, 1 figure\n",
    "authors": [
      "Timothy Greco",
      "Daniel E. Koditschek"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.15617"
  },
  {
    "id": "arXiv:2209.15618",
    "title": "Beyond Bayes-optimality: meta-learning what you know you don't know",
    "abstract": "Meta-training agents with memory has been shown to culminate in Bayes-optimal\nagents, which casts Bayes-optimality as the implicit solution to a numerical\noptimization problem rather than an explicit modeling assumption. Bayes-optimal\nagents are risk-neutral, since they solely attune to the expected return, and\nambiguity-neutral, since they act in new situations as if the uncertainty were\nknown. This is in contrast to risk-sensitive agents, which additionally exploit\nthe higher-order moments of the return, and ambiguity-sensitive agents, which\nact differently when recognizing situations in which they lack knowledge.\nHumans are also known to be averse to ambiguity and sensitive to risk in ways\nthat aren't Bayes-optimal, indicating that such sensitivity can confer\nadvantages, especially in safety-critical situations. How can we extend the\nmeta-learning protocol to generate risk- and ambiguity-sensitive agents? The\ngoal of this work is to fill this gap in the literature by showing that risk-\nand ambiguity-sensitivity also emerge as the result of an optimization problem\nusing modified meta-training algorithms, which manipulate the\nexperience-generation process of the learner. We empirically test our proposed\nmeta-training algorithms on agents exposed to foundational classes of\ndecision-making experiments and demonstrate that they become sensitive to risk\nand ambiguity.",
    "descriptor": "\nComments: 33 pages, 8 figures, technical report\n",
    "authors": [
      "Jordi Grau-Moya",
      "Gr\u00e9goire Del\u00e9tang",
      "Markus Kunesch",
      "Tim Genewein",
      "Elliot Catt",
      "Kevin Li",
      "Anian Ruoss",
      "Chris Cundy",
      "Joel Veness",
      "Jane Wang",
      "Marcus Hutter",
      "Christopher Summerfield",
      "Shane Legg",
      "Pedro Ortega"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15618"
  },
  {
    "id": "arXiv:2209.15619",
    "title": "Point Normal Orientation and Surface Reconstruction by Incorporating  Isovalue Constraints to Poisson Equation",
    "abstract": "Oriented normals are common pre-requisites for many geometric algorithms\nbased on point clouds, such as Poisson surface reconstruction. However, it is\nnot trivial to obtain a consistent orientation. In this work, we bridge\norientation and reconstruction in implicit space and propose a novel approach\nto orient point clouds by incorporating isovalue constraints to the Poisson\nequation. Feeding a well-oriented point cloud into a reconstruction approach,\nthe indicator function values of the sample points should be close to the\nisovalue. Based on this observation and the Poisson equation, we propose an\noptimization formulation that combines isovalue constraints with local\nconsistency requirements for normals. We optimize normals and implicit\nfunctions simultaneously and solve for a globally consistent orientation. Owing\nto the sparsity of the linear system, an average laptop can be used to run our\nmethod within reasonable time. Experiments show that our method can achieve\nhigh performance in non-uniform and noisy data and manage varying sampling\ndensities, artifacts, multiple connected components, and nested surfaces.",
    "descriptor": "",
    "authors": [
      "Dong Xiao",
      "Zuoqiang Shi",
      "Siyu Li",
      "Bailin Deng",
      "Bin Wang"
    ],
    "subjectives": [
      "Graphics (cs.GR)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15619"
  },
  {
    "id": "arXiv:2209.15620",
    "title": "Family-Based Fingerprint Analysis: A Position Paper",
    "abstract": "Thousands of vulnerabilities are reported on a monthly basis to security\nrepositories, such as the National Vulnerability Database. Among these\nvulnerabilities, software misconfiguration is one of the top 10 security risks\nfor web applications. With this large influx of vulnerability reports, software\nfingerprinting has become a highly desired capability to discover distinctive\nand efficient signatures and recognize reportedly vulnerable software\nimplementations. Due to the exponential worst-case complexity of fingerprint\nmatching, designing more efficient methods for fingerprinting becomes highly\ndesirable, especially for variability-intensive systems where optional features\nadd another exponential factor to its analysis. This position paper presents\nour vision of a framework that lifts model learning and family-based analysis\nprinciples to software fingerprinting. In this framework, we propose unifying\ndatabases of signatures into a featured finite state machine and using presence\nconditions to specify whether and in which circumstances a given input-output\ntrace is observed. We believe feature-based signatures can aid performance\nimprovements by reducing the size of fingerprints under analysis.",
    "descriptor": "\nComments: Paper published in the Proceedings A Journey from Process Algebra via Timed Automata to Model Learning: Essays Dedicated to Frits Vaandrager on the Occasion of His 60th Birthday 2022\n",
    "authors": [
      "Carlos Diego Nascimento Damasceno",
      "Daniel Str\u00fcber"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)",
      "Software Engineering (cs.SE)"
    ],
    "url": "https://arxiv.org/abs/2209.15620"
  },
  {
    "id": "arXiv:2209.15621",
    "title": "Neural Unbalanced Optimal Transport via Cycle-Consistent Semi-Couplings",
    "abstract": "Comparing unpaired samples of a distribution or population taken at different\npoints in time is a fundamental task in many application domains where\nmeasuring populations is destructive and cannot be done repeatedly on the same\nsample, such as in single-cell biology. Optimal transport (OT) can solve this\nchallenge by learning an optimal coupling of samples across distributions from\nunpaired data. However, the usual formulation of OT assumes conservation of\nmass, which is violated in unbalanced scenarios in which the population size\nchanges (e.g., cell proliferation or death) between measurements. In this work,\nwe introduce NubOT, a neural unbalanced OT formulation that relies on the\nformalism of semi-couplings to account for creation and destruction of mass. To\nestimate such semi-couplings and generalize out-of-sample, we derive an\nefficient parameterization based on neural optimal transport maps and propose a\nnovel algorithmic scheme through a cycle-consistent training procedure. We\napply our method to the challenging task of forecasting heterogeneous responses\nof multiple cancer cell lines to various drugs, where we observe that by\naccurately modeling cell proliferation and death, our method yields notable\nimprovements over previous neural optimal transport methods.",
    "descriptor": "",
    "authors": [
      "Frederike L\u00fcbeck",
      "Charlotte Bunne",
      "Gabriele Gut",
      "Jacobo Sarabia del Castillo",
      "Lucas Pelkmans",
      "David Alvarez-Melis"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2209.15621"
  },
  {
    "id": "arXiv:2209.15622",
    "title": "A Functional Model For Information Exploration Systems",
    "abstract": "Information exploration tasks are inherently complex, ill-structured, and\ninvolve sequences of actions usually spread over many sessions. When exploring\na dataset, users tend to experiment higher degrees of uncertainty, mostly\nraised by knowledge gaps concerning the information sources, the task, and the\nefficiency of the chosen exploration actions, strategies, and tools in\nsupporting the task solution process. Provided these concerns, exploration\ntools should be designed with the goal of leveraging the mapping between user's\ncognitive actions and solution strategies onto the current systems' operations.\nHowever, state-of-the-art systems fail in providing an expressive set of\noperations that covers a wide range of exploration problems. There is not a\ncommon understanding of neither which operators are required nor in which ways\nthey can be used by explorers. In order to mitigate these shortcomings, this\nwork presents a formal framework of exploration operations expressive enough to\ndescribe at least the majority of state-of-the-art exploration interfaces and\ntasks. We also show how the framework leveraged a new evaluation approach,\nwhere we draw precise comparisons between tools concerning the range of\nexploration tasks they support.",
    "descriptor": "\nComments: 27 pages\n",
    "authors": [
      "Thiago Nunes",
      "Daniel Schwabe"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.15622"
  },
  {
    "id": "arXiv:2209.15623",
    "title": "An Efficient Modular Exponentiation Proof Scheme",
    "abstract": "We present an efficient proof scheme for any instance of left-to-right\nmodular exponentiation, used in the Fermat probable prime test. Specifically,\nwe show that for any $(a,n,r,m)$ the claim $a^n\\equiv r\\pmod m$ can be proven\nand verified with an overhead negligible compared to the computational cost of\nthe exponentiation. Our work generalizes the Gerbicz-Pietrzak double check\nscheme, greatly improving the efficiency of general probabilistic primality\ntests in distributed searches for primes such as PrimeGrid.",
    "descriptor": "",
    "authors": [
      "Darren Li",
      "Yves Gallot"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Number Theory (math.NT)"
    ],
    "url": "https://arxiv.org/abs/2209.15623"
  },
  {
    "id": "arXiv:2209.15625",
    "title": "Anomaly localization for copy detection patterns through print  estimations",
    "abstract": "Copy detection patterns (CDP) are recent technologies for protecting products\nfrom counterfeiting. However, in contrast to traditional copy fakes, deep\nlearning-based fakes have shown to be hardly distinguishable from originals by\ntraditional authentication systems. Systems based on classical supervised\nlearning and digital templates assume knowledge of fake CDP at training time\nand cannot generalize to unseen types of fakes. Authentication based on printed\ncopies of originals is an alternative that yields better results even for\nunseen fakes and simple authentication metrics but comes at the impractical\ncost of acquisition and storage of printed copies. In this work, to overcome\nthese shortcomings, we design a machine learning (ML) based authentication\nsystem that only requires digital templates and printed original CDP for\ntraining, whereas authentication is based solely on digital templates, which\nare used to estimate original printed codes. The obtained results show that the\nproposed system can efficiently authenticate original and detect fake CDP by\naccurately locating the anomalies in the fake CDP. The empirical evaluation of\nthe authentication system under investigation is performed on the original and\nML-based fakes CDP printed on two industrial printers.",
    "descriptor": "",
    "authors": [
      "Brian Pulfer",
      "Yury Belousov",
      "Joakim Tutt",
      "Roman Chaban",
      "Olga Taran",
      "Taras Holotyak",
      "Slava Voloshynovskiy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15625"
  },
  {
    "id": "arXiv:2209.15626",
    "title": "B2RL: An open-source Dataset for Building Batch Reinforcement Learning",
    "abstract": "Batch reinforcement learning (BRL) is an emerging research area in the RL\ncommunity. It learns exclusively from static datasets (i.e. replay buffers)\nwithout interaction with the environment. In the offline settings, existing\nreplay experiences are used as prior knowledge for BRL models to find the\noptimal policy. Thus, generating replay buffers is crucial for BRL model\nbenchmark. In our B2RL (Building Batch RL) dataset, we collected real-world\ndata from our building management systems, as well as buffers generated by\nseveral behavioral policies in simulation environments. We believe it could\nhelp building experts on BRL research. To the best of our knowledge, we are the\nfirst to open-source building datasets for the purpose of BRL learning.",
    "descriptor": "",
    "authors": [
      "Hsin-Yu Liu",
      "Xiaohan Fu",
      "Bharathan Balaji",
      "Rajesh Gupta",
      "Dezhi Hong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15626"
  },
  {
    "id": "arXiv:2209.15632",
    "title": "ExtrudeNet: Unsupervised Inverse Sketch-and-Extrude for Shape Parsing",
    "abstract": "Sketch-and-extrude is a common and intuitive modeling process in computer\naided design. This paper studies the problem of learning the shape given in the\nform of point clouds by inverse sketch-and-extrude. We present ExtrudeNet, an\nunsupervised end-to-end network for discovering sketch and extrude from point\nclouds. Behind ExtrudeNet are two new technical components: 1) an effective\nrepresentation for sketch and extrude, which can model extrusion with freeform\nsketches and conventional cylinder and box primitives as well; and 2) a\nnumerical method for computing the signed distance field which is used in the\nnetwork learning. This is the first attempt that uses machine learning to\nreverse engineer the sketch-and-extrude modeling process of a shape in an\nunsupervised fashion. ExtrudeNet not only outputs a compact, editable and\ninterpretable representation of the shape that can be seamlessly integrated\ninto modern CAD software, but also aligns with the standard CAD modeling\nprocess facilitating various editing applications, which distinguishes our work\nfrom existing shape parsing research. Code is released at\nhttps://github.com/kimren227/ExtrudeNet.",
    "descriptor": "\nComments: Accepted to ECCV 2022\n",
    "authors": [
      "Daxuan Ren",
      "Jianmin Zheng",
      "Jianfei Cai",
      "Jiatong Li",
      "Junzhe Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2209.15632"
  },
  {
    "id": "arXiv:2209.15634",
    "title": "A General Framework for Sample-Efficient Function Approximation in  Reinforcement Learning",
    "abstract": "With the increasing need for handling large state and action spaces, general\nfunction approximation has become a key technique in reinforcement learning\n(RL). In this paper, we propose a general framework that unifies model-based\nand model-free RL, and an Admissible Bellman Characterization (ABC) class that\nsubsumes nearly all Markov Decision Process (MDP) models in the literature for\ntractable RL. We propose a novel estimation function with decomposable\nstructural properties for optimization-based exploration and the functional\neluder dimension as a complexity measure of the ABC class. Under our framework,\na new sample-efficient algorithm namely OPtimization-based ExploRation with\nApproximation (OPERA) is proposed, achieving regret bounds that match or\nimprove over the best-known results for a variety of MDP models. In particular,\nfor MDPs with low Witness rank, under a slightly stronger assumption, OPERA\nimproves the state-of-the-art sample complexity results by a factor of $dH$.\nOur framework provides a generic interface to design and analyze new RL models\nand algorithms.",
    "descriptor": "",
    "authors": [
      "Zixiang Chen",
      "Chris Junchi Li",
      "Angela Yuan",
      "Quanquan Gu",
      "Michael I. Jordan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.15634"
  },
  {
    "id": "arXiv:2209.15635",
    "title": "Vertical Semi-Federated Learning for Efficient Online Advertising",
    "abstract": "As an emerging secure learning paradigm in leveraging cross-silo private\ndata, vertical federated learning (VFL) is expected to improve advertising\nmodels by enabling the joint learning of complementary user attributes\nprivately owned by the advertiser and the publisher. However, the 1) restricted\napplicable scope to overlapped samples and 2) high system challenge of\nreal-time federated serving have limited its application to advertising\nsystems.\nIn this paper, we advocate new learning setting Semi-VFL (Vertical\nSemi-Federated Learning) as a lightweight solution to utilize all available\ndata (both the overlapped and non-overlapped data) that is free from federated\nserving. Semi-VFL is expected to perform better than single-party models and\nmaintain a low inference cost. It's notably important to i) alleviate the\nabsence of the passive party's feature and ii) adapt to the whole sample space\nto implement a good solution for Semi-VFL. Thus, we propose a carefully\ndesigned joint privileged learning framework (JPL) as an efficient\nimplementation of Semi-VFL. Specifically, we build an inference-efficient\nsingle-party student model applicable to the whole sample space and meanwhile\nmaintain the advantage of the federated feature extension. Novel feature\nimitation and ranking consistency restriction methods are proposed to extract\ncross-party feature correlations and maintain cross-sample-space consistency\nfor both the overlapped and non-overlapped data.\nWe conducted extensive experiments on real-world advertising datasets. The\nresults show that our method achieves the best performance over baseline\nmethods and validate its effectiveness in maintaining cross-view feature\ncorrelation.",
    "descriptor": "",
    "authors": [
      "Wenjie Li",
      "Qiaolin Xia",
      "Hao Cheng",
      "Kouyin Xue",
      "Shu-Tao Xia"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2209.15635"
  },
  {
    "id": "arXiv:2209.15637",
    "title": "Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator",
    "abstract": "3D-aware image synthesis aims at learning a generative model that can render\nphoto-realistic 2D images while capturing decent underlying 3D shapes. A\npopular solution is to adopt the generative adversarial network (GAN) and\nreplace the generator with a 3D renderer, where volume rendering with neural\nradiance field (NeRF) is commonly used. Despite the advancement of synthesis\nquality, existing methods fail to obtain moderate 3D shapes. We argue that,\nconsidering the two-player game in the formulation of GANs, only making the\ngenerator 3D-aware is not enough. In other words, displacing the generative\nmechanism only offers the capability, but not the guarantee, of producing\n3D-aware images, because the supervision of the generator primarily comes from\nthe discriminator. To address this issue, we propose GeoD through learning a\ngeometry-aware discriminator to improve 3D-aware GANs. Concretely, besides\ndifferentiating real and fake samples from the 2D image space, the\ndiscriminator is additionally asked to derive the geometry information from the\ninputs, which is then applied as the guidance of the generator. Such a simple\nyet effective design facilitates learning substantially more accurate 3D\nshapes. Extensive experiments on various generator architectures and training\ndatasets verify the superiority of GeoD over state-of-the-art alternatives.\nMoreover, our approach is registered as a general framework such that a more\ncapable discriminator (i.e., with a third task of novel view synthesis beyond\ndomain classification and geometry extraction) can further assist the generator\nwith a better multi-view consistency.",
    "descriptor": "\nComments: Accepted by NeurIPS 2022. Project page: this https URL\n",
    "authors": [
      "Zifan Shi",
      "Yinghao Xu",
      "Yujun Shen",
      "Deli Zhao",
      "Qifeng Chen",
      "Dit-Yan Yeung"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15637"
  },
  {
    "id": "arXiv:2209.15639",
    "title": "F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language  Models",
    "abstract": "We present F-VLM, a simple open-vocabulary object detection method built upon\nFrozen Vision and Language Models. F-VLM simplifies the current multi-stage\ntraining pipeline by eliminating the need for knowledge distillation or\ndetection-tailored pretraining. Surprisingly, we observe that a frozen VLM: 1)\nretains the locality-sensitive features necessary for detection, and 2) is a\nstrong region classifier. We finetune only the detector head and combine the\ndetector and VLM outputs for each region at inference time. F-VLM shows\ncompelling scaling behavior and achieves +6.5 mask AP improvement over the\nprevious state of the art on novel categories of LVIS open-vocabulary detection\nbenchmark. In addition, we demonstrate very competitive results on COCO\nopen-vocabulary detection benchmark and cross-dataset transfer detection, in\naddition to significant training speed-up and compute savings. Code will be\nreleased.",
    "descriptor": "\nComments: 19 pages, 6 figures\n",
    "authors": [
      "Weicheng Kuo",
      "Yin Cui",
      "Xiuye Gu",
      "AJ Piergiovanni",
      "Anelia Angelova"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15639"
  },
  {
    "id": "arXiv:2209.15640",
    "title": "HSD: A hierarchical singing annotation dataset",
    "abstract": "Commonly music has an obvious hierarchical structure, especially for the\nsinging parts which usually act as the main melody in pop songs. However, most\nof the current singing annotation datasets only record symbolic information of\nmusic notes, ignoring the structure of music. In this paper, we propose a\nhierarchical singing annotation dataset that consists of 68 pop songs from\nYoutube. This dataset records the onset/offset time, pitch, duration, and lyric\nof each musical note in an enhanced LyRiCs format to present the hierarchical\nstructure of music. We annotate each song in a two-stage process: first, create\ninitial labels with the corresponding musical notation and lyrics file; second,\nmanually calibrate these labels referring to the raw audio. We mainly validate\nthe labeling accuracy of the proposed dataset by comparing it with an automatic\nsinging transcription (AST) dataset. The result indicates that the proposed\ndataset reaches the labeling accuracy of AST datasets.",
    "descriptor": "",
    "authors": [
      "Xiao Fu",
      "Xin Yuan",
      "Jinglu Hu"
    ],
    "subjectives": [
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2209.15640"
  },
  {
    "id": "arXiv:2209.15009",
    "title": "On Symmetric Pseudo-Boolean Functions: Factorization, Kernels and  Applications",
    "abstract": "A symmetric pseudo-Boolean function is a map from Boolean tuples to real\nnumbers which is invariant under input variable interchange. We prove that any\nsuch function can be equivalently expressed as a power series or factorized.\nThe kernel of a pseudo-Boolean function is the set of all inputs that cause the\nfunction to vanish identically. Any $n$-variable symmetric pseudo-Boolean\nfunction $f(x_1, x_2, \\dots, x_n)$ has a kernel corresponding to at least one\n$n$-affine hyperplane, each hyperplane is given by a constraint $\\sum_{l=1}^n\nx_l = \\lambda$ for $\\lambda\\in \\mathbb{C}$ constant. We use these results to\nanalyze symmetric pseudo-Boolean functions appearing in the literature of spin\nglass energy functions (Ising models), quantum information and tensor networks.",
    "descriptor": "\nComments: 10 pages\n",
    "authors": [
      "Richik Sengupta",
      "Jacob Biamonte"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)",
      "Quantum Physics (quant-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.15009"
  },
  {
    "id": "arXiv:2209.15035",
    "title": "Double negation stable h-propositions in cubical sets",
    "abstract": "We give a construction of classifiers for double negation stable\nh-propositions in a variety of cubical set models of homotopy type theory and\ncubical type theory. This is used to give some relative consistency results:\nclassifiers for double negation stable propositions exist in cubical sets\nwhenever they exist in the metatheory; the Dedekind real numbers can be added\nto homotopy type theory without changing the consistency strength; we construct\na model of homotopy type theory with extended Church's thesis, which states\nthat all partial functions with double negation stable domain are computable.",
    "descriptor": "",
    "authors": [
      "Andrew W. Swan"
    ],
    "subjectives": [
      "Logic (math.LO)",
      "Logic in Computer Science (cs.LO)"
    ],
    "url": "https://arxiv.org/abs/2209.15035"
  },
  {
    "id": "arXiv:2209.15036",
    "title": "Large-Scale Spatial Cross-Calibration of Hinode/SOT-SP and SDO/HMI",
    "abstract": "We investigate the cross-calibration of the Hinode/SOT-SP and SDO/HMI\ninstrument meta-data, specifically the correspondence of the scaling and\npointing information. Accurate calibration of these datasets gives the\ncorrespondence needed by inter-instrument studies and learning-based\nmagnetogram systems, and is required for physically-meaningful photospheric\nmagnetic field vectors. We approach the problem by robustly fitting geometric\nmodels on correspondences between images from each instrument's pipeline. This\ntechnique is common in computer vision, but several critical details are\nrequired when using scanning slit spectrograph data like Hinode/SOT-SP. We\napply this technique to data spanning a decade of the Hinode mission. Our\nresults suggest corrections to the published Level 2 Hinode/SOT-SP data. First,\nan analysis on approximately 2,700 scans suggests that the reported pixel size\nin Hinode/SOT-SP Level 2 data is incorrect by around 1%. Second, analysis of\nover 12,000 scans show that the pointing information is often incorrect by\ndozens of arcseconds with a strong bias. Regression of these corrections\nindicates that thermal effects have caused secular and cyclic drift in\nHinode/SOT-SP pointing data over its mission. We offer two solutions. First,\ndirect co-alignment with SDO/HMI data via our procedure can improve alignments\nfor many Hinode/SOT-SP scans. Second, since the pointing errors are\npredictable, simple post-hoc corrections can substantially improve the\npointing. We conclude by illustrating the impact of this updated calibration on\nderived physical data products needed for research and interpretation. Among\nother things, our results suggest that the pointing errors induce a hemispheric\nbias in estimates of radial current density.",
    "descriptor": "\nComments: Under revisions at ApJS\n",
    "authors": [
      "David F. Fouhey",
      "Richard E. L. Higgins",
      "Spiro K. Antiochos",
      "Graham Barnes",
      "Marc L. DeRosa",
      "J. Todd Hoeksema",
      "K. D. Leka",
      "Yang Liu",
      "Peter W. Schuck",
      "Tamas I. Gombosi"
    ],
    "subjectives": [
      "Solar and Stellar Astrophysics (astro-ph.SR)",
      "Instrumentation and Methods for Astrophysics (astro-ph.IM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15036"
  },
  {
    "id": "arXiv:2209.15051",
    "title": "Circularity of Thermodynamical Material Networks: Indicators, Examples  and Algorithms",
    "abstract": "The transition towards a circular economy has gained importance over the last\nyears since the traditional linear take-make-dispose paradigm is not\nsustainable in the long term. Recently, thermodynamical material networks\n(TMNs) [1] have been proposed as an approach to re-design material flows based\non the idea that any supply chain can be seen as a set of thermodynamic\ncompartments that can be added, removed, modified or connected differently. In\nthis paper, we develop several circularity indicators of TMNs using a\ngraph-based formalism and demonstrate their calculation through examples. The\npaper source code is publicly available.",
    "descriptor": "\nComments: To be added one more example\n",
    "authors": [
      "Federico Zocco",
      "Beatrice Smyth",
      "Pantelis Sopasakis"
    ],
    "subjectives": [
      "Dynamical Systems (math.DS)",
      "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "url": "https://arxiv.org/abs/2209.15051"
  },
  {
    "id": "arXiv:2209.15055",
    "title": "Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear  Functions",
    "abstract": "We show that the representation cost of fully connected neural networks with\nhomogeneous nonlinearities - which describes the implicit bias in function\nspace of networks with $L_2$-regularization or with losses such as the\ncross-entropy - converges as the depth of the network goes to infinity to a\nnotion of rank over nonlinear functions. We then inquire under which conditions\nthe global minima of the loss recover the `true' rank of the data: we show that\nfor too large depths the global minimum will be approximately rank 1\n(underestimating the rank); we then argue that there is a range of depths which\ngrows with the number of datapoints where the true rank is recovered. Finally,\nwe discuss the effect of the rank of a classifier on the topology of the\nresulting class boundaries and show that autoencoders with optimal nonlinear\nrank are naturally denoising.",
    "descriptor": "",
    "authors": [
      "Arthur Jacot"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15055"
  },
  {
    "id": "arXiv:2209.15094",
    "title": "Open-source tool for Airway Segmentation in Computed Tomography using  2.5D Modified EfficientDet: Contribution to the ATM22 Challenge",
    "abstract": "Airway segmentation in computed tomography images can be used to analyze\npulmonary diseases, however, manual segmentation is labor intensive and relies\non expert knowledge. This manuscript details our contribution to MICCAI's 2022\nAirway Tree Modelling challenge, a competition of fully automated methods for\nairway segmentation. We employed a previously developed deep learning\narchitecture based on a modified EfficientDet (MEDSeg), training from scratch\nfor binary airway segmentation using the provided annotations. Our method\nachieved 90.72 Dice in internal validation, 95.52 Dice on external validation,\nand 93.49 Dice in the final test phase, while not being specifically designed\nor tuned for airway segmentation. Open source code and a pip package for\npredictions with our model and trained weights are in\nhttps://github.com/MICLab-Unicamp/medseg.",
    "descriptor": "\nComments: Open source code and a pip package for predictions with our model and trained weights are in this https URL\n",
    "authors": [
      "Diedre Carmo",
      "Leticia Rittner",
      "Roberto Lotufo"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15094"
  },
  {
    "id": "arXiv:2209.15096",
    "title": "AICCA: AI-driven Cloud Classification Atlas",
    "abstract": "Clouds play an important role in the Earth's energy budget and their behavior\nis one of the largest uncertainties in future climate projections. Satellite\nobservations should help in understanding cloud responses, but decades and\npetabytes of multispectral cloud imagery have to date received only limited\nuse. This study reduces the dimensionality of satellite cloud observations by\ngrouping them via a novel automated, unsupervised cloud classification\ntechnique by using a convolutional neural network. Our technique combines a\nrotation-invariant autoencoder with hierarchical agglomerative clustering to\ngenerate cloud clusters that capture meaningful distinctions among cloud\ntextures, using only raw multispectral imagery as input. Thus, cloud classes\nare defined without reliance on location, time/season, derived physical\nproperties, or pre-designated class definitions. We use this approach to\ngenerate a unique new cloud dataset, the AI-driven cloud classification atlas\n(AICCA), which clusters 22 years of ocean images from the Moderate Resolution\nImaging Spectroradiometer (MODIS) on NASA's Aqua and Terra instruments - 800 TB\nof data or 198 million patches roughly 100 km x 100 km (128 x 128 pixels) -\ninto 42 AI-generated cloud classes. We show that AICCA classes involve\nmeaningful distinctions that employ spatial information and result in distinct\ngeographic distributions, capturing, for example, stratocumulus decks along the\nWest coasts of North and South America. AICCA delivers the information in\nmulti-spectral images in a compact form, enables data-driven diagnosis of\npatterns of cloud organization, provides insight into cloud evolution on\ntimescales of hours to decades, and helps democratize climate research by\nfacilitating access to core data.",
    "descriptor": "\nComments: 27 pages, 11 figures, under review in MDPI Remote Sensing\n",
    "authors": [
      "Takuya Kurihana",
      "Elisabeth Moyer",
      "Ian Foster"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15096"
  },
  {
    "id": "arXiv:2209.15097",
    "title": "Likelihood adjusted semidefinite programs for clustering heterogeneous  data",
    "abstract": "Clustering is a widely deployed unsupervised learning tool. Model-based\nclustering is a flexible framework to tackle data heterogeneity when the\nclusters have different shapes. Likelihood-based inference for mixture\ndistributions often involves non-convex and high-dimensional objective\nfunctions, imposing difficult computational and statistical challenges. The\nclassic expectation-maximization (EM) algorithm is a computationally thrifty\niterative method that maximizes a surrogate function minorizing the\nlog-likelihood of observed data in each iteration, which however suffers from\nbad local maxima even in the special case of the standard Gaussian mixture\nmodel with common isotropic covariance matrices. On the other hand, recent\nstudies reveal that the unique global solution of a semidefinite programming\n(SDP) relaxed $K$-means achieves the information-theoretically sharp threshold\nfor perfectly recovering the cluster labels under the standard Gaussian mixture\nmodel. In this paper, we extend the SDP approach to a general setting by\nintegrating cluster labels as model parameters and propose an iterative\nlikelihood adjusted SDP (iLA-SDP) method that directly maximizes the\n\\emph{exact} observed likelihood in the presence of data heterogeneity. By\nlifting the cluster assignment to group-specific membership matrices, iLA-SDP\navoids centroids estimation -- a key feature that allows exact recovery under\nwell-separateness of centroids without being trapped by their adversarial\nconfigurations. Thus iLA-SDP is less sensitive than EM to initialization and\nmore stable on high-dimensional data. Our numeric experiments demonstrate that\niLA-SDP can achieve lower mis-clustering errors over several widely used\nclustering methods including $K$-means, SDP and EM algorithms.",
    "descriptor": "",
    "authors": [
      "Yubo Zhuang",
      "Xiaohui Chen",
      "Yun Yang"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.15097"
  },
  {
    "id": "arXiv:2209.15121",
    "title": "Heterogeneous reconstruction of deformable atomic models in Cryo-EM",
    "abstract": "Cryogenic electron microscopy (cryo-EM) provides a unique opportunity to\nstudy the structural heterogeneity of biomolecules. Being able to explain this\nheterogeneity with atomic models would help our understanding of their\nfunctional mechanisms but the size and ruggedness of the structural space (the\nspace of atomic 3D cartesian coordinates) presents an immense challenge. Here,\nwe describe a heterogeneous reconstruction method based on an atomistic\nrepresentation whose deformation is reduced to a handful of collective motions\nthrough normal mode analysis. Our implementation uses an autoencoder. The\nencoder jointly estimates the amplitude of motion along the normal modes and\nthe 2D shift between the center of the image and the center of the molecule .\nThe physics-based decoder aggregates a representation of the heterogeneity\nreadily interpretable at the atomic level. We illustrate our method on 3\nsynthetic datasets corresponding to different distributions along a simulated\ntrajectory of adenylate kinase transitioning from its open to its closed\nstructures. We show for each distribution that our approach is able to\nrecapitulate the intermediate atomic models with atomic-level accuracy.",
    "descriptor": "\nComments: 8 pages, 1 figure\n",
    "authors": [
      "Youssef Nashed",
      "Ariana Peck",
      "Julien Martel",
      "Axel Levy",
      "Bongjin Koo",
      "Gordon Wetzstein",
      "Nina Miolane",
      "Daniel Ratner",
      "Fr\u00e9d\u00e9ric Poitevin"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Image and Video Processing (eess.IV)",
      "Chemical Physics (physics.chem-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.15121"
  },
  {
    "id": "arXiv:2209.15129",
    "title": "Boundary control of time-harmonic eddy current equations",
    "abstract": "Motivated by various applications, this article develops the notion of\nboundary control for Maxwell's equations in the frequency domain. Surface curl\nis shown to be the appropriate regularization in order for the optimal control\nproblem to be well-posed. Since, all underlying variables are assumed to be\ncomplex valued, the standard results on differentiability do not directly\napply. Instead, we extend the notion of Wirtinger derivatives to complexified\nHilbert spaces. Optimality conditions are rigorously derived and higher order\nboundary regularity of the adjoint variable is established. The state and\nadjoint variables are discretized using higher order N\\'ed\\'elec finite\nelements. The finite element space for controls is identified, as a space,\nwhich preserves the structure of the control regularization. Convergence of the\nfully discrete scheme is established. The theory is validated by numerical\nexperiments, in some cases, motivated by realistic applications.",
    "descriptor": "\nComments: 25 pages, 6 figures\n",
    "authors": [
      "Harbir Antil",
      "Hugo D\u00edaz"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.15129"
  },
  {
    "id": "arXiv:2209.15130",
    "title": "Nonconvex Matrix Factorization is Geodesically Convex: Global Landscape  Analysis for Fixed-rank Matrix Optimization From a Riemannian Perspective",
    "abstract": "We study a general matrix optimization problem with a fixed-rank positive\nsemidefinite (PSD) constraint. We perform the Burer-Monteiro factorization and\nconsider a particular Riemannian quotient geometry in a search space that has a\ntotal space equipped with the Euclidean metric. When the original objective f\nsatisfies standard restricted strong convexity and smoothness properties, we\ncharacterize the global landscape of the factorized objective under the\nRiemannian quotient geometry. We show the entire search space can be divided\ninto three regions: (R1) the region near the target parameter of interest,\nwhere the factorized objective is geodesically strongly convex and smooth; (R2)\nthe region containing neighborhoods of all strict saddle points; (R3) the\nremaining regions, where the factorized objective has a large gradient. To our\nbest knowledge, this is the first global landscape analysis of the\nBurer-Monteiro factorized objective under the Riemannian quotient geometry. Our\nresults provide a fully geometric explanation for the superior performance of\nvanilla gradient descent under the Burer-Monteiro factorization. When f\nsatisfies a weaker restricted strict convexity property, we show there exists a\nneighborhood near local minimizers such that the factorized objective is\ngeodesically convex. To prove our results we provide a comprehensive landscape\nanalysis of a matrix factorization problem with a least squares objective,\nwhich serves as a critical bridge. Our conclusions are also based on a result\nof independent interest stating that the geodesic ball centered at Y with a\nradius 1/3 of the least singular value of Y is a geodesically convex set under\nthe Riemannian quotient geometry, which as a corollary, also implies a\nquantitative bound of the convexity radius in the Bures-Wasserstein space. The\nconvexity radius obtained is sharp up to constants.",
    "descriptor": "\nComments: The abstract is shortened to meet the arXiv submission requirement\n",
    "authors": [
      "Yuetian Luo",
      "Nicolas Garcia Trillos"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2209.15130"
  },
  {
    "id": "arXiv:2209.15136",
    "title": "Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20$\\times$  Speedup",
    "abstract": "Low-dose computed tomography (LDCT) is an important topic in the field of\nradiology over the past decades. LDCT reduces ionizing radiation-induced\npatient health risks but it also results in a low signal-to-noise ratio (SNR)\nand a potential compromise in the diagnostic performance. In this paper, to\nimprove the LDCT denoising performance, we introduce the conditional denoising\ndiffusion probabilistic model (DDPM) and show encouraging results with a high\ncomputational efficiency. Specifically, given the high sampling cost of the\noriginal DDPM model, we adapt the fast ordinary differential equation (ODE)\nsolver for a much-improved sampling efficiency. The experiments show that the\naccelerated DDPM can achieve 20x speedup without compromising image quality.",
    "descriptor": "",
    "authors": [
      "Wenjun Xia",
      "Qing Lyu",
      "Ge Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Machine Learning (cs.LG)",
      "Signal Processing (eess.SP)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.15136"
  },
  {
    "id": "arXiv:2209.15171",
    "title": "Dynamic-Backbone Protein-Ligand Structure Prediction with Multiscale  Generative Diffusion Models",
    "abstract": "Molecular complexes formed by proteins and small-molecule ligands are\nubiquitous, and predicting their 3D structures can facilitate both biological\ndiscoveries and the design of novel enzymes or drug molecules. Here we propose\nNeuralPLexer, a deep generative model framework to rapidly predict\nprotein-ligand complex structures and their fluctuations using protein backbone\ntemplate and molecular graph inputs. NeuralPLexer jointly samples protein and\nsmall-molecule 3D coordinates at an atomistic resolution through a generative\nmodel that incorporates biophysical constraints and inferred proximity\ninformation into a time-truncated diffusion process. The reverse-time\ngenerative diffusion process is learned by a novel stereochemistry-aware\nequivariant graph transformer that enables efficient, concurrent gradient field\nprediction for all heavy atoms in the protein-ligand complex. NeuralPLexer\noutperforms existing physics-based and learning-based methods on benchmarking\nproblems including fixed-backbone blind protein-ligand docking and\nligand-coupled binding site repacking. Moreover, we identify preliminary\nevidence that NeuralPLexer enriches bound-state-like protein structures when\napplied to systems where protein folding landscapes are significantly altered\nby the presence of ligands. Our results reveal that a data-driven approach can\ncapture the structural cooperativity among protein and small-molecule entities,\nshowing promise for the computational identification of novel drug targets and\nthe end-to-end differentiable design of functional small-molecules and\nligand-binding proteins.",
    "descriptor": "",
    "authors": [
      "Zhuoran Qiao",
      "Weili Nie",
      "Arash Vahdat",
      "Thomas F. Miller III",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2209.15171"
  },
  {
    "id": "arXiv:2209.15174",
    "title": "Music Source Separation with Band-split RNN",
    "abstract": "The performance of music source separation (MSS) models has been greatly\nimproved in recent years thanks to the development of novel neural network\narchitectures and training pipelines. However, recent model designs for MSS\nwere mainly motivated by other audio processing tasks or other research fields,\nwhile the intrinsic characteristics and patterns of the music signals were not\nfully discovered. In this paper, we propose band-split RNN (BSRNN), a\nfrequency-domain model that explictly splits the spectrogram of the mixture\ninto subbands and perform interleaved band-level and sequence-level modeling.\nThe choices of the bandwidths of the subbands can be determined by a priori\nknowledge or expert knowledge on the characteristics of the target source in\norder to optimize the performance on a certain type of target musical\ninstrument. To better make use of unlabeled data, we also describe a\nsemi-supervised model finetuning pipeline that can further improve the\nperformance of the model. Experiment results show that BSRNN trained only on\nMUSDB18-HQ dataset significantly outperforms several top-ranking models in\nMusic Demixing (MDX) Challenge 2021, and the semi-supervised finetuning stage\nfurther improves the performance on all four instrument tracks.",
    "descriptor": "",
    "authors": [
      "Yi Luo",
      "Jianwei Yu"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)",
      "Sound (cs.SD)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.15174"
  },
  {
    "id": "arXiv:2209.15180",
    "title": "SCI: A spectrum concentrated implicit neural compression for biomedical  data",
    "abstract": "Massive collection and explosive growth of the huge amount of medical data,\ndemands effective compression for efficient storage, transmission and sharing.\nReadily available visual data compression techniques have been studied\nextensively but tailored for nature images/videos, and thus show limited\nperformance on medical data which are of different characteristics. Emerging\nimplicit neural representation (INR) is gaining momentum and demonstrates high\npromise for fitting diverse visual data in target-data-specific manner, but a\ngeneral compression scheme covering diverse medical data is so far absent. To\naddress this issue, we firstly derive a mathematical explanation for INR's\nspectrum concentration property and an analytical insight on the design of\ncompression-oriented INR architecture. Further, we design a funnel shaped\nneural network capable of covering broad spectrum of complex medical data and\nachieving high compression ratio. Based on this design, we conduct compression\nvia optimization under given budget and propose an adaptive compression\napproach SCI, which adaptively partitions the target data into blocks matching\nthe concentrated spectrum envelop of the adopted INR, and allocates parameter\nwith high representation accuracy under given compression ratio. The\nexperiments show SCI's superior performance over conventional techniques and\nwide applicability across diverse medical data.",
    "descriptor": "",
    "authors": [
      "Runzhao Yang",
      "Tingxiong Xiao",
      "Yuxiao Cheng",
      "Qianni Cao",
      "Jinyuan Qu",
      "Jinli Suo",
      "Qionghai Dai"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15180"
  },
  {
    "id": "arXiv:2209.15183",
    "title": "Graphs with the same truncated cycle matroid",
    "abstract": "The classical Whitney's 2-Isomorphism Theorem describes the families of\ngraphs having the same cycle matroid. In this paper we describe the families of\ngraphs having the same truncated cycle matroid and prove, in particular, that\nevery 3-connected graph, except for K4, is uniquely defined by its truncated\ncycle matroid.",
    "descriptor": "\nComments: 8 pages\n",
    "authors": [
      "Jose De Jesus",
      "Alexander Kelmans"
    ],
    "subjectives": [
      "Combinatorics (math.CO)",
      "Discrete Mathematics (cs.DM)"
    ],
    "url": "https://arxiv.org/abs/2209.15183"
  },
  {
    "id": "arXiv:2209.15193",
    "title": "Quantum Fourier Addition, Simplified to Toffoli Addition",
    "abstract": "Quantum addition circuits are considered being of two types: 1)\nToffolli-adder circuits which use only classical reversible gates (CNOT and\nToffoli), and 2) QFT-adder circuits based on the quantum Fourier\ntransformation. We present the first systematic translation of the QFT-addition\ncircuit into a Toffoli-based adder. This result shows that QFT-addition has\nfundamentally the same fault-tolerance cost (e.g. T-count) as the most\ncost-efficient Toffoli-adder: instead of using approximate decompositions of\nthe gates from the QFT circuit, it is more efficient to merge gates. In order\nto achieve this, we formulated novel circuit identities for multi-controlled\ngates and apply the identities algorithmically. The employed techniques can be\nused to automate quantum circuit optimisation heuristics.",
    "descriptor": "\nComments: accepted in PRA\n",
    "authors": [
      "Alexandru Paler"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2209.15193"
  },
  {
    "id": "arXiv:2209.15207",
    "title": "Mixture of experts models for multilevel data: modelling framework and  approximation theory",
    "abstract": "Multilevel data are prevalent in many real-world applications. However, it\nremains an open research problem to identify and justify a class of models that\nflexibly capture a wide range of multilevel data. Motivated by the versatility\nof the mixture of experts (MoE) models in fitting regression data, in this\narticle we extend upon the MoE and study a class of mixed MoE (MMoE) models for\nmultilevel data. Under some regularity conditions, we prove that the MMoE is\ndense in the space of any continuous mixed effects models in the sense of weak\nconvergence. As a result, the MMoE has a potential to accurately resemble\nalmost all characteristics inherited in multilevel data, including the marginal\ndistributions, dependence structures, regression links, random intercepts and\nrandom slopes. In a particular case where the multilevel data is hierarchical,\nwe further show that a nested version of the MMoE universally approximates a\nbroad range of dependence structures of the random effects among different\nfactor levels.",
    "descriptor": "",
    "authors": [
      "Tsz Chai Fung",
      "Spark C. Tseung"
    ],
    "subjectives": [
      "Statistics Theory (math.ST)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2209.15207"
  },
  {
    "id": "arXiv:2209.15218",
    "title": "EF21-P and Friends: Improved Theoretical Communication Complexity for  Distributed Optimization with Bidirectional Compression",
    "abstract": "The starting point of this paper is the discovery of a novel and simple\nerror-feedback mechanism, which we call EF21-P, for dealing with the error\nintroduced by a contractive compressor. Unlike all prior works on error\nfeedback, where compression and correction operate in the dual space of\ngradients, our mechanism operates in the primal space of models. While we\nbelieve that EF21-P may be of interest in many situations where it is often\nadvantageous to perform model perturbation prior to the computation of the\ngradient (e.g., randomized smoothing and generalization), in this work we focus\nour attention on its use as a key building block in the design of\ncommunication-efficient distributed optimization methods supporting\nbidirectional compression. In particular, we employ EF21-P as the mechanism for\ncompressing and subsequently error-correcting the model broadcast by the server\nto the workers. By combining EF21-P with suitable methods performing\nworker-to-server compression, we obtain novel methods supporting bidirectional\ncompression and enjoying new state-of-the-art theoretical communication\ncomplexity for convex and nonconvex problems. For example, our bounds are the\nfirst that manage to decouple the variance/error coming from the\nworkers-to-server and server-to-workers compression, transforming a\nmultiplicative dependence to an additive one. In the convex regime, we obtain\nthe first bounds that match the theoretical communication complexity of\ngradient descent. Even in this convex regime, our algorithms work with biased\ngradient estimators, which is non-standard and requires new proof techniques\nthat may be of independent interest. Finally, our theoretical results are\ncorroborated through suitable experiments.",
    "descriptor": "",
    "authors": [
      "Kaja Gruntkowska",
      "Alexander Tyurin",
      "Peter Richt\u00e1rik"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.15218"
  },
  {
    "id": "arXiv:2209.15223",
    "title": "ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals",
    "abstract": "A time-frequency diagram is a commonly used visualization for observing the\ntime-frequency distribution of radio signals and analyzing their time-varying\npatterns of communication states in radio monitoring and management. While it\nexcels when performing short-term signal analyses, it becomes inadaptable for\nlong-term signal analyses because it cannot adequately depict signal\ntime-varying patterns in a large time span on a space-limited screen. This\nresearch thus presents an abstract signal time-frequency (ASTF) diagram to\naddress this problem. In the diagram design, a visual abstraction method is\nproposed to visually encode signal communication state changes in time slices.\nA time segmentation algorithm is proposed to divide a large time span into time\nslices.Three new quantified metrics and a loss function are defined to ensure\nthe preservation of important time-varying information in the time\nsegmentation. An algorithm performance experiment and a user study are\nconducted to evaluate the effectiveness of the diagram for long-term signal\nanalyses.",
    "descriptor": "\nComments: 11 pages, 9 figures\n",
    "authors": [
      "Ying Zhao",
      "Luhao Ge",
      "Huixuan Xie",
      "Genghuai Bai",
      "Zhao Zhang",
      "Qiang Wei",
      "Yun Lin",
      "Yuchao Liu",
      "Fangfang Zhou"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Human-Computer Interaction (cs.HC)"
    ],
    "url": "https://arxiv.org/abs/2209.15223"
  },
  {
    "id": "arXiv:2209.15224",
    "title": "Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models",
    "abstract": "Unsupervised learning has been widely used in many real-world applications.\nOne of the simplest and most important unsupervised learning models is the\nGaussian mixture model (GMM). In this work, we study the multi-task learning\nproblem on GMMs, which aims to leverage potentially similar GMM parameter\nstructures among tasks to obtain improved learning performance compared to\nsingle-task learning. We propose a multi-task GMM learning procedure based on\nthe EM algorithm that not only can effectively utilize unknown similarity\nbetween related tasks but is also robust against a fraction of outlier tasks\nfrom arbitrary sources. The proposed procedure is shown to achieve minimax\noptimal rate of convergence for both parameter estimation error and the excess\nmis-clustering error, in a wide range of regimes. Moreover, we generalize our\napproach to tackle the problem of transfer learning for GMMs, where similar\ntheoretical results are derived. Finally, we demonstrate the effectiveness of\nour methods through simulations and a real data analysis. To the best of our\nknowledge, this is the first work studying multi-task and transfer learning on\nGMMs with theoretical guarantees.",
    "descriptor": "\nComments: 149 pages, 7 figures, 2 tables\n",
    "authors": [
      "Ye Tian",
      "Haolei Weng",
      "Yang Feng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2209.15224"
  },
  {
    "id": "arXiv:2209.15283",
    "title": "Sparse tree-based initialization for neural networks",
    "abstract": "Dedicated neural network (NN) architectures have been designed to handle\nspecific data types (such as CNN for images or RNN for text), which ranks them\namong state-of-the-art methods for dealing with these data. Unfortunately, no\narchitecture has been found for dealing with tabular data yet, for which tree\nensemble methods (tree boosting, random forests) usually show the best\npredictive performances. In this work, we propose a new sparse initialization\ntechnique for (potentially deep) multilayer perceptrons (MLP): we first train a\ntree-based procedure to detect feature interactions and use the resulting\ninformation to initialize the network, which is subsequently trained via\nstandard stochastic gradient strategies. Numerical experiments on several\ntabular data sets show that this new, simple and easy-to-use method is a solid\nconcurrent, both in terms of generalization capacity and computation time, to\ndefault MLP initialization and even to existing complex deep learning\nsolutions. In fact, this wise MLP initialization raises the resulting NN\nmethods to the level of a valid competitor to gradient boosting when dealing\nwith tabular data. Besides, such initializations are able to preserve the\nsparsity of weights introduced in the first layers of the network through\ntraining. This fact suggests that this new initializer operates an implicit\nregularization during the NN training, and emphasizes that the first layers act\nas a sparse feature extractor (as for convolutional layers in CNN).",
    "descriptor": "",
    "authors": [
      "Patrick Lutz",
      "Ludovic Arnould",
      "Claire Boyer",
      "Erwan Scornet"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15283"
  },
  {
    "id": "arXiv:2209.15321",
    "title": "Leveraging variational autoencoders for multiple data imputation",
    "abstract": "Missing data persists as a major barrier to data analysis across numerous\napplications. Recently, deep generative models have been used for imputation of\nmissing data, motivated by their ability to capture highly non-linear and\ncomplex relationships in the data. In this work, we investigate the ability of\ndeep models, namely variational autoencoders (VAEs), to account for uncertainty\nin missing data through multiple imputation strategies. We find that VAEs\nprovide poor empirical coverage of missing data, with underestimation and\noverconfident imputations, particularly for more extreme missing data values.\nTo overcome this, we employ $\\beta$-VAEs, which viewed from a generalized Bayes\nframework, provide robustness to model misspecification. Assigning a good value\nof $\\beta$ is critical for uncertainty calibration and we demonstrate how this\ncan be achieved using cross-validation. In downstream tasks, we show how\nmultiple imputation with $\\beta$-VAEs can avoid false discoveries that arise as\nartefacts of imputation.",
    "descriptor": "\nComments: 17 pages, 3 main figures, 6 supplementary figures\n",
    "authors": [
      "Breeshey Roskams-Hieter",
      "Jude Wells",
      "Sara Wade"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15321"
  },
  {
    "id": "arXiv:2209.15338",
    "title": "Many-Body Approximation for Tensors",
    "abstract": "We propose a nonnegative tensor decomposition with focusing on the\nrelationship between the modes of tensors. Traditional decomposition methods\nassume low-rankness in the representation, resulting in difficulties in global\noptimization and target rank selection. To address these problems, we present\nan alternative way to decompose tensors, a many-body approximation for tensors,\nbased on an information geometric formulation. A tensor is treated via an\nenergy-based model, where the tensor and its mode correspond to a probability\ndistribution and a random variable, respectively, and many-body approximation\nis performed on it by taking the interaction between variables into account.\nOur model can be globally optimized in polynomial time in terms of the KL\ndivergence minimization, which is empirically faster than low-rank\napproximations keeping comparable reconstruction error. Furthermore, we\nvisualize interactions between modes as tensor networks and reveal a nontrivial\nrelationship between many-body approximation and low-rank approximation.",
    "descriptor": "\nComments: 12 pages, 5 figures\n",
    "authors": [
      "Kazu Ghalamkari",
      "Mahito Sugiyama"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15338"
  },
  {
    "id": "arXiv:2209.15377",
    "title": "DELAD: Deep Landweber-guided deconvolution with Hessian and sparse prior",
    "abstract": "We present a model for non-blind image deconvolution that incorporates the\nclassic iterative method into a deep learning application. Instead of using\nlarge over-parameterised generative networks to create sharp picture\nrepresentations, we build our network based on the iterative Landweber\ndeconvolution algorithm, which is integrated with trainable convolutional\nlayers to enhance the recovered image structures and details. Additional to the\ndata fidelity term, we also add Hessian and sparse constraints as\nregularization terms to improve the image reconstruction quality. Our proposed\nmodel is \\textit{self-supervised} and converges to a solution based purely on\nthe input blurred image and respective blur kernel without the requirement of\nany pre-training. We evaluate our technique using standard computer vision\nbenchmarking datasets as well as real microscope images obtained by our\nenhanced depth-of-field (EDOF) underwater microscope, demonstrating the\ncapabilities of our model in a real-world application. The quantitative results\ndemonstrate that our approach is competitive with state-of-the-art non-blind\nimage deblurring methods despite having a fraction of the parameters and not\nbeing pre-trained, demonstrating the efficiency and efficacy of embedding a\nclassic deconvolution approach inside a deep network.",
    "descriptor": "\nComments: 9 pages, 7 figures\n",
    "authors": [
      "Tomas Chobola",
      "Anton Theileis",
      "Jan Taucher",
      "Tingying Peng"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15377"
  },
  {
    "id": "arXiv:2209.15379",
    "title": "Multi-Antenna Configuration with Reduced Passive Self-Interference for  Full-Duplex Intelligent Transportation System",
    "abstract": "In this paper, we propose a closely spaced multi-antenna system with\n\\textit{passive} self-interference cancellation (\\textit{p}-SIC) of $\\approx\n90$ dB between the transmitter and receiver antenna for full-duplex\napplication. The \\textit{p}-SIC is achieved by field confinement near\nindividual antennae using shorted metallic vias and the application of U-shaped\nperturbation in the ground plane. The \\textit{p}-SIC technique is initially\nimplemented in a 1-Tx and 1-Rx antenna system and explained using transmission\nline-based theory. Further, it is extended to 1-Tx and 2-Rx configurations.\nHere the proposed full-duplex antenna system is designed at $5.9$ GHz\n($5.855-5.925$ GHz, IEEE 802.11p / WAVE technology) intelligent transportation\nsystem (ITS) application band using a microstrip patch configuration. The\nindividual antenna exhibits an impedance bandwidth of $93$ MHz ($5.850-5.944$\nGHz), $5.63$ dBi gain at $5.9$ GHz operating frequency and X-pol level less\nthan $20$ dB in the broad side direction. The proposed FD configuration\nexhibits $|S_{ij}|$ of less than $-50$ dB over the complete operating band and\n$\\approx -90$ dB is achieved at the operating frequency between the Tx and Rx.\nSimilarly, $|S_{ij}|$ of less the $-30$ dB is achieved between 2-Rx antennas\nfor a three-element FD configuration. The design procedure of the proposed FD\nconfiguration is explained and verified using fabrication and measurement. An\nexperimental demonstration of the self-interference channel and its suppression\nusing the proposed \\textit{p}-SIC technique is also provided. Further, to study\nthe diversity performance of the proposed multi-antenna configuration, the MIMO\nperformance metrics such as \\textit{ECC} and \\textit{CCL} are evaluated using\nsimulation and measurement.",
    "descriptor": "",
    "authors": [
      "Jogesh Chandra Dash",
      "Debdeep Sarkar"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15379"
  },
  {
    "id": "arXiv:2209.15392",
    "title": "Improving the Efficiency of Payments Systems Using Quantum Computing",
    "abstract": "High-value payment systems (HVPS) are typically liquidity-intensive as the\npayment requests are indivisible and settled on a gross basis. Finding the\nright order in which payments should be processed to maximize the liquidity\nefficiency of these systems is an $NP$-hard combinatorial optimization problem,\nwhich quantum algorithms may be able to tackle at meaningful scales. We\ndeveloped an algorithm and ran it on a hybrid quantum annealing solver to find\nan ordering of payments that reduced the amount of system liquidity necessary\nwithout substantially increasing payment delays. Despite the limitations in\nsize and speed of today's quantum computers, our algorithm provided\nquantifiable efficiency improvements when applied to the Canadian HVPS using a\n30-day sample of transaction data. By reordering each batch of 70 payments as\nthey entered the queue, we achieved an average of C\\$240 million in daily\nliquidity savings, with a settlement delay of approximately 90 seconds. For a\nfew days in the sample, the liquidity savings exceeded C\\$1 billion. This\nalgorithm could be incorporated as a centralized preprocessor into existing\nHVPS without entailing a fundamental change to their risk management models.",
    "descriptor": "",
    "authors": [
      "Christopher McMahon",
      "Donald McGillivray",
      "Ajit Desai",
      "Francisco Rivadeneyra",
      "Jean-Paul Lam",
      "Thomas Lo",
      "Danica Marsden",
      "Vladimir Skavysh"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Emerging Technologies (cs.ET)",
      "Quantum Algebra (math.QA)"
    ],
    "url": "https://arxiv.org/abs/2209.15392"
  },
  {
    "id": "arXiv:2209.15408",
    "title": "Equivariant Energy-Guided SDE for Inverse Molecular Design",
    "abstract": "Inverse molecular design is critical in material science and drug discovery,\nwhere the generated molecules should satisfy certain desirable properties. In\nthis paper, we propose equivariant energy-guided stochastic differential\nequations (EEGSDE), a flexible framework for controllable 3D molecule\ngeneration under the guidance of an energy function in diffusion models.\nFormally, we show that EEGSDE naturally exploits the geometric symmetry in 3D\nmolecular conformation, as long as the energy function is invariant to\northogonal transformations. Empirically, under the guidance of designed energy\nfunctions, EEGSDE significantly improves the baseline on QM9, in inverse\nmolecular design targeted to quantum properties and molecular structures.\nFurthermore, EEGSDE is able to generate molecules with multiple target\nproperties by combining the corresponding energy functions linearly.",
    "descriptor": "",
    "authors": [
      "Fan Bao",
      "Min Zhao",
      "Zhongkai Hao",
      "Peiyao Li",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "subjectives": [
      "Chemical Physics (physics.chem-ph)",
      "Machine Learning (cs.LG)",
      "Biomolecules (q-bio.BM)"
    ],
    "url": "https://arxiv.org/abs/2209.15408"
  },
  {
    "id": "arXiv:2209.15414",
    "title": "Predicting the power grid frequency of European islands",
    "abstract": "Modelling, forecasting and overall understanding of the dynamics of the power\ngrid and its frequency is essential for the safe operation of existing and\nfuture power grids. Much previous research was focused on large continental\nareas, while small systems, such as islands are less well-studied. These\nnatural island systems are ideal testing environments for microgrid proposals\nand artificially islanded grid operation. In the present paper, we utilize\nmeasurements of the power grid frequency obtained in European islands: the\nFaroe Islands, Ireland, the Balearic Islands and Iceland and investigate how\ntheir frequency can be predicted, compared to the Nordic power system, acting\nas a reference. The Balearic islands are found to be particularly deterministic\nand easy to predict in contrast to hard-to-predict Iceland. Furthermore, we\nshow that typically 2-4 weeks of data are needed to improve prediction\nperformance beyond simple benchmarks.",
    "descriptor": "\nComments: 16 pages\n",
    "authors": [
      "Thorbj\u00f8rn Lund Onsaker",
      "Heidi S. Nyg\u00e5rd",
      "Dami\u00e0 Gomila",
      "Pere Colet",
      "Ralf Mikut",
      "Richard Jumar",
      "Heiko Maass",
      "Uwe K\u00fchnapfel",
      "Veit Hagenmeyer",
      "Dirk Witthaut",
      "Benjamin Sch\u00e4fer"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)",
      "Data Analysis, Statistics and Probability (physics.data-an)"
    ],
    "url": "https://arxiv.org/abs/2209.15414"
  },
  {
    "id": "arXiv:2209.15415",
    "title": "DynImp: Dynamic Imputation for Wearable Sensing Data Through Sensory and  Temporal Relatedness",
    "abstract": "In wearable sensing applications, data is inevitable to be irregularly\nsampled or partially missing, which pose challenges for any downstream\napplication. An unique aspect of wearable data is that it is time-series data\nand each channel can be correlated to another one, such as x, y, z axis of\naccelerometer. We argue that traditional methods have rarely made use of both\ntimes-series dynamics of the data as well as the relatedness of the features\nfrom different sensors. We propose a model, termed as DynImp, to handle\ndifferent time point's missingness with nearest neighbors along feature axis\nand then feeding the data into a LSTM-based denoising autoencoder which can\nreconstruct missingness along the time axis. We experiment the model on the\nextreme missingness scenario ($>50\\%$ missing rate) which has not been widely\ntested in wearable data. Our experiments on activity recognition show that the\nmethod can exploit the multi-modality features from related sensors and also\nlearn from history time-series dynamics to reconstruct the data under extreme\nmissingness.",
    "descriptor": "\nComments: 5 pages, 2 figures, accepted in ICASSP'2022\n",
    "authors": [
      "Zepeng Huo",
      "Taowei Ji",
      "Yifei Liang",
      "Shuai Huang",
      "Zhangyang Wang",
      "Xiaoning Qian",
      "Bobak Mortazavi"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15415"
  },
  {
    "id": "arXiv:2209.15420",
    "title": "Ensemble-based gradient inference for particle methods in optimization  and sampling",
    "abstract": "We propose an approach based on function evaluations and Bayesian inference\nto extract higher-order differential information of objective functions {from a\ngiven ensemble of particles}. Pointwise evaluation $\\{V(x^i)\\}_i$ of some\npotential $V$ in an ensemble $\\{x^i\\}_i$ contains implicit information about\nfirst or higher order derivatives, which can be made explicit with little\ncomputational effort (ensemble-based gradient inference -- EGI). We suggest to\nuse this information for the improvement of established ensemble-based\nnumerical methods for optimization and sampling such as Consensus-based\noptimization and Langevin-based samplers. Numerical studies indicate that the\naugmented algorithms are often superior to their gradient-free variants, in\nparticular the augmented methods help the ensembles to escape their initial\ndomain, to explore multimodal, non-Gaussian settings and to speed up the\ncollapse at the end of optimization dynamics.}\nThe code for the numerical examples in this manuscript can be found in the\npaper's Github repository\n(https://github.com/MercuryBench/ensemble-based-gradient.git).",
    "descriptor": "",
    "authors": [
      "Claudia Schillings",
      "Claudia Totzeck",
      "Philipp Wacker"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Statistics Theory (math.ST)"
    ],
    "url": "https://arxiv.org/abs/2209.15420"
  },
  {
    "id": "arXiv:2209.15422",
    "title": "Statistical Inference for Fisher Market Equilibrium",
    "abstract": "Statistical inference under market equilibrium effects has attracted\nincreasing attention recently. In this paper we focus on the specific case of\nlinear Fisher markets. They have been widely use in fair resource allocation of\nfood/blood donations and budget management in large-scale Internet ad auctions.\nIn resource allocation, it is crucial to quantify the variability of the\nresource received by the agents (such as blood banks and food banks) in\naddition to fairness and efficiency properties of the systems. For ad auction\nmarkets, it is important to establish statistical properties of the platform's\nrevenues in addition to their expected values. To this end, we propose a\nstatistical framework based on the concept of infinite-dimensional Fisher\nmarkets. In our framework, we observe a market formed by a finite number of\nitems sampled from an underlying distribution (the \"observed market\") and aim\nto infer several important equilibrium quantities of the underlying long-run\nmarket. These equilibrium quantities include individual utilities, social\nwelfare, and pacing multipliers. Through the lens of sample average\napproximation (SSA), we derive a collection of statistical results and show\nthat the observed market provides useful statistical information of the\nlong-run market. In other words, the equilibrium quantities of the observed\nmarket converge to the true ones of the long-run market with strong statistical\nguarantees. These include consistency, finite sample bounds, asymptotics, and\nconfidence. As an extension, we discuss revenue inference in quasilinear Fisher\nmarkets.",
    "descriptor": "",
    "authors": [
      "Luofeng Liao",
      "Yuan Gao",
      "Christian Kroer"
    ],
    "subjectives": [
      "Econometrics (econ.EM)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2209.15422"
  },
  {
    "id": "arXiv:2209.15424",
    "title": "Accurate Long-term Air Temperature Prediction with a Fusion of  Artificial Intelligence and Data Reduction Techniques",
    "abstract": "In this paper three customised Artificial Intelligence (AI) frameworks,\nconsidering Deep Learning (convolutional neural networks), Machine Learning\nalgorithms and data reduction techniques are proposed, for a problem of\nlong-term summer air temperature prediction. Specifically, the prediction of\naverage air temperature in the first and second August fortnights, using input\ndata from previous months, at two different locations, Paris (France) and\nC\\'ordoba (Spain), is considered. The target variable, mainly in the first\nAugust fortnight, can contain signals of extreme events such as heatwaves, like\nthe mega-heatwave of 2003, which affected France and the Iberian Peninsula.\nThus, an accurate prediction of long-term air temperature may be valuable also\nfor different problems related to climate change, such as attribution of\nextreme events, and in other problems related to renewable energy. The analysis\ncarried out this work is based on Reanalysis data, which are first processed by\na correlation analysis among different prediction variables and the target\n(average air temperature in August first and second fortnights). An area with\nthe largest correlation is located, and the variables within, after a feature\nselection process, are the input of different deep learning and ML algorithms.\nThe experiments carried out show a very good prediction skill in the three\nproposed AI frameworks, both in Paris and C\\'ordoba regions.",
    "descriptor": "\nComments: 33 pages, 14 figures, 7 tables, under review\n",
    "authors": [
      "Du\u0161an Fister",
      "Jorge P\u00e9rez-Aracil",
      "C\u00e9sar Pel\u00e1ez-Rodr\u00edguez",
      "Javier Del Ser",
      "Sancho Salcedo-Sanz"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15424"
  },
  {
    "id": "arXiv:2209.15436",
    "title": "XR-RF Imaging Enabled by Software-Defined Metasurfaces and Machine  Learning: Foundational Vision, Technologies and Challenges",
    "abstract": "We present a new approach to Extended Reality (XR), denoted as iCOPYWAVES,\nwhich seeks to offer naturally low-latency operation and cost-effectiveness,\novercoming the critical scalability issues faced by existing solutions.\niCOPYWAVES is enabled by emerging PWEs, a recently proposed technology in\nwireless communications. Empowered by intelligent (meta)surfaces, PWEs\ntransform the wave propagation phenomenon into a software-defined process. We\nleverage PWEs to i) create, and then ii) selectively copy the scattered RF\nwavefront of an object from one location in space to another, where a machine\nlearning module, accelerated by FPGAs, translates it to visual input for an XR\nheadset using PWEdriven, RF imaging principles (XR-RF). This makes for an XR\nsystem whose operation is bounded in the physical layer and, hence, has the\nprospects for minimal end-to-end latency. Over large distances,\nRF-to-fiber/fiber-to-RF is employed to provide intermediate connectivity. The\npaper provides a tutorial on the iCOPYWAVES system architecture and workflow. A\nproof-of-concept implementation via simulations is provided, demonstrating the\nreconstruction of challenging objects in iCOPYWAVES produced computer graphics.",
    "descriptor": "",
    "authors": [
      "C. Liaskos",
      "A. Tsioliaridou",
      "K. Georgopoulos",
      "G. Morianos",
      "S. Ioannidis",
      "I. Salem",
      "D. Manessis",
      "S. Schmid D. Tyrovolas",
      "S. A. Tegos",
      "P.-V. Mekikis",
      "P. D. Diamantoulakis",
      "A. Pitilakis",
      "N. Kantartzis",
      "G. K. Karagiannidis A. Tasolamprou",
      "O. Tsilipakos",
      "M. Kafesaki",
      "I.F. Akyildiz",
      "A. Pitsillides",
      "M. Pateraki",
      "M. Vakalellis",
      "I. Spais"
    ],
    "subjectives": [
      "Signal Processing (eess.SP)",
      "Emerging Technologies (cs.ET)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.15436"
  },
  {
    "id": "arXiv:2209.15449",
    "title": "End-to-End Label Uncertainty Modeling in Speech Emotion Recognition  using Bayesian Neural Networks and Label Distribution Learning",
    "abstract": "To train machine learning algorithms to predict emotional expressions in\nterms of arousal and valence, annotated datasets are needed. However, as\ndifferent people perceive others' emotional expressions differently, their\nannotations are per se subjective. For this, annotations are typically\ncollected from multiple annotators and averaged to obtain ground-truth labels.\nHowever, when exclusively trained on this averaged ground-truth, the trained\nnetwork is agnostic to the inherent subjectivity in emotional expressions. In\nthis work, we therefore propose an end-to-end Bayesian neural network capable\nof being trained on a distribution of labels to also capture the\nsubjectivity-based label uncertainty. Instead of a Gaussian, we model the label\ndistribution using Student's t-distribution, which also accounts for the number\nof annotations. We derive the corresponding Kullback-Leibler divergence loss\nand use it to train an estimator for the distribution of labels, from which the\nmean and uncertainty can be inferred. We validate the proposed method using two\nin-the-wild datasets. We show that the proposed t-distribution based approach\nachieves state-of-the-art uncertainty modeling results in speech emotion\nrecognition, and also consistent results in cross-corpora evaluations.\nFurthermore, analyses reveal that the advantage of a t-distribution over a\nGaussian grows with increasing inter-annotator correlation and a decreasing\nnumber of annotators.",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2207.12135\n",
    "authors": [
      "Navin Raj Prabhu",
      "Nale Lehmann-Willenbrock",
      "Timo Gerkman"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15449"
  },
  {
    "id": "arXiv:2209.15451",
    "title": "Semi-Supervised Domain Generalization for Cardiac Magnetic Resonance  Image Segmentation with High Quality Pseudo Labels",
    "abstract": "Developing a deep learning method for medical segmentation tasks heavily\nrelies on a large amount of labeled data. However, the annotations require\nprofessional knowledge and are limited in number. Recently, semi-supervised\nlearning has demonstrated great potential in medical segmentation tasks. Most\nexisting methods related to cardiac magnetic resonance images only focus on\nregular images with similar domains and high image quality. A semi-supervised\ndomain generalization method was developed in [2], which enhances the quality\nof pseudo labels on varied datasets. In this paper, we follow the strategy in\n[2] and present a domain generalization method for semi-supervised medical\nsegmentation. Our main goal is to improve the quality of pseudo labels under\nextreme MRI Analysis with various domains. We perform Fourier transformation on\ninput images to learn low-level statistics and cross-domain information. Then\nwe feed the augmented images as input to the double cross pseudo supervision\nnetworks to calculate the variance among pseudo labels. We evaluate our method\non the CMRxMotion dataset [1]. With only partially labeled data and without\ndomain labels, our approach consistently generates accurate segmentation\nresults of cardiac magnetic resonance images with different respiratory\nmotions. Code will be available after the conference.",
    "descriptor": "\nComments: Accepted by Statistical Atlases and Computational Modeling of the Heart (STACOM) workshop, MICCAI(2022)\n",
    "authors": [
      "Wanqin Ma",
      "Huifeng Yao",
      "Yiqun Lin",
      "Jiarong Guo",
      "Xiaomeng Li"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15451"
  },
  {
    "id": "arXiv:2209.15466",
    "title": "Sparsity-Constrained Optimal Transport",
    "abstract": "Regularized optimal transport (OT) is now increasingly used as a loss or as a\nmatching layer in neural networks. Entropy-regularized OT can be computed using\nthe Sinkhorn algorithm but it leads to fully-dense transportation plans,\nmeaning that all sources are (fractionally) matched with all targets. To\naddress this issue, several works have investigated quadratic regularization\ninstead. This regularization preserves sparsity and leads to unconstrained and\nsmooth (semi) dual objectives, that can be solved with off-the-shelf gradient\nmethods. Unfortunately, quadratic regularization does not give direct control\nover the cardinality (number of nonzeros) of the transportation plan. We\npropose in this paper a new approach for OT with explicit cardinality\nconstraints on the transportation plan. Our work is motivated by an application\nto sparse mixture of experts, where OT can be used to match input tokens such\nas image patches with expert models such as neural networks. Cardinality\nconstraints ensure that at most $k$ tokens are matched with an expert, which is\ncrucial for computational performance reasons. Despite the nonconvexity of\ncardinality constraints, we show that the corresponding (semi) dual problems\nare tractable and can be solved with first-order gradient methods. Our method\ncan be thought as a middle ground between unregularized OT (recovered in the\nlimit case $k=1$) and quadratically-regularized OT (recovered when $k$ is large\nenough). The smoothness of the objectives increases as $k$ increases, giving\nrise to a trade-off between convergence speed and sparsity of the optimal plan.",
    "descriptor": "",
    "authors": [
      "Tianlin Liu",
      "Joan Puigcerver",
      "Mathieu Blondel"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15466"
  },
  {
    "id": "arXiv:2209.15497",
    "title": "Local dominance unveils clusters in networks",
    "abstract": "Clusters or communities can provide a coarse-grained description of complex\nsystems at multiple scales, but their detection remains challenging in\npractice. Community detection methods often define communities as dense\nsubgraphs, or subgraphs with few connections in-between, via concepts such as\nthe cut, conductance, or modularity. Here we consider another perspective built\non the notion of local dominance, where low-degree nodes are assigned to the\nbasin of influence of high-degree nodes, and design an efficient algorithm\nbased on local information. Local dominance gives rises to community centers,\nand uncovers local hierarchies in the network. Community centers have a larger\ndegree than their neighbors and are sufficiently distant from other centers.\nThe strength of our framework is demonstrated on synthesized and empirical\nnetworks with ground-truth community labels. The notion of local dominance and\nthe associated asymmetric relations between nodes are not restricted to\ncommunity detection, and can be utilised in clustering problems, as we\nillustrate on networks derived from vector data.",
    "descriptor": "",
    "authors": [
      "Fan Shang",
      "Bingsheng Chen",
      "Paul Expert",
      "Linyuan L\u00fc",
      "Ao Yang",
      "H.Eugene Stanley",
      "Renaud Lambiotte",
      "Tim S.Evans",
      "Ruiqi Li"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/2209.15497"
  },
  {
    "id": "arXiv:2209.15543",
    "title": "Bayesian Neural Networks for Geothermal Resource Assessment: Prediction  with Uncertainty",
    "abstract": "We consider the application of machine learning to the evaluation of\ngeothermal resource potential. A supervised learning problem is defined where\nmaps of 10 geological and geophysical features within the state of Nevada, USA\nare used to define geothermal potential across a broad region. We have\navailable a relatively small set of positive training sites (known resources or\nactive power plants) and negative training sites (known drill sites with\nunsuitable geothermal conditions) and use these to constrain and optimize\nartificial neural networks for this classification task. The main objective is\nto predict the geothermal resource potential at unknown sites within a large\ngeographic area where the defining features are known. These predictions could\nbe used to target promising areas for further detailed investigations. We\ndescribe the evolution of our work from defining a specific neural network\narchitecture to training and optimization trials. Upon analysis we expose the\ninevitable problems of model variability and resulting prediction uncertainty.\nFinally, to address these problems we apply the concept of Bayesian neural\nnetworks, a heuristic approach to regularization in network training, and make\nuse of the practical interpretation of the formal uncertainty measures they\nprovide.",
    "descriptor": "\nComments: 27 pages, 12 figures\n",
    "authors": [
      "Stephen Brown",
      "William L. Rodi",
      "Chen Gu",
      "Michael Fehler",
      "James Faulds",
      "Connor M. Smith",
      "Sven Treitel"
    ],
    "subjectives": [
      "Geophysics (physics.geo-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15543"
  },
  {
    "id": "arXiv:2209.15570",
    "title": "Fault Prognosis in Particle Accelerator Power Electronics Using Ensemble  Learning",
    "abstract": "Early fault detection and fault prognosis are crucial to ensure efficient and\nsafe operations of complex engineering systems such as the Spallation Neutron\nSource (SNS) and its power electronics (high voltage converter modulators).\nFollowing an advanced experimental facility setup that mimics SNS operating\nconditions, the authors successfully conducted 21 fault prognosis experiments,\nwhere fault precursors are introduced in the system to a degree enough to cause\ndegradation in the waveform signals, but not enough to reach a real fault. Nine\ndifferent machine learning techniques based on ensemble trees, convolutional\nneural networks, support vector machines, and hierarchical voting ensembles are\nproposed to detect the fault precursors. Although all 9 models have shown a\nperfect and identical performance during the training and testing phase, the\nperformance of most models has decreased in the prognosis phase once they got\nexposed to real-world data from the 21 experiments. The hierarchical voting\nensemble, which features multiple layers of diverse models, maintains a\ndistinguished performance in early detection of the fault precursors with 95%\nsuccess rate (20/21 tests), followed by adaboost and extremely randomized trees\nwith 52% and 48% success rates, respectively. The support vector machine models\nwere the worst with only 24% success rate (5/21 tests). The study concluded\nthat a successful implementation of machine learning in the SNS or particle\naccelerator power systems would require a major upgrade in the controller and\nthe data acquisition system to facilitate streaming and handling big data for\nthe machine learning models. In addition, this study shows that the best\nperforming models were diverse and based on the ensemble concept to reduce the\nbias and hyperparameter sensitivity of individual models.",
    "descriptor": "\nComments: 25 Pages, 13 Figures, 5 Tables\n",
    "authors": [
      "Majdi I. Radaideh",
      "Chris Pappas",
      "Mark Wezensky",
      "Pradeep Ramuhalli",
      "Sarah Cousineau"
    ],
    "subjectives": [
      "Accelerator Physics (physics.acc-ph)",
      "Machine Learning (cs.LG)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.15570"
  },
  {
    "id": "arXiv:2209.15584",
    "title": "Automated Characterization of Catalytically Active Inclusion Body  Production in Biotechnological Screening Systems",
    "abstract": "We here propose an automated pipeline for the microscopy image-based\ncharacterization of catalytically active inclusion bodies (CatIBs), which\nincludes a fully automatic experimental high-throughput workflow combined with\na hybrid approach for multi-object microbial cell segmentation. For automated\nmicroscopy, a CatIB producer strain was cultivated in a microbioreactor from\nwhich samples were injected into a flow chamber. The flow chamber was fixed\nunder a microscope and an integrated camera took a series of images per sample.\nTo explore heterogeneity of CatIB development during the cultivation and track\nthe size and quantity of CatIBs over time, a hybrid image processing pipeline\napproach was developed, which combines an ML-based detection of in-focus cells\nwith model-based segmentation. The experimental setup in combination with an\nautomated image analysis unlocks high-throughput screening of CatIB production,\nsaving time and resources.\nBiotechnological relevance - CatIBs have wide application in synthetic\nchemistry and biocatalysis, but also could have future biomedical applications\nsuch as therapeutics. The proposed hybrid automatic image processing pipeline\ncan be adjusted to treat comparable biological microorganisms, where fully\ndata-driven ML-based segmentation approaches are not feasible due to the lack\nof training data. Our work is the first step towards image-based bioprocess\ncontrol.",
    "descriptor": "",
    "authors": [
      "Karina Ruzaeva",
      "Kira K\u00fcsters",
      "Wolfgang Wiechert",
      "Benjamin Berkels",
      "Marco Oldiges",
      "Katharina N\u00f6h"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.15584"
  },
  {
    "id": "arXiv:2209.15585",
    "title": "Cloud Classification with Unsupervised Deep Learning",
    "abstract": "We present a framework for cloud characterization that leverages modern\nunsupervised deep learning technologies. While previous neural network-based\ncloud classification models have used supervised learning methods, unsupervised\nlearning allows us to avoid restricting the model to artificial categories\nbased on historical cloud classification schemes and enables the discovery of\nnovel, more detailed classifications. Our framework learns cloud features\ndirectly from radiance data produced by NASA's Moderate Resolution Imaging\nSpectroradiometer (MODIS) satellite instrument, deriving cloud characteristics\nfrom millions of images without relying on pre-defined cloud types during the\ntraining process. We present preliminary results showing that our method\nextracts physically relevant information from radiance data and produces\nmeaningful cloud classes.",
    "descriptor": "\nComments: 5 pages, 6 figures, Proceedings for Climate Informatics Workshop 2019 Paris\n",
    "authors": [
      "Takuya Kurihana",
      "Ian Foster",
      "Rebecca Willett",
      "Sydney Jenkins",
      "Kathryn Koenig",
      "Ruby Werman",
      "Ricardo Barros Lourenco",
      "Casper Neo",
      "Elisabeth Moyer"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15585"
  },
  {
    "id": "arXiv:2209.15601",
    "title": "Design and analysis of digital communication within an SoC-based control  system for trapped-ion quantum computing",
    "abstract": "Electronic control systems used for quantum computing have become\nincreasingly complex as multiple qubit technologies employ larger numbers of\nqubits with higher fidelity targets. Whereas the control systems for different\ntechnologies share some similarities, parameters like pulse duration,\nthroughput, real-time feedback, and latency requirements vary widely depending\non the qubit type. In this paper, we evaluate the performance of modern\nSystem-on-Chip (SoC) architectures in meeting the control demands associated\nwith performing quantum gates on trapped-ion qubits, particularly focusing on\ncommunication within the SoC. A principal focus of this paper is the data\ntransfer latency and throughput of several high-speed on-chip mechanisms on\nXilinx multi-processor SoCs, including those that utilize direct memory access\n(DMA). They are measured and evaluated to determine an upper bound on the time\nrequired to reconfigure a gate parameter. Worst-case and average-case bandwidth\nrequirements for a custom gate sequencer core are compared with the\nexperimental results. The lowest-variability, highest-throughput data-transfer\nmechanism is DMA between the real-time processing unit (RPU) and the PL, where\nbandwidths up to 19.2 GB/s are possible. For context, this enables\nreconfiguration of qubit gates in less than 2\\mics\\!, comparable to the fastest\ngate time. Though this paper focuses on trapped-ion control systems, the gate\nabstraction scheme and measured communication rates are applicable to a broad\nrange of quantum computing technologies.",
    "descriptor": "",
    "authors": [
      "Nafis Irtija",
      "Jim Plusquellic",
      "Eirini Eleni Tsiropoulou",
      "Joshua Goldberg",
      "Daniel Lobser",
      "Daniel Stick"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Hardware Architecture (cs.AR)"
    ],
    "url": "https://arxiv.org/abs/2209.15601"
  },
  {
    "id": "arXiv:2209.15608",
    "title": "Shuffled linear regression through graduated convex relaxation",
    "abstract": "The shuffled linear regression problem aims to recover linear relationships\nin datasets where the correspondence between input and output is unknown. This\nproblem arises in a wide range of applications including survey data, in which\none needs to decide whether the anonymity of the responses can be preserved\nwhile uncovering significant statistical connections. In this work, we propose\na novel optimization algorithm for shuffled linear regression based on a\nposterior-maximizing objective function assuming Gaussian noise prior. We\ncompare and contrast our approach with existing methods on synthetic and real\ndata. We show that our approach performs competitively while achieving\nempirical running-time improvements. Furthermore, we demonstrate that our\nalgorithm is able to utilize the side information in the form of seeds, which\nrecently came to prominence in related problems.",
    "descriptor": "",
    "authors": [
      "Efe Onaran",
      "Soledad Villar"
    ],
    "subjectives": [
      "Computation (stat.CO)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.15608"
  },
  {
    "id": "arXiv:2209.15609",
    "title": "$\u03a6$-DVAE: Learning Physically Interpretable Representations with  Nonlinear Filtering",
    "abstract": "Incorporating unstructured data into physical models is a challenging problem\nthat is emerging in data assimilation. Traditional approaches focus on\nwell-defined observation operators whose functional forms are typically assumed\nto be known. This prevents these methods from achieving a consistent model-data\nsynthesis in configurations where the mapping from data-space to model-space is\nunknown. To address these shortcomings, in this paper we develop a\nphysics-informed dynamical variational autoencoder ($\\Phi$-DVAE) for embedding\ndiverse data streams into time-evolving physical systems described by\ndifferential equations. Our approach combines a standard (possibly nonlinear)\nfilter for the latent state-space model and a VAE, to embed the unstructured\ndata stream into the latent dynamical system. A variational Bayesian framework\nis used for the joint estimation of the embedding, latent states, and unknown\nsystem parameters. To demonstrate the method, we look at three examples: video\ndatasets generated by the advection and Korteweg-de Vries partial differential\nequations, and a velocity field generated by the Lorenz-63 system. Comparisons\nwith relevant baselines show that the $\\Phi$-DVAE provides a data efficient\ndynamics encoding methodology that is competitive with standard approaches,\nwith the added benefit of incorporating a physically interpretable latent\nspace.",
    "descriptor": "\nComments: 10 pages, 6 figures\n",
    "authors": [
      "Alex Glyn-Davies",
      "Connor Duffin",
      "\u00d6. Deniz Akyildiz",
      "Mark Girolami"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)",
      "Computation (stat.CO)"
    ],
    "url": "https://arxiv.org/abs/2209.15609"
  },
  {
    "id": "arXiv:2209.15611",
    "title": "Protein structure generation via folding diffusion",
    "abstract": "The ability to computationally generate novel yet physically foldable protein\nstructures could lead to new biological discoveries and new treatments\ntargeting yet incurable diseases. Despite recent advances in protein structure\nprediction, directly generating diverse, novel protein structures from neural\nnetworks remains difficult. In this work, we present a new diffusion-based\ngenerative model that designs protein backbone structures via a procedure that\nmirrors the native folding process. We describe protein backbone structure as a\nseries of consecutive angles capturing the relative orientation of the\nconstituent amino acid residues, and generate new structures by denoising from\na random, unfolded state towards a stable folded structure. Not only does this\nmirror how proteins biologically twist into energetically favorable\nconformations, the inherent shift and rotational invariance of this\nrepresentation crucially alleviates the need for complex equivariant networks.\nWe train a denoising diffusion probabilistic model with a simple transformer\nbackbone and demonstrate that our resulting model unconditionally generates\nhighly realistic protein structures with complexity and structural patterns\nakin to those of naturally-occurring proteins. As a useful resource, we release\nthe first open-source codebase and trained models for protein structure\ndiffusion.",
    "descriptor": "",
    "authors": [
      "Kevin E. Wu",
      "Kevin K. Yang",
      "Rianne van den Berg",
      "James Y. Zou",
      "Alex X. Lu",
      "Ava P. Amini"
    ],
    "subjectives": [
      "Biomolecules (q-bio.BM)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15611"
  },
  {
    "id": "arXiv:2209.15615",
    "title": "A Novel Mixture Model for Characterizing Human Aiming Performance Data",
    "abstract": "Fitts' law is often employed as a predictive model for human movement,\nespecially in the field of human-computer interaction. Models with an assumed\nGaussian error structure are usually adequate when applied to data collected\nfrom controlled studies. However, observational data (often referred to as data\ngathered \"in the wild\") typically display noticeable positive skewness relative\nto a mean trend as users do not routinely try to minimize their task completion\ntime. As such, the exponentially-modified Gaussian (EMG) regression model has\nbeen applied to aimed movements data. However, it is also of interest to\nreasonably characterize those regions where a user likely was not trying to\nminimize their task completion time. In this paper, we propose a novel model\nwith a two-component mixture structure -- one Gaussian and one exponential --\non the errors to identify such a region. An\nexpectation-conditional-maximization (ECM) algorithm is developed for\nestimation of such a model and some properties of the algorithm are\nestablished. The efficacy of the proposed model, as well as its ability to\ninform model-based clustering, are addressed in this work through extensive\nsimulations and an insightful analysis of a human aiming performance study.",
    "descriptor": "\nComments: 29 pages, 3 figures\n",
    "authors": [
      "Yanxi Li",
      "Derek S. Young",
      "Julien Gori",
      "Olivier Rioul"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Numerical Analysis (math.NA)",
      "Computation (stat.CO)",
      "Methodology (stat.ME)"
    ],
    "url": "https://arxiv.org/abs/2209.15615"
  },
  {
    "id": "arXiv:2209.15624",
    "title": "Finding NEEMo: Geometric Fitting using Neural Estimation of the Energy  Mover's Distance",
    "abstract": "A novel neural architecture was recently developed that enforces an exact\nupper bound on the Lipschitz constant of the model by constraining the norm of\nits weights in a minimal way, resulting in higher expressiveness compared to\nother techniques. We present a new and interesting direction for this\narchitecture: estimation of the Wasserstein metric (Earth Mover's Distance) in\noptimal transport by employing the Kantorovich-Rubinstein duality to enable its\nuse in geometric fitting applications. Specifically, we focus on the field of\nhigh-energy particle physics, where it has been shown that a metric for the\nspace of particle-collider events can be defined based on the Wasserstein\nmetric, referred to as the Energy Mover's Distance (EMD). This metrization has\nthe potential to revolutionize data-driven collider phenomenology. The work\npresented here represents a major step towards realizing this goal by providing\na differentiable way of directly calculating the EMD. We show how the\nflexibility that our approach enables can be used to develop novel clustering\nalgorithms.",
    "descriptor": "\nComments: 5 pages, 4 figures\n",
    "authors": [
      "Ouail Kitouni",
      "Niklas Nolte",
      "Mike Williams"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)",
      "High Energy Physics - Phenomenology (hep-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.15624"
  },
  {
    "id": "arXiv:1707.02190",
    "title": "A subexponential parameterized algorithm for Directed Subset Traveling  Salesman Problem on planar graphs",
    "abstract": "Comments: Paper published at SIAM J. Comput. The Steiner Tree part will be moved to a separate paper",
    "descriptor": "\nComments: Paper published at SIAM J. Comput. The Steiner Tree part will be moved to a separate paper\n",
    "authors": [
      "D\u00e1niel Marx",
      "Marcin Pilipczuk",
      "Micha\u0142 Pilipczuk"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)"
    ],
    "url": "https://arxiv.org/abs/1707.02190"
  },
  {
    "id": "arXiv:1902.07516",
    "title": "Emergence of order in random languages",
    "abstract": "Comments: 16 pages + 2 appendices; v2: references added and some explanations expanded. v3: Corrigendum added as Appendix B",
    "descriptor": "\nComments: 16 pages + 2 appendices; v2: references added and some explanations expanded. v3: Corrigendum added as Appendix B\n",
    "authors": [
      "Eric De Giuli"
    ],
    "subjectives": [
      "Disordered Systems and Neural Networks (cond-mat.dis-nn)",
      "Computation and Language (cs.CL)",
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/1902.07516"
  },
  {
    "id": "arXiv:2004.09293",
    "title": "A Social Network Analysis of Occupational Segregation",
    "abstract": "A Social Network Analysis of Occupational Segregation",
    "descriptor": "",
    "authors": [
      "I. Sebastian Buhai",
      "Marco J. van der Leij"
    ],
    "subjectives": [
      "Theoretical Economics (econ.TH)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2004.09293"
  },
  {
    "id": "arXiv:2008.07301",
    "title": "Computational timeline reconstruction of the stories surrounding Trump:  Story turbulence, narrative control, and collective chronopathy",
    "abstract": "Comments: 13 pages, 5 figures (4 main, 1 appendix), 1 table. Analysis complete for 6 calendar years, from 2015/01/01 through to 2021/12/31",
    "descriptor": "\nComments: 13 pages, 5 figures (4 main, 1 appendix), 1 table. Analysis complete for 6 calendar years, from 2015/01/01 through to 2021/12/31\n",
    "authors": [
      "P. S. Dodds",
      "J. R. Minot",
      "M. V. Arnold",
      "T. Alshaabi",
      "J. L. Adams",
      "A. J. Reagan",
      "C. M. Danforth"
    ],
    "subjectives": [
      "Physics and Society (physics.soc-ph)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2008.07301"
  },
  {
    "id": "arXiv:2008.09004",
    "title": "Solving Problems on Generalized Convex Graphs via Mim-Width",
    "abstract": "Solving Problems on Generalized Convex Graphs via Mim-Width",
    "descriptor": "",
    "authors": [
      "Flavia Bonomo-Braberman",
      "Nick Brettell",
      "Andrea Munaro",
      "Dani\u00ebl Paulusma"
    ],
    "subjectives": [
      "Data Structures and Algorithms (cs.DS)",
      "Computational Complexity (cs.CC)",
      "Discrete Mathematics (cs.DM)",
      "Combinatorics (math.CO)"
    ],
    "url": "https://arxiv.org/abs/2008.09004"
  },
  {
    "id": "arXiv:2011.02930",
    "title": "Paralinguistic Privacy Protection at the Edge",
    "abstract": "Comments: 14 pages, 7 figures, Accepted at ACM Transactions on Privacy and Security",
    "descriptor": "\nComments: 14 pages, 7 figures, Accepted at ACM Transactions on Privacy and Security\n",
    "authors": [
      "Ranya Aloufi",
      "Hamed Haddadi",
      "David Boyle"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2011.02930"
  },
  {
    "id": "arXiv:2102.07567",
    "title": "Learning Accurate Decision Trees with Bandit Feedback via Quantized  Gradient Descent",
    "abstract": "Comments: Accepted to TMLR",
    "descriptor": "\nComments: Accepted to TMLR\n",
    "authors": [
      "Ajaykrishna Karthikeyan",
      "Naman Jain",
      "Nagarajan Natarajan",
      "Prateek Jain"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2102.07567"
  },
  {
    "id": "arXiv:2102.08803",
    "title": "Effects of Early Warning Emails on Student Performance",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:1906.09864",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:1906.09864\n",
    "authors": [
      "Jens Klenke",
      "Till Massing",
      "Natalie Reckmann",
      "Janine Langerbein",
      "Benjamin Otto",
      "Michael Goedicke",
      "Christoph Hanck"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Applications (stat.AP)"
    ],
    "url": "https://arxiv.org/abs/2102.08803"
  },
  {
    "id": "arXiv:2103.01661",
    "title": "Incorporating VAD into ASR System by Multi-task Learning",
    "abstract": "Comments: 5 pages, 2 figures",
    "descriptor": "\nComments: 5 pages, 2 figures\n",
    "authors": [
      "Meng Li",
      "Xia Yan",
      "Feng Lin"
    ],
    "subjectives": [
      "Audio and Speech Processing (eess.AS)",
      "Computation and Language (cs.CL)",
      "Sound (cs.SD)"
    ],
    "url": "https://arxiv.org/abs/2103.01661"
  },
  {
    "id": "arXiv:2103.02339",
    "title": "Deep Recurrent Encoder: A scalable end-to-end network to model brain  signals",
    "abstract": "Deep Recurrent Encoder: A scalable end-to-end network to model brain  signals",
    "descriptor": "",
    "authors": [
      "Omar Chehab",
      "Alexandre Defossez",
      "Jean-Christophe Loiseau",
      "Alexandre Gramfort",
      "Jean-Remi King"
    ],
    "subjectives": [
      "Neurons and Cognition (q-bio.NC)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2103.02339"
  },
  {
    "id": "arXiv:2103.02356",
    "title": "Riemannian thresholding methods for row-sparse and low-rank matrix  recovery",
    "abstract": "Riemannian thresholding methods for row-sparse and low-rank matrix  recovery",
    "descriptor": "",
    "authors": [
      "Henrik Eisenmann",
      "Felix Krahmer",
      "Max Pfeffer",
      "Andr\u00e9 Uschmajew"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2103.02356"
  },
  {
    "id": "arXiv:2103.15145",
    "title": "TransCenter: Transformers with Dense Representations for Multiple-Object  Tracking",
    "abstract": "Comments: 17 pages, 10 figures, updated results and add comparisons",
    "descriptor": "\nComments: 17 pages, 10 figures, updated results and add comparisons\n",
    "authors": [
      "Yihong Xu",
      "Yutong Ban",
      "Guillaume Delorme",
      "Chuang Gan",
      "Daniela Rus",
      "Xavier Alameda-Pineda"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2103.15145"
  },
  {
    "id": "arXiv:2105.03692",
    "title": "Provable Guarantees against Data Poisoning Using Self-Expansion and  Compatibility",
    "abstract": "Provable Guarantees against Data Poisoning Using Self-Expansion and  Compatibility",
    "descriptor": "",
    "authors": [
      "Charles Jin",
      "Melinda Sun",
      "Martin Rinard"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Cryptography and Security (cs.CR)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2105.03692"
  },
  {
    "id": "arXiv:2105.05308",
    "title": "Sequential Fair Allocation: Achieving the Optimal Envy-Efficiency  Tradeoff Curve",
    "abstract": "Comments: 42 pages, 5 figures",
    "descriptor": "\nComments: 42 pages, 5 figures\n",
    "authors": [
      "Sean R. Sinclair",
      "Gauri Jain",
      "Siddhartha Banerjee",
      "Christina Lee Yu"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2105.05308"
  },
  {
    "id": "arXiv:2106.01429",
    "title": "Smooth Bilevel Programming for Sparse Regularization",
    "abstract": "Smooth Bilevel Programming for Sparse Regularization",
    "descriptor": "",
    "authors": [
      "Clarice Poon",
      "Gabriel Peyr\u00e9"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2106.01429"
  },
  {
    "id": "arXiv:2106.14321",
    "title": "Draw Me a Flower: Processing and Grounding Abstraction in Natural  Language",
    "abstract": "Comments: Accepted to the TACL journal. This is a pre-MIT Press publication version",
    "descriptor": "\nComments: Accepted to the TACL journal. This is a pre-MIT Press publication version\n",
    "authors": [
      "Royi Lachmy",
      "Valentina Pyatkin",
      "Avshalom Manevich",
      "Reut Tsarfaty"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2106.14321"
  },
  {
    "id": "arXiv:2107.10041",
    "title": "Low Government Performance and Uncivil Political Tweets: Evidence from  the COVID-19 Crisis in the U.S",
    "abstract": "Low Government Performance and Uncivil Political Tweets: Evidence from  the COVID-19 Crisis in the U.S",
    "descriptor": "",
    "authors": [
      "Kohei Nishi"
    ],
    "subjectives": [
      "Applications (stat.AP)",
      "Social and Information Networks (cs.SI)"
    ],
    "url": "https://arxiv.org/abs/2107.10041"
  },
  {
    "id": "arXiv:2110.01052",
    "title": "Learn then Test: Calibrating Predictive Algorithms to Achieve Risk  Control",
    "abstract": "Comments: Code available at this https URL",
    "descriptor": "\nComments: Code available at this https URL\n",
    "authors": [
      "Anastasios N. Angelopoulos",
      "Stephen Bates",
      "Emmanuel J. Cand\u00e8s",
      "Michael I. Jordan",
      "Lihua Lei"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Methodology (stat.ME)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2110.01052"
  },
  {
    "id": "arXiv:2110.02552",
    "title": "Policy iteration method for time-dependent Mean Field Games systems with  non-separable Hamiltonians",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2007.04818 by other authors",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2007.04818 by other authors\n",
    "authors": [
      "Mathieu Lauri\u00e8re",
      "Jiahao Song",
      "Qing Tang"
    ],
    "subjectives": [
      "Optimization and Control (math.OC)",
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2110.02552"
  },
  {
    "id": "arXiv:2110.02892",
    "title": "Probabilistic Metamodels for an Efficient Characterization of Complex  Driving Scenarios",
    "abstract": "Comments: 10 pages, 14 figures, 1 table, associated dataset at this https URL",
    "descriptor": "\nComments: 10 pages, 14 figures, 1 table, associated dataset at this https URL\n",
    "authors": [
      "Max Winkelmann",
      "Mike Kohlhoff",
      "Hadj Hamma Tadjine",
      "Steffen M\u00fcller"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Robotics (cs.RO)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2110.02892"
  },
  {
    "id": "arXiv:2110.09290",
    "title": "The AI Triplet: Computational, Conceptual, and Mathematical Knowledge in  AI Education",
    "abstract": "The AI Triplet: Computational, Conceptual, and Mathematical Knowledge in  AI Education",
    "descriptor": "",
    "authors": [
      "Maithilee Kunda"
    ],
    "subjectives": [
      "Computers and Society (cs.CY)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2110.09290"
  },
  {
    "id": "arXiv:2110.10701",
    "title": "Optimizing Strongly Interacting Fermionic Hamiltonians",
    "abstract": "Comments: 48 pages, 0 figures; v2 minor typo correction; v3 fixed incomplete argument in Theorem 8.6",
    "descriptor": "\nComments: 48 pages, 0 figures; v2 minor typo correction; v3 fixed incomplete argument in Theorem 8.6\n",
    "authors": [
      "Matthew B. Hastings",
      "Ryan O'Donnell"
    ],
    "subjectives": [
      "Quantum Physics (quant-ph)",
      "Strongly Correlated Electrons (cond-mat.str-el)",
      "Computational Complexity (cs.CC)",
      "High Energy Physics - Theory (hep-th)"
    ],
    "url": "https://arxiv.org/abs/2110.10701"
  },
  {
    "id": "arXiv:2110.11155",
    "title": "DeLag: Using Multi-Objective Optimization to Enhance the Detection of  Latency Degradation Patterns in Service-based Systems",
    "abstract": "DeLag: Using Multi-Objective Optimization to Enhance the Detection of  Latency Degradation Patterns in Service-based Systems",
    "descriptor": "",
    "authors": [
      "Luca Traini",
      "Vittorio Cortellessa"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)",
      "Performance (cs.PF)"
    ],
    "url": "https://arxiv.org/abs/2110.11155"
  },
  {
    "id": "arXiv:2110.11281",
    "title": "Fusion of complementary 2D and 3D mesostructural datasets using  generative adversarial networks",
    "abstract": "Fusion of complementary 2D and 3D mesostructural datasets using  generative adversarial networks",
    "descriptor": "",
    "authors": [
      "Amir Dahari",
      "Steve Kench",
      "Isaac Squires",
      "Samuel J. Cooper"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.11281"
  },
  {
    "id": "arXiv:2110.13076",
    "title": "AutoMTL: A Programming Framework for Automating Efficient Multi-Task  Learning",
    "abstract": "Comments: Accepted by NeurIPS 2022",
    "descriptor": "\nComments: Accepted by NeurIPS 2022\n",
    "authors": [
      "Lijun Zhang",
      "Xiao Liu",
      "Hui Guan"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2110.13076"
  },
  {
    "id": "arXiv:2110.15801",
    "title": "Application of the Multi-label Residual Convolutional Neural Network  text classifier using Content-Based Routing process",
    "abstract": "Comments: The paper has mistakes technically",
    "descriptor": "\nComments: The paper has mistakes technically\n",
    "authors": [
      "Tounsi Achraf",
      "Elkefi Safa"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.15801"
  },
  {
    "id": "arXiv:2110.15843",
    "title": "Adaptive Discretization in Online Reinforcement Learning",
    "abstract": "Comments: 77 pages, 7 figures. arXiv admin note: text overlap with arXiv:2007.00717",
    "descriptor": "\nComments: 77 pages, 7 figures. arXiv admin note: text overlap with arXiv:2007.00717\n",
    "authors": [
      "Sean R. Sinclair",
      "Siddhartha Banerjee",
      "Christina Lee Yu"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2110.15843"
  },
  {
    "id": "arXiv:2111.04092",
    "title": "Consistency and Consensus Driven for Hesitant Fuzzy Linguistic Decision  Making with Pairwise Comparisons",
    "abstract": "Comments: Pulished by Expert Systems with Applications (ISSN: 0957-4174)",
    "descriptor": "\nComments: Pulished by Expert Systems with Applications (ISSN: 0957-4174)\n",
    "authors": [
      "Peijia Ren",
      "Zixu Liu",
      "Wei-Guo Zhang",
      "Xilan Wu"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)"
    ],
    "url": "https://arxiv.org/abs/2111.04092"
  },
  {
    "id": "arXiv:2111.09902",
    "title": "A transformer-based model for default prediction in mid-cap corporate  markets",
    "abstract": "A transformer-based model for default prediction in mid-cap corporate  markets",
    "descriptor": "",
    "authors": [
      "Kamesh Korangi",
      "Christophe Mues",
      "Cristi\u00e1n Bravo"
    ],
    "subjectives": [
      "General Finance (q-fin.GN)",
      "Computers and Society (cs.CY)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2111.09902"
  },
  {
    "id": "arXiv:2111.11933",
    "title": "Disentangling Decentralized Finance (DeFi) Compositions",
    "abstract": "Disentangling Decentralized Finance (DeFi) Compositions",
    "descriptor": "",
    "authors": [
      "Stefan Kitzler",
      "Friedhelm Victor",
      "Pietro Saggese",
      "Bernhard Haslhofer"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computers and Society (cs.CY)"
    ],
    "url": "https://arxiv.org/abs/2111.11933"
  },
  {
    "id": "arXiv:2111.15454",
    "title": "Boosting Discriminative Visual Representation Learning with  Scenario-Agnostic Mixup",
    "abstract": "Comments: Preprint under review. 9 pages main body, 8 pages appendix, 4 pages reference",
    "descriptor": "\nComments: Preprint under review. 9 pages main body, 8 pages appendix, 4 pages reference\n",
    "authors": [
      "Siyuan Li",
      "Zicheng Liu",
      "Di Wu",
      "Zihan Liu",
      "Stan Z. Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2111.15454"
  },
  {
    "id": "arXiv:2112.03237",
    "title": "From Coarse to Fine-grained Concept based Discrimination for Phrase  Detection",
    "abstract": "From Coarse to Fine-grained Concept based Discrimination for Phrase  Detection",
    "descriptor": "",
    "authors": [
      "Maan Qraitem",
      "Bryan A. Plummer"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2112.03237"
  },
  {
    "id": "arXiv:2112.04167",
    "title": "Assessment of high-order IMEX methods for incompressible flow",
    "abstract": "Assessment of high-order IMEX methods for incompressible flow",
    "descriptor": "",
    "authors": [
      "Montadhar Guesmi",
      "Martina Grotteschi",
      "J\u00f6rg Stiller"
    ],
    "subjectives": [
      "Numerical Analysis (math.NA)"
    ],
    "url": "https://arxiv.org/abs/2112.04167"
  },
  {
    "id": "arXiv:2112.09827",
    "title": "Scheduling HVAC loads to promote renewable generation integration with a  learning-based joint chance-constrained approach",
    "abstract": "Comments: 9 pages",
    "descriptor": "\nComments: 9 pages\n",
    "authors": [
      "Ge Chen",
      "Hongcai Zhang",
      "Hongxun Hui",
      "Yonghua Song"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2112.09827"
  },
  {
    "id": "arXiv:2112.10212",
    "title": "Hiding pebbles when the output alphabet is unary",
    "abstract": "Comments: 35 pages",
    "descriptor": "\nComments: 35 pages\n",
    "authors": [
      "Ga\u00ebtan Dou\u00e9neau-Tabot"
    ],
    "subjectives": [
      "Formal Languages and Automata Theory (cs.FL)"
    ],
    "url": "https://arxiv.org/abs/2112.10212"
  },
  {
    "id": "arXiv:2112.12596",
    "title": "Explainable Medical Imaging AI Needs Human-Centered Design: Guidelines  and Evidence from a Systematic Review",
    "abstract": "Explainable Medical Imaging AI Needs Human-Centered Design: Guidelines  and Evidence from a Systematic Review",
    "descriptor": "",
    "authors": [
      "Haomin Chen",
      "Catalina Gomez",
      "Chien-Ming Huang",
      "Mathias Unberath"
    ],
    "subjectives": [
      "Human-Computer Interaction (cs.HC)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2112.12596"
  },
  {
    "id": "arXiv:2201.03192",
    "title": "Rate-Splitting Multiple Access: Fundamentals, Survey, and Future  Research Trends",
    "abstract": "Comments: 53 pages, 33 figures, accepted by IEEE Communications Surveys & Tutorials",
    "descriptor": "\nComments: 53 pages, 33 figures, accepted by IEEE Communications Surveys & Tutorials\n",
    "authors": [
      "Yijie Mao",
      "Onur Dizdar",
      "Bruno Clerckx",
      "Robert Schober",
      "Petar Popovski",
      "H. Vincent Poor"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2201.03192"
  },
  {
    "id": "arXiv:2202.00433",
    "title": "TopoOpt: Co-optimizing Network Topology and Parallelization Strategy for  Distributed Training Jobs",
    "abstract": "TopoOpt: Co-optimizing Network Topology and Parallelization Strategy for  Distributed Training Jobs",
    "descriptor": "",
    "authors": [
      "Weiyang Wang",
      "Moein Khazraee",
      "Zhizhen Zhong",
      "Manya Ghobadi",
      "Zhihao Jia",
      "Dheevatsa Mudigere",
      "Ying Zhang",
      "Anthony Kewitsch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2202.00433"
  },
  {
    "id": "arXiv:2202.03163",
    "title": "Patch-Based Stochastic Attention for Image Editing",
    "abstract": "Comments: 17 pages, 11 figures",
    "descriptor": "\nComments: 17 pages, 11 figures\n",
    "authors": [
      "Nicolas Cherel",
      "Andr\u00e9s Almansa",
      "Yann Gousseau",
      "Alasdair Newson"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2202.03163"
  },
  {
    "id": "arXiv:2202.09671",
    "title": "Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial  Auto-Encoders",
    "abstract": "Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial  Auto-Encoders",
    "descriptor": "",
    "authors": [
      "Huangjie Zheng",
      "Pengcheng He",
      "Weizhu Chen",
      "Mingyuan Zhou"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2202.09671"
  },
  {
    "id": "arXiv:2203.02214",
    "title": "Plan Your Target and Learn Your Skills: Transferable State-Only  Imitation Learning via Decoupled Policy Optimization",
    "abstract": "Comments: 22 pages, 3 tables, 17 figures. Published at ICML 2022. Project page and code at this https URL",
    "descriptor": "\nComments: 22 pages, 3 tables, 17 figures. Published at ICML 2022. Project page and code at this https URL\n",
    "authors": [
      "Minghuan Liu",
      "Zhengbang Zhu",
      "Yuzheng Zhuang",
      "Weinan Zhang",
      "Jianye Hao",
      "Yong Yu",
      "Jun Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2203.02214"
  },
  {
    "id": "arXiv:2203.07338",
    "title": "Inverse Online Learning: Understanding Non-Stationary and Reactionary  Policies",
    "abstract": "Inverse Online Learning: Understanding Non-Stationary and Reactionary  Policies",
    "descriptor": "",
    "authors": [
      "Alex J. Chan",
      "Alicia Curth",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.07338"
  },
  {
    "id": "arXiv:2203.07807",
    "title": "End-to-end P300 BCI using Bayesian accumulation of Riemannian  probabilities",
    "abstract": "Comments: 9 pages, 7 figures, 1 table",
    "descriptor": "\nComments: 9 pages, 7 figures, 1 table\n",
    "authors": [
      "Quentin Barth\u00e9lemy",
      "Sylvain Chevallier",
      "Rapha\u00eblle Bertrand-Lalo",
      "Pierre Clisson"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Human-Computer Interaction (cs.HC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2203.07807"
  },
  {
    "id": "arXiv:2203.08057",
    "title": "POETREE: Interpretable Policy Learning with Adaptive Decision Trees",
    "abstract": "POETREE: Interpretable Policy Learning with Adaptive Decision Trees",
    "descriptor": "",
    "authors": [
      "Aliz\u00e9e Pace",
      "Alex J. Chan",
      "Mihaela van der Schaar"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.08057"
  },
  {
    "id": "arXiv:2203.09301",
    "title": "One-Shot Adaptation of GAN in Just One CLIP",
    "abstract": "Comments: Image compressed version",
    "descriptor": "\nComments: Image compressed version\n",
    "authors": [
      "Gihyun Kwon",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2203.09301"
  },
  {
    "id": "arXiv:2203.10304",
    "title": "PACE: A Parallelizable Computation Encoder for Directed Acyclic Graphs",
    "abstract": "Comments: 9 pages main paper, 5 pages appendix",
    "descriptor": "\nComments: 9 pages main paper, 5 pages appendix\n",
    "authors": [
      "Zehao Dong",
      "Muhan Zhang",
      "Fuhai Li",
      "Yixin Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.10304"
  },
  {
    "id": "arXiv:2203.11421",
    "title": "Mobility Equity and Economic Sustainability Using Game Theory",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2110.06403",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2110.06403\n",
    "authors": [
      "Ioannis Vasileios Chremos",
      "Andreas Malikopoulos"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2203.11421"
  },
  {
    "id": "arXiv:2203.13474",
    "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program  Synthesis",
    "abstract": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program  Synthesis",
    "descriptor": "",
    "authors": [
      "Erik Nijkamp",
      "Bo Pang",
      "Hiroaki Hayashi",
      "Lifu Tu",
      "Huan Wang",
      "Yingbo Zhou",
      "Silvio Savarese",
      "Caiming Xiong"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2203.13474"
  },
  {
    "id": "arXiv:2203.14434",
    "title": "Flexible risk design using bi-directional dispersion",
    "abstract": "Comments: Substantial revision to the overall exposition, with new theoretical results and empirical test results",
    "descriptor": "\nComments: Substantial revision to the overall exposition, with new theoretical results and empirical test results\n",
    "authors": [
      "Matthew J. Holland"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2203.14434"
  },
  {
    "id": "arXiv:2203.16151",
    "title": "Manipulative Attacks and Group Identification",
    "abstract": "Comments: 61 pages, 4 figures, 9 tables",
    "descriptor": "\nComments: 61 pages, 4 figures, 9 tables\n",
    "authors": [
      "Emil Junker"
    ],
    "subjectives": [
      "Computer Science and Game Theory (cs.GT)",
      "Computational Complexity (cs.CC)"
    ],
    "url": "https://arxiv.org/abs/2203.16151"
  },
  {
    "id": "arXiv:2204.01681",
    "title": "End-to-end multi-particle reconstruction in high occupancy imaging  calorimeters with graph neural networks",
    "abstract": "End-to-end multi-particle reconstruction in high occupancy imaging  calorimeters with graph neural networks",
    "descriptor": "",
    "authors": [
      "Shah Rukh Qasim",
      "Nadezda Chernyavskaya",
      "Jan Kieseler",
      "Kenneth Long",
      "Oleksandr Viazlo",
      "Maurizio Pierini",
      "Raheel Nawaz"
    ],
    "subjectives": [
      "Instrumentation and Detectors (physics.ins-det)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "High Energy Physics - Experiment (hep-ex)"
    ],
    "url": "https://arxiv.org/abs/2204.01681"
  },
  {
    "id": "arXiv:2204.02782",
    "title": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse  Molecular Simulation Datasets",
    "abstract": "GemNet-OC: Developing Graph Neural Networks for Large and Diverse  Molecular Simulation Datasets",
    "descriptor": "",
    "authors": [
      "Johannes Gasteiger",
      "Muhammed Shuaibi",
      "Anuroop Sriram",
      "Stephan G\u00fcnnemann",
      "Zachary Ulissi",
      "C. Lawrence Zitnick",
      "Abhishek Das"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Materials Science (cond-mat.mtrl-sci)",
      "Chemical Physics (physics.chem-ph)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2204.02782"
  },
  {
    "id": "arXiv:2204.09030",
    "title": "Decentralized Control of Distributed Cloud Networks with Generalized  Network Flows",
    "abstract": "Decentralized Control of Distributed Cloud Networks with Generalized  Network Flows",
    "descriptor": "",
    "authors": [
      "Yang Cai",
      "Jaime Llorca",
      "Antonia M. Tulino",
      "Andreas F. Molisch"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)",
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.09030"
  },
  {
    "id": "arXiv:2204.09583",
    "title": "Improved Group Robustness via Classifier Retraining on Independent  Splits",
    "abstract": "Improved Group Robustness via Classifier Retraining on Independent  Splits",
    "descriptor": "",
    "authors": [
      "Thien Hang Nguyen",
      "Hongyang R. Zhang",
      "Huy Le Nguyen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.09583"
  },
  {
    "id": "arXiv:2204.10196",
    "title": "Multimodal Hate Speech Detection from Bengali Memes and Texts",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2107.00648 by other authors",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2107.00648 by other authors\n",
    "authors": [
      "Md. Rezaul Karim",
      "Sumon Kanti Dey",
      "Tanhim Islam",
      "Md. Shajalal",
      "Bharathi Raja Chakravarthi"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2204.10196"
  },
  {
    "id": "arXiv:2204.10716",
    "title": "Hierarchical Label-wise Attention Transformer Model for Explainable ICD  Coding",
    "abstract": "Hierarchical Label-wise Attention Transformer Model for Explainable ICD  Coding",
    "descriptor": "",
    "authors": [
      "Leibo Liu",
      "Oscar Perez-Concha",
      "Anthony Nguyen",
      "Vicki Bennett",
      "Louisa Jorm"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2204.10716"
  },
  {
    "id": "arXiv:2204.12077",
    "title": "AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in  Ultrasound Images",
    "abstract": "Comments: Breast cancer segmentation, Ultrasound images, Hybrid attention, Adaptive learning, Deep learning",
    "descriptor": "\nComments: Breast cancer segmentation, Ultrasound images, Hybrid attention, Adaptive learning, Deep learning\n",
    "authors": [
      "Gongping Chen",
      "Yu Dai",
      "Jianxun Zhang",
      "Moi Hoon Yap"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.12077"
  },
  {
    "id": "arXiv:2204.12833",
    "title": "Transfer Learning with Pre-trained Conditional Generative Models",
    "abstract": "Comments: 24 pages, 6 figures",
    "descriptor": "\nComments: 24 pages, 6 figures\n",
    "authors": [
      "Shin'ya Yamaguchi",
      "Sekitoshi Kanai",
      "Atsutoshi Kumagai",
      "Daiki Chijiwa",
      "Hisashi Kashima"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2204.12833"
  },
  {
    "id": "arXiv:2204.13544",
    "title": "Generalizing Hybrid Integrator-Gain Systems Using Fractional Calculus",
    "abstract": "Comments: 6 pages, 9 figures",
    "descriptor": "\nComments: 6 pages, 9 figures\n",
    "authors": [
      "S. Ali Hosseini",
      "Mohammad Saleh Tavazoei",
      "Luke F. van Eijk",
      "S. Hassan HosseinNia"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2204.13544"
  },
  {
    "id": "arXiv:2204.13779",
    "title": "Formulating Robustness Against Unforeseen Attacks",
    "abstract": "Comments: NeurIPS 2022",
    "descriptor": "\nComments: NeurIPS 2022\n",
    "authors": [
      "Sihui Dai",
      "Saeed Mahloujifar",
      "Prateek Mittal"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2204.13779"
  },
  {
    "id": "arXiv:2205.00668",
    "title": "Revisiting Classical Multiclass Linear Discriminant Analysis with a  Novel Prototype-based Interpretable Solution",
    "abstract": "Revisiting Classical Multiclass Linear Discriminant Analysis with a  Novel Prototype-based Interpretable Solution",
    "descriptor": "",
    "authors": [
      "Sayed Kamaledin Ghiasi-Shirazi"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.00668"
  },
  {
    "id": "arXiv:2205.01234",
    "title": "Scalable Tail Latency Estimation for Data Center Networks",
    "abstract": "Scalable Tail Latency Estimation for Data Center Networks",
    "descriptor": "",
    "authors": [
      "Kevin Zhao",
      "Prateesh Goyal",
      "Mohammad Alizadeh",
      "Thomas E. Anderson"
    ],
    "subjectives": [
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2205.01234"
  },
  {
    "id": "arXiv:2205.02268",
    "title": "Visual Analysis of Multiple Dynamic Sensitivities along Ascending  Trajectories in the Atmosphere",
    "abstract": "Visual Analysis of Multiple Dynamic Sensitivities along Ascending  Trajectories in the Atmosphere",
    "descriptor": "",
    "authors": [
      "Christoph Neuhauser",
      "Maicon Hieronymus",
      "Michael Kern",
      "Marc Rautenhaus",
      "Annika Oertel",
      "R\u00fcdiger Westermann"
    ],
    "subjectives": [
      "Atmospheric and Oceanic Physics (physics.ao-ph)",
      "Graphics (cs.GR)"
    ],
    "url": "https://arxiv.org/abs/2205.02268"
  },
  {
    "id": "arXiv:2205.02410",
    "title": "Sequential Importance Sampling for Hybrid Model Bayesian Inference to  Support Bioprocess Mechanism Learning and Robust Control",
    "abstract": "Comments: 11 pages, 2 figures",
    "descriptor": "\nComments: 11 pages, 2 figures\n",
    "authors": [
      "Wei Xie",
      "Keqi Wang",
      "Hua Zheng",
      "Ben Feng"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2205.02410"
  },
  {
    "id": "arXiv:2205.02548",
    "title": "Rate-Splitting Multiple Access for 6G -- Part I: Principles,  Applications and Future Works",
    "abstract": "Rate-Splitting Multiple Access for 6G -- Part I: Principles,  Applications and Future Works",
    "descriptor": "",
    "authors": [
      "Anup Mishra",
      "Yijie Mao",
      "Onur Dizdar",
      "Bruno Clerckx"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2205.02548"
  },
  {
    "id": "arXiv:2205.07808",
    "title": "Switch as a Verifier: Toward Scalable Data Plane Checking via  Distributed, On-Device Verification",
    "abstract": "Switch as a Verifier: Toward Scalable Data Plane Checking via  Distributed, On-Device Verification",
    "descriptor": "",
    "authors": [
      "Qiao Xiang",
      "Ridi Wen",
      "Chenyang Huang",
      "Yuxin Wang",
      "Franck Le"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Networking and Internet Architecture (cs.NI)",
      "Programming Languages (cs.PL)"
    ],
    "url": "https://arxiv.org/abs/2205.07808"
  },
  {
    "id": "arXiv:2205.09244",
    "title": "Riemannian Metric Learning via Optimal Transport",
    "abstract": "Riemannian Metric Learning via Optimal Transport",
    "descriptor": "",
    "authors": [
      "Christopher Scarvelis",
      "Justin Solomon"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2205.09244"
  },
  {
    "id": "arXiv:2205.10696",
    "title": "Life after BERT: What do Other Muppets Understand about Language?",
    "abstract": "Life after BERT: What do Other Muppets Understand about Language?",
    "descriptor": "",
    "authors": [
      "Vladislav Lialin",
      "Kevin Zhao",
      "Namrata Shivagunde",
      "Anna Rumshisky"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.10696"
  },
  {
    "id": "arXiv:2205.11234",
    "title": "DagSim: Combining DAG-based model structure with unconstrained data  types and relations for flexible, transparent, and modularized data  simulation",
    "abstract": "Comments: 12 pages, 1 figure, 1 table",
    "descriptor": "\nComments: 12 pages, 1 figure, 1 table\n",
    "authors": [
      "Ghadi S. Al Hajj",
      "Johan Pensar",
      "Geir Kjetil Sandve"
    ],
    "subjectives": [
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.11234"
  },
  {
    "id": "arXiv:2205.12515",
    "title": "Toward Discovering Options that Achieve Faster Planning",
    "abstract": "Toward Discovering Options that Achieve Faster Planning",
    "descriptor": "",
    "authors": [
      "Yi Wan",
      "Richard S. Sutton"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.12515"
  },
  {
    "id": "arXiv:2205.12755",
    "title": "An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale  Multitask Learning Systems",
    "abstract": "An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale  Multitask Learning Systems",
    "descriptor": "",
    "authors": [
      "Andrea Gesmundo",
      "Jeff Dean"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2205.12755"
  },
  {
    "id": "arXiv:2205.13943",
    "title": "Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN",
    "abstract": "Comments: Preprint under review (update reversion). The source code will be released in this https URL",
    "descriptor": "\nComments: Preprint under review (update reversion). The source code will be released in this https URL\n",
    "authors": [
      "Siyuan Li",
      "Di Wu",
      "Fang Wu",
      "Zelin Zang",
      "Baigui Sun",
      "Hao Li",
      "Xuansong Xie",
      "Stan.Z.Li"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2205.13943"
  },
  {
    "id": "arXiv:2205.14310",
    "title": "Approximate Conditional Coverage via Neural Model Approximations",
    "abstract": "Comments: 18 pages, 4 figures",
    "descriptor": "\nComments: 18 pages, 4 figures\n",
    "authors": [
      "Allen Schmaltz",
      "Danielle Rasooly"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computation and Language (cs.CL)"
    ],
    "url": "https://arxiv.org/abs/2205.14310"
  },
  {
    "id": "arXiv:2205.15242",
    "title": "Re-parameterizing Your Optimizers rather than Architectures",
    "abstract": "Comments: Under review",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Xiaohan Ding",
      "Honghao Chen",
      "Xiangyu Zhang",
      "Kaiqi Huang",
      "Jungong Han",
      "Guiguang Ding"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2205.15242"
  },
  {
    "id": "arXiv:2206.00314",
    "title": "Contextual Bandits with Knapsacks for a Conversion Model",
    "abstract": "Comments: Thirty-sixth Conference on Neural Information Processing Systems, 2022, New Orleans, United States",
    "descriptor": "\nComments: Thirty-sixth Conference on Neural Information Processing Systems, 2022, New Orleans, United States\n",
    "authors": [
      "Zhen Li",
      "Gilles Stoltz"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2206.00314"
  },
  {
    "id": "arXiv:2206.02928",
    "title": "Neuro-Symbolic Causal Language Planning with Commonsense Prompting",
    "abstract": "Neuro-Symbolic Causal Language Planning with Commonsense Prompting",
    "descriptor": "",
    "authors": [
      "Yujie Lu",
      "Weixi Feng",
      "Wanrong Zhu",
      "Wenda Xu",
      "Xin Eric Wang",
      "Miguel Eckstein",
      "William Yang Wang"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.02928"
  },
  {
    "id": "arXiv:2206.03171",
    "title": "Look Back When Surprised: Stabilizing Reverse Experience Replay for  Neural Approximation",
    "abstract": "Look Back When Surprised: Stabilizing Reverse Experience Replay for  Neural Approximation",
    "descriptor": "",
    "authors": [
      "Ramnath Kumar",
      "Dheeraj Nagaraj"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.03171"
  },
  {
    "id": "arXiv:2206.04046",
    "title": "Sparse Mixture-of-Experts are Domain Generalizable Learners",
    "abstract": "Comments: remake preprint version",
    "descriptor": "\nComments: remake preprint version\n",
    "authors": [
      "Bo Li",
      "Yifei Shen",
      "Jingkang Yang",
      "Yezhen Wang",
      "Jiawei Ren",
      "Tong Che",
      "Jun Zhang",
      "Ziwei Liu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.04046"
  },
  {
    "id": "arXiv:2206.05282",
    "title": "Learning to Estimate Shapley Values with Vision Transformers",
    "abstract": "Comments: Updated version",
    "descriptor": "\nComments: Updated version\n",
    "authors": [
      "Ian Covert",
      "Chanwoo Kim",
      "Su-In Lee"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05282"
  },
  {
    "id": "arXiv:2206.05909",
    "title": "Local Distance Preserving Auto-encoders using Continuous k-Nearest  Neighbours Graphs",
    "abstract": "Local Distance Preserving Auto-encoders using Continuous k-Nearest  Neighbours Graphs",
    "descriptor": "",
    "authors": [
      "Nutan Chen",
      "Patrick van der Smagt",
      "Botond Cseke"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.05909"
  },
  {
    "id": "arXiv:2206.06487",
    "title": "The Modality Focusing Hypothesis: Towards Understanding Crossmodal  Knowledge Distillation",
    "abstract": "Comments: The first three authors contribute equally",
    "descriptor": "\nComments: The first three authors contribute equally\n",
    "authors": [
      "Zihui Xue",
      "Zhengqi Gao",
      "Sucheng Ren",
      "Hang Zhao"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.06487"
  },
  {
    "id": "arXiv:2206.07796",
    "title": "FixEval: Execution-based Evaluation of Program Fixes for Programming  Problems",
    "abstract": "FixEval: Execution-based Evaluation of Program Fixes for Programming  Problems",
    "descriptor": "",
    "authors": [
      "Md Mahim Anjum Haque",
      "Wasi Uddin Ahmad",
      "Ismini Lourentzou",
      "Chris Brown"
    ],
    "subjectives": [
      "Software Engineering (cs.SE)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.07796"
  },
  {
    "id": "arXiv:2206.07959",
    "title": "Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?",
    "abstract": "Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?",
    "descriptor": "",
    "authors": [
      "Adam W. Harley",
      "Zhaoyuan Fang",
      "Jie Li",
      "Rares Ambrus",
      "Katerina Fragkiadaki"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2206.07959"
  },
  {
    "id": "arXiv:2206.09144",
    "title": "Beyond Real-world Benchmark Datasets: An Empirical Study of Node  Classification with GNNs",
    "abstract": "Comments: Accepted to NeurIPS 2022 Datasets and Benchmarks Track. 20 pages, 15 figures",
    "descriptor": "\nComments: Accepted to NeurIPS 2022 Datasets and Benchmarks Track. 20 pages, 15 figures\n",
    "authors": [
      "Seiji Maekawa",
      "Koki Noda",
      "Yuya Sasaki",
      "Makoto Onizuka"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2206.09144"
  },
  {
    "id": "arXiv:2207.01127",
    "title": "DecisioNet: A Binary-Tree Structured Neural Network",
    "abstract": "Comments: We are happy to announce that the paper has been accepted to the ACCV2022 conference. The final version of the paper will be published soon. In the meantime, we are finally able to share the code (link below)",
    "descriptor": "\nComments: We are happy to announce that the paper has been accepted to the ACCV2022 conference. The final version of the paper will be published soon. In the meantime, we are finally able to share the code (link below)\n",
    "authors": [
      "Noam Gottlieb",
      "Michael Werman"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.01127"
  },
  {
    "id": "arXiv:2207.01405",
    "title": "I-ViT: Integer-only Quantization for Efficient Vision Transformer  Inference",
    "abstract": "I-ViT: Integer-only Quantization for Efficient Vision Transformer  Inference",
    "descriptor": "",
    "authors": [
      "Zhikai Li",
      "Qingyi Gu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.01405"
  },
  {
    "id": "arXiv:2207.01971",
    "title": "DualAfford: Learning Collaborative Visual Affordance for Dual-gripper  Object Manipulation",
    "abstract": "DualAfford: Learning Collaborative Visual Affordance for Dual-gripper  Object Manipulation",
    "descriptor": "",
    "authors": [
      "Yan Zhao",
      "Ruihai Wu",
      "Zhehuan Chen",
      "Yourong Zhang",
      "Qingnan Fan",
      "Kaichun Mo",
      "Hao Dong"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2207.01971"
  },
  {
    "id": "arXiv:2207.03620",
    "title": "More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using  Sparsity",
    "abstract": "Comments: Preprint",
    "descriptor": "\nComments: Preprint\n",
    "authors": [
      "Shiwei Liu",
      "Tianlong Chen",
      "Xiaohan Chen",
      "Xuxi Chen",
      "Qiao Xiao",
      "Boqian Wu",
      "Mykola Pechenizkiy",
      "Decebal Mocanu",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.03620"
  },
  {
    "id": "arXiv:2207.03804",
    "title": "On the Subspace Structure of Gradient-Based Meta-Learning",
    "abstract": "On the Subspace Structure of Gradient-Based Meta-Learning",
    "descriptor": "",
    "authors": [
      "Gustaf Tegn\u00e9r",
      "Alfredo Reichlin",
      "Hang Yin",
      "M\u00e5rten Bj\u00f6rkman",
      "Danica Kragic"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.03804"
  },
  {
    "id": "arXiv:2207.07656",
    "title": "FLOWGEN: Fast and slow graph generation",
    "abstract": "Comments: Accepted at Dynamic Neural Networks Workshop (DyNN), ICML 2022",
    "descriptor": "\nComments: Accepted at Dynamic Neural Networks Workshop (DyNN), ICML 2022\n",
    "authors": [
      "Aman Madaan",
      "Yiming Yang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.07656"
  },
  {
    "id": "arXiv:2207.08745",
    "title": "Amplitude Scintillation Forecasting Using Bagged Trees",
    "abstract": "Comments: This paper was presented at IGARSS 2022, Kuala Lumpur, Malaysia. doi: 10.1109/IGARSS46834.2022.9883380",
    "descriptor": "\nComments: This paper was presented at IGARSS 2022, Kuala Lumpur, Malaysia. doi: 10.1109/IGARSS46834.2022.9883380\n",
    "authors": [
      "Abdollah Masoud Darya",
      "Aisha Abdulla Al-Owais",
      "Muhammad Mubasshir Shaikh",
      "Ilias Fernini"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Atmospheric and Oceanic Physics (physics.ao-ph)"
    ],
    "url": "https://arxiv.org/abs/2207.08745"
  },
  {
    "id": "arXiv:2207.08894",
    "title": "A Deep Reinforcement Learning Approach for Finding Non-Exploitable  Strategies in Two-Player Atari Games",
    "abstract": "A Deep Reinforcement Learning Approach for Finding Non-Exploitable  Strategies in Two-Player Atari Games",
    "descriptor": "",
    "authors": [
      "Zihan Ding",
      "Dijia Su",
      "Qinghua Liu",
      "Chi Jin"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2207.08894"
  },
  {
    "id": "arXiv:2207.09542",
    "title": "Controllable Data Generation by Deep Learning: A Review",
    "abstract": "Controllable Data Generation by Deep Learning: A Review",
    "descriptor": "",
    "authors": [
      "Shiyu Wang",
      "Yuanqi Du",
      "Xiaojie Guo",
      "Bo Pan",
      "Zhaohui Qin",
      "Liang Zhao"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.09542"
  },
  {
    "id": "arXiv:2207.09658",
    "title": "Learning Depth from Focus in the Wild",
    "abstract": "Learning Depth from Focus in the Wild",
    "descriptor": "",
    "authors": [
      "Changyeon Won",
      "Hae-Gon Jeon"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.09658"
  },
  {
    "id": "arXiv:2207.10283",
    "title": "Switching One-Versus-the-Rest Loss to Increase the Margin of Logits for  Adversarial Robustness",
    "abstract": "Comments: 25 pages, 18 figures",
    "descriptor": "\nComments: 25 pages, 18 figures\n",
    "authors": [
      "Sekitoshi Kanai",
      "Shin'ya Yamaguchi",
      "Masanori Yamada",
      "Hiroshi Takahashi",
      "Kentaro Ohno",
      "Yasutoshi Ida"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2207.10283"
  },
  {
    "id": "arXiv:2207.10305",
    "title": "Detecting Small Query Graphs in A Large Graph via Neural Subgraph Search",
    "abstract": "Detecting Small Query Graphs in A Large Graph via Neural Subgraph Search",
    "descriptor": "",
    "authors": [
      "Yunsheng Bai",
      "Derek Xu",
      "Yizhou Sun",
      "Wei Wang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2207.10305"
  },
  {
    "id": "arXiv:2207.11177",
    "title": "Provable Defense Against Geometric Transformations",
    "abstract": "Provable Defense Against Geometric Transformations",
    "descriptor": "",
    "authors": [
      "Rem Yang",
      "Jacob Laurel",
      "Sasa Misailovic",
      "Gagandeep Singh"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2207.11177"
  },
  {
    "id": "arXiv:2207.12559",
    "title": "Static Hand Gesture Recognition for American Sign Language using  Neuromorphic Hardware",
    "abstract": "Comments: Authors MohammedReza Mohammadi, and Peyton Chandarana contributed equally",
    "descriptor": "\nComments: Authors MohammedReza Mohammadi, and Peyton Chandarana contributed equally\n",
    "authors": [
      "MohammedReza Mohammadi",
      "Peyton Chandarana",
      "James Seekings",
      "Sara Hendrix",
      "Ramtin Zand"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Human-Computer Interaction (cs.HC)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2207.12559"
  },
  {
    "id": "arXiv:2207.14502",
    "title": "Language Models Can Teach Themselves to Program Better",
    "abstract": "Comments: 22 pages, 14 figures",
    "descriptor": "\nComments: 22 pages, 14 figures\n",
    "authors": [
      "Patrick Haluptzok",
      "Matthew Bowers",
      "Adam Tauman Kalai"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2207.14502"
  },
  {
    "id": "arXiv:2208.01864",
    "title": "Pyramidal Denoising Diffusion Probabilistic Models",
    "abstract": "Pyramidal Denoising Diffusion Probabilistic Models",
    "descriptor": "",
    "authors": [
      "Dohoon Ryu",
      "Jong Chul Ye"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.01864"
  },
  {
    "id": "arXiv:2208.03211",
    "title": "Why do networks have inhibitory/negative connections?",
    "abstract": "Comments: Submitted",
    "descriptor": "\nComments: Submitted\n",
    "authors": [
      "Qingyang Wang",
      "Michael A. Powell",
      "Ali Geisa",
      "Eric Bridgeford",
      "Joshua T. Vogelstein"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2208.03211"
  },
  {
    "id": "arXiv:2208.06102",
    "title": "Zeus: Understanding and Optimizing GPU Energy Consumption of DNN  Training",
    "abstract": "Comments: NSDI 2023 | Homepage this https URL",
    "descriptor": "\nComments: NSDI 2023 | Homepage this https URL\n",
    "authors": [
      "Jie You",
      "Jae-Won Chung",
      "Mosharaf Chowdhury"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2208.06102"
  },
  {
    "id": "arXiv:2208.08003",
    "title": "The Final Ascent: When Bigger Models Generalize Worse on Noisy-Labeled  Data",
    "abstract": "Comments: added more experiments and discussion on sample size",
    "descriptor": "\nComments: added more experiments and discussion on sample size\n",
    "authors": [
      "Yihao Xue",
      "Kyle Whitecross",
      "Baharan Mirzasoleiman"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.08003"
  },
  {
    "id": "arXiv:2208.09285",
    "title": "Shadows Aren't So Dangerous After All: A Fast and Robust Defense Against  Shadow-Based Adversarial Attacks",
    "abstract": "Comments: This is a draft version - our core results are reported, but additional experiments for journal submission are still being run",
    "descriptor": "\nComments: This is a draft version - our core results are reported, but additional experiments for journal submission are still being run\n",
    "authors": [
      "Andrew Wang",
      "Wyatt Mayor",
      "Ryan Smith",
      "Gopal Nookula",
      "Gregory Ditzler"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.09285"
  },
  {
    "id": "arXiv:2208.10684",
    "title": "K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online  News Comment",
    "abstract": "Comments: Accepted by COLING 2022",
    "descriptor": "\nComments: Accepted by COLING 2022\n",
    "authors": [
      "Jean Lee",
      "Taejun Lim",
      "Heejun Lee",
      "Bogeun Jo",
      "Yangsok Kim",
      "Heegeun Yoon",
      "Soyeon Caren Han"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2208.10684"
  },
  {
    "id": "arXiv:2208.11126",
    "title": "Retrieval-based Controllable Molecule Generation",
    "abstract": "Comments: 29 pages",
    "descriptor": "\nComments: 29 pages\n",
    "authors": [
      "Zichao Wang",
      "Weili Nie",
      "Zhuoran Qiao",
      "Chaowei Xiao",
      "Richard Baraniuk",
      "Anima Anandkumar"
    ],
    "subjectives": [
      "Quantitative Methods (q-bio.QM)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.11126"
  },
  {
    "id": "arXiv:2208.14133",
    "title": "Deep Generative Modeling on Limited Data with Regularization by  Nontransferable Pre-trained Models",
    "abstract": "Deep Generative Modeling on Limited Data with Regularization by  Nontransferable Pre-trained Models",
    "descriptor": "",
    "authors": [
      "Yong Zhong",
      "Hongtao Liu",
      "Xiaodong Liu",
      "Fan Bao",
      "Weiran Shen",
      "Chongxuan Li"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2208.14133"
  },
  {
    "id": "arXiv:2208.14153",
    "title": "Identifying Weight-Variant Latent Causal Models",
    "abstract": "Identifying Weight-Variant Latent Causal Models",
    "descriptor": "",
    "authors": [
      "Yuhang Liu",
      "Zhen Zhang",
      "Dong Gong",
      "Mingming Gong",
      "Biwei Huang",
      "Anton van den Hengel",
      "Kun Zhang",
      "Javen Qinfeng Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.14153"
  },
  {
    "id": "arXiv:2208.14161",
    "title": "Identifying Latent Causal Content for Multi-Source Domain Adaptation",
    "abstract": "Identifying Latent Causal Content for Multi-Source Domain Adaptation",
    "descriptor": "",
    "authors": [
      "Yuhang Liu",
      "Zhen Zhang",
      "Dong Gong",
      "Mingming Gong",
      "Biwei Huang",
      "Kun Zhang",
      "Javen Qinfeng Shi"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2208.14161"
  },
  {
    "id": "arXiv:2208.14197",
    "title": "A Comprehensive Review of Digital Twin -- Part 1: Modeling and Twinning  Enabling Technologies",
    "abstract": "A Comprehensive Review of Digital Twin -- Part 1: Modeling and Twinning  Enabling Technologies",
    "descriptor": "",
    "authors": [
      "Adam Thelen",
      "Xiaoge Zhang",
      "Olga Fink",
      "Yan Lu",
      "Sayan Ghosh",
      "Byeng D. Youn",
      "Michael D. Todd",
      "Sankaran Mahadevan",
      "Chao Hu",
      "Zhen Hu"
    ],
    "subjectives": [
      "Computational Engineering, Finance, and Science (cs.CE)",
      "Artificial Intelligence (cs.AI)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2208.14197"
  },
  {
    "id": "arXiv:2208.14394",
    "title": "Evolutionary Deep Reinforcement Learning for Dynamic Slice Management in  O-RAN",
    "abstract": "Comments: This paper has been accepted for the 2022 IEEE Globecom Workshops (GC Wkshps)",
    "descriptor": "\nComments: This paper has been accepted for the 2022 IEEE Globecom Workshops (GC Wkshps)\n",
    "authors": [
      "Fatemeh Lotfi",
      "Omid Semiari",
      "Fatemeh Afghah"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)",
      "Artificial Intelligence (cs.AI)",
      "Information Theory (cs.IT)",
      "Machine Learning (cs.LG)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2208.14394"
  },
  {
    "id": "arXiv:2209.00188",
    "title": "Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based  Off-Chip Load Prediction",
    "abstract": "Comments: To appear in 55th IEEE/ACM International Symposium on Microarchitecture (MICRO), 2022",
    "descriptor": "\nComments: To appear in 55th IEEE/ACM International Symposium on Microarchitecture (MICRO), 2022\n",
    "authors": [
      "Rahul Bera",
      "Konstantinos Kanellopoulos",
      "Shankar Balachandran",
      "David Novo",
      "Ataberk Olgun",
      "Mohammad Sadrosadati",
      "Onur Mutlu"
    ],
    "subjectives": [
      "Hardware Architecture (cs.AR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.00188"
  },
  {
    "id": "arXiv:2209.01589",
    "title": "Consistent Targets Provide Better Supervision in Semi-supervised Object  Detection",
    "abstract": "Consistent Targets Provide Better Supervision in Semi-supervised Object  Detection",
    "descriptor": "",
    "authors": [
      "Xinjiang Wang",
      "Xingyi Yang",
      "Shilong Zhang",
      "Yijiang Li",
      "Litong Feng",
      "Shijie Fang",
      "Chengqi Lyu",
      "Kai Chen",
      "Wayne Zhang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.01589"
  },
  {
    "id": "arXiv:2209.05433",
    "title": "FP8 Formats for Deep Learning",
    "abstract": "FP8 Formats for Deep Learning",
    "descriptor": "",
    "authors": [
      "Paulius Micikevicius",
      "Dusan Stosic",
      "Neil Burgess",
      "Marius Cornea",
      "Pradeep Dubey",
      "Richard Grisenthwaite",
      "Sangwon Ha",
      "Alexander Heinecke",
      "Patrick Judd",
      "John Kamalu",
      "Naveen Mellempudi",
      "Stuart Oberman",
      "Mohammad Shoeybi",
      "Michael Siu",
      "Hao Wu"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.05433"
  },
  {
    "id": "arXiv:2209.06208",
    "title": "Identification of Cognitive Workload during Surgical Tasks with  Multimodal Deep Learning",
    "abstract": "Identification of Cognitive Workload during Surgical Tasks with  Multimodal Deep Learning",
    "descriptor": "",
    "authors": [
      "Kaizhe Jin",
      "Adrian Rubio-Solis",
      "Ravi Naik",
      "Tochukwu Onyeogulu",
      "Amirul Islam",
      "Salman Khan",
      "Izzeddin Teeti",
      "James Kinross",
      "Daniel R Leff",
      "Fabio Cuzzolin",
      "George Mylonas"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Human-Computer Interaction (cs.HC)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.06208"
  },
  {
    "id": "arXiv:2209.06932",
    "title": "Optimizing Connectivity through Network Gradients for the Restricted  Boltzmann Machine",
    "abstract": "Optimizing Connectivity through Network Gradients for the Restricted  Boltzmann Machine",
    "descriptor": "",
    "authors": [
      "A. C. N. de Oliveira",
      "D. R. Figueiredo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.06932"
  },
  {
    "id": "arXiv:2209.06950",
    "title": "Lossy Image Compression with Conditional Diffusion Models",
    "abstract": "Comments: Accepted at the ECCV 2022 Workshop on Uncertainty Quantification for Computer Vision",
    "descriptor": "\nComments: Accepted at the ECCV 2022 Workshop on Uncertainty Quantification for Computer Vision\n",
    "authors": [
      "Ruihan Yang",
      "Stephan Mandt"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.06950"
  },
  {
    "id": "arXiv:2209.07326",
    "title": "A Continual Development Methodology for Large-scale Multitask Dynamic ML  Systems",
    "abstract": "Comments: arXiv admin note: text overlap with arXiv:2205.12755",
    "descriptor": "\nComments: arXiv admin note: text overlap with arXiv:2205.12755\n",
    "authors": [
      "Andrea Gesmundo"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Neural and Evolutionary Computing (cs.NE)"
    ],
    "url": "https://arxiv.org/abs/2209.07326"
  },
  {
    "id": "arXiv:2209.08248",
    "title": "PlaneSLAM: Plane-based LiDAR SLAM for Motion Planning in Structured 3D  Environments",
    "abstract": "PlaneSLAM: Plane-based LiDAR SLAM for Motion Planning in Structured 3D  Environments",
    "descriptor": "",
    "authors": [
      "Adam Dai",
      "Greg Lund",
      "Grace Gao"
    ],
    "subjectives": [
      "Robotics (cs.RO)"
    ],
    "url": "https://arxiv.org/abs/2209.08248"
  },
  {
    "id": "arXiv:2209.08776",
    "title": "NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes",
    "abstract": "NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes",
    "descriptor": "",
    "authors": [
      "Zhiwen Fan",
      "Peihao Wang",
      "Yifan Jiang",
      "Xinyu Gong",
      "Dejia Xu",
      "Zhangyang Wang"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "url": "https://arxiv.org/abs/2209.08776"
  },
  {
    "id": "arXiv:2209.09580",
    "title": "Carbon: An Asynchronous Voting-Based Payment System for a Client-Server  Architecture",
    "abstract": "Carbon: An Asynchronous Voting-Based Payment System for a Client-Server  Architecture",
    "descriptor": "",
    "authors": [
      "Martina Camaioni",
      "Rachid Guerraoui",
      "Jovan Komatovic",
      "Matteo Monti",
      "Manuel Vidigueira"
    ],
    "subjectives": [
      "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "url": "https://arxiv.org/abs/2209.09580"
  },
  {
    "id": "arXiv:2209.10108",
    "title": "Stochastic MPC with Realization-Adaptive Constraint Tightening",
    "abstract": "Comments: Submitted to ACC 2023",
    "descriptor": "\nComments: Submitted to ACC 2023\n",
    "authors": [
      "Hotae Lee",
      "Monimoy Bujarbaruah",
      "Francesco Borrelli"
    ],
    "subjectives": [
      "Systems and Control (eess.SY)"
    ],
    "url": "https://arxiv.org/abs/2209.10108"
  },
  {
    "id": "arXiv:2209.10831",
    "title": "Boosting as Frank-Wolfe",
    "abstract": "Boosting as Frank-Wolfe",
    "descriptor": "",
    "authors": [
      "Ryotaro Mitsuboshi",
      "Kohei Hatano",
      "Eiji Takimoto"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.10831"
  },
  {
    "id": "arXiv:2209.11224",
    "title": "VToonify: Controllable High-Resolution Portrait Video Style Transfer",
    "abstract": "Comments: ACM Transactions on Graphics (SIGGRAPH Asia 2022). Code: this https URL Project page: this https URL",
    "descriptor": "\nComments: ACM Transactions on Graphics (SIGGRAPH Asia 2022). Code: this https URL Project page: this https URL\n",
    "authors": [
      "Shuai Yang",
      "Liming Jiang",
      "Ziwei Liu",
      "Chen Change Loy"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.11224"
  },
  {
    "id": "arXiv:2209.12104",
    "title": "Conversion Between CT and MRI Images Using Diffusion and Score-Matching  Models",
    "abstract": "Conversion Between CT and MRI Images Using Diffusion and Score-Matching  Models",
    "descriptor": "",
    "authors": [
      "Qing Lyu",
      "Ge Wang"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Medical Physics (physics.med-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.12104"
  },
  {
    "id": "arXiv:2209.12647",
    "title": "PL-kNN: A Parameterless Nearest Neighbors Classifier",
    "abstract": "PL-kNN: A Parameterless Nearest Neighbors Classifier",
    "descriptor": "",
    "authors": [
      "Danilo Samuel Jodas",
      "Leandro Aparecido Passos",
      "Ahsan Adeel",
      "Jo\u00e3o Paulo Papa"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.12647"
  },
  {
    "id": "arXiv:2209.12823",
    "title": "Introductory Review of Swarm Intelligence Techniques",
    "abstract": "Comments: Submitted to Springer",
    "descriptor": "\nComments: Submitted to Springer\n",
    "authors": [
      "Thounaojam Chinglemba",
      "Soujanyo Biswas",
      "Debashish Malakar",
      "Vivek Meena",
      "Debojyoti Sarkar",
      "Anupam Biswas"
    ],
    "subjectives": [
      "Neural and Evolutionary Computing (cs.NE)",
      "Optimization and Control (math.OC)"
    ],
    "url": "https://arxiv.org/abs/2209.12823"
  },
  {
    "id": "arXiv:2209.12867",
    "title": "Trends, Opportunities, and Challenges in Using Restricted Device  Authentication in Fog Computing",
    "abstract": "Trends, Opportunities, and Challenges in Using Restricted Device  Authentication in Fog Computing",
    "descriptor": "",
    "authors": [
      "Wesley dos Reis Bezerra",
      "Carlos Becker Westphal"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Networking and Internet Architecture (cs.NI)"
    ],
    "url": "https://arxiv.org/abs/2209.12867"
  },
  {
    "id": "arXiv:2209.13008",
    "title": "Evaluation of Medical Image Segmentation Models for Uncertain, Small or  Empty Reference Annotations",
    "abstract": "Comments: 16 pages, 10 figures",
    "descriptor": "\nComments: 16 pages, 10 figures\n",
    "authors": [
      "Sophie Ostmeier",
      "Brian Axelrod",
      "Jeroen Bertels",
      "Fabian Isensee",
      "Maarten G.Lansberg",
      "Soren Christensen",
      "Gregory W. Albers",
      "Li-Jia Li",
      "Jeremy J. Heit"
    ],
    "subjectives": [
      "Image and Video Processing (eess.IV)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13008"
  },
  {
    "id": "arXiv:2209.13434",
    "title": "Accelerating hypersonic reentry simulations using deep learning-based  hybridization (with guarantees)",
    "abstract": "Comments: Under review",
    "descriptor": "\nComments: Under review\n",
    "authors": [
      "Paul Novello",
      "Ga\u00ebl Po\u00ebtte",
      "David Lugato",
      "Simon Peluchon",
      "Pietro Marco Congedo"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)",
      "Computational Physics (physics.comp-ph)"
    ],
    "url": "https://arxiv.org/abs/2209.13434"
  },
  {
    "id": "arXiv:2209.13570",
    "title": "Hierarchical Sliced Wasserstein Distance",
    "abstract": "Comments: 28 pages, 7 figures, 3 tables",
    "descriptor": "\nComments: 28 pages, 7 figures, 3 tables\n",
    "authors": [
      "Khai Nguyen",
      "Tongzheng Ren",
      "Huy Nguyen",
      "Litu Rout",
      "Tan Nguyen",
      "Nhat Ho"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.13570"
  },
  {
    "id": "arXiv:2209.13677",
    "title": "Leveraging Voltage Controlled Magnetic Anisotropy to Solve Sneak Path  Issues in Crossbar Arrays",
    "abstract": "Leveraging Voltage Controlled Magnetic Anisotropy to Solve Sneak Path  Issues in Crossbar Arrays",
    "descriptor": "",
    "authors": [
      "Kezhou Yang",
      "Abhronil Sengupta"
    ],
    "subjectives": [
      "Emerging Technologies (cs.ET)"
    ],
    "url": "https://arxiv.org/abs/2209.13677"
  },
  {
    "id": "arXiv:2209.14162",
    "title": "Near Lossless Time Series Data Compression Methods using Statistics and  Deviation",
    "abstract": "Comments: 6 pages, 2 figures and 9 tables are included",
    "descriptor": "\nComments: 6 pages, 2 figures and 9 tables are included\n",
    "authors": [
      "Vidhi Agrawal",
      "Gajraj Kuldeep",
      "Dhananjoy Dey"
    ],
    "subjectives": [
      "Information Theory (cs.IT)",
      "Signal Processing (eess.SP)"
    ],
    "url": "https://arxiv.org/abs/2209.14162"
  },
  {
    "id": "arXiv:2209.14222",
    "title": "Online Subset Selection using $\u03b1$-Core with no Augmented Regret",
    "abstract": "Online Subset Selection using $\u03b1$-Core with no Augmented Regret",
    "descriptor": "",
    "authors": [
      "Sourav Sahoo",
      "Samrat Mukhopadhyay",
      "Abhishek Sinha"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Science and Game Theory (cs.GT)"
    ],
    "url": "https://arxiv.org/abs/2209.14222"
  },
  {
    "id": "arXiv:2209.14243",
    "title": "A Closer Look at Evaluating the Bit-Flip Attack Against Deep Neural  Networks",
    "abstract": "Comments: Extended version from IEEE IOLTS'2022 short paper",
    "descriptor": "\nComments: Extended version from IEEE IOLTS'2022 short paper\n",
    "authors": [
      "Kevin Hector",
      "Mathieu Dumont",
      "Pierre-Alain Moellic",
      "Jean-Max Dutertre"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14243"
  },
  {
    "id": "arXiv:2209.14390",
    "title": "Neighborhood Gradient Clustering: An Efficient Decentralized Learning  Method for Non-IID Data Distributions",
    "abstract": "Comments: 15 pages, 5 figures, 7 tables",
    "descriptor": "\nComments: 15 pages, 5 figures, 7 tables\n",
    "authors": [
      "Sai Aparna Aketi",
      "Sangamesh Kodge",
      "Kaushik Roy"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Distributed, Parallel, and Cluster Computing (cs.DC)",
      "Multiagent Systems (cs.MA)"
    ],
    "url": "https://arxiv.org/abs/2209.14390"
  },
  {
    "id": "arXiv:2209.14440",
    "title": "GeONet: a neural operator for learning the Wasserstein geodesic",
    "abstract": "GeONet: a neural operator for learning the Wasserstein geodesic",
    "descriptor": "",
    "authors": [
      "Andrew Gracyk",
      "Xiaohui Chen"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (stat.ML)"
    ],
    "url": "https://arxiv.org/abs/2209.14440"
  },
  {
    "id": "arXiv:2209.14520",
    "title": "Label driven Knowledge Distillation for Federated Learning with non-IID  Data",
    "abstract": "Comments: 28 pages, 5 figures, 10 tables",
    "descriptor": "\nComments: 28 pages, 5 figures, 10 tables\n",
    "authors": [
      "Minh-Duong Nguyen",
      "Quoc-Viet Pham",
      "Dinh Thai Hoang",
      "Long Tran-Thanh",
      "Diep N. Nguyen",
      "Won-Joo Hwang"
    ],
    "subjectives": [
      "Machine Learning (cs.LG)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.14520"
  },
  {
    "id": "arXiv:2209.14603",
    "title": "Dataset Distillation for Medical Dataset Sharing",
    "abstract": "Dataset Distillation for Medical Dataset Sharing",
    "descriptor": "",
    "authors": [
      "Guang Li",
      "Ren Togo",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "subjectives": [
      "Cryptography and Security (cs.CR)",
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Machine Learning (cs.LG)",
      "Image and Video Processing (eess.IV)"
    ],
    "url": "https://arxiv.org/abs/2209.14603"
  },
  {
    "id": "arXiv:2209.14697",
    "title": "Creative Painting with Latent Diffusion Models",
    "abstract": "Comments: 17pages, 12 figures",
    "descriptor": "\nComments: 17pages, 12 figures\n",
    "authors": [
      "Xianchao Wu"
    ],
    "subjectives": [
      "Computer Vision and Pattern Recognition (cs.CV)",
      "Artificial Intelligence (cs.AI)",
      "Computation and Language (cs.CL)",
      "Graphics (cs.GR)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14697"
  },
  {
    "id": "arXiv:2209.14843",
    "title": "Evaluating Research Dataset Recommendations in a Living Lab",
    "abstract": "Comments: Best of 2021 Labs: LiLAS",
    "descriptor": "\nComments: Best of 2021 Labs: LiLAS\n",
    "authors": [
      "J\u00fcri Keller",
      "Leon Paul Mondrian Munz"
    ],
    "subjectives": [
      "Information Retrieval (cs.IR)"
    ],
    "url": "https://arxiv.org/abs/2209.14843"
  },
  {
    "id": "arXiv:2209.14967",
    "title": "Statistical Learning and Inverse Problems: An Stochastic Gradient  Approach",
    "abstract": "Statistical Learning and Inverse Problems: An Stochastic Gradient  Approach",
    "descriptor": "",
    "authors": [
      "Yuri R. Fonseca",
      "Yuri F. Saporito"
    ],
    "subjectives": [
      "Machine Learning (stat.ML)",
      "Machine Learning (cs.LG)"
    ],
    "url": "https://arxiv.org/abs/2209.14967"
  },
  {
    "id": "arXiv:2209.15003",
    "title": "Compositional Semantic Parsing with Large Language Models",
    "abstract": "Comments: Fixed metadata. No other changes",
    "descriptor": "\nComments: Fixed metadata. No other changes\n",
    "authors": [
      "Andrew Drozdov",
      "Nathanael Sch\u00e4rli",
      "Ekin Aky\u00fcrek",
      "Nathan Scales",
      "Xinying Song",
      "Xinyun Chen",
      "Olivier Bousquet",
      "Denny Zhou"
    ],
    "subjectives": [
      "Computation and Language (cs.CL)",
      "Artificial Intelligence (cs.AI)"
    ],
    "url": "https://arxiv.org/abs/2209.15003"
  }
]